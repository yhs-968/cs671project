{"A00-1020": {"introduction": ["conclusions", "recent availability large bilingual corpora spawned interest several areas multilingual text processing", "research focused bilingual terminology identification either par allel multiwords forms eg champollion sys tem smadja et al1996 technical terminology eg termight system dagan church broadcoverage translation lexicons eg sable system resnik melamed", "addition multilingual entity task met tip ster program1 httpwwwnlpirnistgovjrelated projectsjtipsterjmethtm challenged partici pants message understanding conference muc extract named entities several eign language corpora chinese japanese spanish", "paper present new application aligned multilingual texts", "since coreference reso lution pervasive discourse phenomenon causing performance impediments current ie systems considered corpus aligned english roma nian texts identify coreferring expressions", "task focused kind coreference considered past muc competitions namely 1the tipster text program darpaled government effort advance state art text processing technologies", "steven maiorano ipo washington dc maiorano caiscom identity coreference", "identity coreference links nouns pronouns noun phrases including proper names corresponding antecedents", "created bilingual collection translating muc6 muc7 coreference training texts romanian using native speakers", "train ing data set romanian coreference used wher ever possible coreference identifiers english data incorporated additional tags needed", "claim adding wealth coreferential features provided multilingual data new powerful heuristics coreference resolu tion developed outperform monolingual coreference resolution systems", "languages resolved coreference using swizzle implementation bilingual coreference resolver", "swizzle multilingual en hancement cocktail harabagiu maiorano coreference resolution system operates mixture heuristics combine semantic textual cohesive information2 cocktail applied separately english ro manian texts coreferring links identified english romanian document respectively", "aligned referential expressions corefer nonaligned anaphors swizzle derived new heuris tics coreference", "experiments show swizzle outperformed cocktail english romanian test documents", "rest paper organized follows", "sec tion presents cocktail monolingual coreference resolution system used separately en glish romanian texts", "section details datadriven approach used swizzle presents resources", "section reports discusses experimental results", "section summarizes name cocktail pun cogniac cause cocktail combines larger number heuristics reported baldwin", "swizzle adds new heuristics discovered bilingual aligned corpus"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058524734982232, "p": 0.07692307692307693, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058249868213054494, "p": 0.1111111111111111, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058249868213054494, "p": 0.1111111111111111, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper present new multi lingual datadriven method coreference resolution implemented swizzle system", "results obtained training system bilingual corpus english romanian tagged texts outperformed coreference resolution indi vidual languages"]}, "A00-1024": {"introduction": ["conclusions", "real world use natural language process ing nlp system encounter words lexicon term unknown words", "un known words problematic nlp system perform well recognizes words meant analyze translate words system recognize systems per formance degrade", "even unknown words infrequent disproportionate ef fect system quality", "example min found words emails misspelled meant sen tences contained error discussed min wilson", "words unknown many reasons word proper name misspelling ab breviation number morphological variant known word eg receared missing dictionary", "first step dealing unknown words identify class unknown word whether misspelling proper name ab breviation etc known proper ac tion taken misspellings corrected ab breviations expanded deemed necessary particular text processing applica tion", "paper introduce system cat egorizing unknown words", "system based multi component architecture compo nent responsible identifying category unknown words", "main focus paper components identify names spelling errors", "components use decision tree architecture combine multiple types evidence un known word", "results components combined using weighted voting procedure", "system evaluated using data live closed cap tions genre replete wide variety un known words", "paper organized follows", "section outline overall architecture unknown word categorizer", "name identifier mis spelling identifier introduced section", "perfor mance evaluation issues discussed section", "section considers portability issues", "section", "compares current system relevant preced ing research"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022164436572891397, "p": 0.14285714285714285, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper introduces system categorizing un known words", "system based multi component architecture component sponsible identifying class unknown words", "focus paper components iden tify names spelling errors", "component uses decision tree architecture combine multiple types evidence unknown word", "sys tem evaluated using data live closed captions genre replete wide variety unknown words"]}, "A00-2019": {"introduction": ["concluding comments found section", "good indicator whether person knows meaning word ability use appropriately sentence miller gildea", "much information usage obtained quite limited context choueka lusignan found people typically recognize intended sense polysemous word looking narrow window words around", "statisticallybased computer programs able high level accuracy kilgarriff palmer", "goal work automatically identify inappropriate usage specific vocabulary words essays looking local contextual cues around target word", "developed statistical system alek assessing lexical knowledge uses statistical analysis purpose", "major objective research avoid laborious costly process collecting errors negative evidence word wish evaluate", "instead train alek general corpus english edited text containing example uses target word", "system identifies inappropriate usage based differences words local context cues essay models context derived corpora wellformed sentences", "claudia leacock educational testing service rosedale road princeton nj cleacocketsorg requirement alek steps process automated beyond choosing words tested assessing results", "target word chosen preprocessing building model ofthe words appropriate usage identifying usage errors essays performed without manual intervention", "alek developed using test english foreign language toefl administered educational testing service", "toefl taken foreign students applying us undergraduate graduatelevel programs", "background", "approaches detecting errors nonnative writers typically produce grammars look specific expected error types schneider mccoy park palmer washburn", "approach essays written esl students collected examined errors", "parsers adapted identify error types found essay collection", "take different approach initially viewing error detection extension word sense disambiguation wsd problem", "corpusbased wsd systems identify intended sense polysemous word collecting set example sentences various senses extracting salient contextual cues sets build statistical model sense", "identify intended sense word novel sentence extracting contextual cues selecting similar word sense model eg leacock chodorow miller yarowsky", "golding showed methods used wsd decision lists bayesian classifiers could adapted detect errors resulting common spelling confusions sets theyre", "extracted contexts correct usage confusable word training corpus identified new occurrence error matched wrong context", "grammatical errors result simple word confusions", "complicates task building model incorrect usage", "approach considered proceed without model represent appropriate word usage senses single model compare novel example model", "appealing part formulation could bypass knowledge acquisition bottleneck", "occurrences ofthe word collection edited text could automatically assigned single training set representing appropriate usage", "inappropriate usage would signaled contextual cues occur training", "unfortunately approach effective error detection", "example word usage error often similar model appropriate usage", "incorrect usage contain salient contextual elements well single anomalous element"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1739130384877128, "p": 0.16666666666666666, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07837837837847869, "p": 0.16666666666666666, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03774928774937501, "p": 0.125, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present unsupervised method detecting grammatical errors inferring negative evidence edited textual corpora", "system developed tested using essaylength responses prompts test english foreign language toefl", "error recognition system alek performs precision recall"]}, "A00-2034": {"introduction": ["problem error detection entail finding similarities appropriate usage rather requires identifying element contextual cues simply fit", "diathesis alternations alternate ways arguments verb expressed syntactically", "syntactic changes sometimes accompanied slight changes meaning verb", "ex ample causative alternation given low", "alternation object transitive variant appear subject intransi tive variant", "conative alternation transi tive form alternates prepositional phrase con struction involving either", "example conative alternation given", "", "boy broke window", "window"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose method identifying diathesis alter nations particular argument type seen slots different grammatical roles alternating forms", "method uses selectional pref erences acquired probability distributions wordnet", "preferences target slots com pared using measure distributional similarity", "method evaluated causative cona tive alternations generally applicable require priori knowledge specific alternation"]}, "A97-1025": {"introduction": ["broke", "spelling checkers available major word processing systems", "spelling checkers catch errors result misspelled words", "error results different incor rect word go undetected", "example quite easily mistyped quiet", "another type er ror occurs writer simply doesnt know word set homophones near homophones proper particular context", "ex ample usage affect effect commonly confused", "though cause different types errors treat similarly examining contexts appear", "consequently effort made distinguish er ror types called contextual spelling er rors", "kukich 1992a 1992b reports observed spelling errors contextual er rors", "sets words frequently misused mistyped another identified confusion sets", "thus earlier examples quiet quite affect effect separate confusion sets", "paper introduce latent semantic anal ysis lsa method correcting contextual spelling errors given collection confusion sets", "1homophones words sound spelled differently", "lsa originally developed model infor mation retrieval dumais et al deerwester et al proven useful tasks", "examples include expert expert lo cator streeter lochbaum confer ence proceedings indexer foltz per forms better simple keywordbased index", "recently lsa proposed theory se mantic learning landauer dumais press", "motivation using lsa test effec tiveness predicting words based given sen tence compare bayesian classifier", "lsa makes predictions building highdimensional semantic space used compare sim ilarity words confusion set given context"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1538461489644972, "p": 0.13333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07967781419044126, "p": 0.13333333333333333, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["contextual spelling errors defined use incorrect though valid word particular sentence context", "tra ditional spelling checkers flag misspelled words typically attempt identify words used incorrectly sentence", "explore use latent se mantic analysis correcting incor rectly used words results com pared earlier work based bayesian classifier"]}, "C00-1081": {"introduction": ["experimental results lsa predic tion compared baseline predic tor hybrid predictor based trigrams bayesian classifier", "stochastic language modeling imported speech recognition area successful methodologies natural language processing", "fact language models speech recognition far know based ngram model practical partofspeech pos taggers alsobased word pos ngram model exten sion church cutting et al merialdo1994 dermatas kokkinakis", "pos tagging rst step natural language process ing stochastic taggers solved problem satisfying accuracy many applications", "next step parsing say discovering structure given sentence", "recently many parsers based stochastic approach proposed", "although reported accuracies arehigh accurate enough many appli cations stage attempts made improve", "major applications parser toparse spoken text recognized speech rec ognizer", "attempt clearly aiming spokenlanguage understanding", "consider com bine parser speech recognizer better parser based generative stochastic modelas required language model speech rec ognizer", "generative means sum probabilities possible sentences equal less", "language model generative allows seamless combination parser speech recognizer", "means speech recognizer stochastic parser languagemodel benets richer information normal ngram model", "even though combination possible practices recognizer puts best sentences probabilities parser taking input parses outputs sentence parse tree thathas highest probability possible combina tions", "result parser based generativestochastic language model help speech rec ognizer select syntactically reasonable sentence candidates", "therefore better language model parser generativein paper taking japanese object lan guage propose generative stochastic language model parser based", "model treats sentence word sequence predicts wordfrom left right", "history step predic tion sequence partial parse trees covering preceding words", "predict word model rst predicts partial parse trees stage dependency relation word predicts word selected partial parsetrees", "japanese word depends subse quent word say dependency relationis left right necessary predict di rection dependency relation", "order extend model languages model predict direction dependency", "webuilt parser based model whose parameters estimated sentences nan cial newspaper tested sentences fromthe newspaper"], "introduction_label": [{"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0140065771795307, "p": 0.08333333333333333, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper present stochastic language model using dependency", "model considers sentence word sequence predicts wordfrom left right", "history step prediction sequence partial parse trees covering preceding words", "first model predicts partial parse trees dependency relation next word predictsthe next word trees dependency relation next word", "model generative stochastic model thus used parser language modelof speech recognizer", "experiment prepared syntactically annotated japanese sentences extracted nancial newspaper estimated parameters model", "built aparser based model tested approximately sentences newspaper", "accuracy dependency relation thehighest accuracy level obtained japanese stochastic parsers"]}, "C00-2118": {"introduction": ["accuracy depen dency relation highest obtained japanese stochastic parsers", "detailcd information verbs critical broad range nli ir tasks jnau ual determination large numbers verbs difficult resourc intensive", "hmearch tit automatic acquisition verbbas krtowledg succeded pleaning syntactic proprtis rbs subcategorization frames line resources lhmt briscoe carroll dorr manning", "recently researchers investigated statistical corpus based methods lexical semantic classification syntactic properties verb usage ivlclcc 199g lapata aud brew schulte i111 walde 199h stevenson ivlerlo steven son ct al mccarthy", "corpusbased approaches lexical semantic classification particular drawn levins hypothesis levin verbs classi fied according diathesis alternations alter nations syntactic expressions arguments participatefor example whether fhis research partly sponsored hy us prauts070211 aml li5j8j22 swiss nsf fellowship information sciences cmmcil hutgers university hcs pennsylvania", "research cml clncted first author hutgers university", "paola merlo latl department linguistics university geneva", "rue de caudollc gcncvc tlsuisse", "merlolettresunigech verb occurs dativeprepositional phrase al ternation english", "diagnostic diathesis alternations subcategorization alternatives verb", "classes exhibit subcaegorization possibilities difler argument structures ie content tnatic roles assigned arguments verb", "type situation constitutes particularly difficult case corpusbased classiltcation meth ods", "jn paper apply corpusbased lexica", "acquisition tucthodology distinguish classes verbs allow sante su bcatcgorizations difrer thematic roles", "first assume atttontatically restrict choice classes participate relevant su bcaic gorizations", "lapata", "brew id99", "proposal use statistics diathesis alternants way furthcr distinguish verbs iclt allow tlw sante su bcat gorizations achivinp fin graincd classification st", "work focuses dekrtnining bests lllantic class verb lypr set usages verb document corpusrather single verb lokrn single local context", "way exploit broad behavior verb corpus determine likely class overall", "vve investigate proposed approach depth case study major classes op tionally intransitive verbs english utwrgative unaccusativc objectdrop", "nlore specifically according levins classification la vin unergatives arc manner motion verbs iump march unaccusatives arc verbs change state open explode objectdrop verbs unexpressed object alterna tion verbs played painlcd", "classes support transitive intransi tive subcategorizations arc distinguished pattern thematic role assignments sub jed object position", "au tom atically clas sify verbs basis statistical ap proximations syntactic indicators lying argument structures using numerical fea tures collected large syntactically anno tated tagged parsed corpus", "apply chine learning techniques determine whether frequency distributions features dividually combination support automatic classification verbs", "preview sults demonstrate combining fivc numerical indicators sufficient reduce er ror rate classification task chance", "specifically achieve almost accuracy task whose baseline chance performance whose expertbased per bound calculated"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0397417652978834, "p": 0.05263157894736842, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03633961810930086, "p": 0.037037037037037035, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}], "abstract": ["automatically classify verbs lexical se rnantic classes based distributions indiet tors verb alternations extracted large annotated corpus", "address problem particularly difficult verb classes although semantically diflerr nt sim ilar surface syntactic behavior", "prantmatical feattjres arc suflicient reduce error rate thw dance achieve almost accuracy task whose baseline performance whosexpertbased upptr bound cal culated 8g51l vve conclude corpudriven extraction grannuatical featurcs promiing methodology fin grained verb classification"]}, "C00-2148": {"introduction": ["conclude distributionbased method lexical se mantic verb classification promising avenue research", "difficulty achieving adequate handcrafted semantic representations limited field natural language processing applications contained wellclefinccl subdomains", "despite many different lexicon clcvclopmcnt ap proaches melcuk copestakc sanfil ippo lowe et al field develop clear consensus guidelines computational lexicon", "controver sial areas building lexicon polysemy senses computationally distinguished characterized", "address problem em ploying compositional semantics adjunction syntactic phrases support regular verb sense extensions", "differs lexical concep tual structure lcs approach exemplified voss requires separate lcs representa tion possible sense extension", "pa per describe construction vcrbnet verb lexicon explicitly stated syntactic seman tic information individual lexical items using levin verb classes levin systematically construct lexical entries", "use lexicalizecl tree adjoining grammar ltag joshi schabes capture syntax verb class associate semantic predicates tree", "although similar ideas explored verb sense extension pustejovsky goldberg approach applying ltag prob lem composing extending verb senses novel", "ltags extended domain local ity captures arguments verb local manner", "association semantic predicates tree yields complete semantics verb"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.037496340846416315, "p": 0.04, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present classbased approach building verb lexicon makes explicit close asso ciation syntax semantics levin classes", "used lexicalized tree adjoin ing grammars capture syntax associated verb class augmented trees clude sclcctional restrictions", "addition semantic predicates arc associated tree al low compositional interpretation"]}, "C02-1027": {"introduction": ["operation adjunction ltags provides mechanism extending verb senses", "state art todays full parsing knowledgebased automatic analysis still falls short providing reliable processing framework robust realworld applications automatic abstracting information extraction", "problem especially acute languages benefit wide range processing programs bulgarian", "various projects address different aspects automatic analysis bulgarian morphological analysis krushkov simov et al morphological disambiguation simov et al parsing avgustinova et al previous work pursued development knowledgepoor robust processing environment high level component integrity", "paper reports development implementation robust architecture language processing bulgarian referred lingua includes modules pos tagging sentence splitting clause segmentation parsing anaphora resolution", "text processing framework builds basis considerably shallower linguistic analysis input thus trading depth interpretation breadth coverage workable robust solution"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027735009836184408, "p": 0.02857142857142857, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}], "abstract": ["paper describes lingua architecture text processing bulgarian", "first preprocessing modules tokenisation sentence splitting paragraph segmentation part ofspeech tagging clause chunking noun phrase extraction outlined", "next paper proceeds describe detail anaphora resolution module", "evaluation results reported processing task"]}, "C02-1033": {"introduction": ["lingua uses knowledge poor heuristi cally based algorithms language analysis way getting round lack resources bulgarian", "topic analysis aims identifying topics text delimiting extend finding relations resulting segments recently raised important interest", "largest part dedicated topic segmentation called linear text segmentation tdt topic detection tracking initiative fiscus et al addresses tasks mentioned domaindependent viewpoint necessarily integrated way", "systems implement work categorized according kind knowledge use", "achieve text segmentation rely intrinsic characteristics texts word distribution hearst choi utiyama isahara linguistic cues passonneau litman", "applied without restriction domains low results text doesnt characterize topical structure surface clues", "systems exploit domainindependent knowl edge lexical cohesion network words built dictionary kozima large set collocations collected corpus fer ret kaufmann choi", "extend knowledge permits systems discard false topical shifts without losing independence regard domains", "last main type systems relies knowledge topics encounter texts process", "typically kind approach developed tdt knowledge automatically built set reference texts", "work bigi bigi et al stands perspective focuses much larger topics tdt", "systems limited scope due topic representations precise reason", "hybrid systems combine approaches presented developed illustrated interest combination jobbins evett combined word recurrence collocations thesaurus beeferman et al relied collocations linguistic cues", "topic analysis propose implements hybrid approach relies general language resource collocation network exploits together word recurrence texts", "moreover simultaneously achieves topic segmentation link detection ie determining whether segments discuss topic"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.21052631091412755, "p": 0.25, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07549857549866276, "p": 0.25, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03946333181507231, "p": 0.07692307692307693, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03918918918928951, "p": 0.08333333333333333, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03889570552123663, "p": 0.045454545454545456, "r": 0.03571428571428571}}], "abstract": ["present paper method achieving integrated way tasks topic analysis segmentation link detection", "method combines word repetition lexical cohesion stated collocation network compensate respective weaknesses approaches", "report evaluation method segmentation corpora french english propose evaluation measure specifically suits kind systems"]}, "C02-1039": {"introduction": ["detail paper implementation analysis topicoll system reportevaluations capabilities concerning segmen tation languages french english finally propose evaluation measure integrates segmentation link detection", "task word sense disambiguation consists assigning appropriate meaning polysemous word given con text", "large range applications includingmachine translation knowledge acquisition formation retrieval information extraction andothers require knowledge word mean ings therefore wsd algorithms represent anecessary step applications", "start ing senseval1 wsd received growing attention natural languageprocessing community motivates continuously increasing number researchers de velop wsd systems devote time finding solutions challenging problem", "senseval1 competitions provided goodenvironment development super vised wsd systems making freely available large amounts sense tagged data", "senseval1 data words wasmade available adding examples tagged respect hector dic tionary", "size tagged corpus increasedwith senseval2 additional examples released polyse lhttpwwwitribtonacukeventssensevalmous words", "time semantic annota tions performed respect wordnet", "experiments results reported thispaper pertain senseval2 data", "ever similar experiments performed senseval1 data comparable resultsmost efforts wsd field concentrated far towards supervised learning algorithms methods usu ally achieve best performance cost low recall", "sense tagged occurrence particular word transformed featurevector suitable automatic learning pro cess", "main decisions need made design system set features used learning algorithm", "commonly used features include surrounding words part speechbruce wiebe 1999context keywords ng lee context bigrams pedersen various syntac tic properties fellbaum et al etc learning methodology large range ofalgorithms employed including neu ral networks leacock et al decision trees pedersen decision lists yarowsky memory based learning veenstra et al2000 others", "experimental comparison learning algorithms used disambiguate meaning word line pre sented mooney", "investigate paper use lazy learner namely instance based learning solve semantic ambiguity words contextthe main advantage instance based learn ers fact consider every single training example making classification decision", "characteristic proves particularly useful nlp problems training datais usually expensive exceptions impor tant", "side lazy learners including instance based learners disadvantage easily misled irrelevant features", "inthe algorithm described paper draw back solved improving learner scheme automatic feature selection", "methodology presented integralpart larger system capabil ity performing supervised opentext wsd mihalcea", "reasons clarity space focus paper description supervised component", "knowledge instance based learning per word automatic feature selection new approach wsd field show leads good results", "previous work considered application instance based learning automatic feature selection problem pronoun resolution cardie"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0760286225399182, "p": 0.08333333333333333, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03983890709524641, "p": 0.06666666666666667, "r": 0.03571428571428571}}], "abstract": ["describe algorithm word sense disambiguation wsd relies lazy learner improved automatic feature selection", "algorithm implemented system achieves excellent performance set ofdata released senseval2 competition", "present results obtained discuss performance various features context supervised learning algorithms wsd"]}, "C04-1074": {"introduction": ["wsd work closest reported bruce wiebe decomposable probabilistic models used combination eager naive bayes algorithms", "recent years variety approaches pronoun resolution proposed", "based centering theory strube strube hahn tetreault others machine learning aone bennett ge et al soon et al ng cardie yang et al", "supplement older heuristic approaches hobbs lappin leass", "unfortunately approaches evaluated different corpora making different assumptions direct comparison possible", "appreciation new insights quite hard", "evaluation differs regard size genre corpora along following lines", "scope application approaches deal personal possessive pronouns centering heuristic others consider coreference links general soon et al ng cardie yang et al", "drawback latter view mixes problems different lev els difficulty", "remains unclear much success due virtues approach much due distribution hard easy problems corpus", "paper deal coreferential pronouns ie possessive demonstrative third person pronouns", "thanks go melvin wurster help annotation ciprian gerstenberger discussion", "quality linguistic input proposals evaluated hand annotated strube hahn tree bank input ge et al tetreault", "proposals provide realistic picture work backend parser lappin leass noun chunker mitkov soon et al ng cardie", "evaluation applications presupposing parsing helpful separate errors due parsing intrinsic errors", "hand would like gauge endtoend performance system", "thus provide performance figures ideal handannotated input realistic automatically generated input", "language approaches evaluated english large resources available terms preannotated data muc6 muc7 data lexical information wordnet", "paper deals german", "arguably free wordorder german arguably leads clearer distinction grammatical function surface order information status strube hahn", "paper organized follows", "section describes evaluation corpus", "section describes several factors relevant pronoun resolution", "assesses factors corpus measuring precision restrictiveness", "section describes evaluates algorithms basis factors", "captures algorithms para metric systems proposes parameter settings optimal evaluation data"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper aims deeper understanding several wellknown algorithms proposes ways optimize", "describes discusses factors strategies factor interaction used algorithms", "factors used algorithms algorithms evaluated german corpus annotated syntactic coreference information negra skut et al", "common format pronoun resolution algorithms several open parameters proposed parameter settings optimal evaluation data given"]}, "C04-1075": {"introduction": ["section concludes", "coreference accounts cohesion texts", "especially coreference denotes identity reference holds expressions named entities definite noun phrases pronouns", "coreference resolution process determining whether referring expressions refer entity world", "ability link referring expressions sentence critical discourse language understanding general", "example coreference resolution key task natural language interfaces machine translation text summarization information extraction question answering", "particular information extraction systems like built darpa message understanding conferences muc revealed coreference resolution crucial component information extraction system separate coreference task defined evaluated muc6 muc7", "long tradition work coreference resolution computational linguistics", "many earlier works coreference resolution heavily exploited domain linguistic knowledge carter rich luperfoy carbonell brown", "pressing need development robust inexpensive solutions encouraged drive toward knowledgepoor strategies dagan itai lappin leass mitkov soon ng lim ng cardie motivated emergence cheaper reliable corpus based nlp tools partofspeech taggers shallow parsers alongside increasing availability corpora resources eg ontology", "approaches coreference resolution usually rely set factors include gender number agreements ccommand constraints semantic consistency syntactic parallelism semantic parallelism salience proximity etc factors either constraints discard invalid ones set possible candidates gender number agreements ccommand constraints semantic consistency preferences gives preference certain candidates less others syntactic parallelism semantic parallelism salience proximity", "number approaches use similar set factors computational strategies way antecedents determined ie algorithm formula assigning antecedents differ ie simple cooccurrence rules dagan itai decision trees soon ng lim ng cardie pattern induced rules ng cardie centering algorithms grosz sidner brennan friedman pollard strube tetreault", "paper proposes simple constraintbased multiagent system coreference resolution general noun phrases unrestricted english text", "given anaphor preceding referring expressions antecedent candidates common constraint agent first presented filter invalid antecedent candidates using various kinds general knowledge", "according type anaphor special constraint agent proposed filter invalid antecedent candidates using constraints derived various kinds special knowledge", "finally simple preference agent used choose antecedent anaphor form remaining antecedent candidates based proximity principle", "interesting observation recent antecedent anaphor coreferential chain sometimes indirectly linked anaphor via antecedents chain", "case find recent antecedent always contains little information directly determine coreference relationship anaphor", "therefore given anaphor corresponding special constraint agent always safely filter less informative antecedent candidates", "way rather finding recent antecedent anaphor system tries find direct informative antecedent", "paper focus task determining coreference relations defined muc6 muc7", "order evaluate performance approach coreference resolution utilize annotated corpus scoring programs muc6 muc7", "muc6 dryrun documents annotated coreference information used training data", "annotated training documents muc7", "total size training documents close words muc6 muc7", "testing utilize standard test documents muc standard test documents muc7", "layout paper follows section briefly describe preprocessing determination referring expressions", "section differentiate coreference types discuss restrict possible types direct informative antecedent candidates according anaphor types", "section describe constraintbased multiagent system", "section evaluate multiagent algorithm"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.011049005589507405, "p": 0.14285714285714285, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01123726131062268, "p": 0.06666666666666667, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332868888906, "p": 0.10526315789473684, "r": 0.18181818181818182}, "rouge-l": {"f": 0.022867420349502062, "p": 0.09523809523809523, "r": 0.02197802197802198}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.022662538862818462, "p": 0.1111111111111111, "r": 0.02197802197802198}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01123726131062268, "p": 0.06666666666666667, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022213946732666866, "p": 0.2, "r": 0.02197802197802198}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713948979603, "p": 0.3333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01100055998025679, "p": 0.3333333333333333, "r": 0.01098901098901099}}], "abstract": ["paper presents constraintbased multi agent strategy coreference resolution general noun phrases unrestricted english text", "given anaphor preceding referring expressions antecedent candidates common constraint agent first presented filter invalid antecedent candidates using various kinds general knowledge", "according type anaphor special constraint agent proposed filter invalid antecedent candidates using constraints derived various kinds special knowledge", "finally simple preference agent used choose antecedent anaphor form remaining antecedent candidates based proximity principle", "interesting observation recent antecedent anaphor coreferential chain sometimes indirectly linked anaphor via antecedents chain", "case find recent antecedent always contains little information directly determine coreference relationship anaphor", "therefore given anaphor corresponding special constraint agent always safely filter less informative antecedent candidates", "way rather finding recent antecedent anaphor system tries find direct informative antecedent", "evaluation shows system achieves precision recall fmeasures muc6 muc7 english coreference tasks respectively", "means system achieves significantly better precision rates percent bestreported systems keeping recall rates"]}, "C04-1131": {"introduction": ["finally present conclusions", "task word sense disambiguation wsd identify correct sense word context", "wsd usually performed matching information context word occurs disambiguation knowledge source", "approach uses supervised machinelearning techniques automatically acquire disambiguation knowledge sensetagged corpora", "present type approach widely used seems yield best results kilgarriff rosenzweig ng 1997b", "information conveyed context words morphological form enriched annotations partofspeech tag lemma etc individual piece information called feature", "good feature capture important source knowledge critical determining sense word disambiguated", "choice appropriate set features important issue wsd bruce wiebe perdersen ng zelle pedersen"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["article describes results systematic depth study criteria used word sense disambiguation", "study based target words nouns adjectives verbs", "results always line practices field", "example show omitting non content words decreases performance bigrams yield better results unigrams"]}, "C04-1143": {"introduction": ["thus paper describes results systematic indepth study homogenous criteria ie set features used wsd", "email ubiquitous applications used daily basis millions people worldwide traditionally accessed fixed terminal laptop computer", "past years increasing demand email access mobile phones", "work focused creating email summarisation service provides quality summaries adaptively quickly enough cater tight constrains imposed real time texttospeech system", "work done part european union fasil project aims aims construct conversationally intelligent virtual personal assistant vpa designed manage users personal business information voicebased interface accessible mobile phones", "quality life productivity improved increasingly information dominated society people need access information anywhere anytime", "adaptive information management aim service fasil vpa seeks automatically prioritise present information pertinent mobile users adapt different user preferences"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0395299145297755, "p": 0.05, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03633961810930086, "p": 0.037037037037037035, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["email summarisation presents unique set requirements different general text summarisation", "work describes implementation email summarisation system use voicebased virtual personal assistant developed eu fasil project", "evaluation results first integrated version project presented"]}, "C08-1002": {"introduction": ["aim service comprised main parts email sum mariser email categoriser calendar schedulingpim interaction adaptive prioritisation service optimizes sequence information presented keeping overall duration voicebased dialogue minimum", "organization verbs classes whose members exhibit similar syntactic semantic behavior discussed extensively linguistics literature see eg", "levin rappaport hovav levin", "organization helps avoiding lexicon representation redundancy enables generalizations similar verbs", "great practical use eg compensating nlp statistical models data sparseness", "indeed levins seminal work motivated qc", "licensed creative commons attributionnoncommercialshare alike unported license httpcreativecommonsorglicensesbyncsa30", "rights reserved", "much research aimed automatic discovery verb classes see section", "verbnet vn kipper et al kipper schuler large scale publicly available domain independent verb lexicon builds levin classes extends new verbs new classes additional information semantic roles selectional restrictions", "vn classes proven beneficial semantic role labeling srl swier stevenson semantic parsing shi mihalcea building conceptual graphs hensman dun nion", "levininspired classes used several nlp tasks machine translation dorr document classification klavans kan", "many applications use vn need map verb instances onto vn classes", "verbs polysemous respect vn classes", "sem link loper et al dataset maps verb instance wsj penn treebank vn class", "mapping created using combination automatic manual methods", "yi et al", "used semlink improve srl", "paper present first largescale experimentation supervised machine learning classification algorithm disambiguating verb instances vn classes", "use rich syntactic features extracted treebankstyle parse tree utilize learning algorithm capable imposing class membership constraints thus taking advantage nature task", "use semlink training set", "evaluate algorithm indomain corpus adaptation scenarios", "former test wsj using semlink obtaining accuracy error reduction er strong baseline frequent proceedings 22nd international conference computational linguistics coling pages manchester august class using modern statistical parser", "corpus adaptation scenario disambiguate verbs sentences taken outside training domain", "since manual annotation new corpora costly since vn designed domain independent resource adaptation results important usability nlp practice", "manually annotated sentences genia kim et al medical domain corpus1 testing achieved accuracy er", "adaptation scenario complete sense parser use trained different corpus wsj", "report experiments done using goldstandard manually created parses", "relevant previous works addressing verb instance class classification lapata brew li brew girju et al", "former use verbnet experiments narrower cannot compare results", "latter mapped vn used preliminary highly restricted setup instances monosemous", "completeness compared method theirs2 achieving similar results", "review related work section discuss task section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012255612925692647, "p": 0.16666666666666666, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012276354830712096, "p": 0.14285714285714285, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.10526315378116359, "p": 0.07407407407407407, "r": 0.18181818181818182}, "rouge-l": {"f": 0.039286960161965145, "p": 0.10714285714285714, "r": 0.036585365853658534}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012299775313490156, "p": 0.125, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012276354830712096, "p": 0.14285714285714285, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["verbnet vn major largescale english verb lexicon", "mapping verb instances vn classes proven useful several nlp tasks", "verbs polysemous respect vn classes", "introduce novel supervised learning model mapping verb instances vn classes using rich syntactic features class membership constraints", "evaluate algorithm indomain corpus adaptation scenarios", "cases use manually tagged sem link wsj corpus training data", "domain testing semlink wsj data achieve accuracy error reduction er strong baseline", "adaptation test genia corpus achieve accuracy er", "first largescale experimentation automatic algorithms task"]}, "C08-1014": {"introduction": ["section introduces model section describes experimental setup section presents results", "stateoftheart statistical machine translation smt systems usually adopt twopass search strategy och koehn et al shown figure", "first pass decoding algorithm applied generate nbest list translation hypotheses second pass final translation selected rescoring reranking nbest translations additional feature functions", "fundamental assumption behind using second pass generated nbest list contain better transla", "licensed creative commons attributionnoncommercialshare alike unported license httpcreativecommonsorglicensesbync sa30", "rights reserved", "tions best choice found decoder", "therefore performance twopass smt system improved aspects ie scoring models quality nbest hypotheses", "rescoring pass improves performance machine translation enhancing scoring models global sophisticated dis criminative feature functions", "idea applying passes instead global feature functions cannot easily decomposed local scores computed decoding", "furthermore rescoring allows feature functions word ngram posterior probabilities estimated nbest list ueffing chen et al zens ney", "twopass method translation performance hinges nbest hypotheses generated first pass since rescoring occurs adding translation candidates generated mt systems hypotheses could potentially improve performance", "technique called system combination bangalore et al matusov et al sim et al rosti et al 2007a rosti et al 2007b", "instead chosen regenerate new hypotheses original nbest list technique call regeneration", "regeneration intermediate pass decoding rescoring depicted figure", "given original nbest list nbest1 generated decoder regeneration pass creates new translation hypotheses list form another nbest list nbest2", "nbest lists combined given rescoring pass derive best translation", "implement methods regenerate new hypotheses redecoding ngram expansion confusion network", "redecoding rosti et al 2007a based regeneration redecodes source sentence using original lm well new trans proceedings 22nd international conference computational linguistics coling pages manchester august figure structure typical twopass machine translation system", "nbest translations generated decoder 1best translation returned rescored additional feature functions"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908715909106, "p": 0.06060606060606061, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04809676378546985, "p": 0.06060606060606061, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper studies techniques improve quality nbest hypotheses additional regeneration process", "unlike multisystem consensus approach multiple translation systems used improvement achieved expansion best hypotheses single system", "explore different methods implement regeneration process decoding ngram expansion confusion networkbased regeneration", "experiments chinesetoenglish nist iwslt tasks show methods obtain consistent improvements", "moreover combination strategies achieves improvements outperforms baseline bleuscore iwslt06 nist03 nist05 test set respectively"]}, "C08-1088": {"introduction": ["didates mt systems", "information extraction key tasks natural language processing", "attempts identify relevant information large amount natural language text documents", "sub tasks defined ace program1 paper focuses exclusively relation detection characterization rdc task detects classifies semantic relationships predefined types entities ace corpus", "", "licensed creative commons attribution noncommercialshare alike unported license httpcreativecommonsorglicensesbyncsa30", "rights reserved", "httpwwwldcupenneduprojectsace example sentence microsoft corp based redmond wa conveys relation gpeaffbased microsoft corp org redmond gpe", "due limited accuracy stateoftheart syntactic semantic parsing reliably extracting semantic relationships named entities natural language documents still difficult unresolved problem", "literature featurebased methods dominated research semantic relation extraction", "featuredbased methods achieve promising performance competitive efficiency transforming relation example set syntactic semantic features lexical knowledge entityrelated information syntactic parse trees deep semantic information", "detailed research zhou et al shows difficult extract new effective features improve extraction accuracy", "therefore researchers turn kernelbased methods avoids burden feature engineering computing similarity discrete objects eg parse trees directly", "prior work zelenko et al culotta sorensen bunescu mooney current research zhang et al zhou et al kernel methods showing potential relation extraction", "key problem kernel methods relation extraction represent capture structured syntactic information inherent relation instances", "kernel methods using dependency tree culotta sorensen shortest dependency path bunescu andmooney suffer low recall perform ance convolution tree kernels zhang et al zhou et al syntactic parse trees achieve comparable even better performance featurebased methods", "still exist problems regarding currently widely used tree spans", "zhang et al", "discover shortest path proceedings 22nd international conference computational linguistics coling pages manchester august enclosed tree spt achieves best performance", "zhou et al", "extend contextsensitive shortest pathenclosed tree cs spt dynamically includes necessary predicatelinked path information", "problem spt csspt still contain unnecessary information", "problem considerable number useful contextsensitive information missing sptcsspt although csspt includes contextual information relating predicate linked path", "paper proposes new approach dynamically determine tree span relation extraction exploiting constituent dependencies remove noisy information well keep necessary information parse tree", "motivation integrate dependency information proven useful relation extraction structured syntactic information construct concise effective tree span specifically targeted relation extraction", "moreover explore interesting combined entity features relation extraction via unified parse semantic tree", "sections paper organized follows", "previous work first reviewed section", "section proposes dynamic syntactic parse tree entityrelated semantic tree described section", "evaluation ace rdc corpus given section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03016579555494728, "p": 0.041666666666666664, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03021330786371127, "p": 0.043478260869565216, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04761904375283478, "p": 0.03225806451612903, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027386983731762453, "p": 0.027777777777777776, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030227891877784848, "p": 0.045454545454545456, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.3999999960888889, "p": 0.75, "r": 0.2727272727272727}, "rouge-l": {"f": 0.08192519273618294, "p": 0.75, "r": 0.08108108108108109}}], "abstract": ["paper proposes new approach dynamically determine tree span tree kernelbased semantic relation extraction", "exploits constituent dependencies keep nodes head children along path connecting entities removing noisy information syntactic parse tree eventually leading dynamic syntactic parse tree", "paper explores entity features combined features unified parse semantic tree integrates structured syntactic parse information entityrelated semantic information", "evaluation ace rdc corpus shows dynamic syntactic parse tree outperforms previous tree spans composite kernel combining tree kernel linear stateoftheart featurebased kernel achieves far best performance"]}, "C08-2009": {"introduction": ["finally conclude work section", "discourse structures cannot always described completely either ambiguous stede discourse parser fails analyse completely", "either case underspecification formalisms ufs used represent partial information discourse structure", "ufs used semantics model structural ambiguity without disjunctive enumeration readings van deemter peters", "underspecified descriptions discourse must handle kinds incomplete information configuration discourse segments combine larger units discourse relations bring configuration corpus studies rst discourse treebank carlson et al showed interdependencies relations configuration phenomenon first noted corstonoliver", "interdependencies formulated constraints contribute disambiguation underspecified descriptions discourse structure", "eg discourse segments constituted relation condition premiss tends dis qc", "licensed creative commons attributionnoncommercialshare alike unported license httpcreativecommonsorglicensesbyncsa30", "rights reserved", "course atom least maximally short1 similarly evidence interdependency constraint relation purpose1", "cases purpose1 discourse atom nucleus", "corpus evaluation furthermore shows patterns never occur exclusively tendencies", "realised soft constraints tendencies help sort set readings according established preferences allows focus best reading nbest readings", "high value ufbased approach discourse structure must cope extremely high numbers readings", "model interdependency constraints use regular tree grammars rtgs comon et al", "rtgs straightforwardly extended weighted regular tree grammars wrtgs represent soft hard constraints", "apart corpusextracted examples consider hard interdependency constraint similar right frontier constraint"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["introduce new type discourse constraints interaction discourse relations configuration discourse segments", "examine corpusextracted examples soft constraints", "show use regular tree gramamrs process constraints representation constraints depends expressive power formalism"]}, "C10-1018": {"introduction": ["show integrate attachment constraint formalism representation depends expressiveness rtgs", "relation extraction task detecting characterizing semantic relations expressed entities text", "instance given sentence cone kansas city native originally signed royals broke majors team relations might want extract employment relation pair entity mentions cone royals", "important many nlp applications building ontology entities biomedical information extraction question answering", "prior work employed diverse approaches towards resolving task", "approach build supervised systems using sentences annotated entity mentions predefined target relations", "given new sentence system detect disambiguate presence predefined relations might exist mention pairs sentence", "building systems researchers used wide variety features kambhatla zhou et al jiang zhai", "common features used analyze target sentence include words appearing sentence partof speech pos tags syntactic parse sentence dependency path pair mentions", "related line work researchers proposed various kernel functions based different structured representations eg dependency syntactic tree parses target sentences bunescu mooney zhou et al zelenko et al zhang et al", "additionally researchers tried automatically extract examples supervised learning resources wikipedia weld et al databases mintz et al attempted open information extraction ie banko et al extract possible relations", "work focus supervised", "prior work feature kernel functions employed usually restricted defined various representations eg lexical structural target sentences", "recognizing relations humans thus constrained rely abundance implicit world knowledge background information", "quantifies world background knowledge rarely explored literature attempt provide complete precise definitions paper", "show considering relationship relations interest proceedings 23rd international conference computational linguistics coling pages beijing august well relate existing knowledge resources improve performance", "specifically contributions paper following relations interest clustered organized hierarchical ontology show use information improve performance", "defining appropriate con straints predictions relations different levels hierarchy obtain globally coherent predictions well improved performance", "coreference generic relationship might exists entity mentions show exploit information assuming coreferring mentions interesting relations", "capture intuition using coreference information constraint predictions system", "characterizing relationship pair mentions use large encyclopedia wikipedia infer knowledge mentions", "work probabilistically mapping mentions respective wikipedia pages check whether mentions related", "another generic relationship might exists pair mentions whether parentchild relation use additional information", "sparsity features especially lexical features common problem supervised systems", "work show make fruitful use unlabeled data using word clusters automatically gathered unlabeled texts way generalizing lexical features", "combine various relational predictions background knowledge global inference procedure formalize via integer linear programming ilp framework constraint optimization problem roth yih", "allows us easily incorporate various constraints encode background knowledge", "roth yih develop relation extraction approach exploits constraints entity types relations allowed", "extend view significantly similar computational framework exploit relations target relations background information world knowledge way improve relation extraction make globally coherent predictions", "rest paper first describe features used basic system section", "describe make use background knowledge section", "section show experimental results perform analysis section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024256852399887556, "p": 0.03333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713948979603, "p": 0.3333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021825539475989305, "p": 0.3333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.12121211676767694, "p": 0.09090909090909091, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04815528226412383, "p": 0.09090909090909091, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04730148883385132, "p": 0.1111111111111111, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0220600295227433, "p": 0.16666666666666666, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0220600295227433, "p": 0.16666666666666666, "r": 0.021739130434782608}}], "abstract": ["relation extraction task recognizing semantic relations entities", "given particular sentence supervised approaches relation extraction employed feature kernel functions usually single sentence scope", "overall aim paper propose methods using knowledge resources external target sentence way improve relation extraction", "demonstrate exploiting background knowledge relationships target relations well considering target relations relate existing knowledge resources", "methods general suggest could applied nlp tasks"]}, "C10-1064": {"introduction": ["section discuss related work concluding section", "relation extraction aims identify semantic relations entities document", "many relation extraction studies followed relation detection characterization rdc task organized automatic content extraction project doddington et al make multilingual corpora english chinese arabic", "although datasets encourage development evaluation statistical relation extractors languages would scarcity labeled training samples learning new system another language korean", "since manual annotation entities relations resourcepoor languages expensive would like consider instead weaklysupervised learning technique order learn relation extractor without significant annotation efforts", "propose leverage parallel corpora project relation annotation source language eg english target eg korean", "many supervised machine learning approaches successfully applied rdc task kambhatla zhou et al zelenko et al culotta sorensen bunescu mooney zhang et al focused weaklysupervised relation extraction", "example zhang chen et al utilized weaklysupervised learning techniques relation extraction consider weak supervision context crosslingual relation extraction", "key hypothesis use parallel corpora learning relation extraction system referred crosslingual annotation projection", "early studies crosslingual annotation projection accomplished lexicallybased tasks example partofspeech tagging yarowsky ngai namedentity tagging yarowsky et al verb classification merlo et al", "recently increasing interest applications annotation projection dependency parsing hwa et al mention detection zitouni florian semantic role labeling pado lapata", "best knowledge work reported rdc task", "paper apply crosslingual annotation projection approach binary relation detection task identifying relation entities", "simple projection method propagates relations source language sentences proceedings 23rd international conference computational linguistics coling pages beijing august wordaligned target sentences target relation detector bootstrap projected annotation", "automatic annotation unreliable misclassification source text word alignment errors causes critical fallingoff annotation projection quality", "alleviate problem present noise reduction strategies heuristic filtering alignment correction dictionary instance selection based assessment combine yield better result", "provide quantitive evaluation method new korean rdc dataset", "experiment leverage englishkorean parallel corpus collected web demonstrate annotation projection approach noise reduction method beneficial build initial korean relation detection system", "example combined model noise reduction methods achieves f1scores precision recall favorably comparing shown supervised baseline1 remainder paper structured follows", "section describe crosslingual annotation projection approach relation detection task", "present noise reduction methods section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.23529411307958487, "p": 0.3333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05523993001635577, "p": 0.3333333333333333, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}], "abstract": ["extensive studies relation extraction conducted last decade statistical systems based supervised learning still limited require large amounts training data achieve high performance", "paper develop crosslingual annotation projection method leverages parallel corpora bootstrap relation detector without significant annotation efforts resourcepoor language", "order make method reliable introduce simple projection noise reduction methods", "merit method demonstrated novel korean relation detection task"]}, "C10-1081": {"introduction": ["experiment proposed korean rdc evaluation set shown section section conclude paper section", "syntaxbased statistical machine translation ssmt achieved significant progress recent years galley et al knight liu et al huang et al showing deep linguistic knowledge used properly improve mt performance", "semanticsbased smt natural extension ssmt begun receive attention researchers liu gildea wu fung", "semantic structures major advantages syntactic structures terms helping machine translation", "first semantic roles tend agree better languages syntactic constituents fung et al", "property motivates approach using consistency semantic roles select mt outputs wu fung", "secondly set semantic roles predicate models skeleton sentence crucial readability mt output", "skeleton mean main structure sentence including verbs arguments", "spite theoretical potential semantic roles much success using improve smt systems", "liu gildea proposed semantic role based treetostring tts transducer adding semantic roles tts templates", "approach differentiate semantic roles different predicates always improve tts transducers performance", "wu fung took output phrasebased smt system moses koehn et al kept permuting semantic roles mt output best matched semantic roles source sentence", "approach shows positive effect applying semantic role constraints requires retagging semantic roles every permuted mt output scale well longer sentences", "paper explores ways tightly integrating semantic role features srfs mt system rather using postprocessing best reranking", "semantic role labeling srl systems usually use sentencewide features xue palmer pradhan et al toutanova et al thus difficult compute target side semantic roles incrementally decoding", "noticing source side semantic roles easy compute apply compromise approach target side semantic roles generated projecting source side semantic roles using word alignments source target sentences", "since approach perform true srl target string cannot fully evaluate whether source target semantic structures consistent", "approach capture semanticlevel reordering sentences", "assume mt system capable providing word alignment equivalent information decoding generally true current statistical mt systems", "specifically types semantic role features proposed paper semantic role reordering feature designed capture skeleton level permutation semantic role deletion fea proceedings 23rd international conference computational linguistics coling pages beijing august ture designed penalize missing semantic roles target sentence", "use features decoding need keep track semantic role sequences srs partial translations generated based sourceside semantic role sequence corresponding word alignments", "since srl system mt system separate translation rule eg phrase pair phrasebased smt could cover partial sourceside semantic roles", "cases partial srss must recorded way combined later partial srss", "dealing problem increase complexity decoding algorithm", "fortunately treeto string transducer based mt systems liu et al huang et al avoid problem using syntax tree srl mt arrangement guarantees tts template either covers parts sourceside semantic role complete semantic roles", "advantage motivates us use tts transducer mt system demonstrate use proposed semantic role features", "since hard design generative model combine semantic role features tts templates use loglinear model estimate feature weights maximizing conditional probabilities target strings given source syntax trees", "loglinear model latent variables discussed blunsom et al", "apply technique combine tts templates semantic role features"], "introduction_label": [{"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.035066140134551615, "p": 0.034482758620689655, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03918918918928951, "p": 0.08333333333333333, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0395299145297755, "p": 0.05, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.036935842946350037, "p": 0.038461538461538464, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.049999996012500325, "p": 0.034482758620689655, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03235502426995308, "p": 0.030303030303030304, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose semantic role features treetostring transducer model reorderingdeletion sourceside semantic roles", "semantic features well treetostring templates trained based conditional loglinear model shown significantly outperform systems trained based maxlikelihood em", "show significant improvement sentence fluency using semantic role features loglinear model based manual evaluation"]}, "C10-2052": {"introduction": ["remainder paper organized follows section describes semantic role features proposed machine translation section describes semantic role features used trained tts transducer section presents experimental results section gives conclusion", "ambiguity central topics nlp", "substantial amount work devoted disambiguating prepositional attachment words names", "prepositions word types ambiguous", "example word assume temporal spatial us meanings well others less easily classifiable vein", "prepositions typically senses nouns verbs litkowski hargraves making difficult disambiguate", "preposition sense disambiguation psd many potential uses", "example due relational nature prepositions disambiguating senses help allword sense disambiguation", "machine translation different senses english preposition often correspond different translations foreign language", "thus disambiguating prepositions correctly help improve translation quality1 coarsegrained psd valuable information extraction sense acts label", "recent study hwang et al", "identified preposition related features coarsegrained pp labels used informative feature identifying causedmotion constructions", "understanding constraints hold prepositional constructions could help improve pp attachment parsing frequent sources parse errors", "several papers successfully addressed psd variety different approaches rudzicz mokhov ohara wiebe ye baldwin ohara wiebe tratz hovy", "often possible increase accuracy using different classifier andor features adding features creates problems lead overfitting possibly improving accuracy always clear improvement comes features actually informative", "parameter studies exist general word sense disambiguation wsd tasks yarowsky florian psd accuracy steadily increasing exploration parameters prepositions guide engineering decisions", "go beyond simply improving accuracy analyze various parameters order determine ones actually informative", "explore different options context feature se see chan et al relevance word sense disambiguation chiang et al role prepositions mt coling poster volume pages beijing august lection influence different preprocessing methods different levels sense granularity", "using resulting parameters maximum entropy classifier able improve significantly existing results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024265011818283992, "p": 0.04, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["choosing right parameters word sense disambiguation task critical success experiments", "explore idea prepositions often overlooked word class", "examine parameters must considered preposition disambiguation namely context features granularity", "delivers increased performance significantly improves stateof theart systems shows potential improving word sense disambiguation tasks", "report accuracies coarse finegrained preposition sense disambiguation respectively"]}, "C10-2087": {"introduction": ["general outline present potentially extended word classes improve wsd general", "recent years automatically extracting biomedical information subject significant research efforts due rapid growth biomedical development discovery", "wide concern characterize protein interaction partners since crucial understand functional role individual proteins organization entire biological process", "manual collection relevant proteinprotein interaction ppi information thousands research papers published every day timeconsuming automatic extraction approaches help natural language processing nlp techniques become necessary", "various machine learning approaches relation extraction applied biomedical domain classified categories featurebased methods mitsumori et al giuliano et al stre et al kernelbased methods bunescu et al erkan et al airola et al kim et al", "provided largescale manually annotated corpus task ppi extraction formulated classification problem", "typically featuredbased learning protein pair represented vector whose features extracted sentence involving protein names", "early studies identify existence protein interactions using bagofwords features usually unigram bigram around protein names well various kinds shallow linguistic information pos tag lemma orthographical features", "systems achieve promising results since disregard syntactic semantic information altogether useful task relation extraction newswire domain zhao grishman zhou et al", "furthermore featurebased methods fail effectively capture structural information essential corresponding author coling poster volume pages beijing august wide application kernelbased methods many nlp tasks various kernels subsequence kernels bunescu mooney tree kernels li et al applied ppi detection", "particularly dependencybased kernels edit distance kernels erkan et al graph kernels airola et al kim et al show promising results ppi extraction", "suggests dependency information play critical role ppi extraction well relation extraction newswire stories culotta sorensen", "order appreciate advantages featurebased methods kernelbased methods composite kernels miyao et al miwa et al 2009a miwa et al 2009b employed combine structural syntactic information flat word features significantly improve performance ppi extraction", "critical challenge kernelbased methods computation complexity prevents widely deployed realworld applications regarding large amount biomedical literature archived everyday", "considering potential dependency information ppi extraction challenge computation complexity kernelbased methods naturally ask question essential dependency information maximally exploited featuredbased ppi extraction enhance performance without loss efficiency answer yes paper addresses problems focusing application dependency information featurebased ppi extraction", "starting baseline system common lexical syntactic features incorporated using support vector machines svm augment baseline various features related dependency information including predicates dependency tree", "moreover order reveal linguistic difference distinct domains compare effects various features ppi extraction biomedical texts relation extraction newswire narratives", "evaluation aimed ppi cor rest paper organized follows", "featurebased ppi extraction baseline system given section section describes dependencydriven method", "report experiments section compare work related ones section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11428570997551038, "p": 0.08333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0485937064884717, "p": 0.07692307692307693, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024217344368538975, "p": 0.041666666666666664, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["recent kernelbased ppi extraction systems achieve promising performance capability capture structural syntactic information expense computational complexity", "paper incorporates dependency information well lexical syntactic knowledge featurebased framework", "motivation considering large amount biomedical literature archived daily featurebased methods comparable performance suitable practical applications", "additionally explore difference lexical characteristics biomedical newswire domains", "experimental evaluation aimed corpus shows system achieves comparable performance f1score stateoftheart ppi extraction systems best performance featurebased ones"]}, "C10-2101": {"introduction": ["section concludes paper gives future directions", "dictionary plays important role japanese morphological analysis joint task segmentation partofspeech pos tagging kurohashi et al asahara mat sumoto kudo et al", "like chinese thai japanese delimit words whitespace", "makes first step natural language processing ambiguous simple pos tagging", "accordingly morphemes predefined dictionary compactly represent knowledge segmentation pos", "obvious problem dictionarybased approach caused unknown morphemes morphemes defined dictionary", "even though historically extensive human resources used build highcoverage dictionaries yokoi texts newspaper articles particular web pages contain large number unknown morphemes", "unknown morphemes often cause segmentation errors", "example morphological analyzer ju man wrongly segments phrase saqporo eki sapporo station saqporo unknown morpheme follows sa nouncommon difference unk po unk ro nouncommon sumac eki nouncommon station unk refers unknown morphemes automatically identified analyzer", "erroneous sequence disastrous effects applications morphological analysis", "example hardly identified location named entity recognition", "solution unknown morpheme problem unknown morpheme acquisition mori nagao murawaki kurohashi", "task automatically augmenting dictionary acquiring unknown morphemes text", "example goal acquire morpheme saqporo pos tag nounlocation name unknown morpheme acquisition usually adopts coarser pos tagset represents morphology level distinction noun verb adjective", "means saqporo acquired noun semantic label location name remains assigned", "reason morphology level distinction made httpnlpkueekyotouacjp nlresourcejumanehtml coling poster volume pages beijing august semantic level distinction cannot easily captured morphological clues exploited unknown morpheme acquisition", "paper investigate remaining problem introduce new task semantic classification applied automatically acquired nouns", "task exploit syntactic clues addition morphological ones result acquisition rely automatic parsing", "exam ple since text containing saqporo noununclassified correctly segmented extract phrase saqporo station tree fragment go saqporo determine semantic label", "japanese semantic classification poses interesting challenge proper nouns need distinguished common nouns", "like chinese thai japanese orthographic distinction common proper nouns thing capitalization", "addition seems morphosyntactic ie grammatical distinction", "paper explore lexicosyntactic clues extracted automatically parsed text", "train classification model manually registered nouns apply automatically acquired nouns"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01894739339873598, "p": 0.07692307692307693, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper present twostage approach acquire japanese unknown morphemes text full pos tags assigned", "first acquire unknown morphemes making morphology level distinction apply semantic classification acquired nouns", "advantage approach second stage exploit syntactic clues addition morphological ones result first stage acquisition rely automatic parsing", "japanese semantic classification poses interesting challenge proper nouns need distinguished common nouns", "japanese orthographic distinction common proper nouns apparent morphosyntactic distinction", "explore lexicosyntactic clues extracted automatically parsed text investigate effects"]}, "C10-2104": {"introduction": ["investigate effects lexicosyntactic clues", "reranking promising computational framework drawn special attention natural language processing nlp community", "basically method first employs probabilistic model generate list topn candidates reranks nbest list additional features", "appeal approach flexibility incorporating arbitrary features model", "features help discriminating good bad hypotheses consequently automatic learning", "various algorithms applied reranking nlp applications huang shen et al collins 2002b collins koo including parsing name tagging machine translation", "work exploited disciminative property key criterion reranking algorithm", "reranking appears extremely interesting coupled kernel methods dinarelli et al moschitti collins duffy latter allow extracting ranking hypotheses huge amount features along dependencies", "indeed featurebased learning algorithms involve dotproduct feature vectors kernel methods allow higher generalization replacing dot product function pairs linguistic objects", "functions kind similarity measure satisfying certain properties", "example tree kernel collins duffy objects syntactic trees encode grammatical derivations kernel function computes number common subtrees", "similarly sequence kernels lodhi et al count number common subsequences shared input strings", "namedentities nes essential defining semantics document", "nes objects referred names chinchor robinson people organizations locations", "research ner promoted message understanding conferences mucs shared task conference natural language learning conll automatic content extraction program ace", "literature exist various learning approaches extract namedentities text", "ner sys coling poster volume pages beijing august tem often builds generativediscriminative model either uses classifier car reras et al combines many classifiers using heuristics florian et al", "best knowledge reranking applied ner except reranking algorithms defined collins 2002b collins 2002a targeted entity detection entity classification task", "besides since kernel methods offer natural way exploit linguistic properties applying kernels ne reranking worthwhile", "paper describe kernel methods applied reranking ie detection classification namedentities standard corpora italian english", "key aspect reranking approach structured flat features employed discriminating candidate tagged sequences", "purpose apply tree kernels tree structure encoding ne tags sentence combined polynomial kernel efficiently exploits global features", "main contribution show tree kernels used define general features merely syntactic using appropriate algorithms features reranking effective namedentity recognition", "study demonstrates composite kernel effective reranking namedentity sequences", "without need producing heuristically combining learning models like previous work ner composite kernel captures flat features efficiently exploits structured features", "interestingly kernel yields significant improvement applied corpora different languages"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407672862, "p": 0.03571428571428571, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02377273381650473, "p": 0.05263157894736842, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02377273381650473, "p": 0.05263157894736842, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present novel kernels based structured unstructured features reranking nbest hypotheses conditional random fields crfs applied entity extraction", "former features generated polynomial kernel encoding entity features whereas tree kernels used model dependencies amongst tagged candidate examples", "experiments standard corpora languages ie italian evalita english conll datasets show large improvement crfs fmeasure ie respectively", "analysis reveals kernels provide comparable improvement crfs baseline", "additionally combination improves crfs much sum individual contributions suggesting interesting kernel synergy"]}, "C16-1060": {"introduction": ["evaluation italian corpus shows method outperforms best reported methods whereas english data reaches stateoftheart", "word alignment problem identifying translationally equivalent words languages parallel text", "found widespread use enabling applications statistical machine translation brown et al koehn et al annotation transfer yarowsky et al word sense disambiguation diab resnik lexicon extraction wu xia", "although many types algorithms explored main line research last couple decades based generative ibm models introduced brown et al", "", "models common unsupervised asymmetric models assuming languages bitext source language generates corresponding text language target language word word", "often variant expectationmaximization algorithm dempster et al used inference models recently work using bayesian alignment models using gibbs sampling inference denero et al mermer saraclar gal blunsom", "incorporation bayesian priors models shown improve accuracy since provide flexible way biasing model towards empirical observations language importantly given word type tends limited number translations", "basic word alignment models use lexical cooccurrence word order lexical data tends sparse number authors explored usefulness information sources", "toutanova et al", "showed part speech pos tags integrated ibm models improve word alignment accuracy others reported similar results dependency cherry lin wang zong phrasestructure yamada knight parse trees lemmatized texts bojar prokopov", "addition studies mentioned showed various types linguistic annotation used guide word alignment research showing reverse holds wordaligned parallel texts used transfer annotations models languages resources exist languages", "pioneering work yarowsky et al", "explored tasks pos work licenced creative commons attribution international license", "license details http creativecommonsorglicensesby40 proceedings coling 26th international conference computational linguistics technical papers pages osaka japan december", "tagging shallow parsing lemmatization followed eg dependency parsing hwa et al", "present work combines previous lines work exploring joint models word alignment annotation transfer pos tags bayesian framework"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12121211676767694, "p": 0.09090909090909091, "r": 0.18181818181818182}, "rouge-l": {"f": 0.08948475887154199, "p": 0.1111111111111111, "r": 0.08108108108108109}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05967678076530816, "p": 0.11764705882352941, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["current methods word alignment require considerable amounts parallel text deliver accurate results requirement met small minority worlds approximately languages", "show jointly performing word alignment annotation transfer novel bayesian model alignment accuracy improved language pairs annotations available languagesa finding could facilitate study processing vast number lowresource languages", "present evaluation method used perform singlesource multisource partofspeech transfer translations text different languages", "allows us quantify considerable variation accuracy depending specific source texts used even different translations language"]}, "C92-1059": {"introduction": ["source code implementation available httpwwwlingsusespacos", "unificationbased informationbased linguisticframeworks important objects struc tures called aposfeature structuresapos fss usedto describe linguistic objects phenomena", "fea ture structure either atomic complex atomic fs denoted atomic symbol complex fs consists set featurevalue pairs describes aspect object", "partial information object merged applying unification operation fss", "research unificationbased linguistic theories accompanied research fss", "several extensions fss feature descriptions formal treatments extensions proposed", "disjunctive negative descriptions fss help make linguistic descriptions simple compact andthus easy understand", "disjunctive feature descriptions kay14 introduces fug functional unification grammar gives procedural semantics", "karttunen111 proposes proce dural treatments disjunctions conjunction relatively simple negations", "rounds kasper19 propose logicbased formalismfeature logicwhich uses automata model fss treat disjunctive feature descriptions obtain impor tant results", "negative descriptions fss mostfundamental properties fss partiality formation carry makes insufficient adopt relatively simple treatments", "classical interpretation negation example allow evaluation negations freely interleaved unification", "moshier rounds171 propose formal framework treats negative feature descriptions basis intuitionistic logic", "formalism trouble treating double negations", "dawar5 proposes formal treatment based threevalued logic", "order treat feature domains complex fssand treat taxonomic hierarchies symbolic feature values type sort hierarchies troduced allowing definition typed sorted featurestructures ms", "tiaposs consists type symbol lattice set featurevalue pairs", "tfs seen generalized concept bothatomic complex fss", "pollard sag i8 intro duce sorts ii psg headdriven phrase structtire grammar use sorted pis describe linguistic objects", "ahkacill proposes algebraic framework usingthe sittypes ctypes promising formalizations 1tss based lattice theory", "formalization originally aimed formalizing tegrating various kinds knowledge representation frameworks al approach types defined equivalence classes complex term structures", "asubsumption relation defined term structures", "join meet operations correspond generalization opera tions tfss respectively", "approach essentially adopts apostypeassetapos semantics", "subtype relationships type correspond subsumption relationships denotations types", "based frau aapos work extension prolog login developed", "smolka20 proposes feature logic subsoilsin approach negative descriptions decoin posed kinds primitive negations namely negations sorts complement sorts denotethe complements sets positive counterparts de note negations feature existences negationsof featureaddress agreement featureaddress dis agreement", "smolka extends feature descriptions buta featurestructure interpretation extended de scription include negative fort nation corresponds simple tiaposssome wsbased natural language processing systems developed7", "car penter pollard propose interface build type lattices", "formalizations extended fss extended featuredescriptions described classified classes extensions fss extensions pis hut featuredescriptions", "previous attempts introducetype hierarchies fall former class previous treatments disjunctive negative descrip tions mainly fall latter", "acres us colingt92 nantes amyr proc", "coling92 nantes aug paper proposes extension aithaciapossvapos type incorporates kinds primitive negative descriptions described 0type", "aitkaciaposs 0type formalization uses ter structures", "paper type structures type symbol lattice term structures definedare extended treat negative descriptions", "nega tions type symbols treated extending type symbol lattices negations feature existwicesand featureaddress disagreements treated ex tending term structures", "extension seen intuitionistic", "extension classified class abovebased papers formalization unification al gorithms developed using graph unificationtechniques23", "programs based algo rithms implemented common lisp"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016568588454494517, "p": 0.05263157894736842, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015838206627705572, "p": 0.125, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["formal treatment typed feature structurestfss developed augment tfss negative descriptions treated", "negative descriptions tfss make linguistic descriptionscompact thus easy understand", "negative descriptions classified primitive negative descriptions negations type symbols negations feature existences negations featureaddress value agreements", "formalizationproposed paper based aitkaciaposs complex terms", "first description treated extending type symbol lattices include complement typesymbols", "second third treated augmenting terns structures structures representingthese negations", "algorithms augmentedws unification developed using graph unificationand programs using algorithms written common lisp"]}, "D07-1012": {"introduction": ["bodytext sectionheader confidence0650502 genericheadermethod", "paper concerned task predicting whether sentence contains grammatical error", "accurate method carrying automatic affiliated ibm cas dublin", "grammaticality judgements uses areas computerassisted language learning grammar checking", "comparative evaluation existing error detection approaches hampered lack large commonly used evaluation error corpora", "attempt overcome automatically creating large error corpus containing different types frequently occurring grammatical errors", "use corpus evaluate performance approaches task automatic error detection", "approach uses lowlevel detection techniques based pos ngrams", "approach novel parserbased method employs deep linguistic processing discriminate grammatical input ungrammatical", "approaches implement basic solution attempt improve upon solution using decision tree classifier", "show combining methods improves upon individual methods", "ngrambased approaches problem error detection proposed implemented various forms atwell1987 bigert knutsson chodorow leacock amongst others", "existing approaches hard compare since evaluated different test sets vary size error density", "furthermore approaches concentrate type grammatical error namely contextsensitive real word spelling errors", "implement vanilla grambased approach tested large test set containing different types error", "idea behind parserbased approach error detection use broadcoverage handcrafted precision grammar detect ungrammatical sen proceedings joint conference empirical methods natural language processing computational natural language learning pp", "prague june", "qc association computational linguistics tences", "approach exploits fact precision grammar designed traditional generative grammar sense chomsky distinguish grammatical sentences ungrammatical sentences", "contrast treebankbased grammars tend massively overgenerate generally aim discriminate", "order approach work coverage precision grammars must broad enough parse large corpus grammatical sentences reason choose xle maxwell kaplan efficient robust parsing system lexical functional grammar lfg kaplan bresnan pargram english grammar butt et al experiments", "system employs robustness techniques borrowed optimality theory ot prince smolen sky parse extragrammatical input frank et al crucially still distinguishes optimal suboptimal solutions", "evaluation corpus subset ungrammatical version british national corpus bnc million word balanced corpus british english burnard", "corpus obtained automatically inserting grammatical errors original bnc sentences based analysis manually compiled real error corpus", "paper makes following contributions task automatic error detection"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.044444440750617584, "p": 0.029411764705882353, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01746113989640373, "p": 0.027777777777777776, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper compares deep shallow processing approach problem classifying sentence grammatically well formed illformed", "deep processing approach uses xle lfg parser english grammar versions presented uses xle directly perform classification another uses decision tree trained features consisting xles output statistics", "shallow processing approach predicts gram maticality based ngram frequency statistics present versions uses frequency thresholds uses decision tree trained frequencies rarest ngrams input sentence", "find use decision tree improves basic approach deep parserbased approach", "show combining shallow deep decision tree features effective", "evaluation carried using large test set grammatical ungrammatical sentences", "ungrammatical test set generated automatically inserting grammatical errors wellformed bnc sentences"]}, "D07-1018": {"introduction": ["novel deep processing xlebased approach", "paper reports series experiments explore automatic acquisition semantic classes catalan adjectives", "important challenge classification task model assignment polysemous lexical instances multiple semantic classes combining stateoftheart machine learning architecture multilabel classification schapire singer ghamrawi mccallum ensemble classifier dietterich definition features various levels linguistic description", "proper treatment polysemy essential area lexical acquisition since polysemy repre sents pervasive phenomenon natural language", "previous approaches automatic acquisition semantic classes mostly disregarded problem cf", "merlo stevenson stevenson joanis english semantic verb classes schulte im walde german semantic verb classes", "exceptions tradition pereira et al", "rooth et al", "korhonen et al", "used soft clustering methods multiple assignment verb semantic classes", "work addresses lack methodology modelling polysemous classification", "implement multilabel classification architecture handle polysemy", "paper concentrates classification catalan adjectives general nature architecture allow related tasks profit insights", "target classification experiments set catalan adjectives manually classified experts simple polysemous semantic classes", "deliberately decided favour smallscale broad classification", "far little work semantic classification adjectives opposed verbal semantic classification", "semantic classification propose first step characterising adjectival meaning refined extended subsequent work", "experiments provide thorough comparison feature sets based different levels linguistic description morphology syntax semantics", "set features defined level description performance assessed series experiments", "ensemble classifier comple proceedings joint conference empirical methods natural language processing computational natural language learning pp", "prague june", "qc association computational linguistics ments classification architecture optimising combination different types linguistic evidence", "task motivated fact adjectives play important role sentential semantics crucial determining reference nps defining properties entities", "even using different classes information acquired could applied eg identify referents given context dialog question answering systems induce properties objects information extraction tasks", "furthermore semantic classes corresponding broad sense representations exploited word sense disambiguation", "remainder paper organised follows", "section provides background catalan adjectives section presents gold standard classification"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027806102439457355, "p": 0.14285714285714285, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12121211676767694, "p": 0.09090909090909091, "r": 0.18181818181818182}, "rouge-l": {"f": 0.06042661572744894, "p": 0.08695652173913043, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper assesses role multilabel classification modelling polysemy language acquisition tasks", "focus acquisition semantic classes catalan adjectives show polysemy acquisition naturally suits architectures used multi label classification", "furthermore explore performance information drawn different levels linguistic description using feature sets based morphology syntax semantics ngram distribution", "finally demonstrate ensemble classifiers powerful adequate way combine different types linguistic evidence simple majority voting ensemble classifier improves accuracy best single classifier"]}, "D07-1076": {"introduction": ["section introduces methodology multilabel classification experiments section discusses results improved ensemble classifier presented section", "relation extraction find various predefined semantic relations pairs entities text", "research relation extraction pr omoted message understanding conferences mucs muc nist automatic content extraction ace program ace", "according ace program entity object set objects world relation explicitly implicitly stated relationship entities", "example sentence bill gates chairman chief software architect microsoft corporation onveys acestyle relation employmentexec entities bill gates person name microsoft corporation organization name", "extraction semantic relations entities useful many applic tions question answering eg answer query president united states information retrieval eg expand query george bushwith pres ident united statesvia relationship united states", "many researches done relation extraction", "featurebased methods kambhatla zhou et al achieve certain success employing large amount diverse linguistic features varying lexical knowledge entity related information syntactic parse trees dependency trees semantic information ever difficult effectively capture struc tured parse tree information zhou et al critical performance improvement relation extraction", "alternative featurebased methods tree kernelbased methods provide elegant solution explore implic itly structured features directly computing simila rity trees", "although earlier researches zelenko et al culotta sorensen bunescu mooney 2005a achieve success simple tasks fail complex tasks ace rdc task tree kernelbased methods achieve much progress recently", "stateoftheart zhang et al applied onvolution tree kernel collins duffy achieved comparable performance stateofthe art linear kernel zhou et al relation types ace rdc corpus", "problems collins duffys convolution tree kernel relation extrac tion", "first subtrees enumerated tree kernel computation contextfree", "subtree enumerated tree kernel computation proceedings joint conference empirical methods natural language processing computational natural language learning pp", "prague june", "qc association computational linguistics consider context information outside subtree", "second dec ide proper tree span relation extraction", "zhang et al explored tree spans relation extraction bit surprising find shortest pathenclosed tree spt ie subtree enclosed shortest path linking involved entities parse tree performed best", "contrast intuition", "example got married critical determine relationship john maryin sentence john mary got married shown figure 1e", "obvious information contained spt john marry enough determine relationshipthis paper proposes contextsensitive convolu tion tree kernel relation extraction resolve problems", "first automatically determines dynamic contextsensitive tree span relation extraction extending shortest pathenclosed tree spt include necessary context information outside spt", "proposes context sensitive convolution tree kernel whic enumerates context free subtrees context sensitive subtrees considering ancestor node paths contexts", "moreover paper evaluates complementary nature different linear kernels tree kernels via composite kernel", "layout paper follows", "section review related work details", "dynamic contextsensitive tree span context sensitive convolution tree ker nel proposed section section shows experimental results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.040816323048729994, "p": 0.02631578947368421, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01732673967983619, "p": 0.022222222222222223, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0157128749490135, "p": 0.2, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.374999995703125, "p": 0.6, "r": 0.2727272727272727}, "rouge-l": {"f": 0.04713862484701898, "p": 0.6, "r": 0.046875}}], "abstract": ["paper proposes tree kernel context sensitive structured parse tree information relation extraction", "resolves critical problems previous tree kernels relation extraction ways", "first automatically determines dynamic contextsensitive tree span relation extraction extending widely used shortest pathenclosed tree spt include necessary context information outside spt", "second proposes contextsensitive convolution tree kernel enumerates contextfree context sensitive subtrees consid ering ancestor node paths contexts", "moreover paper evaluates complementary nature tree kernel stateoftheart linear kernel", "evaluation ace rdc corpora shows dynamic contextsensitive tree span much uitable relation extraction spt tree kernel outperforms stateoftheart collins duffys convolution tree kernel", "shows tree kernel achieves much better performance stateoftheart linear kernels finally shows featurebased tree kernelbased methods much complement composite kernel well integrate flat structured features"]}, "D09-1025": {"introduction": ["finally conclude work sec tion", "mounting evidence shows combining information sources information extraction algorithms leads improvements several tasks fact extraction pasca et al open domain ie talukdar et al entailment rule acquisition mirkin et al", "paper show large gains entity extraction combining stateoftheart distributional pattern based systems large set features million document webcrawl year query logs snapshot wikipedia", "generalize mixing sources features framework called ensemble semantics", "distributional patternbased extraction algorithms capture aspects paradigmatic syntagmatic dimensions semantics respectively believed quite complementary", "pasca et al", "showed filtering facts extracted patternbased system according arguments distributional similarity seed facts yielded large precision gains", "mirkin et al", "showed similar gains task acquiring lexical entailment rules exploring supervised combination distributional patternbased algorithms using mlbased svm classifier", "paper builds work studying impact various sources features external distributional patternbased algorithms task entity extraction", "mirkin et als results corroborated task large significant gains baseline obtained incorporating features webcrawl query logs wikipedia", "extracted candidate entities classes actors athletes musicians webcrawl using variant pasca et als patternbased engine pantel et als distributional extraction system", "gradient boosted decision tree used learn regression function feature space ranking candidate entities", "experimental results show gains nominal mean average precision mirkin et als method gains nominal mean average precision unsupervised baseline similar pasca et als method", "summarize contributions paper explore hypothesis although dis tributional patternbased algorithms complementary exhaust semantic space sources evidence leveraged better combine model mixing knowledge sourcesand features novel general informa tion extraction framework called ensemble semantics experiments entity extraction taskshow model achieves large sig nificant gains stateoftheart extractors", "detailed analysis feature correlations interactions shows query log bcrawl features yield highest gains easily accessible wikipedia features improve current stateoftheart systems", "proceedings conference empirical methods natural language processing pages singapore august", "qc acl afnlp figure ensemble semantics framework information extraction", "remainder paper organized follows", "next section present ensemble semantics framework outline various information extraction systems cast framework", "section presents entity extraction system instance ensemble semantics comparing contrasting previous information extraction systems", "experimental methodology analysis described section shows empirical evidence extractor significantly outperforms prior art"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030209967620282052, "p": 0.047619047619047616, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.03703703379286723, "p": 0.023255813953488372, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023938833951019988, "p": 0.022222222222222223, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.23529411307958487, "p": 0.3333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05523993001635577, "p": 0.3333333333333333, "r": 0.05405405405405406}}], "abstract": ["combining information extraction systems yields significantly higher quality resources system isolation", "paper generalize mixing sources features framework called ensemble semantics", "show large gains entity extraction combining stateoftheart distributional pattern based systems large set features webcrawl query logs wikipedia", "experimental results web scale extraction actors athletes musicians show significantly higher mean average precision scores gain compared current state art"]}, "D09-1115": {"introduction": ["finally section concludes discussion future work", "system combination aims find consensus translations different machine translation systems", "proven consensus translations usually better output individual systems frederking nirenburg", "recent several years system combination methods based confusion networks developed rapidly bangalore et al matusov et al sim et al rosti et al 2007a rosti et al 2007b rosti et al et al show stateoftheart performance benchmarks", "confusion network consists sequence sets candidate words", "candidate word associated score", "optimal consensus translation obtained selecting word set maximizing overall score", "construct confusion network first need choose hypotheses ie candidate translations backbone called skeleton literature decide word alignments hypotheses backbone", "hypothesis alignment plays crucial role confusion networkbased system combination direct effect selecting consensus translations", "confusion network restricted way 1to1 mappings allowed hypothesis alignment", "fact even word alignments languages", "common several words connected another several words", "example capable able meaning", "although confusionnetworkbased approaches resort inserting null words alleviate problem face risk producing degenerate translations capable able", "paper propose new system combination method based lattices", "general form confusion network lattice capable describing arbitrary mappings hypothesis alignment", "lattice edge associated sequence words rather single word", "therefore select phrases instead words candidate set minimize chance produce unexpected translations capable", "compared approach stateoftheart confusionnetworkbased system et al achieved significant absolute improvement bleu points nist chineseto", "english test set bleu point nist chinesetoenglish test set", "proceedings conference empirical methods natural language processing pages singapore august", "qc acl afnlp feels like apples prefer apples feels like apples fond apples unidirectional alignments feels like apples prefer apples feels like apples fond apples bidirectional alignments lations", "note phrase fond attached edge", "unlikely obtain translation like like apples", "lattice directed acyclic graph formally weighted finite state automation fsa set nodes set edges"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.056439998443106744, "p": 0.2222222222222222, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["current system combination methods usually use confusion networks find consensus translations different systems", "requiring onetoone mappings words candidate translations confusion networks difficulty handling general situations several words connected another several words", "instead propose latticebased system combination model allows phrase alignments uses lattices encode candidate translations", "experiments show approach achieves significant improvements stateof theart baseline system chinesetoenglish translation test sets"]}, "D09-1138": {"introduction": ["nodes lattice usually labeled according prefer feels like apples fond confusion network prefer feels like apples fond lattice figure comparison confusion network lattice", "lexicons invaluable resources semantic processing", "many cases lexicons necessary restrict set semantic classes assigned word", "fact considerable number works semantic processing implicitly explicitly presupposes availability lexicon word sense disambiguation wsd mccarthy et al tokenlevel verb class disambiguation lapata brew girju et al li brew abend et al", "words methods heavily dependent availability semantic lexicon", "therefore recent research efforts invested developing semantic resources wordnet fellbaum framenet baker et al verbnet kipper et al kipperschuler greatly advanced research semantic processing", "construction resources expensive unrealistic presuppose availability fullcoverage lexicons case unknown words always appear real texts wordsemantics associations vary abend et al", "paper explores method supervised learning probabilistic model verbnet lexicon", "target automatic classification arbitrary verbs including polysemous verbs verbnet classes target estimation probabilistic model represents saliences verbclass associations polysemous verbs", "approach existing lexicon andor annotated corpus used training data", "since verbnet classes designed represent distinctions syntactic frames verbs take features representing statistics syntactic frames extracted unannotated corpora", "additionally classes represent semantic commonalities semantically inspired features like distributionally similar words used", "features considered generalized representation verbs expect obtained probabilistic model predicts verbnet classes unknown words", "model evaluated tasks type level verb classification classification monosemous verbs small subset classes studied previous works joanis stevenson joanis et al", "task classification verbs full set verbnet classes proceedings conference empirical methods natural language processing pages singapore august", "qc acl afnlp attempted", "experiments training instances obtained verbnet andor sem link loper et al features extracted british national corpus wall street journal", "empirically compare several settings model learning varying set features source domain size corpus feature extraction use tokenlevel statistics obtained manually disambiguated corpus", "provide analysis remaining errors lead us improve supervised learning probabilistic semantic lexicon", "supervised methods automatic verb classification extensively investigated stevenson et al stevenson merlo merlo stevenson stevenson joanis joanis stevenson joanis et al", "focus limited small subset verb classes limited number monosemous verbs"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02017650410095058, "p": 0.037037037037037035, "r": 0.01818181818181818}}], "abstract": ["work presented paper explores supervised method learning probabilistic model lexicon verbnet classes", "intend probabilistic model provide probability distribution verbclass associations known unknown verbs including pol ysemous words", "approach training instances obtained existing lexicon andor annotated corpus features represent syntactic frames semantic similarity selectional preferences extracted unannotated corpora", "model evaluated typelevel verb classification tasks measure prediction accuracy verbnet classes unknown verbs measure dissimilarity learned observed probability distributions", "empirically compare several settings model learning vary use features source corpora feature extraction disambiguated corpora", "task verb classification verbnet classes best model achieved error reduction classification accuracy previously proposed model"]}, "D09-1149": {"introduction": ["main contributions present work provide empirical results automatic classification verbs including polysemous ones verbnet classes ii empirically explore effective settings supervised learning probabilistic lexicon verb semantic classes", "dramatic increase amount textual information available digital archives www growing interest techniques automatically extracting information text documents", "information extraction ie technology ie systems expected identify relevant information usually predefined types text documents certain domain put structured format", "according scope nist automaticcontent extraction ace program ace current research ie main objectives entity detection tracking edt relation detection characterization rdc event detection characterization edc", "paper focuses ace rdc subtask many machine learning methods proposed including supervised methods miller et al zelenko et al culotta soresen kambhatla zhou et al zhang et al qian et al semisupervised methods brin agichtein gravano zhang chen et al zhou et al unsupervised methods hasegawa et al zhang et al current work semantic relation extraction task mainly uses supervised learning methods since achieves relatively better performance", "method requires large amount manually labeled relation instances timeconsuming laborious", "contrast unsupervised methods need definitions relation types handtagged data difficult evaluate performance since criteria evaluation", "therefore semisupervised learning received attention balance advantages disadvantages supervised unsupervised methods", "plenitude unlabeled natural language data hand semisupervised learning significantly reduce need labeled data limited sacrifice performance", "specifically bootstrapping algorithm chooses unlabeled instances highest probability correctly labeled use augment labeled training data iteratively", "although previous work yarowsky blum mitchell abney zhang tackled bootstrapping approach theoretical practical point view many key problems still remain unresolved selection initial seed set", "since size initial seed set usually small eg proceedings conference empirical methods natural language processing pages singapore august", "qc acl afnlp instances imbalance relation types manifold structure cluster structure severely weaken strength bootstrapping", "therefore critical bootstrapping approach select appropriate initial seed set", "current systems zhang chen et al use randomly sampling strategy fails explore affinity nature training instances", "alternatively zhou et al", "bootstrap set weighted support vectors labeled unlabeled data using svm", "nevertheless initial labeled data still randomly generated ensure least instances every relation subtype", "paper presents new approach selecting initial seed set based stratified sampling strategy bootstrapping procedure semisupervised semantic relation classification", "motivation behind stratified sampling every relation type much possible represented initial seed set thus leading instances diverse structures added labeled set", "addition explore different strategies augment reliably classified instances labeled data iteratively attempt find stoppage criterion iteration procedure greatly decrease training time using unlabeled set", "rest paper organized follows", "first section reviews related work semi supervised relation extraction", "present underlying supervised learner section", "section details various key aspects bootstrapping procedure including stratified sampling strategy", "experimental results reported section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.03773584576717722, "p": 0.023809523809523808, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016591350392049113, "p": 0.014925373134328358, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024265011818283992, "p": 0.04, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024217344368538975, "p": 0.041666666666666664, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.3999999960888889, "p": 0.75, "r": 0.2727272727272727}, "rouge-l": {"f": 0.06566735112937659, "p": 0.75, "r": 0.06521739130434782}}], "abstract": ["paper presents new approach selecting initial seed set using stratified sampling strategy bootstrappingbased semisupervised learning semantic relation classification", "first training data partitioned several strata according relation typessubtypes relation instances randomly sampled stratum form initial seed set", "investigate different augmentation strategies iteratively adding reliable instances labeled set find bootstrapping procedure stop reasonable point significantly decrease training time without degrading much performance", "experiments ace rdc corpora show stratified sampling strategy contributes bootstrapping procedure", "suggests proper sampling strategy critical semisupervised learning"]}, "D10-1005": {"introduction": ["finally conclude work section", "name suggests mlslda extension latent dirichlet allocation lda blei et al modeling approach takes corpus unannotated documents input produces outputs set topics assignments documents topics", "topics assignments probabilistic topic represented probability distribution words corpus document assigned probability distribution topics", "topic models built foundations lda appealing sentiment analysis learned topics cluster together sentiment bearing words topic distributions parsimonious way represent document1 lda used discover latent structure text eg discourse segmentation purver et al authorship rosenzvi et al", "mlslda extends approach ensuring latent structure underlying topics consistent languages", "discuss multilingual topic modeling section section show enables supervised regression regardless documents language", "capturing semantic correlations", "topic models posit straightforward generative process creates observed corpus", "document distribution unobserved topics chosen", "word position document topic selected", "finally word position generated selecting topic indexed", "recall lda topic distribution words", "monolingual topic models topic distribution usually drawn dirichlet distribution", "using dirichlet distributions makes easy specify sparse priors simplifies posterior inference dirichlet distributions conjugate multinomial distributions", "drawing topics dirichlet distributions suffice vocabulary includes multiple languages", "working english german chinese time dirichlet prior way fa vor distributions pgoodz pgutz latter property made lda popular information retrieval wei croft", "phaoz tend high time low time", "generally structure model must encourage topics consistent languages dirichlet distributions cannot encode correlations elements", "possible solution problem use multivariate normal distribution produce correlated multinomials blei lafferty place dirichlet distribution", "done successfully multilingual settings cohen smith", "models complicate inference conjugate", "instead appeal treebased extensions dirichlet distribution used induce correlation semantic ontologies boydgraber et al encode clustering constraints drzejewski et al", "key idea approach assume vocabularies languages organized according shared semantic structure represented tree", "con creteness section use wordnet miller representation multilingual semantic bridge since well known offers convenient intuitive terminology demonstrates full flexibility approach", "model describe generalizes treestructured representation multilingual knowledge discuss alternatives section", "wordnet organizes vocabulary rooted directed acyclic graph nodes called synsets short synonym sets synset child another synset satisfies hyponomy relationship child specific instantiation parent concept thus hyponomy often called isa relationship", "example dog canine animal living thing etc approximation unreasonable assume wordnets structure meaning language independent ie concept encoded synset realized using terms different languages share meaning", "practice organization used create many alignments international wordnets original english wordnet ordan wintner sagot fiser isahara et al", "using structure wordnet describe generative process produces distribution multilingual vocabulary encourages correlations words similar mean ings regardless language word", "synset create multilingual word distribution synset follows"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "D10-1034": {"introduction": ["draw transition probabilities dir", "semantic relation extraction aims detect classify semantic relationships pair named entities occurring natural language text", "many machine learning approaches proposed attack problem including supervised miller et al zelenko et al culotta soresen kambhatla zhao grishman zhou et al zhang et al zhou zhang zhou et al qian et al zhou et al semi supervised brin agichtein gravano zhang chen et al qian et al zhou et al unsupervised methods hasegawa et al zhang et al chen et al", "current work relation extraction mainly adopts supervised learning methods since achieve much better performance", "theynormally require large number manually la beled relation instances whose acquisition time consuming labor intensive", "contrast unsupervised methods need manually labeled instances", "nevertheless difficult assess performance due lack evaluation criteria", "something semi supervised learning received attention recently", "plenitude unlabeled natural language text hand semisupervised learning significantly reduce need labeled data limited sacrifice performance", "example abney proposes bootstrapping algorithm chooses unlabeled instances highest probability correctly labeled add turn labeled training data iteratively", "paper focuses bootstrappingbased semi supervised learning relation extraction", "since performance bootstrapping depends much quality quantity seed set researchers tend employ seeds possible eg instances save time labor quality seed set plays critical role bootstrapping", "furthermore imbalance different classes proceedings conference empirical methods natural language processing pages mit massachusetts usa october", "qc association computational linguistics inherent structural complexity instances severely weaken strength bootstrapping semisupervised learning well", "therefore critical bootstrapping procedure select appropriate seed set representative diverse", "current semi supervised relation extraction systems zhang chen et al use random seed sampling strategy fails fully exploit affinity nature training data derive seed set", "alternatively zhou et al", "bootstrap set weighted support vectors labeled unlabeled data using svm feed instances semisupervised relation extraction", "seed set sequentially generated ensure least instances relation class", "previous work qian et al attempts solve problem via simple stratified sampling strategy selecting seed set", "experimentation ace rdc corpus shows stratified sampling strategy achieves promising results semisupervised learning", "nevertheless success strategy relies assumption true distribution relation types already known impractical real nlp applications", "paper presents clusteringbased stratified seed sampling approach semisupervised relation extraction without assumption true distribution different relation types", "motivations behind approach unlabeled data partitioned number strata using clustering algorithm representative diverse seeds derived strata framework stratified sampling neyman oracle annotate", "particularly employ diversitymotivated intrastratum sampling scheme pick center additional instances seeds stratum", "experimental results show effectiveness clustering based stratified seed sampling semisupervised relation classification", "rest paper organized follows", "first overview related work given section", "section introduces stratifiedbootstrapping framework including intra stratum sampling scheme section describes various clustering algorithms", "experimental results ace rdc corpus reported section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01410298625953104, "p": 0.07142857142857142, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014154213133862024, "p": 0.06666666666666667, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014154213133862024, "p": 0.06666666666666667, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01420711093254263, "p": 0.0625, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014544681170572223, "p": 0.045454545454545456, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013783517841510365, "p": 0.16666666666666666, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.3333333285802469, "p": 0.42857142857142855, "r": 0.2727272727272727}, "rouge-l": {"f": 0.04143723032670566, "p": 0.42857142857142855, "r": 0.0410958904109589}}], "abstract": ["seed sampling critical semisupervised learning", "paper proposes clustering based stratified seed sampling approach semisupervised learning", "first various clustering algorithms explored partition unlabeled instances different strata stratum represented center", "diversitymotivated intrastratum sampling adopted choose center additional instances stratum form unlabeled seed set oracle annotate", "finally labeled seed set fed bootstrapping procedure initial labeled data", "systematically evaluate stratified bootstrapping approach semantic relation classification subtask ace rdc relation detection classification task", "particular compare various clustering algorithms stratified bootstrapping performance", "experimental results ace rdc corpus show clustering based stratified bootstrapping approach achieves best f1score sub task semantic relation classification approaching golden clustering"]}, "D10-1100": {"introduction": ["finally conclude work indicate future directions section", "paper introduces novel natural language processing nlp task social event extraction", "interested task contributes overall research goal extract social network written text", "extracted social network used various applications summarization questionanswering detection main characters story", "example manually extracted social network characters alice wonderland ran standard social network analysis algorithms network", "influential characters story correctly detected", "moreover characters occurring scene together given social roles positions", "social network extraction recently applied literary theory elson et al potential help organize novels becoming machine readable", "take social network network consisting individual human beings groups human beings connected virtue participating social events", "define social events events occur people least person aware event taking place", "example sentence john talks mary entities john mary aware talking event", "sentence john thinks mary great john aware mary event thinking event", "sentence rabbit ran alice evidence cognitive states rabbit alice rabbit could run alice without noticing", "text describe social network ways explicitly stating type relationship individuals eg husbandwife implicitly describing event creates perpetuates social relationship eg john talked mary", "call types events social events", "define types social events interaction parties aware social event eg conversation observation party aware interaction eg thinking proceedings conference empirical methods natural language processing pages mit massachusetts usa october", "qc association computational linguistics spying someone", "note notion cognitive state crucial definition", "paper first attempt detect classify social events present text", "task different related tasks notably automated content extraction ace relation event extraction tasks events different class events defined effect participants cognitive state linguistic realization different", "mentions entities1 engaged social event often quite distant sentence unlike ace relations relations local social event annotation events local", "fact average number words entities participating social event", "use tree kernel methods structures derived phrase structure trees dependency trees conjunction support vector machines svms solve tasks", "design structures type kernel take motivation system proposed nguyen et al", "state oftheart system relation extraction", "data skew ness turns big challenge task relation detection since many pairs entities without relation compared pairs entities relation", "paper discuss data sampling techniques deal skewness allow us gain f1 measure baseline system", "moreover introduce new sequence kernel outperforms previously proposed sequence kernels task social event detection plays role achieve best performing system task social event detection classification", "paper structured follows", "section compare work existing work notably ace extraction literature", "section present task detail explain annotated corpus", "show novel task different ace extraction tasks", "discuss kernel methods structures use introduce new structure section", "section present sampling methods used experiments", "section present exper entity mention reference entity text", "use entities people interchangeably since entities interested people groups people", "iments results social event detection social event classification tasks"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027761670421273366, "p": 0.2222222222222222, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013783517841510365, "p": 0.16666666666666666, "r": 0.0136986301369863}}], "abstract": ["paper introduce new task social event extraction text", "distinguish broad types social events depending whether parties aware social contact", "annotate part automatic content extraction ace data perform experiments using support vector machines kernel methods", "use combination structures derived phrase structure trees dependency trees", "characteristic events distinguishes ace events participating entities spread far parse trees", "use syntactic semantic insights devise new structure derived dependency trees show plays role achieving best performing system social event detection classification tasks", "use data sampling approaches solve problem data skewness", "sampling methods improve f1measure task relation detection absolute baseline"]}, "D11-1044": {"introduction": ["conclude section mention future direction research", "approaches currently dominate statistical machine translation mt research", "phrasebased models koehn et al excel capturing local reordering phenomena memorizing multiword translations", "models employ syntax syntax like representations chiang galley et al zollmann venugopal huang et al handle longdistance reordering better phrasebased systems auli et al often require constraints formalism rule extraction method order achieve computational tractability", "result certain instances syntactic divergence naturally handled phrasebased systems deneefe et al", "paper present new way combining advantages phrasebased syntaxbased mt propose model phrases organized tree structure inspired dependency syntax", "instead standard dependency trees words vertices trees phrases vertices", "describe simple heuristic extract phrase dependencies aligned parallel corpus parsed target side use compute targetside tree features", "define additional stringtotree features sourceside dependency parser available treetotree features capture properties phrase dependencies interact reordering", "leverage standard phrasebased features alongside novel features require formalism supports flexible feature combination efficient decoding", "quasisynchronous grammar qg provides backbone smith eisner describe coarsetofine approach decoding framework advancing substantially earlier qg machine translation systems gimpel smith", "decoder involves generating phrase lattice ueffing et al coarse pass using phrasebased model followed lattice dependency parsing phrase lattice", "approach allows us feasibly explore combined search space segmentations phrase alignments target phrase dependency trees", "experiments demonstrate average improvement bleu chineseenglish translation test sets improvement bleu urduenglish translation phrasebased baseline", "describe experiments replace supervised dependency parsers unsupervised parsers reporting promising results using supervised chinese parser stateoftheart unsupervised english parser provides best results giving averaged gain bleu baseline", "discuss model improves translation quality discuss future possibilities combining approaches proceedings conference empirical methods natural language processing pages edinburgh scotland uk july"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019545816641176612, "p": 0.05263157894736842, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02017650410095058, "p": 0.037037037037037035, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}], "abstract": ["present quasisynchronous dependency grammar smith eisner machine translation leaves tree phrases rather words previous work gimpel smith", "formulation allows us combine structural components phrasebased syntaxbased mt single model", "describe method extracting phrase dependencies parallel text using targetside dependency parser", "decoding describe coarsetofine approach based lattice dependency parsing phrase lattices", "demonstrate performance improvements chineseenglish urduenglish translation phrasebased baseline", "investigate use unsupervised dependency parsers reporting encouraging preliminary results"]}, "D11-1076": {"introduction": ["qc association computational linguistics chine translation using framework", "pattern based relation acquisition methods rely lexicosyntactic patterns hearst extracting relation instances", "templates natural language expressions causes signal instance semantic relation ie causality", "pattern based methods agichtein gravano pantel pennacchiotti 2006b pasca et al de saeger et al learn many work done authors national institute information communications technology", "patterns extract new instances word pairs corpus", "since extraction patterns learned using statistical methods require certain frequency observations pattern based methods fail capture relations complex expressions pattern connecting words rarely observed", "consider following sentence curing hypertension alleviates deterioration speed renal function thereby lowering risk causing intracranial bleeding humans infer sentence expresses causal relation underlined noun phrases", "actual pattern connecting ie curing alleviates deterioration speed renal function thereby lowering risk causing rarely observed even page web corpus", "sense term pattern implies recurring event expression contains pattern detecting causal relation hypertension intracranial bleeding", "mean long tail instances words cooccur infrequently sparse extraction contexts", "important application relation extraction mining web socalled unknown unknowns words rumsfeld things dont know dont know torisawa et al", "knowledge discovery applications like risk management innovation usefulness relation extraction lies ability find many unexpected remedies diseases causes social problems", "give example relation extrac proceedings conference empirical methods natural language processing pages edinburgh scotland uk july", "qc association computational linguistics tion system found blog post mentioning japanese automaker toyota hidden cause japans deflation", "several months later connection made article published authoritative economic magazine", "propose semisupervised relation extraction method rely direct pattern evidence connecting words sentence", "argue role binary patterns replaced combination types indirect evidence semantic class information target relation partial patterns fragments sub patterns binary patterns", "intuition sentence like example sentence contains word belonging class medical conditions another word class traumas matches partial pattern", "causing decent chance sentence expresses causal relation show using combination indirect evidence pick semantic relations roughly precision regardless complexity frequency expression words cooccur", "furthermore combining idea straightforward machine learning approach overall performance method par stateoftheart pattern based methods", "method manages extract large number instances sentences contain pattern learned pattern induction methods", "method twostage system", "figure presents overview", "stage apply state oftheart pattern based relation extractor web corpus obtain initial batch relation instances", "stage supervised classifier built various components obtained output stage", "given output stage access web corpus stage extractor completely selfsufficient whole method requires supervision handful seed patterns start first stage extractor", "whole procedure therefore minimally supervised", "semantic word classes partial patterns play crucial role throughout steps process", "evaluate method relation acquisition tasks causation prevention material relations using million japanese web page cor figure proposed method data flow", "pus shinzato et al show system successfully acquire relations frequent infrequent patterns", "system extracted causal relations precision prevention relations precision material relations precision", "extreme case acquired several thousand word pairs cooccurring patterns appear entire corpus", "call patterns single occurrence patterns", "word pairs cooccur patterns represent theoretical limiting case relations cannot acquired using existing pattern based methods"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024265011818283992, "p": 0.04, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024217344368538975, "p": 0.041666666666666664, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024154589372054745, "p": 0.043478260869565216, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14814814331961607, "p": 0.125, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04677025001981951, "p": 0.125, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper proposes semisupervised relation acquisition method rely extraction patterns eg causes causal relations instead learns combination indirect evidence target relation semantic word classes partial patterns", "method extract long tail instances semantic relations like causality rare complex expressions large japanese web corpus extreme cases patterns occur entire corpus", "patterns beyond reach current pattern based methods", "show method performs par stateoftheart pattern based methods maintains reasonable level accuracy even instances acquired infrequent patterns", "ability acquire long tail instances crucial risk management innovation exhaustive database highlevel semantic relations like causation vital importance"]}, "D11-1081": {"introduction": ["sense method seen complementary pattern based approaches merging methods output pattern based method beneficial", "discriminative model och ney easily incorporate nonindependent overlapping features dominating research field statistical machine translation smt last decade", "recent work shown smt benefits lot exploiting large amount features liang et al tillmann zhang watanabe et al blunsom et al chiang et al", "training large number features always restricted fairly small data sets", "systems limit number training examples others use short sentences maintain efficiency", "overfitting problem often comes training many features small data watanabe et al chiang et al", "obviously using much data alleviate problem", "furthermore large data enables us globally train millions sparse lexical features offer accurate clues smt", "despite advantages best knowledge previous discriminative training paradigms scale use large amount training data", "main obstacle comes complexity packed forests nbest lists generation requires search possible translations training example computationally prohibitive practice smt", "make normalization efficient contrastive estimation smith eisner poon et al introduce neighborhood unsupervised loglinear model presented positive results various tasks", "motivated work use translation forest section contains reference derivations potentially yield reference translation neighboring nonreference derivations fail produce reference translation1however complexity generating translation forest on6 still need bi parsing create reference derivations", "consequently propose method fast generate subset forest", "key idea section initialize reference derivation tree maximum score help word alignment traverse tree generate subset forest linear time", "besides efficiency improvement forest allows us train model without resort exactly reference derivations since derivation latent variable smt", "call reference derivation convenience", "proceedings conference empirical methods natural language processing pages edinburgh scotland uk july", "qc association computational linguistics hyper edge rule e1 r1 x1 bei x2 x1 x2 e2 r2 qiangshou bei x1 e3 r3 jingfang x1 x1 police e4 r4 jingfang x1 police x1 e5 r5 qiangshou gunman e6 r6 jibi shot dead figure translation forest running example throughout paper", "reference translation gunman killed police", "solid hyperedges denote reference derivation tree t1 exactly yields reference translation", "replacing e3 t1 e4 results competing nonreference derivation t2 fails swap order x34", "removing e1 e5 t1 adding e2 leads another reference derivation t3 generally done deleting node x01 ing constructing oracle reference liang et al watanabe et al chiang et al nontrivial smt needs determined experimentally", "given forests globally learn loglinear model using stochastic gradient descend section", "overall generation forests training algorithm scalable enabling us train millions features largescale data", "show effect framework globally quence scfg rules ri", "translation forest miet al li eisner compact repre sentation derivations given sentence scfg see figure", "tree forest corresponds derivation", "paper tree means derivation", "formally forest pair set nodes set hyperedge", "given source sentence node form denotes recognitiontrain millions word level context features moti vated word sense disambiguation chan et al together features used traditional smt system section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03016579555494728, "p": 0.041666666666666664, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02930876266467362, "p": 0.07142857142857142, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028685822514372142, "p": 0.03125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["although discriminative training guarantees improve statistical machine translation incorporating large amount overlapping features hard scale large data due decoding complexity", "propose new algorithm generate translation forest training data linear time help word alignment", "algorithm alleviates oracle selection problem ensuring forest always contains derivations exactly yield reference translation", "millions features trained 519k sentences second per sentence system achieves significant improvement bleu baseline system nist chineseenglish test sets"]}, "D11-1084": {"introduction": ["training 519k sentence pairs seconds per sentence achieve significantly improvement traditional pipeline bleu", "last decade tremendous work done improve quality statistical machine corresponding author", "translation smt systems", "still huge performance gap stateofthe art smt systems human translators", "bond suggested ways improve machine translation imitating best practices human translators nida parsing entire document translation first priority", "smt systems still treat parallel corpora list independent sentencepairs ignore documentlevel information", "documentlevel information used help documentlevel machine translation", "least topic document help choose specific translation candidates since taken context document words phrases even sentences rather ambiguous thus difficult understand", "another advantage documentlevel machine translation ability keeping consistent translation", "documentlevel translation drawn little attention smt research community", "reasons manifold", "first parallel corpora lack annotation document boundaries tam", "secondly although easy incorporate new feature classical loglinear model och difficult capture documentlevel information model via simple features", "thirdly reference translations test document written human translators tend flexible expressions order avoid producing monotonous texts", "makes evaluation documentlevel smt systems extremely difficult", "tiedemann showed repetition consistency important modeling natural language translation", "proposed employ cachebased language translation models phrasebased smt system domain proceedings conference empirical methods natural language processing pages edinburgh scotland uk july", "qc association computational linguistics adaptation", "especially cache translation model dynamically grows adding bilingual phrase pairs best translation hypotheses previous sentences", "problem dynamic cache initial sentences test document benefit dynamic cache", "another problem dynamic cache prone noise cause error propagation", "explains dynamic cache fails much improve performance", "paper proposes cachebased approach documentlevel smt using static cache dynamic cache", "approach applies phrasebased syntaxbased smt paper focuses phrasebased smt", "particular static cache employed store relevant bilingual phrase pairs extracted similar bilingual document pairs ie source documents similar test document target counterparts training parallel corpus dynamic cache employed store bilingual phrase pairs best translation hypotheses previous sentences test document", "way cachebased approach provide useful data beginning translation process via static cache", "translation process continues dynamic cache grows contributes translation subsequent sentences", "motivation employ similar bilingual document pairs training parallel corpus simple human translator often collects similar bilingual document pairs help translation", "translation pairs sentencesphraseswords similar bilingual document pairs makes translation much easier", "given test document approach imitates procedure first retrieving similar bilingual document pairs training parallel corpus often applied irbased adaptation smt systems zhao et al2004 hildebrand et al2005 lu et al2007 extracting bilingual phrase pairs similar bilingual document pairs store static cachehowever cachebased approach troduce many noisyunnecessary bilingual phrase pairs static dynamic caches", "order resolve problem paper employs topic model weaken noisyunnecessary bilingual phrase pairs recommending decoder choose likely phrase pairs according topic words extracted targetside text similar bilingual document pairs", "like human translator even big bilingual dictionary often confused meets source phrase corresponds several possible translations", "case topic words help reduce perplexity", "paper topic words stored topic cache", "sense similar effect employing adaptive language model advantage avoiding interpolation global language model specific domain language model", "rest paper organized follows", "section reviews related work", "section presents cachebased approach document level smt", "section presents experimental results", "session gives new insights cache based documentlevel translation"], "introduction_label": [{"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016005313597158736, "p": 0.09090909090909091, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016346153846228638, "p": 0.0625, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015945641929930903, "p": 0.1, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016005313597158736, "p": 0.09090909090909091, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.034946479066006605, "p": 0.05128205128205128, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.040816323048729994, "p": 0.02631578947368421, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01688383918612522, "p": 0.019230769230769232, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015682206492562606, "p": 0.25, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.26666666275555556, "p": 0.5, "r": 0.18181818181818182}, "rouge-l": {"f": 0.031364412985118134, "p": 0.5, "r": 0.03125}}], "abstract": ["statistical machine translation systems usually trained large amount bilingual sentence pairs translate sentence time ignoring documentlevel information", "paper propose cachebased approach documentlevel translation", "since caches mainly depend relevant data supervise subsequent decisions critical fill caches highlyrelevant data reasonable size", "paper present kinds caches store relevant documentlevel information dynamic cache stores bilingual phrase pairs best translation hypotheses previous sentences test document static cache stores relevant bilingual phrase pairs extracted similar bilingual document pairs ie source documents similar test document corresponding target documents training parallel corpus topic cache stores targetside topic words related test document sourceside", "particular new features designed explore various kinds documentlevel information kinds caches", "evaluation shows effectiveness cachebased approach documentlevel translation performance improvement blue score moses", "especially detailed analysis discussion presented give new insights documentlevel translation"]}, "D11-1095": {"introduction": ["finally conclude paper section", "variety verb classifications built support nlp tasks", "include syntactic semantic classifications well ones integrate aspects grishman et al miller baker et al palmer et al kipper hovy et al", "classifications integrate wide range linguistic properties particularly useful tasks suffering data sparseness", "classification taxonomy english verbs proposed levin based shared morphosyntactic semantic properties verbs", "levins taxonomy extended version verbnet kipper proved helpful various nlp application tasks including eg parsing word sense disambiguation semantic role labeling information extraction questionanswering machine translation swier stevenson dang shi mihalcea zapirain et al", "verbs change meaning haviour domains important able tune existing classifications well build novel ones costeffective manner required", "recent years variety approaches proposed automatic induction levin style classes corpus data could used purpose schulte im walde joanis et al sun et al li brew korhonen et al seaghdha copestake vlachos et al", "best approaches yielded promising results", "mostly focussed acquiring evaluating flat classifications", "levins classification flat taxonomic nature practical nlp purposes since applications differ terms granularity require classification", "paper experiment hierarchical levinstyle clustering", "adopt baseline method wellknown hierarchical method agglomerative clustering agg previously used acquire flat levinstyle classifications stevenson joanis well hierarchical verb classifications based levin fer rer schulte im walde", "method popular related task noun clus proceedings conference empirical methods natural language processing pages edinburgh scotland uk july", "qc association computational linguistics tering ushioda matsuo et al bassiou kotropoulos", "introduce new method called hierarchical graph factorization clustering hgfc yu et al", "graphbased probabilistic clustering algorithm clear advantages agg eg delays decision verbs cluster membership level full graph available minimising problem error propagation shown perform better several hierarchical clustering methods recent comparisons yu et al", "method applied identification social network communities lin et al used best knowledge nlp", "modify hgfc new tree extraction algorithm ensures consistent result propose novel extensions", "first method automatically determining tree structure ie number clusters produced level hierarchy", "avoids need predetermine number clusters manually", "second addition soft constraints guide clustering performance vlachos et al", "useful situations partial eg flat verb classification available goal extend", "adopting set lexical syntactic features performed well previous works compare performance methods test sets extracted levin verbnet", "evaluated flat clustering task hgfc outperforms agg performs similarly best flat clustering method reported test set sun korhonen", "evaluated hierarchical task hgfc performs considerably better agg levels gold standard classification", "constrained version hgfc performs best expected demonstrating usefulness soft constraints extending partial classifications", "qualitative analysis shows hgfc capable detecting novel information included gold standards"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407672862, "p": 0.03571428571428571, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.047042024860982905, "p": 0.11764705882352941, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["previous research verb clustering focussed acquiring flat classifications corpus data although many manually built classifications taxonomic nature", "natural language processing nlp applications benefit taxonomic classifications vary terms granularity require classification", "introduce new clustering method called hierarchical graph factorization clustering hgfc extend optimal task", "results show hgfc outperforms frequently used agglomerative clustering hierarchical test set extracted verbnet yields stateoftheart performance flat test set", "demonstrate method used acquire novel classifications well extend existing ones basis prior knowledge classification"]}, "D11-1119": {"introduction": ["unconstrained version used acquire novel classifications scratch constrained version used extend existing ones additional class members classes levels hierarchy", "function contextsensitive text correction identify wordchoice errors text bergsma et al", "viewed lexical disambiguation task lapata keller system selects predefined confusion word set affect effect complement compliment provides appropriate word choice given context", "typically determines word used correctly based lexical syntactic semantic information context word", "top performing models spelling correction bergsma et al based webscale ngram counts reflect syntax meaning", "even largescale ngram corpus data sparsity hurt performance ways", "work done first author intern educational testing service", "first ngram based methods require exact word order matches", "low frequency word context persons name little evidence ngram data support usage", "second target confusable word rare enough ngram support training data render confident decision", "data sparsity problem language modeling always sufficient capture meaning sentence correct usage word", "take sentence new york times nyt example fellows war dean capitals press corps david broder announced meet press complimenting president great sense authority command exhibited flight suit unfortunately neither phrase complementing president complimenting president exists webscale google ngram corpus brants franz", "ngram models decide solely based frequency bigrams compleimenting compleimenting common usages words", "real question whether likely compliment complement person president", "several clues could help us answer question", "dependency parser identify word president subject compliment complement case training data", "lexical cooccurrence edmonds semantic word relatedness measurements random indexing sahlgren could provide evidence compliment likely cooccur president complement", "fur proceedings conference empirical methods natural language processing pages edinburgh scotland uk july", "qc association computational linguistics thermore important clues quite distant target word eg outside 9word context window bergsma et al", "carlson used", "consider another sentence nyt corpus gm says addition onstar includes system automatically notifies onstar operator vehicle involved collision complements vues top fivestar safety rating driver front passenger front side impact crash tests dependency parser finds object complement rating outside 9word window", "propose enhancing stateoftheart webscale ngram models spelling correction syntactic structures distributional information", "work build baseline system combines ngram lexical features bergsma et al", "specifically paper makes following contributions", "show baseline system"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose novel way incorporating dependency parse word cooccurrence information stateoftheart webscale gram model spelling correction", "syntactic distributional information provides extra evidence addition provided webscale ngram corpus especially helps data sparsity problems", "experimental results show introducing syntactic features ngram based models significantly reduces errors current stateoftheart", "word cooccurrence information shows potential improves overall accuracy slightly"]}, "D11-1125": {"introduction": ["improved augmenting dependency parse features", "mert algorithm och currently popular way tune parameters statistical machine translation mt system", "mert wellunderstood easy implement runs quickly behave erratically scale beyond handful features", "lack scalability significant weakness inhibits systems using couple dozen features discriminate candidate translations stymies feature development innovation", "several researchers attempted address weakness", "recently watanabe et al", "chiang et al", "2008b developed tuning methods using mira algorithm crammer singer nucleus", "mira technique chiang et al shown perform well largescale tasks hundreds thousands features", "technique complex architecturally quite different mert", "tellingly entire proceedings acl hajic et al paper describing statistical mt system cited use mira tuning chiang used mert1 propose simpler approach tuning scales similarly highdimensional feature spaces", "cast tuning ranking problem chen et al explicit goal learn correctly rank candidate translations", "specifically follow pairwise approach ranking herbrich et al freund et al burges et al cao et al ranking problem reduced binary classification task deciding candidate translation pairs", "primary concern us ease adoption proposed technique", "adhere closely possible established mert architecture use freely available machine learning software", "end result technique scales performs well mirabased tuning implemented couple hours anyone existing mert implementation", "mindful many wouldbe enhancements remainder either specify tuning method", "though number used moses toolkit koehn et al uses mert tuning case set weights hand", "stateoftheart false positives show improvement narrowly defined setting limited data validate claims syntax phrasebased systems using multiple language pairs large data sets"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09302325200649016, "p": 0.0625, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0464332450285652, "p": 0.05128205128205128, "r": 0.043478260869565216}}], "abstract": ["offer simple effective scalable method statistical machine translation parameter tuning based pairwise approach ranking herbrich et al", "unlike popular mert algorithm och pairwise ranking optimization pro method limited handful parameters easily handle systems thousands features", "moreover unlike recent approaches built upon mira algorithm crammer singer watanabe et al chi ang et al 2008b pro easy implement", "uses offtheshelf linear binary classifier software built top existing mert framework matter hours", "establish pros scalability effectiveness comparing mert mira demonstrate parity phrasebased syntaxbased systems variety language pairs using large scale data scenarios"]}, "D12-1016": {"introduction": ["describe tuning abstract somewhat formal terms section describe mert algorithm context terms illustrate scalability issues via synthetic experiment section introduce pairwise ranking optimization method section present numerous largescale mt experiments validate claims section discuss related work section conclude section", "discourse coherence important aspect natural language generation nlg applications", "number theories investigated coherence inducing factors", "prominent example centering theory grosz et al models local coherence relating choice referring expressions importance entity certain stage discourse", "datadriven model based theory entitybased approach barzilay lap ata models coherence phenomena observing sentencetosentence transitions entity occurrences", "barzilay lapata show approach discriminate coherent noncoherent set ordered sentences", "model able generate alternative entity realizations", "furthermore entitybased approach investigates realization patterns individual entities discourse terms core grammatical functions", "investigate interplay entity transitions realization patterns full fledged semantic structures", "interplay important factor semanticsbased generative model discourse coherence", "main hypothesis work automatically learn contextspecific realization patterns predicate argument structures pas semantically parsed corpus comparable text pairs", "assumption builds success previous research comparable parallel texts exploited range related learning tasks eg unsupervised discourse segmentation barzilay lee bootstrapping semantic analyzers titov kozhevnikov", "purposes interested finding corresponding pas comparable texts known talk events hence involve set underlying event participants", "aligning predicates texts investigate factors determine discourse coherence realization patterns involved arguments", "include specific forms argument realization pronoun specific type referential expression studied prior work nlg belz et al inter alia", "specific setup examine allows us investi proceedings joint conference empirical methods natural language processing computational natural language learning pages jeju island korea july", "qc association computational linguistics gate factors govern nonrealization argument position special form coherence inducing element discourse", "example extracted corpus aligned textsillustrates point texts report event locating victims avalanche", "1a explicitly talks location event role remains implicit second sentence 1b given recovered preceding sentence", "fact realization argument role would impede fluency discourse overly repetitive", "", "official said bodiesarg1 recovered avalanchesarg2 occurred late friday central asian country near afghan border kilometers miles southeast capital dushanbe", "victims trapped avalanche village khichikh", "none victims bodiesarg1 found argmloc phenomenon clearly relates problem discourselinking implicit roles challenging task discourse processing1 work consider problem contentbased generation perspective concentrating discourse factors allow omission role", "thus aim identify comparable predications aligned texts study discourse coherence factors determine realization patterns arguments respective discourses", "achieved considering full set arguments recovered aligned predications", "paper focuses first tasks henceforth called predicate alignment2 line datadriven approaches nlp automatically align predicates suitable corpus paired texts", "induced alignments serve identify events described comparable texts ii provide information underlying argument structures realized context establish coherent discourse"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01944752536201737, "p": 0.05555555555555555, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019545816641176612, "p": 0.05263157894736842, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019246675387691467, "p": 0.0625, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020225458909306583, "p": 0.03571428571428571, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["generating coherent discourse important aspect natural language generation", "aim learn factors constitute coherent discourse data focus realize predicateargument structures model exceeds sentence level", "present important subtask overall goal align predicates comparable texts admitting partial argument structure correspondence", "contribution work twofold first construct large corpus resource comparable texts including evaluation set manual predicate alignments", "secondly present novel approach aligning predicates comparable texts using graphbased clustering mincuts", "method significantly outperforms alignment techniques applied novel alignment task margin least percentage points f1 score"]}, "D12-1074": {"introduction": ["investigate graphbased clustering method induc see recent semeval task linking events participants discourse ruppenhofer et al", "many nlp tasks inherently tied syntax stateoftheart solutions tasks often rely syntactic annotations either source useful features zhang et al path features relation extraction scaffolding upon narrow specialized classification occur often done semantic role labeling", "decou pling end task intermediate representation sometimes known twostage approach chang et al comes several drawbacks", "notably decomposition prohibits learning method utilizing labels end task predicting intermediate representation structure must correlation end task provide benefit", "relying intermediate representations specifically syntactic nature introduces unique set problems", "large amounts syntactically annotated data difficult obtain costly produce often tied particular domain vary greatly desired end task", "additionally current systems often utilize small amount annotation particular task", "instance performing named entity recognition ner jointly constituent parsing shown improve performance tasks aspect syntax leveraged ner component location noun phrases finkel manning", "instead discovering latent representation jointly end task address concerns alleviating need syntactic annotations simultaneously attempting learn latent syntax relevant particular domain structure end task", "phrase joint model factor graph marginalize hidden structure intermediate representation training test time optimize performance end task", "inference done via loopy belief propagation making framework trivially extensible graph structures", "computation latent syntactic rep proceedings joint conference empirical methods natural language processing computational natural language learning pages jeju island korea july", "qc association computational linguistics resentations made tractable use special combinatorial factors implement unlabeled variants common dynamicprogramming parsing algorithms constraining hidden representation realize valid dependency graphs constituency trees", "apply strategy common nlp tasks coupling model end task prediction latent general syntactic representations via specialized logical factors learn associations latent observed structure", "comparisons identical models observe gold syntactic annotations derived offtheshelf parsers provided corpora find hidden marginalization method comparable tasks almost every language tested sometimes significantly outperforming models observe true syntax", "following sections serves preliminary introducing inventory factors variables constructing factor graph representations syntacticallycoupled nlp tasks", "section explores benefits method relation extraction compare use dependency constituency structure latent representations"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["many nlp tasks make predictions inherently coupled syntactic relations many languages resources required provide syntactic annotations unavailable", "others unclear exactly much syntactic annotations effectively leveraged current models structures syntactic trees relevant current task", "propose novel method avoids need syntactically annotated data predicting related nlp task", "method couples latent syntactic representations constrained form valid dependency graphs constituency parses prediction task via specialized factors markov random field", "training test time marginalize hidden structure learning optimal latent representations problem", "results show approach provides significant gains syntactically uninformed baseline outperforming models observe syntax english relation extraction task performing comparably semantic role labeling"]}, "D13-1060": {"introduction": ["turn established semantic role label ing srl task evaluate wide range languages", "multiword expression mwe noncompositional compound sequence words whose meaning cannot composed directly meanings constituent words", "idiosyncratic phrases prevalent lexicon language jackendoff estimates number order magnitude single words sag et al", "suggest much common though quantifying challenging church", "task identifying mwes relevant lexical semantics applications machine translation koehn et al ren et al pal et al information retrieval xu et al acosta et al syntactic parsing sag et al", "awareness mwes empirically proven useful number domains finlayson kulkarni example use mwes attain significant perfor focus particular subset mwes english phrasal verbs", "phrasal verb consists head verb followed particles meaning phrase cannot determined combining simplex meanings constituent words baldwin villavicencio dixon bannard et al examples phrasal verbs include count rely look tend take remove meanings involve counting looking taking", "contrast verbs followed particles phrasal verbs meaning compositional walk towards sit behind paint", "identify phrasal verbs using frequency statistics calculated parallel corpora consisting bilingual pairs documents translation document english", "leverage observation verb translate atypical way occurring head phrasal verb", "example word look context look tend translate differently look translates generally", "order characterize difference calculate frequency distribution translations look compare distribution translations look followed word", "expect idiomatic phrasal verbs tend unexpected translation head verbs measured kullbackleibler divergence distributions", "polyglot ranking approach motivated hypothesis using many parallel corpora different languages help determine degree semantic idiomaticity phrase", "order com mance improvement word sense disambiguation venkatapathy joshi use features associated mwes improve word alignment", "research conducted internship google", "nomenclature varies term verbparticle construction", "used denote call phrasal verbs term phrasal verb sometimes used denote broader class constructions", "bine evidence multiple languages develop novel boosting algorithm tailored task ranking multiword expressions degree id iomaticity", "train evaluate disjoint subsets phrasal verbs english wiktionary2", "1experiments set phrasal verbs identified au tomatically method achieves heldout recall nears performance phrasal verbs wordnet humancurated set"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["address problem identifying multiword expressions language focusing english phrasal verbs", "polyglot ranking approach integrates frequency statistics translated corpora different languages", "experimental evaluation demonstrates combining statistical evidence many parallel corpora using novel rankingoriented boosting algorithm produces comprehensive set english phrasal verbs achieving performance comparable humancurated set"]}, "D13-1141": {"introduction": ["approach strongly outperforms monolingual system continues improve incrementally adding translation statistics different languages", "difficult recognize quantify semantic similarities languages", "fren phrasepair un cas de force majeure case absolute necessity zhen phrase pair persist stubborn manner similar semantics", "co occurrences exact word combinations rare training parallel text difficult classical statistical mt methods identify similarity produce reasonable translation given source phrase", "introduce unsupervised neural model learn bilingual semantic embedding words languages", "extension monolingual counterpart turian et al huang et al bengio et al bilingual embeddings capture semantic information monolingual words semantic relationships different languages", "prop erty allows define semantic similarity metrics phrasepairs making perfect features machine translation", "learn bilingual embeddings use new objective function embodies monolingual semantics bilingual translation equivalence", "latter utilizes word alignments natural subtask machine translation pipeline", "large scale curriculum training bengio et al obtain bilingual distributed representations lie feature space", "embeddings direct translations overlap semantic relationships bilingual embeddings improved unsupervised learning large unlabeled corpus", "consequently produce research community first set mandarin chinese word embed dings words trained chinese gigaword corpus", "evaluate embedding chinese word semantic similarity semeval jin wu", "embeddings significantly outperform prior work pruned tfidf baselines", "addition learned embeddings give rise f1 improvement named entity recognition ontonotes dataset hovy et al neural network model", "apply bilingual embeddings endto end phrasebased mt system computing semantic similarities phrase pairs", "nist08 chineseenglish translation task obtain improvement bleu competitive baseline bleu bleu stanford phrasal mt system", "proceedings conference empirical methods natural language processing pages seattle washington usa october"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["introduce bilingual word embeddings semantic embeddings associated languages context neural language models", "propose method learn bilingual embeddings large unlabeled corpus utilizing mt word alignments constrain translational equivalence", "new em beddings significantly outperform baselines word semantic similarity", "single semantic similarity feature induced bilingual em beddings adds near half bleu point results nist08 chineseenglish machine translation task"]}, "D13-1161": {"introduction": ["qc association computational linguistics", "semantic parsers map sentences formal representations underlying meaning", "recently algorithms developed learn parsers many applications including question answering qa kwiatkowski et al liang et al relation extraction krishnamurthy mitchell robot control matuszek et al krishnamurthy kollar interpreting instruc tions chen mooney artzi zettlemoyer generating programs kushman barzilay", "case parser uses predefined set logical constants ontology construct meaning representations", "practice choice ontology significantly impacts learning", "example consider following questions candidate meaning representations mr q1 population seattle", "q2 many people live seattle", "mr1 xpopulationseattle mr2 countxpersonx livex seattle semantic parser might aim construct mr1 q1 mr2 q2 pairings align constants count person etc directly phrases many people etc", "unfortunately ontologies sufficient coverage support meaning representations example many qa databases would include population relation required mr1", "existing approaches would given deficiency simply aim produce mr1 q2 thereby introducing significant lexical ambiguity complicates learning", "ontological mismatches become increasingly common domain language complexity increases", "paper introduce semantic parsing approach supports scalable opendomain ontological reasoning", "parser first constructs linguistically motivated domainindependent meaning representation", "example possibly producing mr1 q1 mr2 q2", "uses learned ontology matching model transform represen proceedings conference empirical methods natural language processing pages seattle washington usa october", "qc association computational linguistics many people visit public library new york annually l0 xeqx countypeopley evisity zpublicz libraryz new york annuallye xlibrarypublic library systemannual visitsx new york public library works mozart dedicate joseph haydn l0 xworksx ededicatemozart tohaydn xdedicated workx ededicated bymozart dedicationx dedicated tohaydn string quartet haydn quartets string quartet string quartet string quartet figure examples sentences domainindependent underspecified logical forms l0 fully specified logical forms answers drawn freebase domain", "tation target domain", "example producing either mr1 mr2 another appropriate option depending qa database schema", "stage approach enables parsing without domaindependent lexicon pairs words logical constants", "instead word meaning filled onthefly ontology matching enabling parser infer meaning previously unseen words easily transfer domains", "figure shows desired outputs example freebase sentences", "first parsing stage uses probabilistic combinatory categorial grammar ccg steedman clark curran map sentences new underspecified logicalform meaning representations containing generic logical constants tied specific ontology", "approach enables us share grammar structure domains instead repeatedly relearning different grammars target ontology", "ontologymatching step considers large number typeequivalent domainspecific meanings", "enables us incorporate number cues including target ontology structure lexical similarity names domainindependent dependent constants construct final logical forms", "learning estimate linear model derivations include ccg parsing decisions choices ontology matching", "following number recent approaches clarke et al liang et al treat intermediate decisions latent learn data containing easily gathered question answer pairs", "approach aligns naturally twostage parsing setup final logical expression directly used provide answers", "report performance benchmark datasets geoquery zelle mooney freebase qa fq cai yates 2013a", "geo query includes geography database small ontology questions relatively complex compositional structure", "fq includes questions freebase large communityauthored database spans many subdomains"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}], "abstract": ["consider challenge learning semantic parsers scale large opendomain problems question answering freebase", "settings sentences cover wide variety topics include many phrases whose meaning difficult represent fixed target ontology", "example even simple phrases daughter number people living cannot directly represented freebase whose ontology instead encodes facts gender parenthood population", "paper introduce new semantic parsing approach learns resolve ontological mismatches", "parser learned questionanswer pairs uses probabilistic ccg build linguistically motivated logical form meaning representations includes ontology matching model adapts output logical forms target ontology", "experiments demonstrate stateoftheart performance benchmark semantic parsing datasets including point accuracy improvement recent freebase qa corpus"]}, "E06-1030": {"introduction": ["experiments demonstrate stateoftheart performance cases including point improvement recall fq test", "traditional written corpora linguistics research created primarily printed text newspaper articles books", "growth world wide web information resource increasingly used training data natural language processing nlp tasks", "many advantages creating corpus web data rather printed text", "web data already electronic form therefore readable computers whereas printed data available electronically", "vast amount text available web major advantage keller lapata estimating billion words indexed google", "performance nlp systems tends improve increasing amount training data", "banko brill showed context sensitive spelling correction increasing training data size increases accuracy billion words experiments", "date nlp tasks utilised web data accessed search engines using hit counts examining limited number results pages", "tasks reduced determining ngram probabilities estimated hit counts search engine queries", "method gathers information hit counts require computationally expensive downloading actual text analysis", "unfortunately search engines designed nlp research reported hit counts subject uncontrolled variations approximations nakov hearst", "volk proposed linguistic search engine extract word relationships accurately", "created billion word topicdiverse web corpus spidering websites set seed urls", "seed set selected open directory ensure diverse range topics included corpus", "process text cleaning transforms html text form useable nlp systems tokenised words sentence per line", "text filtering removes unwanted text corpus nonenglish sentences lines text grammatical sentences", "compare vocabulary web corpus newswire", "web corpus evaluated nlp tasks", "contextsensitive spelling correction disambiguation problem correction word aconfusion set eg theyre needs se lected given context", "thesaurus extraction similarity task synonyms target word extracted corpus unlabelled text", "evaluation demonstrates web text used tasks search engine hit counts newspaper text"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["web text successfully used training data many nlp applications", "previous work accesses web text search engine hit counts created web corpus downloading web pages create topicdiverse collection billion words english", "show contextsensitive spelling correction web corpus results better using search engine", "thesaurus extraction achieved similar overall results corpus newspaper text", "many words available web better results obtained collecting much larger web corpora"]}, "E06-2012": {"introduction": ["much larger quantity freely available web text exploit", "goal support discovery complex events text", "complex events mean events might structured multiple occurrences events might occur span time", "financial analysis domain concerns us example mean problem understanding corporate acquisition practices", "gauge companys modus operandi acquiring companies isnt enough know acquisition occurred important understand degree debtleveraged whether performed reciprocal stock exchanges", "words complex events often composed multiple facets beyond basic event", "concerns therefore enable end users access complex events combination possible facets", "another key characteristic rich domains like financial analysis facts events subject interpretation context", "financial analyst makes difference whether multimilliondollar loss occurs context recurring operations potentially chronic problem context onetime event merger layoff", "second concern thus enable end users interpret facts events automated context assessment", "route taken towards end model domain corporate finance interactive suite language processing tools", "maytag prototype makes following novel contribution", "rather trying model complex events monolithically provide range multipurpose information extraction text classification methods allow end user combine interactively"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present novel application nlp text mining analysis financial documents", "particular describe implemented prototype maytag combines information extraction subject classification tools interactive exploratory framework", "present experimental results performance tailored financial domain forwardlooking extensions approach enables users specify classifications fly"]}, "E09-1045": {"introduction": ["think boolean queries query terms keywords extracted facts events entities contextual text classifications", "word sense disambiguation wsd intermediate natural language processing nlp task consists assigning correct semantic interpretation ambiguous words context", "successful approaches last years supervised learning examples statistical machine learning classification models induced semantically annotated corpora marquez et al", "generally supervised systems obtained better results unsupervised ones shown experimental work international evaluation exercises paper supported european union projects qallme fp6 ist033860 kyoto fp7 ict211423 spanish government project textmess tin200615265c0601 know tin200615049c0301 senseval1", "annotated corpora usually manually tagged lexicographers word senses taken particular lexical semantic resource commonly wordnet2 wn fell baum", "wn widely criticized sense repository often provides finegrained sense distinctions higher level applications like machine translation question answering", "fact wsd level granularity resisted attempts inferring robust broad coverage models", "seems many wordsense distinctions subtle captured automatic systems current small volumes wordsense annotated examples", "possibly building classbased classifiers would allow avoid data sparseness problem wordbased approach", "recently using wn sense repository organizers english allwords task senseval3 reported interannotation agreement snyder palmer", "interestingly result difficult outperform stateoftheart sensebased wsd systems", "thus research focused deriving different wordsense groupings overcome finegrained distinctions wn hearst schu tze peters et al mihalcea moldovan agirre lopezdelacalle navigli snow et al", "provide methods grouping senses word thus producing coarser word sense groupings better disambiguation", "wikipedia3 recently used overcome problems automatic learning methods excessively finegrained definition meanings lack annotated data strong domain dependence existing annotated corpora", "way wikipedia provides new large source annotated data constantly expanded mihalcea", "httpwwwsensevalorg httpwordnetprincetonedu httpwwwwikipediaorg proceedings 12th conference european chapter acl pages athens greece march april", "qc association computational linguistics contrast research focused using predefined sets sensegroupings learning classbased classifiers wsd segond et al ciaramita johnson villarejo et al curran ciaramita altun", "grouping senses different words explicit comprehensive semantic class", "later approaches used original lexicographical files wn recently called supersenses coarsegrained sense distinctions", "much attention paid learning classbased classifiers available sensegroupings wordnet domains magnini cavaglia sumo labels niles pease eurowordnet base concepts vossen et al top concept ontology labels alvez et al basic level concepts izquierdo et al", "obviously resources relate senses level abstraction using different semantic criteria properties could interest wsd", "possibly combination could improve overall results since offer different semantic perspectives data", "furthermore knowledge date comparative evaluation performed senseval data exploring different levels abstraction", "fact villarejo et al studied performance classbased wsd comparing supersenses sumo 10fold crossvalidation semcor provide results senseval2 senseval3", "paper empirically explores supervised wsd task performance different levels abstraction provided wordnet domains magnini cavaglia sumo labels niles pease basic level concepts izquierdo et al", "refer approach classbased wsd since classifiers created class level instead sense level", "classbased wsd clusters senses different words explicit comprehensive grouping", "cases belonging semantic class grouped train classifier", "example coarser word grouping obtained snow et al remaining sense church", "using set base level concepts izquierdo et al senses church still represented faithn3 buildingn1 religious ceremonyn1", "contribution work threefold", "empirically demonstrate basic level concepts group senses adequate level abstraction order perform supervised class based wsd semantic classes successfully used semantic features boost performance classifiers classbased approach wsd reduces dramatically required amount training examples obtain competitive classifiers", "introduction section presents sensegroupings used study", "section approach followed build classbased system explained", "experiments results shown section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04545454170454576, "p": 0.030303030303030304, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017394600796492802, "p": 0.030303030303030304, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016273788316535746, "p": 0.06666666666666667, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017059917060020587, "p": 0.038461538461538464, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016203074507023917, "p": 0.07142857142857142, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016346153846228638, "p": 0.0625, "r": 0.015625}}, {"rouge-1": {"f": 0.14285713948979603, "p": 0.3333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015657719579972886, "p": 0.3333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015682206492562606, "p": 0.25, "r": 0.015625}}], "abstract": ["empirically demonstrated last senseval exercises assigning appropriate meaning words context resisted attempts successfully addressed", "possible reason could use inappropriate set meanings", "fact wordnet used defacto standard repository meanings", "knowledge meanings represented wordnet used wsd finegrained sense level coarsegrained class level", "suspect selecting appropriate level abstraction could levels", "use simple method deriving small set appropriate meanings using basic structural properties wordnet", "empirically demonstrate automatically derived set meanings groups senses adequate level abstraction order perform classbased word sense disambiguation allowing accuracy figures"]}, "E09-1072": {"introduction": ["finally conclusions drawn section", "property animacy influences linguistic phenomena range different languages case marking aissen argument realization bresnan et al de swart et al shown constitute important factor production comprehension syntactic structure branigan et al weckerly kutas computational linguistic work animacy shown provide important information anaphora resolution orasan evans argument disambiguation dellorletta et al syntactic parsing general vrelid nivre 2007the dimension animacy roughly distin guishes entities alive entities distinctions parts research reported paper supported deutsche forschungsgemeinschaft dfg son derforschungsbereich project d4", "relevant animacy dimension often viewed continuum ranging humans inanimate objects", "following silverstein several animacy hierarchies proposed typological studies focusing linguistic category animacy ie distinctions relevant linguistic phenomena", "example animacy hierarchy taken aissen provided human animate inanimate clearly nonhuman animates like animals less animate humans biological sense humans animals show differing linguistic behaviourempirical studies animacy require human notation efforts particular welldefined annotation task", "annotation studies animacy differ distinctly treatment macy type tokenlevel phenomenon well terms granularity categories", "use annotated data computational resource furthermore poses requirements annotation necessarily agree theoretical considerations", "methods induction animacy information use practical applications require resolution issues level representation well granularitythis article addresses issues em pirical experimental evaluation", "present indepth study manually annotated data set indicates animacy treated lexical semantic property type level", "evaluate proposal supervised machine learning animacy information focus indepth error analysis resulting classifier addressing issues granularity animacy dimension", "finally automatically proceedings 12th conference european chapter acl pages athens greece march april", "qc association computational linguistics notated data set employed order train syntactic parser investigate effect imacy information contrast automatically acquired features gold standard ones", "rest article structured follows", "section briefly discuss annotation schemes animacy annotation strategies categories proposed", "go describe annotation binary distinction human reference found swedish dependency treebank section perform evaluation consistency human annotation terms linguistic level", "section present experiments lexical acquisition animacy based morphosyntactic features extracted considerably larger corpus", "section presents experiments acquired animacy information applied datadriven dependency parsing swedish"], "introduction_label": [{"rouge-1": {"f": 0.027027024495982706, "p": 0.015873015873015872, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013573820868448482, "p": 0.012987012987012988, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058002707487074444, "p": 0.07142857142857142, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04975186104159399, "p": 0.047619047619047616, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058524734982232, "p": 0.07692307692307693, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04677809313506047, "p": 0.043478260869565216, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058852258852275095, "p": 0.09090909090909091, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05765839099182598, "p": 0.125, "r": 0.05263157894736842}}], "abstract": ["article presents empirical evaluations aspects annotation linguistic property animacy swedish ranging manual human annotation automatic classification finally external evaluation task syntactic parsing", "show treatment animacy lexical semantic property noun types enables generalization distributional properties nouns proves beneficial automatic classification furthermore gives significant improvements terms parsing accuracy swedish compared stateofthe art baseline parser gold standard macy information"]}, "E12-1020": {"introduction": ["finally section concludes article provides suggestions future research", "relation extraction aims detecting categorizing semantic relations pairs entities text", "important nlp task many practical applications answering factoid questions building knowledge bases improving web search", "supervised methods relation extraction studied extensively since rich annotated linguistic resources eg automatic content extraction1 ace training corpus released", "give summary related methods section", "methods rely accurate complete annotation", "obtain high quality annotation common wisdom let httpwwwitlnistgoviadmigtestsace annotators independently annotate corpus asking senior annotator adjudicate disagreements", "annotation procedure roughly requires passes3 corpus", "therefore expensive", "ace annotation relations conducted way", "paper analyzed snapshot ace training data found annotator missed significant fraction relation mentions annotated spurious ones", "found possible separate missing examples vast majority truenegative unlabeled examples contrast relation mentions adjudicated incorrect contain useful expressions learning relation extractor", "based observation propose algorithm purifies negative examples applies transductive inference utilize missing examples training process singlepass annotation", "results show extractor trained singlepass annotation proposed algorithm performance close extractor trained 3pass annotation", "show proposed algorithm trained singlepass annotation complete set documents higher performance extractor trained 3pass annotation documents corpus although effort singlepass annotation entire set costs less half passes documents"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["wellstudied supervised relation extraction algorithms require training data accurate good coverage", "obtain gold standard common practice independent double annotation followed adjudication", "takes significantly human effort annotation done single annotator", "detailed analysis snapshot ace annotation files understand differences singlepass annotation expensive nearly threepass process propose algorithm learns much cheaper singlepass annotation achieves performance par extractor trained multipass annotated data", "furthermore show given amount human labor better way relation annotation annotate highcost quality assurance annotate"]}, "E99-1007": {"introduction": ["perspective learning highperformance relation extractor suggests better way relation annotation annotate highcost quality assurance annotate", "recent years witnessed shift grammar development methodology crafting large grammars annotation corpora", "correspond ingly change developing rulebased parsers developing statistical meth ods inducing grammatical knowledge notated corpus data", "shift mostly oc curred building widecoverage grammars timeconsuming error prone difficult", "said crafting rich lexical rep resentations central component lin guistic knowledge research automatic lex ical acquisition sought address dorr jones dorr others", "attempts learn fine grained lexical classifications statisti cal analysis distributional data analogously induction syntactic knowledge though see eg brent klavans chodorow resnik", "paper propose approach automatic classification verbs lexical semantic classes1 express issues raised ap proach follows", "", "linguistic distinctions lexical"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03889570552123663, "p": 0.045454545454545456, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03889570552123663, "p": 0.045454545454545456, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["apply machine learning techniques classify automatically set verbs lexical semantic classes based distributional approximations diathe ses extracted large anno tated corpus", "distributions gram matical features sufficient reduce error rate chance", "con clude corpus data usable repos itory verb class information corpusdriven extraction grammatical features promising methodology automatic lexical acquisition"]}, "E99-1031": {"introduction": ["classes expect find corpus", "building natural language understand ing systems choosing best technique anaphora resolution challenging task", "system builder must decide whether adopt existing technique design new approach", "huge variety techniques described literature many achieving high suc cess rates evaluation texts cf", "hobbs strube mitkov", "technique makes different assumptions data available reference resolution ex ample assume perfect parses others sume postagged input assume se mantic information available etc chances high published technique ex actly match data available particular sys tems reference resolution component authors thank james allen help project well anonymous reviewers helpful comments paper", "material based work supported usafrome labs contract f3060295l0025 onr grant n0001495l1088 columbia univ grant opgi307", "apparent method work best", "choosing technique especially problematic designers dialogue systems trying pre dict anaphora resolution techniques devel oped written monologue perform adapted spoken dialogue", "ideal world system designer would implement com pare many techniques input data available system", "good software engineer would ensure pronoun resolution code implements ported future ap plications different language domains without modification", "architecture described paper designed provide functionality", "anaphora resolution code developed architecture encapsulated ensure portabil ity parsers language genres domains", "using architectural guidelines testbed system comparing pronoun resolution tech niques developed university rochester", "testbed provides highly config urable environment uses pronoun resolution code regardless parser frontend language type analysis"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.03773584576717722, "p": 0.023809523809523808, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02234743317481458, "p": 0.02, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03983890709524641, "p": 0.06666666666666667, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0363372093023574, "p": 0.25, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03946333181507231, "p": 0.07692307692307693, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes architecture performing anaphora resolution flexible way", "systems con form guidelines well encapsulated portable used compare anaphora resolu tion techniques new language un derstanding applications", "im plementation architecture pronoun resolution testing platform demonstrates flexibility ap proach"]}, "H01-1052": {"introduction": ["used inter alia compare anaphora resolution tech niques given application compare new techniques published baselines compare particular techniques performance lan guage types", "significant amount work empirical natural language processing involves developing refining machine learning techniques automatically extract linguistic knowledge online text corpora", "number learning variants various problems increasing size training sets learning algorithms use remained essentially unchanged", "instance muchstudied problems part speech tagging base noun phrase labeling parsing penn treebank first released remains de facto training corpus", "average training corpus size reported papers published aclsponsored workshop large corpora essentially unchanged proceedings proceedings", "amount available online text growing amazing rate last years estimations currently billion readily accessible words web size training corpora used field remained static", "confusable word set disambiguation problem choosing correct use word given set words commonly confused eg youre prototypical problem nlp", "level task identical many natural language problems including word sense disambiguation determining lexical features pronoun case determiner number machine translation part speech tagging named entity labeling spelling correction formulations skeletal parsing", "problems involve disambiguating relatively small set tokens based upon string context", "disambiguation problems lexical confusables possess fortunate property supervised training data free since differences members confusion set surfaceapparent set wellwritten text", "date papers published topic confusion set disambiguation used training sets supervised learning less million words", "true disambiguationinstringcontext problems", "paper explore happens significantly larger training corpora used"], "introduction_label": [{"rouge-1": {"f": 0.1290322534859523, "p": 0.1, "r": 0.18181818181818182}, "rouge-l": {"f": 0.060320870202765726, "p": 0.1, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper discuss experiments applying machine learning techniques task confusion set disambiguation using orders magnitude training data previously used disambiguationinstringcontext problem", "attempt determine current learning methods cease benefit additional training data analyze residual errors made learners issues sparse data significantly mitigated", "finally context results discuss possible directions empirical natural language research community", "keywords learning curves data scaling large corpora natural language disambiguation"]}, "H05-1001": {"introduction": ["results suggest make sense field concentrate considerably effort enlarging training corpora addressing scalability issues rather continuing explore different learning methods applied relatively small extant training corpora", "many approaches summarization broadly characterized ba attempt identify main topics generally extract document important information terms hovy lin", "approaches divided broadly lex kennedy azzam et al bergler et al stuckardt identify terms running coreference anaphoric resolver text1 aware attempt use lexical anaphoric information identify main terms", "addition knowledge authors convincingly demonstrated feeding anaphoric information summarizer significantly improves performance summarizer using standard evaluation procedure reference corpus baseline widely accepted evaluation measures", "paper compare sentence extraction based summarizers", "use latent semantic analysis landauer identify main terms text summarization first system steinberger jezek discussed section uses lexical information identify main topics whereas second system exploits lexical anaphoric information", "second system uses existing anaphora resolution system resolve anaphoric expressions ta poesio kabadjov crucially different ways using information summarization tested", "section", "sum marizers tested corpus orasan et al discussed section sig ical approaches would include abased approaches coreferencebased approaches lexical approaches termbased sum marization use lexical relations identify central terms barzilay elhadad gong liu coreference anaphora based approaches baldwin morton boguraev terms anaphora resolution coreference resolu", "tion variously defined stuckardt latter term generally used refer coreference task defined ac use term anaphora resolution refer task identifying successive mentions discourse entity realized via type noun phrase proper noun definite description pronoun whether discourse entities refer objects world", "proceedings human language technology conference conference empirical methods natural language processing hltemnlp pages vancouver october"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12121211676767694, "p": 0.09090909090909091, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0906058115353764, "p": 0.08333333333333333, "r": 0.10526315789473684}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05121475200160483, "p": 0.05, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose approach summarization exploiting lexical information output automatic anaphoric resolver using singular value decomposition identify main terms", "demonstrate adding anaphoric information results significant performance improvements previously developed system lexical terms used input show anaphoric information used crucial whereas using information add new terms result improved performance simple substitution makes performance worse"]}, "H05-1083": {"introduction": ["qc association computational linguistics nificant improvements observed baseline system previous abased summarizer", "coreference resolution system aims group together mentions referring entity mention instance reference object collection mentions referring object document form entity", "following example john believes best student mentions underlined", "mentions john best student type name pronoun nominal respectively", "form entity since refer person", "syntactic information plays important role coreference resolution", "example binding theory haege man beatrice kroch provides good account constraints antecedent english pronouns", "theory relies syntactic parse trees determine governing category defines scope pronoun paper refers anaphor normal pronoun", "binding constraints", "use theory guideline help us design features machine learning framework", "previous pronoun resolution work hobbs lappin leass ge et al stuckardt explicitly utilized syntactic information", "unique challenges study syntactic information extracted parse trees automatically generated", "possible availability statistical parsers trained humanannotated tree banks marcus et al xia et al maamouri bies multiple languages binding theory used guideline syntactic structures encoded features maximum entropy coreference system syntactic features evaluated languages arabic chinese english goal see features motivated english language help coreference resolution languages", "con trastive experiments done publiclyavailable data coreference system resolves coreferential relationships annotated mentions pronouns", "using machinegenerated parse trees eliminates need handlabeled trees coreference system", "major challenge extract useful information noisy parse trees", "approach encoding structures contained parse tree set computable features associated weight automatically determined machine learning algorithm", "contrasts approach extracting rules assigning weights rules hand lap pin leass stuckardt", "advantage approach robustness particular structure helpful assigned high weight feature extracted highly noisy parse tree informative coreference resolution assigned small weight", "avoiding writing rules automatically incorporate useful information model time limit potentially negative impact noisy parsing output", "proceedings human language technology conference conference empirical methods natural language processing hltemnlp pages vancouver october"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper study impact group features extracted automatically machinegenerated parse trees coreference resolution", "focus designing syntactic features using binding theory guideline improve pronoun resolution although linguistic phenomenon apposition modeled", "features applied arabic chinese english coreference resolution systems effectiveness evaluated data automatic content extraction ace task", "syntactic features improve arabic english systems significantly play limited role chinese", "detailed analyses done understand syntactic features impact coreference systems"]}, "H89-2014": {"introduction": ["qc association computational linguistics", "paper describes refinements currently investigated model partofspeech assignment words unrestricted text", "model advantage pretagged training corpus required", "words represented equivalence classes reduce number parameters required provide essentially vocabularyindependent model", "state chains used model selective higherorder conditioning model obviates proliferation parameters attendant uniformly higherorder models", "structure state chains based analysis errors linguistic knowledge"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "I05-1065": {"introduction": ["examples show word dependency phrases modeled", "correct identification antecedents anaphor essential message understanding systems well knowledge acquisition systems", "example efficient anaphora resolution needed enhance protein interaction extraction biomedical literature mining protein entity instances represented pronouns general concepts", "biomedical literature pronominal nominal anaphora common types anaphora", "past literature different strategies identify antecedents anaphor presented using syntactic semantic pragmatic clues", "example grammatical roles noun phrases used", "addition syntactic information statistical information like cooccurring patterns obtained corpus employed antecedent finding", "large corpus needed acquiring sufficient cooccurring patterns dealing data sparseness", "hand outer resources like wordnet1 applied proved helpful improve system like described macy information exploited analyzing hierarchical relation nouns verbs surrounding context learned wordnet", "nevertheless using word net alone acquiring semantic information sufficient solving unknown words", "tackle problem richer resource web exploited httpwordnetprincetonedu anaphoric information mined google search results expense less precision", "domainspecific ontologies like umls2 unified medical language system employed way frequent semantic types associated agent subject patient object role subjectaction actionobject patterns extracted", "result showed kind patterns could gain increase precision recall", "hand kim park built bioar relate protein names swissprot entries using centering theory presented salience measures", "paper resolution system presented tackling nominal anaphora pronominal anaphora biomedical literature using various kinds syntactic semantic features", "unlike previous approaches verification semantic association anaphors antecedents facilitated help general domain domainspecific resources", "example semantic type checking resolving nominal anaphora done domain ontology umls pubmed3 search engine medline databases", "umls used tagging semantic type noun phrase chunks umls generating key lexicons type use tag chunks umls", "type information obtained chunk type finding implemented web mining pubmed", "hand domain corpus genia 302p corpus exploited solve semantic type checking pronominal anaphora", "simple weight calculation key saao subjectaction actionobject patterns type mined corpus turn helpful resolution", "semantic type agreement implicit resemblance anaphor antecedents another evidence useful verifying semantic association", "hence general domain thesaurus wordnet supporting relationship concepts subconcepts employed enhance resemblance extraction", "presented resolution system constructed basis salience grading", "order boost system implemented simple genetic algorithm selection rich feature set", "system developed small evaluation corpus medstract nevertheless constructed larger test corpus denoted 100medline instances anaphors resolved", "experimental results show resolution medstract yield fscores resolving pronominal nominal anaphora respectively"], "introduction_label": [{"rouge-1": {"f": 0.18181817681818196, "p": 0.18181818181818182, "r": 0.18181818181818182}, "rouge-l": {"f": 0.057769038391880946, "p": 0.16666666666666666, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02910122989603423, "p": 0.07692307692307693, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper resolution system presented tackle nominal pronominal anaphora biomedical literature using rich set syntactic semantic features", "unlike previous researches verification semantic association anaphors antecedents facilitated exploiting outer resources including umls wordnet genia corpus 302p pubmed", "moreover resolution implemented genetic algorithm feature selection", "experimental results different biomedical corpora showed approach could achieve promising results resolving common types anaphora"]}, "I05-2040": {"introduction": ["promising results obtained larger corpus terms fscores resloving pronominal nominal anaphora respectively", "task entity detection tracking edt suggested automatic content extraction ace project nist", "goal detect entities given text track mentions refer entity", "task fundamental many natural languageprocessing nlp applications informa tion retrieval extraction text classification summarization question answering machine translation", "edt extension task coreference resolution edt resolve coreference mentions detect entities", "entities mentions", "ace project types entities defined edt person per geography political entity gpe organization org location loc facility fac", "many traditional coreference techniques extended edt entity tracking", "early work pronoun anaphora resolution usually uses rulebased methods eg hobbs ge et al mitkov try mine cues relation pronouns antecedents", "recent research soon et al yang et al ng cardie ittycherah et al luo et al focuses use statistical machine learning methods tries resolve references kinds noun phases including namenominal pronoun phrase", "common ap proach applied first train binary statistical model measure likely pair work done first author visiting microsoft research asia", "mentions corefer followed greedy procedure group mentions entities", "mention detection find named entity noun noun phrase pronoun pronoun phrase", "therefore needs named entity rec ognition", "though detection entity mentions essential problem edtcoreference relatively less previous research", "ng cardie shows improving recall noun phrase identification improve performance coreference system", "florian et al", "formulate mention detection problem characterbased classification problem", "assign character text label indicating whether start specific mention inside specific mention outside mention", "paper propose unified edt model based transformation based learning tbl brill framework chinese", "model consists sub models mention detection model coreference model", "first submodel used adapt existing chinese word segmentation named entity ne recognition system specific edt standard", "tbl widely used machine learning method first time applied coreference resolution", "addition feedback technique proposed improve performance system", "rest paper organized follows", "raw document msrsegpos tagging segposne document coreference model mentions entities mention detection model section propose unified tbl chinese edt model framework", "describe key techniques chinese edt word segmentation adaptation model mention detection model coreference model feedback technique section accordingly"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016716714872705825, "p": 0.047619047619047616, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016642975598294377, "p": 0.05, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper proposes unified transformation based learning tbl brill framework chinese entity detection tracking edt", "consists sub models mention detection model entity trackingcoreference model", "first submodel used adapt existing chinese word segmentation named entity ne recognition results specific edt standard find mentions", "second submodel used find coreference relation mentions", "addition feedback technique proposed improve performance system", "evaluated methods automatic content extraction ace nist chinese edt corpus", "results show outperforms baseline achieves comparable performance state oftheart methods"]}, "I05-3013": {"introduction": ["experimental results ace chinese edt corpus shown section", "rapid global proliferation internet applications showing deceleration since new millennium", "example commerce physical customer servicescall centers replaced internet solutions eg via msn icq etc network informal language nil actively used applications", "following trend forecast nil would become key language human communication via network", "today nil expressions ubiquitous", "appear example chat rooms bbs email text message etc growing importance understanding nil expressions technology humanity research points view", "instance comprehension customeroperator dialogues aforesaid commercial application would facilitate effective customer relationship management crm", "recently sociologists showed many interests studying impact networkmediated communication language evolution psychological cognitive perspectives danet mcelhearn nishimura", "researchers claim languages never changing fast today since inception internet language internet communication ie nil gets concise effective formal language", "processing nil text requires unconventional linguistic knowledge techniques", "unfortunately developed handle formal language text existing natural language processing nlp approaches exhibit less effectiveness dealing nil text", "example use ictclas zhang et al tool process sentence going attend meeting", "word segmentation result", "sentence xi4 ba1 xi4 nil expression means case", "concluded without identifying expression chinese text processing techniques able produce reasonable result", "problem leads recent research nil nothing project aims produce techniques nil processing thus avails understanding change patterns behaviors language particularly internet language evolution", "latter could make us adaptive dynamic language environment cyber worldrecently linguistic works car ried nil english", "shared dictionary compiled made available online", "contains english nil expressions including english abbreviations acronyms emoticons", "similar efforts chinese rare", "chinese language widely used internet years ago", "moreover chinese nil expression involves processing chinese pinyin dialects results higher complexity chinese nil processing", "nil nothing project develop comprehensive chinese nil dictionary", "difficult task resource nil text rather restricted", "download collection bbs text internet bbs system construct nil corpus annotating nil expressions collection hand", "empirical study conducted nil expressions nil corpus knowledge mining tool designed construct nil dictionary generate statistical nil features automatically", "knowledge resources nil processing system ie niler developed extract nil expressions nil text employing stateoftheart information extraction techniques", "remaining sections paper organized follow", "section observe formation nil expressions", "section present related works", "section describe nil corpus knowledge engineering component nil dictionary construction nil features generation", "section present methods nil expression recognition"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013844925538298282, "p": 0.125, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0143731875638612, "p": 0.05263157894736842, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014316865434707999, "p": 0.05555555555555555, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01405368928519407, "p": 0.07692307692307693, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013919906055406692, "p": 0.1, "r": 0.0136986301369863}}], "abstract": ["informal language actively used networkmediated communication eg chat room bbs email text message", "refer anomalous terms used context network informal language nil expressions", "example ou3 used replace wo3 chinese icq", "without unconventional resource knowledge techniques existing natural language processing approaches exhibit less effectiveness dealing nil text", "propose study nil expressions nil corpus investigate techniques processing nil expressions", "methods chinese nil expression recognition designed niler system", "experimental results show pattern matching method produces higher precision support vector machines method higher f1 measure", "results encouraging justify future research effort nil processing"]}, "I08-1004": {"introduction": ["outline experiments discussions error analysis section finally section concludes paper", "well known syntactic structured information plays critical role many critical nlp applications parsing semantic role labeling semantic relation extraction coreference resolution", "still open question kinds syntactic structured information effective well incorporate structured information applications", "much research work done direction", "prior researches apply featurebased methods select define set flat features mined parse trees represent particular structured information parse tree grammatical role eg subject object according particular application", "indeed featurebased methods widely applied parsing collins charniak semantic role labeling pradhan et al semantic relation extraction zhou et al coreference resolution lapin leass aone bennett mitkov yang et al luo zitouni bergsma lin", "major problem featurebased methods exploring structured information fail well capture complex structured information critical performance improvement", "current trend explore kernelbased methods haussler implicitly explore features high dimensional space employing kernel calculate similarity objects directly", "particular kernelbased methods could effective reducing burden feature engineering structured objects nlp eg parse tree structure coreference resolution", "recent years various tree kernels convolution tree kernel collins duffy shallow parse tree kernel zelenko et al dependency tree kernel culota sorensen proposed literature", "previous tree kernels convolution tree kernel represents stateoftheart successfully applied collins duffy parsing moschitti semantic role labeling zhang et al semantic relation extraction yang et al pronoun resolution", "exist problems collins duffys kernel", "first subtrees enumerated tree kernel contextfree", "subtree enumerated tree kernel consider context information outside subtree", "second decide proper tree span tree kernel computation according particular application", "resolve problems paper proposes new tree span scheme applies new tree kernel better capture syntactic structured information pronoun resolution whose task find corresponding antecedent given pronominal anaphor text", "rest paper organized follows", "section review related work exploring syntactic structured information pronoun resolution comparison method", "section first presents dynamicexpansion tree span scheme automatically expanding shortest path include necessary structured information predicate antecedent competitor related information", "presents context sensitive convolution tree kernel enumerates contextfree subtrees context sensitive subtrees considering ancestor node paths contexts", "section shows experimental results"], "introduction_label": [{"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021967761463579497, "p": 0.2, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.3999999960888889, "p": 0.75, "r": 0.2727272727272727}, "rouge-l": {"f": 0.06566735112937659, "p": 0.75, "r": 0.06521739130434782}}], "abstract": ["paper proposes contextsensitive convolution tree kernel pronoun resolution", "resolves critical problems previous researches ways", "first given parse tree pair anaphor antecedent candidate implements dynamicexpansion scheme automatically determine proper tree span pronoun resolution taking predicate antecedent competitorrelated information consideration", "second applies contextsensitive convolution tree kernel enumerates contextfree contextsensitive subtrees considering ancestor node paths contexts", "evaluation ace corpus shows dynamicexpansion tree span scheme well cover necessary structured information parse tree pronoun resolution contextsensitive tree kernel much outperforms previous tree kernels"]}, "I08-2080": {"introduction": ["finally conclude work section", "named entity recognition ner task identifying classifying phrases certain classes named entities nes names persons organizations locations", "japanese texts focus written without using blank spaces", "therefore japanese ner tight relation morphological analysis thus often performed immediately morphological analysis masayuki matsumoto yamada", "approaches rely local context", "japanese ner system proposed nakano hirai achieved highest fmeasure conventional systems introduced bunsetsu1 feature order consider wider context considers adjacent bunsetsus", "research fellow japan society promotion science jsps bunsetsu commonly used linguistic unit japanese consisting adjacent content words following functional words", "hand english chinese various ner systems explored global information reported effectiveness", "malouf chieu ng information features assigned instances token utilized", "ji grishman uses information obtained coreference analysis ner", "mohit hwa uses syntactic features building semisupervised ne tagger", "paper present japanese ner system uses global information obtained several structural analyses", "specic system based svm recognizes nes syntactic case coreference analyses uses information obtained analyses ner results previous context integrally", "point true ner results useful syntactic case coreference analyses thus analyses ner performed complementary way"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper presents approach uses structural information japanese named entity recognition ner", "ner system based support vector machine svm utilizes types structural information cache features coreference relations syntactic features caseframe features obtained structural analyses", "evaluated approach crl ne data obtained higher fmeasure existing approaches use structural information", "conducted experiments irex ne data neannotated web corpus conrmed structural information improves performance ner"]}, "I08-4015": {"introduction": ["since focus ner recognize ne structural analyses", "since chinese word segmentation firstly treated characterbased tagging task xue converse method widely accepted developed researchers peng et al tseng et al low et al zhao et al", "thus powerful sequence tagging model crf became dominant method bakeoff levow 2006in paper improve basic segmenter un der crf work frame aspects namely iv oov identification respectively", "use result wordbased segmentation revise crf output gain higher iv word recall", "oov part postprocessing rule proposed find oov words wrongly segmented several fractions", "system performs well fourth bakeoff achieving second best fifth corpora", "following paper describe method detail", "rest paper organized follows", "section first give brief review basic crf tagging approach propose methods improve iv oov performance respectively", "section give experiment results fourth bakeoff corpora show method effective improve performance segmenter"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02982825295707428, "p": 0.037037037037037035, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.28571428234693885, "p": 0.6666666666666666, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05438042620364216, "p": 0.6666666666666666, "r": 0.05405405405405406}}], "abstract": ["paper describes chinese word segmenter fourth international chinese language processing bakeoff", "base conditional random field crf model basic segmenter designed problem characterbased tagging", "improve performance segmenter employ wordbased approach increase invocabulary iv word recall postprocessing increase outofvocabulary oov word recall", "participate word segmentation closed test corpora system achieved second best fifth corpora"]}, "J00-4003": {"introduction": ["section conclude work", "models definite description processing proposed literature tend emphasise anaphoric role elements1 heim perhaps best malization type theory", "approach challenged results exper iments reported previously poesio vieira subjects asked classify uses definite descriptions wall street journal articles according schemes derived proposals hawkins prince", "results experiments indicated definite descriptions primarily anaphoric half time used introduce new entity discourse", "paper present implemented system processing definite descriptions based results earlier study", "system techniques recognizing discoursenew descriptions play role important techniques identifying antecedent anaphoric ones", "central characteristic work described intended start develop system whose performance could evaluated using texts notated experiments mentioned", "assessing performance nlp system large number examples increasingly seen much thorough evaluation performance trying come counterexamples con sidered essential language engineering applications", "advantages thought many offset obvious disadvantages way developing nlp theoriesin particular fact given current state language processing technology many hypotheses interest cannot tested see", "result quantitative evaluation commonplace areas language engineering parsing quantitative evaluation techniques proposed semantic univcrsidade vale rio dos sinosunisinos av", "unisinos 950cx", "postal sao leopoldo rs brazil", "email renataexatasunisinosbr university edinburgh iccs informatics buccleuch place ehs 9lw edinburgh uk", "email massimopoesioedacuk use term definite description russell indicate definite noun phrases definite", "article car", "concerned types definite noun phrases pronouns demonstratives possessive deiptions", "anaphoric expressions linguistic expressions used signat evoke refer previously mentioned entities", "association computational linguistics interpretation well example sixth seventh message understa nd ing conferences muc6 muc7 sundheim chinchor included evaluations systems socalled coreference task subtask resolution definite descriptions", "system present developed evaluated quantitative fashion well problems concerning agreement annotators observed previous study evaluated sys tem measuring precisionrecall gold standard done muc measuring agreement annotations produced system proposed annotators", "decision develop system could quantitatively evaluated large number examples resulted important constraint could make use inference mechanisms assumed traditional computational theories definite description resolution eg sidner carter alshawi poesio", "many facts axioms would encoded hand theories type tested even mediumsized corpus", "system therefore based shallowprocessing approach radical even attempted first advocate approach carter systems participated muc evaluations appelt et al gaizaukas et al humphreys et al since made attempt finetune system maximize performance particular domain", "system relies structural information formation provided preexisting lexical sources wordnet fellbaum minimal amounts general handcoded information information could acquired automatically corpus", "result system really resources correctly resolve definite descriptions whose interpretation require complex reasoning grouped call bridging class", "nevertheless developed heuristic techniques processing types definites well idea heuristics provide baseline gains performance due use commonsense knowledge assessed clearly2 paper organized follows first summarize results previous corpus study poesio vieira section discuss model defi nite description processing adopted result work general architecture system section", "section discuss heuristics developed resolving anaphoric definite descriptions recognizing discoursenew descriptions processing bridging descriptions section per formance heuristics evaluated using annotated corpus"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07988088591102555, "p": 0.11764705882352941, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03633961810930086, "p": 0.037037037037037035, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0344010459219721, "p": 0.03333333333333333, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03924646781770945, "p": 0.047619047619047616, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110786694109, "p": 0.06976744186046512, "r": 0.2727272727272727}, "rouge-l": {"f": 0.07452443909834229, "p": 0.06666666666666667, "r": 0.10714285714285714}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03889570552123663, "p": 0.045454545454545456, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07905982905969006, "p": 0.1, "r": 0.07142857142857142}}], "abstract": ["present implemented system processing definite descriptions arbitrary domains", "design system based results corpus analysis previousl reported highlighted prevalence discoursenew descriptions newspa per corpora", "annotated corpus used extensivel evaluate pro posed techniques matching definite descriptions antecedents discourse segmentation recognizing discoursenew descriptions suggesting anchors bridging descriptions"]}, "J01-2004": {"introduction": ["finally present final configuration versions system developed section review systems perform similar tasks section present conclusions indicate future work section", "certain exceptions computational linguists past generally formed separate research community speech recognition researchers despite obvious overlap interest", "perhaps reason relatively cently methods come natural language processing community shown improve upon simple language models still standardly use speech recognition systems", "past years improve ments made language models use statistical meth ods natural language processing development innovative linguistically wellmotivated techniques improving language models speech recognition generating interest computational linguists", "language models built around shallow local dependencies still standard stateoftheart speech recognition systems reason hope better language models developed computational linguists task", "paper examine language modeling speech recognition nat ural language processing point view", "recent literature investigating approaches use syntactic structure attempt capture longdistance depen dencies language modeling reviewed", "new language model based probabilistic topdown parsing outlined compared previous liter ature extensive empirical results presented demonstrate utility", "features topdown parsing approach emerge key success", "first topdown parsing algorithm builds set rooted candidate parse trees left right string allows calculate generative probability department cognitive linguistic sciences box brown university providence ri association computational linguistics prefix string probabilistic grammar hence conditional probability word given previous words probabilistic grammar", "leftto right parser whose derivations rooted ie derivations consist disconnected tree fragments lr shiftreduce parser cannot incrementally calculate probability prefix string generated probabilistic grammar derivations include probability mass unrooted structures", "point derivations become rooted end string generative string probabilities calculated grammar", "parsers calculate word probabilities based upon parser stateas chelba jelinek 1998abut distribution generative probabilistic grammar", "parser left right rooted derivations eg head first parser able calculate generative joint probabilities entire strings able calculate probabilities word conditioned previously generated words unless derivation generates words string exactly order", "example suppose possible verbs could head sentence", "headfirst parser derivations first verb head sentence second verb generated first hence second verbs probability conditioned first verb", "derivations second verb head sentence first verbs probability conditioned second verb", "scenario way decompose joint probability calculated set derivations product conditional probabilities using chain rule", "course joint probability used language model cannot interpolated wordbyword basis say trigram model demonstrate useful thing", "thus topdown parser allows incremental calculation generative conditional word probabilities property shares lefttoright parsers rooted derivations earley parsers earley leftcorner parsers rosenkrantz lewis ii", "second key feature approach topdown guidance improves efficiency search conditioning events extracted derivation use probabilistic model", "rooted partial derivation fully connected conditioning information might extracted topdown left context already specified conditional probability model built information impose additional burden search", "contrast earley leftcorner parser underspecify certain connections constituents left context underspecified information used conditional probability model become specified", "course done expense search efficiency done less benefit underspecification", "topdown parser contrast derive efficiency benefit precisely information underspecified approaches", "thus topdown parser makes easy condition probabilistic gram mar arbitrary number values extracted rooted fully specified deriva tion", "lead us formulation conditional probability model terms values returned treewalking functions contextually sen sitive", "topdown guidance provided makes approach quite efficient practice", "following section provide background probabilistic contextfree grammars language modeling speech recognition", "brief review previous work using syntactic information language modeling introduce model section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1739130384877128, "p": 0.16666666666666666, "r": 0.18181818181818182}, "rouge-l": {"f": 0.045627069369375425, "p": 0.16666666666666666, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["pa per describes thefunctioning broadcoverage probabilistic topdown parser application problem language modelingfor speech recognition", "pa per first introduces key notions language modeling probabilistic parsing briefly reviews previous approaches using syntactic structure language modeling", "lexicalized probabilistic top parser presented performs well terms accuracy returned parses efficiency arefound relative best broadcoverage statistical parsers", "new language model utilizes probabilistic topdown parsing outlined empirical results show improves upon previous work test corpus per plexit interpolation trigram model yields exceptional improvement relative improvement observed models demonstrating degree information captured parsing model orthogonal captured trigram model", "small recognition experiment demonstrates utility model"]}, "J98-1006": {"introduction": ["st st stop stop np vp np np vp spot vbd np chased dt nn ball spot vbd jp chased dt nn ball spot figure parse trees complete parse tree complete parse tree explicit stop symbol partial parse tree", "impressive array statistical methods developed word sense identi fication", "range dictionarybased approaches rely definitions veronis ide wilks et al corpusbased approaches use word co occurrence frequencies extracted large textual corpora schiitze dagan itai", "drawn traditions using corpusbased cooccurrence lexical knowledge base embodied wordnet lexicon", "traditions complement", "corpusbased approaches advantage generally applicable new texts domains corpora without needing costly perhaps errorprone parsing semantic analysis", "require training corpora sense distinctions marked therein lies weakness", "obtaining training materials statistical methods costly time consumingit knowledge acquisition bottleneck gale church yarowsky 1992a", "open bottleneck use wordnets lexical relations locate unsuper vised training examples", "section describes statistical classifier tlc topicallocal classifier uses topical context openclass words cooccur particular sense local con text open closedclass items occur small window around word combination", "results combining types context disambiguate noun line verb serve adjective hard presented", "following questions discussed topical context superior local context vice versa", "combination superior either type alone", "answers questions depend size training", "depend syntactic category target", "division cognitive instructional science princeton nj email cleacocketsorg", "work reported done author princeton university", "department psychology park avenue new york ny email mschccunyvmcunyedu cognitive science laboratory nassau street princeton nj email geoclarityprincetonedu association computational linguistics manually tagged training materials used development tlc experiments section", "cognitive science laboratory princeton univer sity support nsfarpa producing textual corpora used developing evaluating automatic methods disambiguation", "examples different meanings thousand common polysemous openclass english words manually tagged", "results effort useful resource train ing statistical classifiers next thousand polysemous words next", "order identify senses words necessary learn harvest training examples automatically", "section describes wordnets lexical relations role monosemous relatives polysemous words play creating unsupervised training materials"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.18181817681818196, "p": 0.18181818181818182, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04534444140398479, "p": 0.18181818181818182, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0220600295227433, "p": 0.16666666666666666, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["corpusbased approaches word sense identification flexibility generality suffer knowledge acquisition bottleneckwe show knowledgebased techniques used open bottleneck automatically locating training corpora", "describe statistical classifier combines topical context local cues identify word sense", "classifier used disambiguate nouna verband adjective", "knowledge base form ofwordnets lexical relations used automatically locate training examples general text corpus", "test results compared manually tagged training examples"]}, "N01-1012": {"introduction": ["tlc trained automatically extracted examples performance compared obtained manually tagged training materials", "semantic interpretation algorithm ex plained paper offers solution thefollowing interpretation problems determina tion meaning verb identification thematic roles adjuncts attachments prepositional phrases pps", "interesting aspect algorithm solution ofall problems interdependent", "inter pretation algorithm uses wordnet miller et al lexical knowledgebase", "predicatesor verbal concepts defined word net verb classes reorganized considerably following criteria imposed bythe interpretation algorithm", "wordnet ontology nouns undergone reorga nization redefinition conform entries thematic roles predicates", "views guides research syntax many verbs determined theirmeaning", "verbs highly ambigu ous say senses light verbswhich lexicalize anything need spe cial definitions", "briefly algorithm follows", "every verb sentence wordnet provides list ofverb synsets defined predicates", "predicates viewed contenders meaning verb", "syntac tic relations parsed interpreter checks predicate order see predicatehas thematic role realized syn tactic relation", "interpreter records fact gets next syntactic relation", "thepredicate realizes syntactic rela tions sentence selected meaning verb", "paper organized follows"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["algorithm semantic interpretation explained", "algorithm based predicatesdefined wordnet verb classes", "algorithm driven definition predicates whose thematic roles linked thewordnet ontology nouns syntactic relations realize", "algorithm tested identification meaning verb thematic roles temporal spatial adjuncts"]}, "N03-2035": {"introduction": ["first part paper sections explains themethodology building predicates word net verb classes second part sections describes semantic interpretation algorithm testing conclusions", "traditional thai tts consists main modules word segmentation graphemetophoneme prosody generation speech signal processing", "accuracy pronunciation thai tts mainly depends accuracies modules word segmentation graphemetophoneme", "word segmentation process word boundaries cannot identified correctly leads thai tts incorrect pronunciation string separated different ways different meanings pronunciations", "first eye round pronounced ta0 klom0and expose wind pronounced tak1 lom0", "graphemetophoneme mod ule produce error pronunciations homograph pronounced way word pronounced phlaw0 phe0 la0", "therefore improve accuracy thai tts focus solving problems word boundary ambiguity homograph ambiguity viewed disambiguation task", "number featurebased methods tried several disambiguation tasks nlp including decision lists bayesian hybrids winnow", "methods superior previously proposed methods combine evidence various sources disambiguation", "apply methods task treat problems word boundary homograph ambiguity task word pronunciation disambiguation", "task decide using context actually intended", "instead using type syntactic evidence ngram approaches employ synergy several types features", "following previous works adopted types features context words collections", "contextword feature used test presence particular word words target word collocation test pattern contiguous words andor partofspeech tags surrounding target word", "automatically extract discriminative features feature space combine disambiguation investigate efficient technique task", "problem becomes select combine various kinds features", "yarowsky proposed decision list way pool several types features solve target problem applying single strongest feature whatever type golding proposed bayesian hybrid method take account available evidence instead strongest", "method applied task contextsentitive spelling correction reported superior decision lists", "later golding roth applied winnow algorithm task found algorithm performs comparably bayesian hybrid method using pruned feature sets better using unpruned sets unfamiliar test set", "paper propose unified framework solving problems word boundary ambiguity homograph ambiguity altogether", "approach employs local longdistance contexts automatically extracted machine learning technique", "task employ machine learning technique called winnow"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015749352035386324, "p": 0.16666666666666666, "r": 0.015625}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03343342974531318, "p": 0.09523809523809523, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12121211676767694, "p": 0.09090909090909091, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05098841123389943, "p": 0.12, "r": 0.046875}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["homograph ambiguity original issue texttospeech tts", "disambiguate homograph several efficient approaches proposed partofspeech pos ngram bayesian classifier decision tree bayesianhybrid approaches", "methods need words orand pos tags surrounding question homographs disambiguation", "languages thai chinese japanese wordboundary delimiter", "therefore solving homograph ambiguity need identify word boundaries", "paper propose unique framework solves word segmentation homograph ambiguity problems altogether", "model employs local long distance contexts automatically extracted machine learning technique called winnow"]}, "N04-1016": {"introduction": ["construct system based algorithm evaluate comparing existing approaches thai homograph problems", "keller lapata investigated validity web counts range predicateargument bigrams verb object adjectivenoun nounnoun bigrams", "presented simple method retrieving bigram counts web querying search engine demonstrated web counts correlate frequencies obtained carefully edited balanced corpus 100m words british national corpus bnc correlate frequencies recreated using smoothing methods case unseen bigrams reliably predict human plausibility judgments yield stateoftheart performance pseudodisambiguation tasks", "keller lapatas results suggest web based frequencies viable alternative bigram frequencies obtained smaller corpora recreated using smoothing", "demonstrate realistic nlp tasks benefi web counts", "order show web counts would applied diverse range nlp tasks syntactic seman task pos ling type mt candidate select", "sem generation spelling correction synsem generation adjective ordering adj sem generation compound bracketing syn analysis compound interpret", "sem analysis countability detection det sem analysis table overview tasks investigated paper size ngram pos parts speech ling linguistic knowledge type type task tic involving analysis eg disambiguation generation eg selection competing outputs", "remains shown webbased approach scales larger ngrams eg trigrams combinations different parts speech keller lapata tested bigrams involving nouns verbs adjectives", "another important question whether webbased methods defi nition unsupervised competitive alternatives supervised approaches used tasks literature", "paper aims address questions", "start using web counts generation tasks use large data sets shown promising results target language candidate selection machine translation grefenstette context sensitive spelling correction banko brill 2001ab", "investigate generality webbased approach applying range analysis generations tasks involving syntactic semantic knowledge ordering prenominal adjectives compound noun bracketing compound noun interpretation noun count ability detection", "table gives overview tasks properties", "cases propose simple unsupervised ngram based model whose parameters estimated using web counts"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.039215682891195994, "p": 0.025, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01961165048515681, "p": 0.022222222222222223, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02033492822966117, "p": 0.030303030303030304, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02017650410095058, "p": 0.037037037037037035, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020119488342091166, "p": 0.038461538461538464, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01894739339873598, "p": 0.07692307692307693, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["previous work demonstrated web counts used approximate bigram frequencies thus useful wide variety nlp tasks", "far generation tasks candidate selection machine translation confusionset disambiguation tested using webscale data sets", "present paper investigates results generalize tasks covering syntax semantics generation analysis larger range ngrams", "majority tasks fi nd simple unsupervised models perform better ngram frequencies obtained web rather large corpus", "cases webbased models fail outperform sophisticated stateofthe art models trained small corpora", "argue webbased models therefore used baseline rather alternative standard models"]}, "N06-1017": {"introduction": ["compare model baseline model parameters estimated bnc stateoftheart models literature either supervised ie use annotated training data unsupervised rely taxonomies recreate missing counts", "system seen positive examples recognize negative example", "problem addressed outlier detection called novelty detection1 markou singh 2003a markou singh 2003b marsland detect novel unknown items differ seen training data", "outlier detection approaches typically derive model normal objects training set use distance measure threshold detect abnormal items", "paper apply outlier detection techniques task unknown sense detection identification corpus occurrences covered given sense inventory", "training set term novelty detection used distinction novel repeated information information retrieval different related topic", "figure wrong assignment due missing sense hound baskervilles ch", "new occurrences compared consist senseannotated text", "unknown sense detection related word sense disambiguation wsd word sense discrimination schu tze differs", "wsd senses assumed known task select unknown sense detection task decide whether given occurrence matches known senses none training instances regardless sense belong modeled group known data", "unknown sense detection differs word sense discrimination sense inventory given task group occurrences senses", "unknown sense detection model respects given word senses", "main motivation study comes shallow semantic parsing mean combination wsd automatic assignment semantic roles free text", "cases sense missing inventory wsd wrongly assign existing senses", "figure shows example sentence hound baskervilles analyzed sh erk pado shallow semantic parser", "analysis based framenet baker et al resource lists senses semantic roles english expressions", "framenet lacking sense expectation mentally prepared verb prepare prepared assigned sense co possible improbable analysis2", "erroneous labels fatal processing builds results shallow semantic parsing eg drawing inferences", "unknown sense detection prevent mistakes", "sense inventories face problem missing senses either small overall size case nonenglish wordnets encounter domainspecific senses", "study evaluated framenet main aim improving shallow semantic parsing method propose applicable sense inventory annotated data particular applicable wordnet", "paper model unknown sense detection outlier detection using simple nearest neighborbased method tax duin compares local probability density test item nearest training item", "knowledge exists approach date problem detecting unknown senses", "approaches complementary problem determining closest known sense unknown words widdows curran burchardt et al viewed logical next step unknown sense detection", "plan paper", "brief sketch framenet section describe experimental setup used throughout paper section", "section tests whether simple model suffices detecting unknown senses threshold confidence scores returned sh wsd unfortunately semantic roles misassigned system", "word fill fo role hound could assigned optional role", "system", "result recall much low", "section introduces nnbased outlier detection approach use section unknown sense detection better results first experiment still low recall"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1290322534859523, "p": 0.1, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0769659134203902, "p": 0.08695652173913043, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03774928774937501, "p": 0.125, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["address problem unknown word sense detection identification corpus occurrences covered given sense inventory", "model instance outlier detection using simple nearest neighborbased approach measuring resemblance new item training set", "combination method alleviates data sparseness sharing training data lemmas approach achieves precision recall"]}, "N06-1037": {"introduction": ["section repeats experiment section added training data making use fact semantic class framenet typically pertains several lemmas achieving marked improvement results", "relation extraction subtask information extraction finds various predefined semantic relations location affiliation rival etc pairs entities text", "example sentence george bush president united states conveys semantic relation president entities george bush per united states gpe geopolitical entity entity land government ace", "prior featurebased methods task kambhatla zhou et al employed large amount diverse linguistic features varyingfrom lexical knowledge entity mention informa tion syntactic parse trees dependency trees semantic features", "since parse tree contains rich syntactic structure information principle features extracted parse tree contribute much performance improvement relation extraction", "reported zhou et al kambhatla hierarchical structured syntactic features contributes less performance improvement", "mainly due fact syntactic structure information parse tree hard explicitly describe vector linear features", "alternative kernel methods collins duffy provide elegant solution implicitly explore tree structure features directly computing similarity trees", "surprise sole tworeported dependency tree kernels relation extraction ace corpus bunescu mooney culotta sorensen showed much lower performance featurebased methods", "ask syntactic tree features useful relation extraction", "tree kernel methods effectively capture syntactic tree features various features proven useful featurebased methods", "paper demonstrate effectiveness syntactic tree features relation extraction study capture features via convolution tree kernel", "study select optimal feature space eg set subtrees represent relation instances optimize system performance", "experimental results show convolution tree kernel plus entity features achieves slightly better performance previous bestreported featurebased methods", "shows method significantly outperforms dependency tree kernels bunescu mooney culotta sorensen ace relation types", "rest paper organized follows", "section review previous work", "section discusses tree kernel based learning algorithm", "proceedings human language technology conference north american chapter acl pages new york june", "qc association computational linguistics section shows experimental results compares work related work"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029653605123417946, "p": 0.03571428571428571, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02730839757874066, "p": 0.25, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.057769038391880946, "p": 0.16666666666666666, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.28571428234693885, "p": 0.6666666666666666, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05438042620364216, "p": 0.6666666666666666, "r": 0.05405405405405406}}], "abstract": ["paper proposes use convolution kernel parse trees model syntactic structure information relation extraction", "study reveals syntactic structure features embedded parse tree effective relation extraction features well captured convolution tree kernel", "evaluation ace corpus shows convolution kernel parse trees achieve comparable performance previous bestreported featurebased methods ace relation subtypes", "shows method significantly outperforms previous dependency tree kernels ace relation major types"]}, "N07-1015": {"introduction": ["conclude work section", "important information extraction task relation extraction whose goal detect characterize semantic relations entities text", "example text fragment hundreds palestinians converged square contains located relation person entity hundreds palestinians boundedarea entity square", "relation extraction applications many domains including finding affiliation relations web pages finding proteinprotein interactions biomedical literature", "recent studies relation extraction shown advantages discriminative modelbased statistical machine learning approach problem", "generally lines work following approach", "first utilizes set carefully selected features obtained different levels text analysis partofspeech pos tagging full parsing dependency parsing kambhatla zhao grishman zhou et al", "second line work designs kernel functions structured representation sequences trees relation instances capture similarity relation instances zelenko et al culotta sorensen bunescu mooney 2005a bunescu mooney 2005b zhang et al 2006a zhang et al 2006b", "particular interest various kernels proposed convolution kernels bunescu mooney 2005b zhang et al 2006a efficiently compute similarity instances huge feature space due recursive nature", "apart computational efficiency convolution kernels implicitly correspond feature space", "therefore lines work rely appropriately de although zhao grishman defined number kernels relation extraction method essentially similar featurebased methods", "proceedings naacl hlt pages rochester ny april", "qc association computational linguistics fined set features", "learning problem choice features affect performance significantly", "despite importance feature selection systematic exploration feature space relation extraction choices features existing work somewhat arbitrary", "paper conduct systematic study feature space relation extraction evaluate effectiveness different feature subspaces", "motivations twofold", "first based previous studies want identify characterize types features potentially useful relation extraction define relatively complete structured feature space systematically explored", "second want compare effectiveness different features", "study guide us choose effective feature set relation extraction design convolution kernels effective way", "propose define unified graphic representation feature space experiment feature subspaces corresponding sequences syntactic parse trees dependency parse trees", "experiment results show subspace effective syntactic parse tree subspace effective", "combining subspaces generate much improvement", "feature subspace using basic unit features already give reasonably good performance", "adding complex features improve performance much even hurt performance", "taskoriented heuristics used prune feature space appropriately done improve performance"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018318318318332562, "p": 0.2, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020310633213816317, "p": 0.02857142857142857, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019545816641176612, "p": 0.05263157894736842, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019246675387691467, "p": 0.0625, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["relation extraction task finding semantic relations entities text", "stateoftheart methods relation extraction mostly based statistical learning thus deal feature selection significantly affect classification performance", "paper systematically explore large space features relation extraction evaluate effectiveness different feature subspaces", "present general definition feature spaces based graphic representation relation instances explore different representations relation instances features different complexities framework", "experiments show using basic unit features generally sufficient achieve stateoftheart performance inclusion complex features hurt performance", "combination features different levels complexity different sentence representations coupled taskoriented feature pruning gives best performance"]}, "N07-1024": {"introduction": ["combination features different levels complexity different sentence representations coupled taskoriented feature pruning gives best performance", "research semantic annotation focused primarily word sense disambiguation wsd ie task determining appropriate sense instance polysemous word set senses defined word lexicon", "much less work done semantic classification unknown words ie words listed lexicon", "real texts typically contain large number unknown words", "successful classification unknown words useful lexical acquisition necessary natural language processing nlp tasks require semantic annotation", "paper addresses problem classifying chinese unknown words finegrained semantic categories defined chinese thesaurus cilin mei et al", "thesaurus classifies words major categories including human concrete object time space abstract object attributes actions mental activities activities physical states relations auxiliaries honorifics", "major categories divided medium categories turn subdivided small categories", "small category contains synonyms close meaning", "example major category medium category dm groups words refer institutions small category dm05 groups words refer educational institutions eg xuxio school", "unknown word classification involves much larger search space wsd", "classifying words small categories cilin search space polysemous known word consists categories word belongs unknown word consists small categories", "research wsd concentrated using contextual information limited infrequent unknown words", "hand chinese characters carry semantic information useful predicting semantic properties words containing", "present novel knowledgebased models capture relationship semantic categories unknown word component characters different ways combine corpusbased model uses contextual information classify unknown words", "experiments show combined knowledgebased model achieves accuracy classifying unknown words small categories cilin use contextual information improve performance", "rest paper organized follows", "section details novel knowledge based models proposed task", "section describes corpusbased model", "section reports experiment results proposed proceedings naacl hlt pages rochester ny april", "qc association computational linguistics models", "section compares results previous results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper addresses problem classifying chinese unknown words finegrained semantic categories defined chinese thesaurus", "describe novel knowledgebased models capture relationship semantic categories unknown word component characters different ways", "combine knowledgebased models corpusbased model classifies unknown words using contextual information", "experiments show knowledgebased models outperform previous methods task use contextual information improve performance"]}, "N09-1057": {"introduction": ["section concludes paper points avenues research", "pang lee observe last several years seen land rush research sentiment analysis opinion mining frequent emphasis identification opinions evaluative text movie product reviews", "sentiment carried implicitly statements nonevaluative even visibly subjective", "consider example following descriptions invented event 1a november soldier veered jeep crowded market killed civilians", "november soldiers jeep veered crowded market causing civilian deaths", "descriptions appear surface objective statements use nearly words", "lexically sentences first clauses differ difference express relationship soldier jeep second clauses kill death terms negative connotations least according general inquirer lexicon stone", "descriptions clearly differ feelings evoke soldier tried role happened november surely prosecutor would likely say 1a jury defense attorney 1b rather reverse1 description like 1a perceived less sympathetic soldier 1b", "difference words must way put together structure sentence", "section offer specific hypothesis connection structure implicit sentiment suggest relationship mediated set grammatically relevant semantic properties well known important cross linguistically characterizing interface syntax lexical semantics", "section validate hypothesis means human ratings study showing properties highly predictive human sentiment ratings", "section introduce observable proxies underlying semantics opus practical way approximate relevant semantic properties automatically features supervised learning setting", "section show features improve existing state ofthe art automatic sentiment classification", "sec work done first author student department linguistics university maryland", "refer readers sharing intuition section", "human language technologies annual conference north american chapter acl pages boulder colorado june"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.036935842946350037, "p": 0.038461538461538464, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}], "abstract": ["work sentiment analysis often focuses words phrases people use overtly opinionated text", "paper introduce new approach problem focuses lexical indicators syntactic packaging ideas well suited investigating identification implicit sentiment perspective", "establish strong predictive connection linguistically well motivated features implicit sentiment show computational approximations features used improve existing stateoftheart sentiment classification results"]}, "N09-2003": {"introduction": ["qc association computational linguistics tions discuss related work summarize", "hypergraph demonstrated huang chi ang compact datastructure encode exponential number hypotheses generated regular phrasebased machine translation mt system eg koehn et al", "syntax based mt system eg chiang", "hypergraph represents large set translations quite possible desired translations eg reference translations contained hypergraph due pruning inherent deficiency translation model", "case often required find translations hypergraph similar desired translations similarity computed via automatic metric bleu papineni et al", "maximally similar translations called oracle best translations process extracting oracle extraction", "oracle extraction nontrivial task computing similarity hypothesis requires information scattered many items hypergraph exponentially large number hypotheses makes bruteforce linear search intractable", "therefore efficient algorithms exploit structure hypergraph required", "present efficient oracle extraction algorithm involves key ideas", "firstly view oracle extraction bottomup model scoring process hypergraph model trained reference translations", "similar algorithm proposed lattice dreyer et al", "", "algorithm requires maintaining separate dynamic programming state distinguished sequence state words number sequences huge making search slow", "secondly therefore present novel lookahead technique called equivalent oraclestate maintenance merge multiple states equivalent similarity computation", "experiments show equivalent oracle state maintenance technique significantly speeds times oracle extraction", "efficient oracle extraction least important applications machine translation", "discriminative training discriminative training objective tune model parameters eg weights perceptron model conditional random field reference translations preferred competitors", "reference translations reachable translation system case oraclebest hypotheses substituted training", "proceedings naacl hlt short papers pages boulder colorado june", "qc association computational linguistics system combination typical system combination task eg rosti et al", "component system produces set translations grafted form confusion network", "confusion network rescored often employing additional language models select final translation", "measuring goodness hypothesis confusion network requires score component system", "translations confusion network reachable component systems case systems score similar reachable translation serves good approximation", "multisource translation multisource translation task och ney input given multiple source languages"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["hypergraphs used several syntax inspired methods machine translation compactly encode exponentially many translation hypotheses", "hypotheses closest given reference translations therefore cannot found via brute force particularly popular measures closeness bleu", "develop dynamic program extracting called oraclebest hypothesis hyper graph viewing problem finding likely hypothesis ngram language model trained reference translations", "identify remove massive redundancies dynamic program state due sparsity ngrams present reference translations resulting efficient program", "present run time statistics program demonstrate successful application hypotheses thus found targets discriminative training translation system components"]}, "N09_csl2013": {"introduction": ["leads situation analogous system combination except component translation system corresponds specific source language", "sentiment subjectivity analysis seeks automatically identify opinions beliefs speculations emotions sentiments private states natural text wiebe et al", "quirk et al", "define private state state lend objective external validation words person observed assert god exists believe god exists", "belief sense private", "field natural language processing researchers used term subjectivity analysis denote identifying private states text namely separating objective subjective instances sentiment polarity analysis refines subjective text positive negative neutral", "sentiment subjectivity analysis stemmed prolific area research mainly due fact numerous text processing applications stand gain incorporating sentiment dimensions models including automatic expressive texttospeech synthesis alm et al tracking sentiment timelines online forums news balog et al lloyd et al mining opinions product reviews hu liu", "many natural language processing tasks subjectivity sentiment classification used first phase filtering generate viable data", "research benefited additional layering ranges question answering yu hatzivassiloglou conversation summarization carenini et al text semantic analysis wiebe mihalcea esuli sebastiani lexical substitution su markert", "experiments carried english wiebe mihalcea shown robust subjectivity delineation occurs sense word level", "following finegrained perspective esuli sebastiani andreevskaia bergler proposed methods embed senselevel automatic sentiment annotations objectiveneutral negative positive english wordnet structure miller using relationships synonymy antonymy meronymy etc", "hand noticing scarcity hand crafted senselevel subjectivitypolarity lexica markert su explored ways infer data annotated either word sentence level", "senselevel subjectivity crosslingual subjectivity sentiment analysis received considerable attentions recent years paper explores area lies intersection topics", "knowledge area formally investigated techniques similar applied sentiment subjectivity analysis sentence review level work explores difficult task senselevel subjectivity involves deep semantic aspects language", "manual annotation study performed task cross lingual senselevel subjectivity annotations well methods proposed crosslingual multilingual learning using dictionaries multiple languages novel knowledge", "work seeks answer following questions", "first word senses aligned languages subjectivity content consistent words subjective sense language map subjective sense language similarly objective sense", "second employ multilingual framework automatically discover new subjectiveobjective senses starting limited amount annotated data", "seek answer first question conducting manual annotation study section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.039215682891195994, "p": 0.025, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019494825774881076, "p": 0.021739130434782608, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02017650410095058, "p": 0.037037037037037035, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.17647058385813158, "p": 0.13043478260869565, "r": 0.2727272727272727}, "rouge-l": {"f": 0.039966925454734924, "p": 0.08333333333333333, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.18181817737373748, "p": 0.13636363636363635, "r": 0.2727272727272727}, "rouge-l": {"f": 0.05946684894063716, "p": 0.13636363636363635, "r": 0.05454545454545454}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018318318318332562, "p": 0.2, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["recent research english word sense subjectivity shown subjective aspect entity characteristic better delineated sense level instead traditional word level", "paper seek explore whether senses aligned languages exhibit trait consistently case investigate property leveraged automatic fashion", "first conduct manual annotation study gauge whether subjectivity trait sense robustly transferred language boundaries", "automatic framework introduced able predict subjectivity labeling unseen senses using either cross lingual multilingual training enhanced bootstrapping", "show multilingual model consistently outperforms crosslingual accuracy iterations", "keywords sentiment text classification multilingual subjectivity analysis sense level subjectivity email addresses carmenbaneagmailcom carmen banea radacsuntedu rada mihalcea wiebecspittedu janyce wiebe preprint submitted computer speech language december"]}, "N09prod": {"introduction": ["second question propose models see section cross lingual multilingual able simultaneously use information extracted several languages making subjectivity senselevel predictions", "development internet customers get used purchasing products ecommerce sites 360buy1 newegg2", "write reviews products using produce large number reviews internet", "addition microblog system allows users post messages words share information instantaneously based relationship users rapid development twitter3 sina microblog4 tencent microblog5", "lot microblogs contain latest product reviews", "reviews large data sources contain much useful information users companies", "users make better purchasing decisions based reviews companies analyze customers satisfaction according reviews improve quality products", "since mass httpwww360buycom httpwwwneweggcomcn httpwwwtwittercom httpweibocom httptqqcn ds", "huang et al", "eds icic lncs pp", "", "springerverlag berlin heidelberg product reviews single user cannot read automatically mining reviews multiple sources particularly important", "reviews chinese ecommerce sites labeled advantage disadvantage naturally suitable binary classification", "stateofart research chinese sentiment analysis mainly focuses whole review classification customers often desire detailed understanding products", "example want know others opinion battery cell phone", "therefore propose framework sentiment classification aspect level solve problem", "framework explicit implicit aspects taken account", "knowledge implicit aspect discovery work product review chinese language reported", "reviews aspect unigram features words used text features", "compare performance feature weighing strategies reduction dimension classification approaches", "sentiment analysis reviews products microblogs infancy", "besides microblogs express opinion products microblogs give statements relative products contain sentiment polarity neutral"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013880835210649011, "p": 0.1111111111111111, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.027839812110783763, "p": 0.2, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["product reviews emerge ecommerce sites microblog systems nowadays", "information useful consumers know others opinion products purchasing companies want learn public sentiment products", "order effectively utilize information paper done sentiment analysis multisource reviews", "thing binary classification framework based aspects product proposed", "explicit implicit aspect considered multiple kinds feature weighing classifiers compared framework", "another use several machine learning algorithms classify product reviews microblog systems positive negative neutral classes find ovasvms perform best", "part work paper applied chinese product review mining system", "keywords product review sentiment analysis microblog svm"]}, "N10-1019": {"introduction": ["therefore paper exploit linear regression multiclass classification twostage classification mincut model optimization classify product related microblogs classes compare performance methods", "research automatic correction grammatical errors undergone renaissance past decade", "least part based recognition nonnative speakers english outnumber native speakers estimates tool domain could tremendous value", "earlier work native nonnative error correction focused construction grammars analysis systems detect correct specific errors see heift schulze detailed overview recent approaches based datadriven methods", "majority datadriven methods use classification technique determine whether word used appropriately context continuing tradition established contextual spelling correction golding golding roth", "words investigated typically articles prepositions", "distinct advantages subject matter investigation closed class comprise substantial proportion learners errors", "investigation preposition corrections even narrowed amongst english prepositions usage frequent prepositions accounts preposition errors million word cambridge university press learners corpus", "learning correct article use difficult native speakers l1 overtly mark definiteness indefiniteness english", "prepositions hand pose difficulties language learners l1 backgrounds dalgish bitchener et al", "contextual classification methods represent context preposition article feature vector gleaned window words around prepositionarticle", "different systems typically vary along dimensions choice features choice classifier choice training data", "features range words morphological information knight chander inclusion partofspeech tags minnen et al han et al chodorow et al gamon et al izumi et al tetrault chodorow features based linguistic analysis wordnet lee defelice pulman", "knight chander gamon et al", "used decision tree classifiers general maximum entropy classifiers become classification human language technologies annual conference north american chapter acl pages los angeles california june", "qc association computational linguistics algorithm choice", "training data normally drawn sizeable corpora native english text british national corpus defelice pulman wall street journal knight chander mix reuters encarta gamon et al", "", "order partially address problem domain mismatch learners writing newsheavy data sets often used datadriven nlp applications han et al", "use million words metametrics corpus diverse corpus fiction nonfiction textbooks categorized reading level", "addition classification approach error detection line research going back least atwell uses language models", "idea detect errors areas language model score suspiciously low", "atwell uses partofspeech tag language model detect errors chodorow leacock use mutual information chi square statistics identify unlikely function word partofspeech tag sequences turner charniak employ language model based generative statistical parser stehouwer van zaanen investigate diverse set language models different backoff strategies determine choice set confusable words likely given context", "gamon et al", "use combination errorspecific classifiers large generic language model hand tuned heuristics combining scores maximize precision", "finally yi et al", "met et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016996137078036107, "p": 0.04, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015682206492562606, "p": 0.25, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present results range experiments article preposition error correction nonnative speakers english", "first compare language model error specific classifiers trained large english corpora respect performance error detection correction", "combine language model classifiers metaclassification approach combining evidence classifiers language model input features metaclassifier", "metaclassifier turn trained errorannotated learner data optimizing error detection correction performance domain", "metaclassification approach results substantial gains classifier languagemodelonly scenario", "since metaclassifier requires errorannotated data training investigate much training data needed improve results baseline using metaclassifier", "evaluations conducted large error annotated corpus learner english"]}, "N10-1040": {"introduction": ["use ngram counts web language model approximation identify likely errors correction candidates", "recently number successful attempts improving phrasebased statistical machine translation exploiting linguistic knowledge morphology partofspeech tags syntax", "many translation models use knowledge decoding xia mccord decoding birch et al gimpel smith koehn hoang chiang et al limited simpler features practical reasons often restricted conditioning leftto right target sentence", "traditionally nbest rerankers shen et al applied expensive analysis translation process source target side though suffer limited whatever nbest list hasan et al", "argue desirable pretranslate parts source text sentencelevel decoding begins using richer model would typically reach sentencelevel decoding", "paper describe particular method generating additional bilingual phrase pairs new source text using call phrase prototypes learned bilingual training data", "goal generate improved translations relatively short phrase pairs provide smt decoder better phrasal choices", "validate idea experiments arabicenglish translation"], "introduction_label": [{"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028964248420955387, "p": 0.03225806451612903, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030160435101406875, "p": 0.05, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}], "abstract": ["investigate methods generating additional bilingual phrase pairs phrase based decoder translating short sequences source text", "translation task constrained use model employs linguistically rich features traditional decoder", "implemented example approach", "experimental results suggest phrase pairs produced method useful decoder lead improved sentence translations"]}, "N12-1006": {"introduction": ["method produces bleu score increase relative test set", "arabic language wellknown example diglossia ferguson formal variety language taught schools used written communication formal speech religion politics etc differs significantly grammatical properties informal varieties acquired natively used mostly verbal communication", "spoken varieties arabic language refer collectively dialectal arabic differ widely depending geographic distribution socioeconomic conditions speakers diverge formal variety known modern standard arabic msa embarki ennaji", "significant differences phonology morphology lexicon even syntax render varieties mutually incomprehensible", "use dialectal arabic traditionally confined informal personal speech writ ing done almost exclusively using msa ancestor classical arabic", "situation quickly changing rapid proliferation social media arabicspeaking part world much communication composed dialect", "focus arabic nlp research community mostly msa turning towards dealing informal communication introduction darpa bolt program", "new focus presents new challenges obvious lack dialectal linguistic resources", "dialectal text usually user generated noisy lack standardized orthography means users often improvise spelling", "dialectal data includes wider range topics formal data genres newswire due informal nature", "challenges require innovative solutions nlp applications deal dialectal arabic effectively", "paper describe process cheaply quickly developing parallel corpora levantineenglish egyptianenglish using amazons mechanical turk crowdsourcing service", "use data perform variety machine translation experiments showing impact morphological analysis limited value adding msa parallel data usefulness crossdialect training effects translating dialect msa english", "find collecting dialect translations low cost 003word relatively small amounts data dramatic impact translation quality", "trained 15m words dialectal data system performs bleu points higher trained times msa data mismatching domain", "conference north american chapter association computational linguistics human language technologies pages", "montreal canada june"], "introduction_label": [{"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024204142314291258, "p": 0.03225806451612903, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["arabic dialects present many challenges machine translation least lack data resources", "use crowdsourcing cheaply quickly build levantineenglish egyptianenglish parallel corpora consisting 11m words 380k words respectively", "dialectal sentences selected large corpus arabic web text translated using amazons mechanical turk", "use data build dialectal arabic mt systems find small amounts dialectal data dramatic impact translation quality", "translating egyptian levantine test sets dialectal arabic mt system performs bleu points higher modern standard arabic mt system trained 150mword arabicenglish parallel corpus"]}, "N13-1093": {"introduction": ["qc association computational linguistics", "negation linguistic phenomenon negation cue eg alter meaning particular text segment fact", "text segment fact said inside scope negation cue", "context much work aims exploit scope negations1 work aware sanchezgraillet poesio used various heuristics extract negative protein interaction", "despite recent interest automatically detecting scope negation2 till seems empirical evidence supporting exploitation purpose", "even could manage obtain highly accurate automatically detected context event extraction closely related task", "efforts bionlp shared tasks nonmandatory subtask event negation detection participants kim et al kim et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05994511817304724, "p": 0.1111111111111111, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}], "abstract": ["paper presents approach exploits scope negation cues relation extraction without need using specifically annotated dataset building separate negation scope detection classifier", "new features proposed used different stages", "include nontarget entity specific features", "proposed approach outperforms previous state art drugdrug interaction ddi extraction"]}, "N13-1095": {"introduction": ["participants approached subtask using either predefined patterns heuristics", "relation extraction wellstudied problem miller et al zhou et al kambhatla min et al 2012a", "recently distant supervision ds craven kumlien mintz et al emerged popular choice training relation extractors without using manually labeled data", "automatically generates training examples labeling relation mentions1 source corpus according whether argument pair listed target relational tables knowledge base kb", "method significantly reduces human efforts relation extraction", "labeling heuristic serious flaw", "knowledge bases usually highly incomplete", "exam occurrence pair entities source sentence", "ple persons freebase2 place birth nationality section", "previous work typically assumes argument entity pair listed kb relation corresponding relation mentions considered negative examples3 crude assumption labeled many entity pairs negative fact mentions express relation", "number false negative matches even exceeds number positive pairs times leading significant problem training", "previous approaches riedel et al hoffmann et al surdeanu et al bypassed problem heavily undersampling negative class", "instead deal learning scenario entitypair level labels either positive unlabeled", "proposed extension surdeanu et al", "train dataset", "contribution includes analysis incompleteness freebase false negative match rate datasets labeled examples generated ds"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021967761463579497, "p": 0.2, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02431229116640058, "p": 0.037037037037037035, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["distant supervision heuristically labeling corpus using knowledge base emerged popular choice training relation extractors", "paper show significant number negative examples generated labeling process false negatives knowledge base incomplete", "therefore heuristic generating negative examples serious flaw", "building stateoftheart distantlysupervised extraction algorithm proposed algorithm learns positive unlabeled labels pairofentity level", "experimental results demonstrate advantage existing algorithms"]}, "N13-1104": {"introduction": ["experimental results realistic challenging dataset demonstrate advantage algorithm existing solutions", "events causal temporal relations tend occur near text", "example bombing scenario article terrorism might begin detonation event terrorists set bomb", "damage event might ensue describe resulting destruction casualties followed investigation event research undertaken authors internship microsoft research", "covering subsequent police investigations", "afterwards bombing scenario transition criminalprocessing scenario begins police catching terrorists proceeds trial sentencing etc common set participants serves event arguments eg agent subject detonation often theme object investigation corresponds perpetrator", "structures formally captured notion frame aka template scenario consists set events prototypical transitions well set slots representing common participants", "identifying frames explicit implicit prerequisite many nlp tasks", "information extraction example stipulates types events slots extracted frame template", "online applications dialogue systems personalassistant applications model users goals subgoals using framelike representations", "naturallanguage generation frames often used represent contents expressed well support surface realization", "recently frames related representations manually constructed limited applicability relatively small number domains slots domain", "furthermore additional manual effort needed frames defined order extract frame components text eg annotating examples designing features train supervised learning model", "paradigm makes generalizing tasks difficult might suffer annotator biasrecently increasing interest au proceedings naaclhlt pages atlanta georgia june", "qc association computational linguistics tomatically inducing frames text", "notable example chambers jurafsky first clusters related verbs form frames clusters verbs syntactic arguments identify slots", "chambers jurafsky represents major step forward frame induction limited several aspects", "clustering used ad hoc steps customized similarity metrics well additional retrieval step large external text corpus slot generation", "makes hard replicate approach adapt new domains", "lacking coherent model difficult incorporate additional linguistic insights prior knowledge", "paper present profinder probabilistic frame inducer first probabilistic approach frame induction", "profinder defines joint distribution words document frame assignments modeling frame event transitions correlations events slots surface realizations", "given set documents profinder outputs set induced frames learned parameters well probable frame assignments used event entity extraction", "numbers events slots dynamically determined novel application splitmerge approach syntactic parsing petrov et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016568588454494517, "p": 0.05263157894736842, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01649401438943414, "p": 0.05555555555555555, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015945641929930903, "p": 0.1, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01649401438943414, "p": 0.05555555555555555, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016716714872705825, "p": 0.047619047619047616, "r": 0.015625}}], "abstract": ["naturallanguage discourse related events tend appear near describe larger scenario", "structures formalized notion frame aka template comprises set related events prototypical participants event transitions", "identifying frames prerequisite information extraction natural language generation usually done manually", "methods inducing frames proposed recently typically use ad hoc procedures difficult diagnose extend", "paper propose first probabilistic approach frame induction incorporates frames events participants latent topics learns frame event transitions best explain text", "number frame components inferred novel application splitmerge method syntactic parsing", "endtoend evaluations text induced frames extracted facts method produces stateoftheart results substantially reducing engineering effort"]}, "N13-1110": {"introduction": ["endtoend evaluations text entity extraction using standard muc tac datasets profinder achieved stateoftheart results significantly reducing engineering effort requiring external data", "repetition common coreferential devices written text making stringmatch features important coreference resolution systems", "fact scores achieved head match rudimentary form pronominal resolution1 far stateoftheart systems recasens hovy", "suggests opaque mentions ie lexically different ipad cupertino slate serious problem modern systems comprise nonpronominal closest np gender number", "errors made stanford system conll data", "solving problem critical overcoming recall gap stateoftheart systems haghighi klein stoyanov et al", "previous systems turned either ontologies ponzetto strube uryupina et al rahman ng distributional semantics yang su kobdani et al bansal klein help solve errors", "neither semantic similarity hypernymy coreference microsoft google distributionally similar coreferent people hypernym voters scientists people corefer voters less likely corefer scientists", "thus ontologies lead precision problems recall problems like missing ne descriptions eg apple iphone maker metonymies eg agreement wording distributional systems lead precision problems like coreferring microsoft mountain view giant similar vector representation release software update", "increase precision drawing intuition referents similar participate event likely corefer", "restrict dis tributional similarity collections articles discuss event", "following documents nexus different sources take subjects identical verb release google mountain view giantas coreferent", "document google released software update", "document mountain view giant released update", "based idea introduce new unsupervised method uses verbs comparable corpora proceedings naaclhlt pages atlanta georgia june", "qc association computational linguistics pivots extracting hard cases coreference resolution build dictionary opaque coreferent mentions ie dictionary entries pairs mentions", "dictionary integrated stanford coreference system lee et al resulting average improvement f1 score evaluation measures", "work points importance context decide whether specific mention pair coreferent", "hand need know semantic relations potentially coreferent eg content video", "need distinguish contexts compatible coreference1 2a not1 2b", "elemental helps big media entities process content full slate mobile devices", "elemental provides picks shovels make video work multiple devices", "elemental powering video hbo go"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}], "abstract": ["coreference resolution systems rely heavily string overlap eg google inc google performing badly mentions different words opaque mentions like google search giant", "prior attempts resolve opaque pairs using ontologies distributional semantics hurt precision improved recall", "present new unsupervised method mining opaque pairs", "intuition restrict distributional semantics articles event thus promoting referential match", "using english comparable corpus tech news built dictionary opaque coreferent mentions wordnet", "dictionary integrated coreference system increases performance stateoftheart system f1 measures easily extendable using news aggregators"]}, "N13-1118": {"introduction": ["dictionary opaque coreferent pairs solution first problem report preliminary work context compatibility address second problem", "metaphor traditionally viewed form linguistic creativity gives expression vividness distinction artistism", "true surface mechanisms metaphor much deeper origin reasoning", "today metaphor widely understood cognitive phenomenon operating level mental processes whereby concept domain systematically viewed terms properties another lakoff johnson", "consider examples shot arguments attacked every weak point argument", "demonstrate metaphorical mapping concept argument war", "argument target concept viewed terms battle war source concept", "existence link allows us systematically describe arguments using war terminology thus leading number metaphorical expressions", "lakoff johnson call generalisations sourcetarget domain mapping conceptual metaphor", "ubiquity metaphor language established number corpus studies cameron martin steen et al shutova teufel role plays human reasoning confirmed psychological experiments thibodeau boroditsky", "makes metaphor important research area computational cognitive linguistics automatic processing indispensable semantics oriented nlp application", "problem metaphor modeling gaining interest nlp growing number approaches exploiting statistical techniques mason gedigian et al shutova shutova et al turney et al shutova et al", "compared traditional approaches based handcoded knowledge fass martin narayanan narayanan feldman narayanan barnden lee agerri et al recent methods tend wider coverage well efficient accurate robust", "even statistical metaphor processing approaches far often focused limited domain subset phenomena gedigian et al krishnakumaran zhu addressed metaphor processing sub tasks identification metaphorical mappings mason identification metaphorical expressions shutova et al turney et al", "paper present first computational method proceedings naaclhlt pages atlanta georgia june", "qc association computational linguistics identifies generalisations govern production metaphorical expressions ie conceptual metaphors uses generalisations identify metaphorical expressions unrestricted text", "opposed previous works statistical metaphor processing supervised semisupervised thus required training data method fully unsupervised", "relies building hierarchical graph concepts connected association strength using hierarchical clustering searching metaphorical links graph", "shutova et al", "introduced hypothesis clustering association claimed course distributional noun clustering abstract concepts tend cluster together associated source domain concrete concepts cluster meaning similarity", "share intuition take idea significant step", "approach theoretically grounded cognitive science findings suggesting abstract concrete concepts organised differently human brain crutch warrington binder et al wiemerhastings xu huang et al crutch warring ton adorni proverbio", "according crutch warrington differences emerge general patterns relation concepts", "nlp systems date treat abstract concrete concepts identical", "contrast incorporate distinction model creating network graph concepts automatically learning different patterns association abstract concrete concepts concepts", "expect concrete concepts would tend naturally organise treelike structure specific terms descending general terms abstract concepts would exhibit complex pattern associations", "consider example figure", "figure schematically shows small portion graph describing concepts mechanism concrete political system relationship abstract levels generality", "see graph concrete concepts bike engine tend connected concept higher level hierarchy mechanism abstract concepts multiple higherlevel associates literal ones metaphorical ones", "ex ample abstract concept democracy literally associated general concept political system well metaphorically associated concept mechanism", "multiple associations due fact political systems metaphorically viewed mechanisms function break oiled etc often discuss using mechanism terminology thus corpusbased distributional learning approach would learn share features political systems literal uses well mechanisms metaphorical uses shown next respective graph edges figure", "system discovers association patterns graph uses identify metaphorical connections concepts", "best knowledge method first use hierarchical clustering model metaphor processing task", "original graph concepts built using hierarchical graph factorization clustering hgfc yu et al nouns yielding network clusters different levels generality", "weights edges graph indicate association clusters concepts", "hgfc previously employed noun clustering nlp showed successful results verb clustering task sun korhonen", "summary system builds graph concepts using hgfc traverses find metaphorical associations clusters using weights edges graph generates lists salient features metaphorically connected clusters searches british national corpus bnc burnard metaphorical expressions describing target domain concepts using verbs set salient features", "evaluated performance system aid human judges precision recalloriented settings"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11428570997551038, "p": 0.08333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04023897668409529, "p": 0.07692307692307693, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019246675387691467, "p": 0.0625, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.08333332980034738, "p": 0.05405405405405406, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04000067999437927, "p": 0.04878048780487805, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019641577061035545, "p": 0.05, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.060227889310749856, "p": 0.075, "r": 0.05454545454545454}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}], "abstract": ["present novel approach automatic metaphor identification discovers metaphorical associations metaphorical expressions unrestricted text", "system first performs hierarchical graph factorization clustering hgfc nouns searches resulting graph metaphorical connections concepts", "makes use salient features metaphorically connected clusters identify actual metaphorical expressions", "contrast previous work method fully unsupervised", "despite fact operates encouraging precision recall", "approach first nlp exploit cognitive findings differences ganisation abstract concrete concepts human brain"]}, "N15-1071": {"introduction": ["addition compared performance baselines unsupervised baseline using agglomerative clustering agg supervised baseline built upon wordnet fellbaum wn", "sentiment classification find general tendency documentlevel classification towards finegrained approaches yield detailed appraisal judgement performed text particular using composition syntactic structure get detailed approach phrases", "english movie reviews work using stanford sentiment treebank sstb shown subsentential sentiment information yield approaches high accuracy socher et al dong et al hall et al precise information role phrase information subsequently used extracting summarizing sentiment expressed text", "effort creating sentiment treebank sstb seems prohibitive wanted create resource pair relevant domain language compared documentlevel annotations sentiment easy come eg star ratings annotating individual syntactic phrases requires considerable effort", "main focus paper question possible reach sensible performance compositional sentiment classification limited resources spend inlanguage indomain sentiment treebank", "goal use new resource heidelberg sentiment treebank heist germanlanguage counterpart stanford sentiment treebank sense makes explicit composition sentiment expression syntactic phrases", "experiments heist provide direct comparison different techniques harnessing crosslingual crossdomain crosstask information first kind specifically target compositional sentiment analysis"], "introduction_label": [{"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.037496340846416315, "p": 0.04, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09756097168352187, "p": 0.06666666666666667, "r": 0.18181818181818182}, "rouge-l": {"f": 0.058002707486556136, "p": 0.05263157894736842, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.10526315378116359, "p": 0.07407407407407407, "r": 0.18181818181818182}, "rouge-l": {"f": 0.06471004854053988, "p": 0.06060606060606061, "r": 0.07142857142857142}}], "abstract": ["sentiment analysis undergone shift documentlevel analysis labels expresses sentiment whole document whole sentence subsentential approaches assess contribution individual phrases particular including composition sentiment terms phrases negators intensifiers", "starting small sentiment treebank modeled stanford sentiment treebank socher et al", "investigate suitable methods perform compositional sentiment classification german datascarce setting harnessing crosslingual methods well existing generaldomain lexical resources"]}, "P01-1005": {"introduction": ["figure next page shows schematic overview experiments beyond supervised baseline experiments using svm classification supervised rntn model section evaluated cross lingual projection section lexiconbased approaches section well semisupervised approaches based word clusters section", "machine learning techniques automatic ally learn linguistic information online text corpora applied number natural language problems throughout last decade", "large percentage papers published area involve comparisons different learning approaches trained tested commonly used corpora", "amount available online text increasing dramatic rate size training corpora typically used learning", "part due standardization data sets used field well potentially large cost annotating data learning methods rely labeled text", "empirical nlp community put substantial effort evaluating performance large number machine learning methods fixed relatively small data sets", "since access significantly data wonder conclusions drawn small data sets carry learning methods trained using much larger corpora", "paper present study effects data size machine learning natural language disambiguation", "particular study problem selection confusable words using orders magnitude training data ever applied problem", "first show learning curves different machine learning algorithms"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["amount readily available online text reached hundreds billions words continues grow", "core natural language tasks algorithms continue optimized tested compared training corpora consisting million words less", "paper evaluate performance different learning methods prototypical natural language disambiguation task confusion set disambiguation trained orders magnitude labeled data previously used", "fortunate particular application correctly labeled training data free", "since often case examine methods effectively exploiting large corpora labeled data comes cost"]}, "P03-1028": {"introduction": ["next consider efficacy voting sample selection partially unsupervised learning large training corpora hopes able obtain benefits come significantly larger training corpora without incurring large cost", "explosive growth online texts written natural language prompted much research information extraction ie task automatically extracting specific information items interest natural language texts", "extracted information used fill database records known templates ie literature", "research efforts ie tackle variety tasks", "include extracting information semi structured texts seminar announcements rental job advertisements etc well free texts newspaper articles soderland", "ie semistructured texts easier free texts since layout format semistructured text provide additional useful clues ayacucho jan today people wounded bomb exploded san juan bautista municipality", "officials said shining path members responsible attack", "police sources stated bomb attack involving shining path caused serious damages", "figure snippet muc4 document aid extraction", "several benchmark data sets used evaluate ie approaches semi structured texts soderland ciravegna chieu ng 2002a", "task extracting information free texts series message understanding conferences muc provided benchmark data sets evaluation", "several subtasks ie free texts identified", "named entity ne task extracts person names organization names location names etc template element te task extracts information centered around entity like acronym category location company", "template relation tr task extracts relations entities", "finally fullscale ie task scenario template st task deals extracting generic information items free texts", "tackle full st task ie system needs merge information multiple sentences general since information needed fill template come multiple sentences thus discourse processing needed", "fullscale st task considerably harder ie tasks subtasks outlined", "case many natural language processing nlp tasks main approaches ie namely knowledgeengineering approach learning approach", "early ie systems adopted knowledgeengineering ap message id tst3muc40014"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper present learning approach scenario template task information extraction information filling template could come multiple sentences", "tested muc4 task learning approach achieves accuracy competitive best muc4 systems built manually engineered rules", "analysis reveals use full parsing stateoftheart learning algorithms contributed good performance", "knowledge first research demonstrated learning approach fullscale information extraction task could achieve performance rivaling knowledge engineering approach"]}, "P04-1046": {"introduction": ["message template", "paper deals definition verbpredicates make possible determination verb meaning semantic roles adjuncts attachment meaning postverbal pps", "hence adequacy defini tions measured comparing output asemantic interpretation algorithm lution semantic interpretation tasks sentences randomly taken corpus typed user console", "algorithmthus must provide immediate feedback test ing definitions randomly selected sentences", "gomez generic predicates defined wordnet henceforthwn verb classes fellbaum", "se mantic roles predicates linked selectional restrictions categories wordnetontology nouns grammatical relations realize", "selectional restric tions predicates solidly grounded wordnet ontology nouns miller whose upper level ontology modified rearranged gomez based thefeedback obtained testing predicate def initionshowever found ini tial idea work defining predicates wn verb class valid verbs class proven optimistic many verb forms included class realize theirsemantic roles different selectional restric tions grammatical relations", "due fact many verbs given class grouped many instances bysome kind implication troponymy fellbaum rather sharing seman tic roles hierarchy predicates", "even many cases verbs synset list differ semantically syntactically", "instance third sense ampquotgainampquot wn ampquotprofit gain benefit derive benefit fromampquot theme thing obtained syntactically realized pp npfor verbs listed synset ampquotbene fitampquot ampquotprofitampquot theme ampquotgainampquot realized direct object", "differences grammatical relations even prevailing verbs given class", "cases verb polysemy high predicate definition verbclass apply many verb forms un der class", "notwithstanding problems wn verb classes provided important basis construction general ontology predicates cover every english verb", "moreover considers verbs wordnet sense and2199 verbs exactly senses pred icates constructed verb classes verbs close done", "method explained paper deviates considerably wn approach constructing verb meaning", "particular iteschews synset list favor predicate def initions individual verbs opposed listof synonymous verbs allows easy tegration new predicate list already defined predicates given verb", "briefly algorithm gomez tests predicates follows", "every verb sentence provide list predicates verb", "predicates viewed contenders meaning verb", "goals algorithm select predicate list thus determining sense verb identify semantic roles adjuncts attach postverbal pps", "tasks aresimultaneously achieved", "grammati cal relation cr clause starting np complements every predicate list predicates algorithm checks predicate explains cr", "predicate explainsa cr semantic role predi cate realized grammatical relation selectional restrictions semantic role subsume ontological category head noun grammatical relation", "process repeated cr clause predicate list predicates verb clause", "predicate explains crs selected meaning verb", "semantic roles predicate identified result process", "case ties predicate greatest number semantic roles realized selected", "every grammatical relation mapped semantic role must adjunct np modifier", "entries adjuncts stored root node action description stative verbs inherited predicates categories", "adjuncts identified predicate verb determined adjuncts part argument structure predicatein next section show de fine predicates individual verbs different wn verb classes predicates individual verbs reuse entries generic predicates wordnet verb classes howthey integrated ontology pred icates defined help ofwn verb classes", "section provides discus sion semiautomatic construction thepredicates"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016005313597158736, "p": 0.09090909090909091, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.03571428255739824, "p": 0.022222222222222223, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016799628242545626, "p": 0.018867924528301886, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["method definition verb predicates proposed", "definition predicates isessentially tied semantic interpretation algorithm determines predicate theverb semantic roles adjuncts", "predicate definitions complete tested running algorithm sentences andverifying resolution predicate semantic roles adjuncts sentences", "predicates defined semiautomatically help software environment uses several sections corpus provide feedback definition predicates forthe subsequent testing refining definitions", "method flexible adding anew predicate list already defined predicates given verb", "method builds existing approach defines predicates wordnet verb classes plans definepredicates every english verb", "definitions predicates semantic interpretation algorithm used automatically create corpus annotated verb predicates semantic roles adjuncts"]}, "P05-1004": {"introduction": ["section gives view upper level ontology predicates section discusses testing sections related research conclusions respectively", "limited coverage lexicalsemantic resources significant problem nlp systems alleviated automatically classifying unknown words", "supersense tagging assigns unknown nouns broad semantic categories used lexicographers organise manual insertion wordnet", "ciaramita johnson present tagger uses synonym set glosses annotated training examples", "describe unsupervised approach based vectorspace similarity require annotated examples significantly outperforms tagger"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "P05-1020": {"introduction": ["demonstrate use extremely large shallowparsed corpus calculating vectorspace semantic similarity", "recent research coreference resolution problem determining noun phrases nps text dialogue refer realworld entity exhibited shift knowledge based approaches datadriven approaches yielding learningbased coreference systems rival handcrafted counterparts performance eg soon et al", "ng cardie 2002b strube et al", "yang et al", "luo et al", "", "central idea behind majority learning based approaches recast coreference resolution binary classification task", "specifically classifier first trained determine whether nps document coreferring", "separate clustering mechanism coordinates possibly contradictory pairwise coreference classification decisions constructs partition given set nps cluster set coreferent nps", "though reasonably successful standard approach robust think", "first de sign decisions choice learning algorithm clustering procedure apparently critical system performance often made adhoc unprincipled manner suboptimal empirical point view", "second approach makes attempt search space possible partitions given set nps clustered employing instead greedy clustering procedure construct partition far optimal", "another potential weakness approach concerns inability directly optimize clustering level accuracy coreference classifier trained optimized independently clustering procedure used hence improvements classification accuracy guarantee corresponding improvements clusteringlevel accuracy", "goal paper improve robustness standard approach addressing weaknesses", "specifically propose following procedure coreference resolution given set nps clustered use preselected learning based coreference systems generate candidate partitions nps apply automatically acquired ranking model rank candidate hypotheses selecting best final partition", "key features approach minimal human decision making", "contrast standard approach method obviates large extent need make tough potentially suboptimal design decisions1 instance still need determine coreference systems employed framework", "fortunately choice flexible large want subject proceedings 43rd annual meeting acl pages ann arbor june", "qc association computational linguistics cannot decide whether learner better use learner coreference system simply create copies system employing add pre selected set coreference systems", "generation multiple candidate partitions", "although exhaustive search best partition computationally feasible even document moderate number nps approach explores larger portion search space standard approach via generating multiple hypotheses making possible find potentially better partition nps consideration", "optimization clusteringlevel accuracy via ranking", "mentioned standard approach trains optimizes coreference classifier without necessarily optimizing clusteringlevel accuracy", "contrast attempt optimize ranking model respect target coreference scoring function essentially training way higher scored candidate partition according scoring function would assigned higher rank see section details", "perhaps even importantly approach provides general framework coreference resolution", "instead committing particular resolution method previous approaches framework makes possible leverage strengths different methods allowing participate generation candidate partitions", "evaluate approach standard coreference data sets using different scoring metrics"], "introduction_label": [{"rouge-1": {"f": 0.04761904375283478, "p": 0.03225806451612903, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03235502426995308, "p": 0.030303030303030304, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.049999996012500325, "p": 0.034482758620689655, "r": 0.09090909090909091}, "rouge-l": {"f": 0.033040935671900556, "p": 0.03125, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper view coreference resolution problem ranking candidate partitions generated different coreference systems", "propose set partitionbased features learn ranking model distinguishing good bad partitions", "approach compares favorably stateoftheart coreference systems evaluated standard coreference data sets"]}, "P05-1021": {"introduction": ["experiments approach compares favorably stateoftheart coreference systems adopting standard machine learning approach outperforming much data sets performance metrics", "semantic compatibility important factor pronoun resolution", "since pronouns especially neutral pronouns carry little semantics compatibility anaphor antecedent candidate commonly evaluated examining relationships candidate anaphors context based statistics corresponding predicateargument tuples occur particular large corpus", "consider example given work dagan itai know full well companies held tax money aside collection later basis government said it1 going collect it2", "anaphor it1 candidate government higher semantic compatibility money government collect supposed occur frequently money collect large corpus", "similar pattern could observed it2", "far corpusbased semantic knowledge successfully employed several anaphora resolution systems", "dagan itai proposed heuristicsbased approach pronoun resolution", "determined preference candidates based predicateargument frequencies", "recently bean riloff presented unsupervised approach coreference resolution mined coreferring np pairs similar predicate arguments large corpus using bootstrapping method", "utility corpusbased semantics pronoun resolution often argued", "kehler et al", "example explored usage corpusbased statistics supervised learning based systems found information produce apparent improvement overall pronoun resolution", "indeed existing learningbased approaches anaphor resolution performed reasonably well using limited shallow knowledge eg mitkov soon et al", "strube muller", "could relatively noisy semantic knowledge give us system improvement", "paper focus improving pronominal anaphora resolution using automatically computed semantic compatibility information", "propose enhance utility statisticsbased knowledge aspects statistics source", "corpusbased knowledge usually suffers data sparseness problem", "many predicateargument tuples would unseen even large corpus", "possible solution proceedings 43rd annual meeting acl pages ann arbor june", "qc association computational linguistics web", "believed size web thousands times larger normal large corpora counts obtained web highly correlated counts large balanced corpora predicateargument bigrams keller lapata", "far web utilized nominal anaphora resolution modjeska et al poesio et al determine semantic relation anaphor candidate pair", "knowledge using web help pronoun resolution still remains unexplored", "learning framework", "commonly predicate argument statistics incorporated anaphora resolution systems feature", "kind learning framework suitable feature", "previous approaches anaphora resolution adopt single candidate model resolution done anaphor candidate time soon et al ng cardie", "purpose predicateargument statistics evaluate preference candidates semantics possible statisticsbased semantic feature could effectively applied twin candidate yang et al focusses preference relationships candidates", "work explore acquisition semantic compatibility information corpus web incorporation semantic information singlecandidate model twincandidate model", "systematically evaluate combinations different statistics sources learning frameworks terms effectiveness helping resolution", "results muc data set show neutral pronoun resolution anaphor specific semantic category webbased semantic information would effective applied twincandidate model could system significantly improve baseline without semantic feature outperforms system combination corpus singlecandidate model success", "rest paper organized follows"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705444636694, "p": 0.08695652173913043, "r": 0.18181818181818182}, "rouge-l": {"f": 0.06042661572744894, "p": 0.08695652173913043, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030160435101406875, "p": 0.05, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.2068965470154579, "p": 0.16666666666666666, "r": 0.2727272727272727}, "rouge-l": {"f": 0.0899176772595304, "p": 0.16666666666666666, "r": 0.08108108108108109}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027806102439457355, "p": 0.14285714285714285, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1999999950500001, "p": 0.2222222222222222, "r": 0.18181818181818182}, "rouge-l": {"f": 0.056439998443106744, "p": 0.2222222222222222, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}], "abstract": ["paper focus improve pronoun resolution using statistics based semantic compatibility information", "investigate unexplored issues influence effectiveness information statistics source learning framework", "specifically first time propose utilize web twincandidate model addition previous combination corpus singlecandidate model compute apply semantic information", "study shows semantic compatibility obtained web effectively incorporated twincandidate learning model significantly improve resolution neutral pronouns"]}, "P05-1045": {"introduction": ["section describes acquisition semantic com finally section gives conclusion", "statistical models currently used natural language processing represent local structure", "although constraint critical enabling tractable model inference key limitation many tasks since natural language contains great deal non local structure", "general method solving problem relax requirement exact inference substituting approximate inference algorithms instead thereby permitting tractable inference models nonlocal structure", "algorithm gibbs sampling simple monte carlo algorithm appropriate inference factored probabilistic model including sequence models probabilistic context free grammars geman ge man", "although gibbs sampling widely used elsewhere extremely little use natural language processing1 use add nonlocal dependencies sequence models information extraction", "statistical hidden state sequence models hidden markov models hmms leek freitag mccallum conditional markov models cmms borthwick conditional random fields crfs lafferty et al prominent recent approach information extraction tasks", "models encode markov property decisions state particular position sequence depend small local window", "property allows tractable computation viterbi forward backward clique calibration algorithms become intractable without", "information extraction tasks benefit modeling nonlocal structure", "example several authors see section mention value enforcing label consistency named entity recognition ner tasks", "example given figure second occurrence token tanjug mis labeled crfbased statistical ner system looking local evidence unclear whether person organization", "first occurrence tanjug provides ample evidence organization enforcing label consistency system able get right", "show incorporate constraints form crf model using gibbs sampling instead viterbi algorithm inference procedure demonstrate technique yields significant improvements established ie tasks", "prior uses nlp aware include kim et al", "della pietra et al", "abney", "proceedings 43rd annual meeting acl pages ann arbor june", "qc association computational linguistics news agency tanjug reported"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["current statistical natural language processing models use local features permit dynamic programming inference makes unable fully account long distance structure prevalent language use", "show solve dilemma gibbs sampling simple monte carlo method used perform approximate inference factored probabilistic models", "using simulated annealing place viterbi decoding sequence models hmms cmms crfs possible incorporate nonlocal structure preserving tractable inference", "use technique augment existing crfbased information extraction system longdistance dependency models enforcing label consistency extraction template consistency constraints", "technique results error reduction stateoftheart systems established information extraction tasks"]}, "P05-1051": {"introduction": ["airport tanjug said figure example label consistency problem excerpted document conll english dataset", "systems extract relations events document typically perform number types linguistic analysis preparation information extraction", "include name identification classification parsing partial parsing semantic classification noun phrases coreference analysis", "tasks reflected evaluation tasks introduced muc6 named entity coreference template element muc7 template relation", "extraction systems stages analysis arranged sequentially stage using results prior stages generating single analysis gets enriched stage", "provides simple modular organization extraction systemunfortunately stage introduces cer tain level error analysis", "furthermore errors compounded example errors name recognition lead errors parsing", "net result final output relations events quite inaccurate", "paper considers interactions stages exploited reduce error rate", "example results coreference analysis relation identification helpful name classification results relation event extraction helpful coreference", "interactions easily exploited simple sequential model name classification performed beginning pipeline cannot make use results subsequent stages", "even difficult use information implicitly using features used later stages representation used initial stages limitedto address limitations recent sys tems used parallel designs single classifier incorporating wide range features encompasses previously several separate stages kambhatla zelenko et al", "reduce compounding errors sequential design", "leads large feature space makes difficult select linguistically appropriate features particular analysis tasks", "furthermore decisions made parallel becomes much harder express interactions levels analysis based linguistic intuitions", "proceedings 43rd annual meeting acl pages ann arbor june", "qc association computational linguistics order capture interactions explicitly employed sequential design multiple hypotheses forwarded stage next hypotheses reranked pruned using information later stages", "shall apply design show named entity classification improved feedback coreference analysis relation extraction"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04545454170454576, "p": 0.030303030303030304, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02338904656756372, "p": 0.02631578947368421, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024154589372054745, "p": 0.043478260869565216, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["information extraction systems incorporate multiple stages linguistic analysis", "although errors typically compounded stage stage possible reduce errors stage harnessing results stages", "demonstrate using results coreference analysis relation extraction reduce errors produced chinese name tagger", "use nbest approach generate multiple hypotheses reranked subsequent stages processing", "obtained thereby reduction spurious incorrect name tags reduction missed tags"]}, "P05-1053": {"introduction": ["shall show approach capture interactions natural efficient manner yielding substantial improvement name identification classification", "extracting semantic relationships entities challenging", "paper investigates incorporation diverse lexical syntactic semantic knowledge featurebased relation extraction using svm", "study illustrates base phrase chunking information effective relation extraction contributes performance improvement syntactic aspect additional information full parsing gives limited enhancement", "suggests useful information full parse trees relation extraction shallow captured chunking", "demonstrate semantic information wordnet name list used featurebased relation extraction improve performance"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "P06-1016": {"introduction": ["evaluation ace corpus shows effective incorporation diverse features enables system outperform previously bestreported systems ace relation subtypes significantly outperforms tree kernelbased systems fmeasure ace relation types", "dramatic increase amount textual information available digital archives www growing interest techniques automatically extracting information text", "information extraction ie technology ie systems expected identify relevant information usually predefined types text documents certain domain put structured format", "according scope ace program ace current research ie main objectives entity detection tracking edt relation detection characterization rdc event detection characterization edc", "paper focus ace rdc task detects classifies various semantic relations entities", "example want determine whether person location based evidence context", "extraction semantic relationships entities useful applications question answering eg answer query president united states", "major challenge relation extraction due data sparseness problem zhou et al", "largest annotated corpus relation extraction ace rdc corpus shows different subtypestypes relations much unevenly distributed relation subtypes subtype founder type role suffers small amount annotated data", "experimentation paper please see figure shows relation subtypes suffer lack training data fail achieve steady performance given current corpus size", "given relative large size corpus timeconsuming expensive expand corpus reasonable gain performance", "even somehow expend corpus achieve steady performance major relation subtypes still far beyond practice minor sub types given much unevenly distribution different relation subtypes", "various machine learning approaches generative modeling miller et al maximum entropy kambhatla support vector machines zhao grisman zhou et al applied relation extraction task explicit learning strategy proposed deal inherent data sparseness problem caused much uneven distribution different relations", "paper proposes novel hierarchical learning strategy deal data sparseness problem modeling commonality related classes", "organizing various classes hierarchically linear discriminative function determined class top way using perceptron algorithm lowerlevel weight vector derived upperlevel weight vector", "evaluation ace rdc corpus shows hierarchical proceedings 21st international conference computational linguistics 44th annual meeting acl pages sydney july", "qc association computational linguistics strategy achieves much better performance flat strategy least mediumfrequent relations", "shows system based hierarchical strategy outperforms previous bestreported system", "rest paper organized follows", "section presents related work", "section describes hierarchical learning strategy using perceptron algorithm"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019733995308583406, "p": 0.047619047619047616, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01827095812881336, "p": 0.25, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.23529411307958487, "p": 0.3333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.036876641994291845, "p": 0.2857142857142857, "r": 0.03636363636363636}}], "abstract": ["paper proposes novel hierarchical learning strategy deal data sparseness problem relation extraction modeling commonality related classes", "class hierarchy either manually predefined automatically clustered linear dis criminative function determined top way using perceptron algorithm lowerlevel weight vector derived upperlevel weight vector", "upperlevel class normally much positive training examples lowerlevel class corresponding linear discriminative function determined reliably", "upper level discriminative function effectively guide discriminative function learning lowerlevel otherwise might suffer limited training data", "evaluation ace rdc corpus shows hierarchical strategy much improves performance fmeasure least medium frequent relations respectively", "shows system outperforms previous bestreported system fmeasure subtypes using feature set"]}, "P06-1017": {"introduction": ["finally present experimentation section conclude paper section", "relation extraction task detecting classifying relationships entities text", "many machine learning methods proposed address problem eg supervised learning algorithms miller et al zelenko et al culotta soresen kambhatla zhou et al semisupervised learning algorithms brin agichtein gravano zhang unsupervised learning algorithms hasegawa et al", "supervised methods relation extraction perform well ace data require large amount manually labeled relation instances", "unsupervised methods need definition relation types manually labeled data cannot detect relations entity pairs result cannot directly used many nlp tasks since relation type label attached instance clustering result", "considering availability large amount untagged corpora direct usage extracted relations semi supervised learning methods received great attention", "dipre dual iterative pattern relation expansion brin bootstrappingbased system used pattern matching system classifier exploit duality sets patterns relations", "snowball agichtein gravano another system used bootstrap ping techniques extracting relations unstructured text", "snowball shares much common dipre including employment boot strapping framework well use pattern matching extract new candidate relations", "third system approaches relation classification problem bootstrapping top svm proposed zhang", "system focuses ace sub problem rdc extracts various lexical syntactic features classification task", "zhang 2004s method doesnt actually detect laitons performs relation classification entities given known related", "bootstrapping works iteratively classifying unlabeled examples adding confidently classified examples labeled data using model learned augmented labeled data previous iteration", "proceedings 21st international conference computational linguistics 44th annual meeting acl pages sydney july", "qc association computational linguistics found affinity information unlabeled examples fully explored boot strapping process", "recently promising family semisupervised learning algorithm introduced effectively combine unlabeled data labeled data learning process exploiting manifold structure cluster structure data belkin niyogi blum chawla blum et al zhu ghahramani zhu et al", "graphbased semisupervised methods usually define graph nodes represent labeled unlabeled examples dataset edges weighted reflect similarity examples", "wants labeling function satisfy constraints time close given labels labeled nodes smooth whole graph", "expressed regularization framework first term loss function second term regularizer", "methods differ traditional semi supervised learning methods use graph structure smooth labeling function", "best knowledge work done using graph based semisupervised learning algorithms relation extraction", "investigate label propagation algorithm lp zhu ghahramani relation extraction task", "algorithm works representing labeled unlabeled examples vertices connected graph propagating label information vertex nearby vertices weighted edges iteratively finally inferring labels unlabeled examples propagation process converges"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030160435101406875, "p": 0.05, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.26086956022684316, "p": 0.25, "r": 0.2727272727272727}, "rouge-l": {"f": 0.08665355758777259, "p": 0.25, "r": 0.08108108108108109}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029972592299338248, "p": 0.038461538461538464, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["shortage manually labeled data obstacle supervised relation extraction methods", "paper investigate graph based semisupervised learning algorithm label propagation lp algorithm relation extraction", "represents labeled unlabeled examples distances nodes weights edges graph tries obtain labeling function satisfy constraints fixed labeled nodes smooth whole graph", "experiment results ace corpus showed lp algorithm achieves better performance svm labeled examples available performs better bootstrap ping relation extraction task"]}, "P06-1104": {"introduction": ["paper focus ace rdc task1", "goal relation extraction find various predefined semantic relations pairs entities text", "research relation extraction promoted message understanding conferences mucs muc automatic content extraction ace program ace", "according ace program entity object set objects world relation explicitly implicitly stated relationship entities", "example sentence bill gates chairman chief software architect microsoft corporation conveys acestyle relation employmentexec entities bill gates personname microsoft corporation organization", "commercial", "paper address problem relation extraction using kernel methods schlkopf smola", "many featurebased learning algorithms involve dotproduct feature vectors", "kernel methods regarded replacing dotproduct kernel function vectors even objects", "kernel function similarity function satisfying properties symmetric positivedefinite", "recently kernel methods attracting interests nlp study due ability implicitly exploring huge amounts structured features using original representation objects", "example kernels structured natural language data parse tree kernel collins duffy string kernel lodhi et al graph kernel suzuki et al example instances well known convolution kernels1 nlp", "relation extraction typical work kernel methods includes zelenko et al", "culotta sorensen bunescu mooney", "paper presents novel composite kernel explore diverse knowledge relation extraction", "composite kernel consists entity kernel convolution parse tree kernel", "study demonstrates composite kernel effective relation extraction", "alsoshows without need extensive feature en gineering composite kernel capture flat features used previous work exploit useful syntactic structure features effectively", "advantage method composite kernel easily cover knowledge introducing kernels", "evaluation ace corpus shows method outperforms previous best reported methods significantly outperforms previous kernel methods due effective exploration various syntactic features", "rest paper organized follows", "section review previous work", "section discusses composite kernel", "section reports experimental results observations", "section compares method convolution kernels proposed discrete structure", "haussler machine learning field", "framework defines kernel input objects applying convolution subkernels kernels decompositions parts objects", "proceedings 21st international conference computational linguistics 44th annual meeting acl pages sydney july", "qc association computational linguistics previous work viewpoint feature exploration"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02377273381650473, "p": 0.05263157894736842, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407672862, "p": 0.03571428571428571, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021889117043134277, "p": 0.25, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.2499999957031251, "p": 0.4, "r": 0.18181818181818182}, "rouge-l": {"f": 0.06618008856817548, "p": 0.5, "r": 0.06521739130434782}}], "abstract": ["paper proposes novel composite kernel relation extraction", "composite kernel consists individual kernels entity kernel allows entityrelated features convolution parse tree kernel models syntactic information relation examples", "motivation method fully utilize nice properties kernel methods explore diverse knowledge relation extraction", "study illustrates composite kernel effectively capture flat structured features without need extensive feature engineering easily scale include features", "evaluation ace corpus shows method outperforms previous bestreported methods significantly outperforms previous dependency tree kernels relation extraction"]}, "P06-1117": {"introduction": ["conclude work indicate future work section", "last years noticeable effort devoted design lexical resources provide training ground automatic semantic role labelers", "unfortunately systems developed confined scope resource used training", "recent example sense provided conll shared task carreras marquez propbank pb kingsbury palmer role labeling", "systems participated task trained wall street journal corpus wsj tested portions wsj brown corpora", "best fmeasure recorded wsj brown corpus fmeasure dropped", "significant causes performance decay highly ambiguous unseen predicates ie predicates training examples", "problem highlighted results obtained without frame information senseval3 competition litkowski framenet johnson et al role labeling task", "information used systems performance decreases percent points", "quite intuitive semantics many roles strongly depends focused frame", "thus cannot expect good performance new domains information available", "solution problem automatic frame detection", "unfortunately preliminary experiments showed given framenet fn predicateargument structure task identifying associated frame performed good results verb predicates enough training examples becomes challenging otherwise", "predicates belonging new application domains ie included fn especially problematic since training data available", "therefore rely semantic context alternative frame giuglea moschitti", "context wide coverage easily derivable fn data", "good candidate seems intersective levin class ilc dang et al found well predicate resources like pb verbnet vn kipper et al", "paper investigated claim designing semiautomatic algorithm assigns ilcs fn verb predicates carrying several semantic role labeling srl experiments replace frame ilc information", "used support vector proceedings 21st international conference computational linguistics 44th annual meeting acl pages sydney july", "qc association computational linguistics chines vapnik polynomial kernels learn semantic role classification tree kernels moschitti learning frame ilc classification", "tree kernels applied syntactic trees encode subcategorization structures verbs", "means although fn contains types predicates nouns adjectives verbs concentrated verb predicates roles", "results show ilc derived high accuracy fn probank ilc replace frame feature almost loss accuracy srl systems", "time ilc provides better predicate coverage learned corpora eg pb", "remainder paper section summarizes previous work done fn automatic role detection", "explains detail models based exclusively corpus suitable freetext parsing", "section focuses vn pb enhance robustness semantic parser", "section describes mapping frames ilcs whereas section presents experiments support thesis"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021889117043134277, "p": 0.25, "r": 0.021739130434782608}}], "abstract": ["article describes robust semantic parser uses broad knowledge base created interconnecting major resources framenet verbnet propbank", "framenet corpus contains examples annotated semantic roles whereas verbnet lexicon provides knowledge syntactic behavior verbs", "connect verbnet framenet mapping framenet frames verbnet intersective levin classes", "propbank corpus tightly connected verbnet lexicon used increase verb coverage test effectiveness approach", "results indicate model interesting step towards design robust semantic parsers"]}, "P06-1141": {"introduction": ["finally section summarizes conclusions", "named entity recognition ner seeks locate classify atomic elements unstructured text predefined entities names persons organizations locations expressions times quantities monetary values percentages etc particular problem named entity recognitionner systems exploit presence useful information regarding labels assigned long distance given entity", "example labelconsistency constraint text occurrences new york separated tokens would want learner encourage entities get label", "statistical models currently used named entity recognition use sequence models thereby capture local structure", "hidden markov models hmms leek freitag mccallum conditional markov models cmms borthwick mccallum et al conditional random fields crfs lafferty et al successfully employed ner information extraction tasks", "models encode markov property ie labels directly depend labels assigned small window around", "models exploit property tractable computation allows forwardbackward viterbi clique calibration algorithms become tractable", "although constraint essential make exact inference tractable makes us unable exploit nonlocal structure present natural language", "label consistency example nonlocal dependency important ner", "apart label consistency token sequences would like exploit richer sources dependencies similar token sequences", "example shown figure would want encourage einstein labeled person strong evidence albert einstein labeled person", "sequence models unfortu proceedings 21st international conference computational linguistics 44th annual meeting acl pages sydney july", "qc association computational linguistics told albert einstein proved", "seeing einstein figure example label consistency problem", "would like model encourage entities albert einstein einstein get label improve chance labeled person", "nately cannot model due markovian assumption", "recent approaches attempting capture non local dependencies model nonlocal dependencies directly use approximate inference algorithms since exact inference general tractable graphs nonlocal structure", "bunescu mooney define relational markov network rmn explicitly models longdistance dependencies use represent relations entities", "sutton mccallum augment sequential crf skipedges ie edges different occurrences token document", "approaches use loopy belief propagation pearl yedidia et al approximate inference", "finkel et al", "handset penalties inconsistency entity labeling different occurrences text based statistics training data", "employ gibbs sampling geman geman dealing local feature weights nonlocal penalties approximate inference", "present simple twostage approach second crf uses features derived output first crf", "gives us advantage defining rich set features model nonlocal dependencies eliminates need approximate inference since explicitly capture nonlocal dependencies single model like complex existing approaches", "enables us inference efficiently since inference time merely inference time sequential crfs contrast finkel et al", "reported increase running time factor sequential crf gibbs sampling approximate inference"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper shows simple twostage approach handle nonlocal dependencies named entity recognition ner outperform existing approaches handle nonlocal dependencies much computationally efficient", "ner systems typically use sequence models tractable inference makes unable capture long distance structure present text", "use conditional random field crf based ner system using local features make predictions train another crf uses local information features extracted output first crf", "using features capturing nonlocal dependencies document approach yields relative error reduction f1 score stateofthe art ner systems using localinformation alone compared relative error reduction offered best systems exploit nonlocal information", "approach makes easy incorporate nonlocal information documents test corpus gives us error reduction ner systems using localinformation alone", "additionally running time inference inference time sequential crfs much less complicated approaches directly model dependencies approximate inference"]}, "P06-2012": {"introduction": ["approach simpler yields higher f1 scores much computationally efficient existing approaches modeling non local dependencies", "paper address task relation extraction find relationships name entities given context", "many methods proposed deal task including supervised learning algorithms miller et al zelenko et al culotta soresen kambhatla zhou et al semisupervised learning algorithms brin agichtein gravano zhang unsupervised learning algorithm hasegawa et al", "methods supervised learning usually preferred large amount labeled training data available", "timeconsuming laborintensive manually tag large amount training data", "semisupervised learning methods put forward minimize corpus annotation requirement", "semisupervised methods employ bootstrapping framework need predefine initial seeds particular relation bootstrap seeds acquire relation", "often quite difficult enumerate class labels initial seeds decide optimal number", "compared supervised semisupervised methods hasegawa et al", "2004s unsupervised approach relation extraction overcome difficulties requirement large amount labeled data enumeration class labels", "hasegawa et al", "2004s method use hierarchical clustering method cluster pairs named entities according similarity context words intervening named entities", "drawback hierarchical clustering required providing cluster number users", "furthermore clustering performed original high dimensional space induce nonconvex clusters hard identified", "paper presents novel application spectral clustering technique unsupervised relation extraction problem", "works calculating eigenvec tors adjacency graphs laplacian recover submanifold data high dimensional space performing cluster number estimation transformed space defined first eigen vectors", "method help us find nonconvex clusters", "need predefine number context clusters prespecify similarity threshold clusters hasegawa et al", "2004s method", "rest paper organized follows", "section formulates unsupervised relation extraction presents apply spectral clustering proceedings colingacl main conference poster sessions pages sydney july", "qc association computational linguistics technique resolve task", "section reports experiments results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.2499999957031251, "p": 0.4, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0732889432441462, "p": 0.4, "r": 0.07142857142857142}}], "abstract": ["paper presents unsupervised learning approach disambiguate various relations name entities use various lexical syntactic features contexts", "works calculating eigen vectors adjacency graphs laplacian recover submanifold data high dimensionality space performing cluster number estimation eigenvectors", "experiment results ace corpora show spectral clustering based approach outperforms clustering methods"]}, "P06-3008": {"introduction": ["finally give conclusion work section", "rhetorical structure proven useful nlp projects text generation summarization machine translation essay scoring", "automatic discourse parsing remains elusive task despite much rulebased research lexical cues anaphora conjunctions", "parsing machine learning encountered bottleneck due limited resourcesthere english rst treebank publicly available rstannotated german corpus way", "punctuation marks pms proven useful rst annotation well many nlp tasks partofspeech tagging word sense disambiguation nearduplicate detection bilingual alignment eg chuang yeh etc dale noticed role pms determining rhetorical relations", "say study roles english discourse structure", "marcu corstonoliver based automatic discourse parser partially pms orthographical cues", "tsou et al", "chan et al", "use pms disambiguate candidate discourse markers chinese summarization system", "reitter used pms distinguish attribution elaboration relations featurerich svm rhetorical analysis system", "inspired us survey rhetorical patterns around chinese pms provide direct priori scores coarse rhetorical analyzer zhang et al", "hybrid summarization system", "paper organized parts section gives overview chinese rst treebank construction survey syntax main pms corpus colon dash ellipses exclamation mark question mark semicolon", "section reports rhetorical patterns around pms", "section discussion effectiveness pms comparison chinese cue phrases"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04878048387864398, "p": 0.03333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028964248420955387, "p": 0.03225806451612903, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}], "abstract": ["rhetorical structure punctuation helpful discourse processing", "based corpus annotation project paper reports discursive usage chinese punctuation marks news commentary texts colon dash ellipsis exclamation mark question mark semicolon", "rhetorical patterns marks compared patterns around cue phrases general", "results show chinese punctuation marks though fewer number cue phrases easy identify strong correlation certain relations used distinctive indicators nuclearity chinese texts"]}, "P07-1061": {"introduction": ["section summary section directions future work", "article address problem linear topic segmentation consists segmenting documents topically homogeneous segments overlap", "part discourse analysis field received constant interest since initial work domain hearst", "criterion classifying topic segmentation systems kind knowledge depend", "rely surface features documents word reiteration hearst choi utiyama isahara galley et al discourse cues passonneau lit man galley et al", "systems require external knowledge sensitive domains limited type documents applied lexical reiteration reliable concepts frequently ex pressed several means synonyms etc discourse cues often rare corpusspecific", "overcome difficulties systems make use domainindependent knowledge lexical cohesion lexical network built dictionary kozima thesaurus morris hirst large set lexical co occurrences collected corpus choi et al", "certain extent lexical networks enable topic segmenters exploit sort concept reiteration", "lack explicit topical structure makes kind knowledge difficult use lexical ambiguity high", "simple solution problem exploit knowledge topics occur documents", "topic models generally built large set example documents yam ron et al blei moreno component beeferman et al", "statistical topic models enable segmenters improve precision restrict scope", "hybrid systems combine approaches presented developed illustrated interest combination job bins evett combined word recurrence cooccurrences thesaurus beeferman et al relied lexical modeling discourse cues galley et al made use word reiteration lexical chains discourse cues", "work report article takes place first category presented", "rely priori knowledge exploits word usage rather discourse cues", "precisely present new method enhancing results proceedings 45th annual meeting association computational linguistics pages prague czech republic june"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024296853244250034, "p": 0.038461538461538464, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02431229116640058, "p": 0.037037037037037035, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022279453846826853, "p": 0.125, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}], "abstract": ["topic segmentation identification often tackled separate problems whereas part topic analysis", "article study topic identification help improve topic segmenter based word reiteration", "first present unsupervised method discovering topics text", "detail topics used segmentation finding topical similarities text segments", "finally show results evaluation done french english interest method propose"]}, "P07-1066": {"introduction": ["qc association computational linguistics segmentation systems based word reiteration without relying external knowledge", "language model adaptation crucial numerous speech translation tasks enables higher level contextual information effectively incorporated background lm improving recognition translation performance", "approach employ latent semantic analysis lsa capture indomain word unigram distributions integrated background ngram lm", "approach successfully applied automatic speech recognition asr tam schultz using latent dirichlet allocation lda blei et al", "lda model viewed bayesian topic mixture model topic mixture weights drawn dirichlet distribution", "lm adaptation topic mixture weights estimated based indomain adaptation text eg asr hypotheses", "adapted mixture weights used interpolate topic dependent unigram lm finally integrated background ngram lm using marginal adaptation kneser et al paper propose framework perform lm adaptation languages enabling adaptation lm language based adaptation text another language", "statistical machine translation smt approach apply lm adaptation target language based initial translation input references kim khudanpur paulik et al", "scheme limited coverage translation model overall quality translation", "since approach allows apply lm adaptation translation available knowledge cannot applied extend coverage", "propose bilingual lsa model blsa crosslingual lm adaptation applied translation", "blsa model consists lsa models side language trained parallel document corpora", "key property blsa model proceedings 45th annual meeting association computational linguistics pages prague czech republic june", "qc association computational linguistics latent topic source target lsa models assumed onetoone correspondence thus share common latent topic space since training corpora consist bilingual parallel data", "instance say topic chinese lsa model politics", "topic english lsa model set correspond politics forth", "lm adaptation first infer topic mixture weights source text using source lsa model", "transfer inferred mixture weights target lsa model thus obtain target lsa marginals", "challenge enforce onetoone topic correspon dence", "proposal share common variational asr hypo chinese asr chineseenglish smt chinese ngram lm english ngram lm adapt adapt topic distribution chinese lsa english lsa chinese text english text chineseenglish parallel document corpus mt hypo dirichlet posteriors topic mixture weights document pair ldastyle model", "beauty blsa framework model searches common latent topic space unsupervised fashion rather require manual interaction", "since topic space language independent approach supports topic transfer multiple language pairs number languages", "related work includes bilingual topic admixture model bitam word alignment proposed zhao xing", "basically bitam model consists topicdependent transla tion lexicons modeling rce denotes source chinese word target english word topic index respectively", "hand blsa framework models rck rek different bitam model", "different modeling nature blsa model usually supports topics bitam model", "another work kim khudanpur employed crosslingual lsa using singular value decomposition concatenates bilingual documents single input supervector projection", "organize paper follows section introduce blsa framework including latent dirichlettree allocation ldta tam schultz correlated lsa model blsa training crosslingual lm adaptation", "section present effect lm adaptation word perplexity followed smt experiments reported bleu speech text input section", "section describes conclusions fu figure topic transfer bilingual lsa model"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999601250016, "p": 0.06896551724137931, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0405668658551404, "p": 0.05555555555555555, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01894739339873598, "p": 0.07692307692307693, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01894739339873598, "p": 0.07692307692307693, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01894739339873598, "p": 0.07692307692307693, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13793102977407862, "p": 0.1111111111111111, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03889505072393645, "p": 0.1111111111111111, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose novel approach crosslingual language model lm adaptation based bilingual latent semantic analysis blsa", "blsa model introduced enables latent topic distributions efficiently transferred languages enforcing onetoone topic correspondence training", "using proposed blsa framework crosslingual lm adaptation performed first inferring topic posterior distribution source text applying inferred distribution target language ngram lm via marginal adaptation", "proposed framework enables rapid bootstrapping lsa models new languages based source lsa model another language", "chinese english speech text translation proposed blsa framework successfully reduced word perplexity english lm unigram lm 4gram lm", "furthermore proposed approach consistently improved machine translation quality speech text based adaptation"]}, "P07-1067": {"introduction": ["ture works", "semantic relatedness important factor coreference resolution noun phrases used refer entity certain semantic relation", "obtain semantic information previous work reference resolution usually leverages semantic lexicon like wordnet vieira poesio harabagiu et al soon et al ng cardie", "drawback wordnet many expressions especially proper names word senses semantic relations available database vieira poesio", "recent years increasing interest seen mining semantic relations large text corpora", "common solution utilize pattern represent specific semantic relation eg isa relation otherrelation", "instantiated given noun phrases pattern searched large corpus occurrence number used measure semantic relatedness markert et al modjeska et al poesio et al", "previous pattern based approaches selection patterns represent specific semantic relation done ad hoc way usually linguistic intuition", "manually selected patterns nevertheless necessarily effective ones coreference resolution following concerns accuracy", "patterns eg find many np pairs specific semantic relation eg isa possible high precision", "breadth", "patterns cover wide variety semantic relations isa coreference relationship realized", "example annotation schemes like ace beijingchina coreferential capital country could used represent government", "pattern common relation fail identify np pairs capitalcountry relation", "deal problem paper propose approach automatically discover effective patterns represent semantic relations proceedings 45th annual meeting association computational linguistics pages prague czech republic june", "qc association computational linguistics coreference resolution", "explore issues study automatically acquire evaluate patterns", "utilize set coreferential np pairs seeds", "seed pair search large corpus texts noun phrases co occur collect surrounding words surface patterns", "evaluate pattern based commonality association positive seed pairs", "mine patterns obtain semantic relatedness information coreference resolution", "present strategies exploit patterns choosing top best patterns set pattern features computing reliability semantic relatedness single feature", "either strategy obtained features applied coreference resolution supervisedlearning way", "knowledge work first effort systematically explores issues coreference resolution task", "evaluate approach ace data set", "experimental results show pattern based semantic relatedness information helpful coreference resolution", "remainder paper organized follows", "section gives related work", "section introduces framework coreference resolution", "section presents model obtain pattern based semantic relatedness information", "section discusses experimental results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01990568045624127, "p": 0.043478260869565216, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03734129947726912, "p": 0.2, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01827095812881336, "p": 0.25, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01827095812881336, "p": 0.25, "r": 0.01818181818181818}}], "abstract": ["semantic relatedness important factor coreference resolution task", "obtain semantic information corpus based approaches commonly leverage patterns express specific semantic relation", "patterns designed manually thus necessarily effective ones terms accuracy breadth", "deal problem paper propose approach automatically find effective patterns coreference resolution", "explore automatically discover evaluate patterns exploit patterns obtain semantic relatedness information", "evaluation ace data set shows pattern based semantic information helpful coreference resolution"]}, "P07-1068": {"introduction": ["finally section summarizes conclusions", "past decade knowledgelean approaches significantly influenced research noun phrase np coreference resolution problem determining nps refer realworld entity document", "knowledgelean approaches coreference resolvers employ morphosyntactic cues knowledge sources resolution process eg mitkov tetreault", "approaches reasonably successful see mitkov kehler et al", "speculate deeper linguistic knowledge needs made available resolvers order reach next level performance", "fact semantics plays crucially important role resolution common nps allowing us identify coreference relation lexically dissimilar common nouns eg talks negotiations eliminate george bush list candidate antecedents city instance", "result researchers readopted oncepopular knowledgerich approach investigating variety semantic knowledge sources common noun resolution semantic relations nps eg ji et al", "semantic similarity computed using wordnet eg poesio et al", "wikipedia ponzetto strube contextual role played np see bean riloff", "another type semantic knowledge employed coreference resolvers semantic class sc np used disallow coreference semantically incompatible nps", "learningbased resolvers able benefit sc agreement feature presumably method used compute sc np simplistic sc proper name computed fairly accurately using named entity ne recognizer many resolvers simply assign common noun first ie frequent wordnet sense sc eg soon et al", "markert nissim", "easy measure accuracy heuristic fact sc agreement feature used soon et als decision tree coreference classifier seems suggest sc values nps computed accurately firstsense heuristic", "motivated part observation examine whether automatically induced semantic class knowledge improve performance learningbased coreference resolver reporting evaluation results commonlyused ace corefer proceedings 45th annual meeting association computational linguistics pages prague czech republic june", "qc association computational linguistics ence corpus", "investigation proceeds follows", "train classifier labeling sc np", "ace primarily concerned classifying np belonging ace semantic classes", "instance part ace phase evaluation involves classifying np per izatio pe geographicalpolitical region fac ility lo atio oth er adopt corpusbased approach sc determination recasting problem sixclass classification task", "derive knowledge sources coreference resolution induced scs", "first knowledge source ks semantic class agreement sca", "following soon et al", "represent sca binary value indicates whether induced scs nps involved", "second ks mention represented binary value indicates whether np belongs ace scs mentioned", "hence mention value np readily derived induced sc value sc oth er es otherwise", "ks could useful ace coreference since ace concerned resolving nps mentions", "incorporate knowledge sources coreference resolver", "next investigate whether kss improve learningbased baseline resolver employs fairly standard feature set", "since kss represented resolver constraint filtering nonmentions disallowing coreference semantically incompatible nps feature applied resolver isolation combination ways incorporating kss baseline resolver"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03946333181507231, "p": 0.07692307692307693, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03924646781770945, "p": 0.047619047619047616, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.041666663133680865, "p": 0.02702702702702703, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027125768929573087, "p": 0.024390243902439025, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04545454170454576, "p": 0.030303030303030304, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03235502426995308, "p": 0.030303030303030304, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.037362637362712156, "p": 0.14285714285714285, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03774928774937501, "p": 0.125, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03664447162209609, "p": 0.2, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.03703703379286723, "p": 0.023255813953488372, "r": 0.09090909090909091}, "rouge-l": {"f": 0.035561702781079216, "p": 0.03225806451612903, "r": 0.07142857142857142}}], "abstract": ["paper examines whether learning based coreference resolver improved using semantic class knowledge automatically acquired version penn treebank noun phrases labeled semantic classes", "experiments ace test data show resolver employs induced semantic class knowledge yields statistically significant improvement fmeasure exploits heuristically computed semantic class knowledge", "addition induced knowledge improves accuracy common noun resolution"]}, "P07-3016": {"introduction": ["experiments ace phase coreference corpus found sc induction method yields significant improvement accuracy soon et als firstsense heuristic method described coreference resolver incorporates induced sc knowledge means kss mentioned yields significant improvement fmeasure resolver exploits sc knowledge computed soon et als method mention ks used baseline resolver constraint improves resolver approximately measure sca employed feature baseline resolver improves accuracy common noun resolution", "decade automatic construction wide coverage structured lexicons center interest natural language processing community", "hand structured lexical databases easier handle expand allow making generalizations classes words", "hand interest automatic acquisition lexical information corpora due fact manual construction resources timeconsuming resulting database difficult update", "work field acquisition verbal lexical properties aims learning subcategorization frames corpora eg", "pereira et al briscoe carroll sass", "semantic group ing verbs basis syntactic distribution quantifiable features gained attention schulte im walde schulte im walde brew merlo stevenson dorr jones", "goal investigations either validation verb classes based levin finding algorithms categorization new verbs", "unlike projects report attempt cluster verbs basis syntactic properties goal identifying semantic classes relevant description hungarian verbs alternation behavior", "theoretical grounding clustering attempts provided socalled semantic base hypothesis levin koenig et al", "founded observation semantically similar verbs tend occur similar syntactic contexts leading assumption verbal semantics determines argument structure surface realization arguments", "english semantic argument roles mapped configurational positions tree structure hungarian codes complement structure highly rich nominal inflection system", "therefore start examination casemarked nps context verbs", "experiment discussed paper first stage ongoing project finding semantic verb classes syntactically relevant hungarian", "presuppositions classes used chose unsupervised clustering method described schulte im walde", "frequent hungarian verbs categorized according comp proceedings acl student research workshop pages prague june", "qc association computational linguistics lementation structures syntactically annotated corpus szeged treebank csendes et al", "seeking answer questions", "resulting clusters semantically coherent"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper reports attempt apply unsupervised clustering algorithm hungarian treebank order obtain semantic verb classes", "starting hypothesis semantic metapredicates underlie verbs syntactic realization investigate obtain semantically motivated verb classes automatic means", "frequent hungarian verbs clustered basis complementation patterns yielding set basic classes hints features determine verbal subcategorization", "resulting classes serve basis subsequent analysis alternation behavior"]}, "P08-1090": {"introduction": ["thus reinforcing semantic base hypothesis", "paper induces new representation structured knowledge called narrative event chains narrative chains", "narrative chains partially ordered sets events centered around common protagonist", "related structured sequences participants events called scripts schank abelson fillmorean frames", "participants events filled instantiated particular text situation draw inferences", "chains focus single actor facili tate learning thus paper addresses tasks chain induction narrative event induction temporal ordering events structured selection pruning event space discrete sets", "learning prototypical schematic sequences events important rich understanding text", "scripts central natural language understanding research 1970s 1980s proposed tasks summarization coreference resolution question answering", "example schank abelson proposed understanding text restaurants required knowledge restaurant script including participants customer waiter cook tables etc events constituting script entering sitting asking menus etc various preconditions ordering results constituent actions", "consider distinct narrative chains", "accused joined claimed served argued oversaw dismissed resigned would useful question answering textual entailment know denied likely event left chain replaces temporally follows right", "narrative chains firing employee executive resigns offer structure power directly infer new subevents providing critical background knowledge", "part due complexity automatic induction addressed since early non statistical work mooney dejong", "first step narrative induction uses entity based model learning narrative relations fol proceedings acl08 hlt pages columbus ohio usa june", "qc association computational linguistics lowing protagonist", "narrative progresses series events event characterized grammatical role played protagonist protagonists shared connection surrounding events", "algorithm unsupervised distributional learning approach uses core ferring arguments evidence narrative relation", "show using new evaluation task called narrative cloze protagonistbased method leads better induction verbonly approach", "next step order events narrative chain", "apply work area temporal classification create partial orders learned events", "show using coherencebased evaluation temporal ordering partial orders lead better coherence judgements real narrative instances extracted documents"], "introduction_label": [{"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012417374199087205, "p": 0.08333333333333333, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04878048387864398, "p": 0.03333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013264034403051713, "p": 0.03125, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012606937860599514, "p": 0.058823529411764705, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012452015571831648, "p": 0.07692307692307693, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012526521290086582, "p": 0.06666666666666667, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.02470816557080309, "p": 0.2, "r": 0.024390243902439025}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012606937860599514, "p": 0.058823529411764705, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012452015571831648, "p": 0.07692307692307693, "r": 0.012195121951219513}}], "abstract": ["handcoded scripts used 197080s knowledge backbones enabled inference nlp tasks requiring deep semantic knowledge", "propose unsupervised induction similar schemata called narrative event chains raw newswire text", "narrative event chain partially ordered set events related common protagonist", "describe step process learning narrative event chains", "first uses unsupervised distributional methods learn narrative relations events sharing corefer ring arguments", "second applies temporal classifier partially order connected events", "finally third prunes clusters selfcontained chains space events", "introduce evaluations narrative cloze evaluate event relatedness order coherence task evaluate narrative order", "show improvement baseline narrative prediction temporal coherence"]}, "P08-2021": {"introduction": ["finally space narrative events temporal orders clustered pruned create discrete sets narrative chains", "large improvements machine translation mt result combining different approaches mt mutually complementary strengths", "systemlevel combination translation outputs promising path towards improvements", "significant hurdles path", "must somehow align multiple outputsto identify different hypotheses reinforce offer alternatives", "must work partially supported darpa gale program contract hr00110620001", "would like thank ibm rosetta team availability several mt system outputs", "use alignment hypothesize set new composite translations select best composite hypothesis set", "alignment step difficult different mt approaches usually reorder translated words differently", "training selection step difficult identifying best hypothesis relative known reference translation means scoring composite hypotheses exponentially many", "mt combination methods create exponentially large hypothesis set representing confusion network strings target language eg english", "confusion network lattice every node every path ie time step presents independent choice several phrases", "note contributions paper could applied arbitrary lattice topologies", "example bangalore et al", "show build confusion network following multistring alignment procedure several mt outputs", "procedure used primarily biology thompson et al yields monotone alignments minimize number insertions deletions substitutions", "unfortunately monotone alignments often poor since machine translations particularly different models vary significantly word order", "thus matusov et al", "use procedure deterministically reorder translation prior monotone alignment", "procedure described rosti et al", "shown yield significant improvements translation quality uses estimate translation error rate ter guide alignment", "ter defined minimum number inser proceedings acl08 hlt short papers companion volume pages columbus ohio usa june", "qc association computational linguistics tions deletions substitutions block shifts strings", "remarkable feature procedure performs alignment output translations without knowledge translation model used generate translations ii without knowledge target words translation align back source words", "fact requires procedure creating pairwise alignments translations allow appropriate reorderings", "rosti et al", "use tercom script snover et al uses number heuristics well dynamic programming finding sequence edits insertions deletions substitutions block shifts convert input string another", "paper show build better confusion networks terms best translation possible confusion network pairwise alignments computed tercom approximately minimizes ter instead exact minimization invwer leusch et al restricted version ter permits properly nested sets block shifts computed polynomial time"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0603315911099514, "p": 0.08333333333333333, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03016579555494728, "p": 0.041666666666666664, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["given several systems automatic translations sentence show combine confusion network whose various paths represent composite translations could considered subsequent rescoring step", "build confusion networks using method rosti et al", "instead forming alignments using tercom script snover et al create alignments minimize invwer leusch et al form edit distance permits properly nested block movements substrings", "oracle experiments chinese newswire weblog translations show confusion networks contain paths significantly better terms bleu ter tercombased confusion networks"]}, "P09-1045": {"introduction": ["paper organized follows summary ter tercom invwer presented section", "iterative bootstrapping algorithms proposed extract semantic lexicons tasks limited linguistic resources", "bootstrapping initially proposed riloff jones since successfully applied extracting general semantic lexicons riloff jones thelen riloff biomedical entities yu agichtein facts pasca et al coreference data yang su", "bootstrapping approaches attractive domain language independent require minimal linguistic preprocessing applied raw text efficient enough terascale extraction pasca et al", "bootstrapping minimally supervised initialised small number seed instances information extract", "semantic lexicons seeds terms category interest", "seeds identify contextual patterns express particular semantic category turn recognise new terms riloff jones", "unfortunately semantic drift often occurs ambiguous erroneous terms andor patterns introduced dominate iterative process curran et al", "bootstrapping algorithms typically compared using single set handpicked seeds", "first show different seeds cause algorithms generate diverse lexicons vary greatly precision", "makes evaluation unreliable seeds perform well algorithm perform surprisingly poorly another", "fact random goldstandard seeds often outperform seeds carefully chosen domain experts", "second contribution exploits diversity identified", "present unsupervised bagging algorithm samples extracted lexicon rather relying existing gazetteers handselected seeds", "sample fed back seeds bootstrapper results combined using voting", "improves precision lexicon robustness algorithms choice initial seeds", "unfortunately semantic drift still dominates later iterations since erroneous extracted terms andor patterns eventually shift categorys direction", "third contribution focuses detecting censoring terms introduced semantic drift", "integrate distributional similarity filter directly mcintosh curran", "filter judges whether new term similar earlier recently extracted terms sign potential semantic drift", "demonstrate methods extracting biomedical semantic lexicons using bootstrap ping algorithms", "unsupervised bagging approach outperforms carefully handpicked seeds later iterations", "distributionalsimilarity filter gives similar performance im provement", "allows us produce large lexicons accurately efficiently domainspecific language processing", "proceedings 47th annual meeting acl 4th ijcnlp afnlp pages suntec singapore august"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["iterative bootstrapping algorithms typically compared using single set handpicked seeds", "demonstrate performance varies greatly depending seeds favourable seeds algorithm perform poorly others making comparisons unreliable", "exploit wide variation bagging sampling automatically extracted seeds reduce semantic drift", "semantic drift still occurs later iterations", "propose integrated distributional similarity filter identify censor potential semantic drifts ensuring higher precision extracting large semantic lexicons"]}, "P09-1068": {"introduction": ["qc acl afnlp", "learned", "even unsupervised attempts learn semantic roles required pre defined set roles grenager manning often handlabeled seed corpus swier stevenson gildea", "paper describe attempts learn scriptlike information world including event structures roles participants without pre defined frames roles tagged corpora", "consider following narrative schema defined formally later", "events left follow set participants series connected events constitute narrative events roles paper describes new approach event semantics jointly learns event relations search arrest plead police suspect plea participants unlabeled corpora", "early years natural language processing nlp took topdown approach language acquit convict sentence jury understanding using representations like scripts schank abelson structured representations events causal relationships participants frames drive interpretation syntax word use", "knowledge structures provided interpreter rich information many aspects meaning", "problem rich knowledge structures need hand construction specificity domain dependence prevents robust flexible language understanding", "instead modern work understanding focused shallower representations like semantic roles express least aspect semantics events proved amenable supervised learning corpora like propbank palmer et al framenet baker et al", "unfortunately creating supervised corpora expensive difficult multiyear effort requiring complex decisions exact set roles able robustly learn sets related events left framespecific role information argument types fill right could assist variety nlp applications question answering machine translation", "previous work chambers jurafsky relied intuition coherent text events participants likely part story narrative", "model learned simple aspects narrative structure narrative chains extracting events share single participant protagonist", "paper extend work represent sets situationspecific events unlike scripts caseframes bean riloff framenet frames baker et al", "paper shows verbs distinct narrative chains merged improved single narrative schema shared arguments verbs provide rich information inducing semantic roles", "proceedings 47th annual meeting acl 4th ijcnlp afnlp pages suntec singapore august"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04761904375283478, "p": 0.03225806451612903, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028386649728229026, "p": 0.030303030303030304, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029450174568674967, "p": 0.034482758620689655, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02930876266467362, "p": 0.07142857142857142, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["describe unsupervised system learning narrative schemas coherent sequences sets events arrestedpolicesuspect convicted judge suspect whose arguments filled participant semantic roles defined words judge judge jury court police police agent authorities", "unlike previous work inevent structure semantic role learning system use supervised techniques handbuilt knowledge predefined classes events roles", "unsupervised learning algorithm uses corefer ring arguments chains verbs learn rich narrative event structure argument roles", "jointly addressing tasks improve previous results narrativeframe learning induce rich framespecific semantic roles"]}, "P09-1074": {"introduction": ["qc acl afnlp", "common many natural language processing problems stateoftheart noun phrase np coreference resolution typically quantified based system performance manually annotated text corpora", "spite availability several benchmark data sets eg muc6 ace nist use many formal evaluations field make surprisingly conclusive statements stateofthe art np coreference resolution", "particular remains difficult assess effectiveness different coreference resolution approaches even relative terms", "example fmeasure reported mccallum well ner produced system using perfect information several linguistic subproblems", "contrast fmeasure reported yang et al", "represents fully automatic endtoend resolver", "impossible assess approach truly performs best dramatically different assumptions evaluation", "results vary widely data sets", "coreference resolution scores range ace data sets much lower muc data sets eg soon et al", "yang et al", "", "accounts differences", "due properties documents domains", "differences coreference task definitions account differences performance", "given new text collection domain level performance expect", "little understanding aspects coreference resolution problem handled well poorly stateoftheart systems", "except fairly general statements example proper names easier resolve pronouns easier common nouns little analysis aspects problem achieved success remain elusive", "goal paper take initial steps toward making sense disparate performance results reported np coreference resolution", "investigations employ stateoftheart classificationbased np coreference resolver focus widely used muc ace coreference resolution data sets", "hypothesize performance variation coreference resolvers least part function sometimes unstated assumptions evaluation methodologies relative difficulty benchmark text corpora", "mind section first examines subproblems play important role coreference resolution named entity recognition anaphoricity determination coreference element detection", "quantitatively measure impact subproblems coreference resolution performance whole", "results suggest availability accurate detectors anaphoricity coreference elements could substantially improve performance stateof theart resolvers improvements named entity recognition likely offer little gains", "results confirm assumptions adopted proceedings 47th annual meeting acl 4th ijcnlp afnlp pages suntec singapore august", "qc acl afnlp muc ace relative pronouns yes gerunds yes nested nonnp nouns yes nested nes gpe loc premod semantic types classes singletons yes table coreference definition differences muc ace", "gpe refers geopolitical entities", "evaluations dramatically simplify resolution task rendering unrealistic surrogate original problem", "section quantify difficulty text corpus respect coreference resolution analyzing performance different resolution classes", "goals twofold measure level performance stateoftheart coreference resolvers different types anaphora develop quantitative measure estimating coreference resolution performance new data sets", "introduce coreference performance prediction cpp measure show accurately predicts performance coreference resolver"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.15999999507200013, "p": 0.14285714285714285, "r": 0.18181818181818182}, "rouge-l": {"f": 0.058617525329242776, "p": 0.14285714285714285, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["aim shed light stateoftheart np coreference resolution teasing apart differences muc ace task definitions assumptions made evaluation methodologies inherent differences text corpora", "first examine subproblems play role coreference resolution named entity recognition anaphoricity determination coreference element detection", "measure impact subproblem coreference resolution confirm certain assumptions regarding subproblems evaluation methodology dramatically simplify overall task", "second measure performance stateoftheart coreference resolver several classes anaphora use results develop quantitative measure estimating coreference resolution performance new data sets"]}, "P09-1113": {"introduction": ["side effect research provide new set muchneeded benchmark results coreference resolution common sets fullyspecified evaluation assumptions", "least learning paradigms applied task extracting relational facts text example learning person employed particular organization geographic entity located particular region", "supervised approaches sentences corpus first handlabeled presence entities relations", "nist automatic content extraction ace rdc corpora example include documents pairs entities labeled major relation types subrelations totaling relation instances", "ace systems extract wide variety lexical syntactic semantic features use supervised classifiers label relation mention holding given pair entities test set sentence optionally combining relation men tions zhou et al zhou et al surdeanu ciaramita", "supervised relation extraction suffers number problems", "labeled training data expensive produce thus limited quantity", "relations labeled particular corpus resulting classifiers tend biased toward text domain", "alternative approach purely unsupervised information extraction extracts strings words entities large amounts text clusters simplifies word strings produce relationstrings shinyama sekine banko et al", "unsupervised approaches use large amounts data extract large numbers relations resulting relations easy map relations needed particular knowledge base", "third approach use small number seed instances patterns bootstrap learning brin riloff jones agichtein gravano ravichandran hovy etzioni et al pennacchiotti pantel bunescu mooney rozenfeld feldman", "seeds used large corpus extract new set patterns used extract instances used extract patterns iterative fashion", "resulting patterns often suffer low precision semantic drift", "propose alternative paradigm distant supervision combines advantages approaches", "distant supervision extension paradigm used snow et al", "exploiting wordnet extract hyper nym isa relations entities similar use weakly labeled data bioinformatics craven kumlien morgan et al proceedings 47th annual meeting acl 4th ijcnlp afnlp pages suntec singapore august", "qc acl afnlp relation name new instance locationlocationcontains locationlocationcontains musicartistorigin peopledeceased personplace death peoplepersonnationality peoplepersonplace birth bookauthorworks written businesscompanyfounders peoplepersonprofession paris montmartre ontario fort erie mighty wagon cincinnati fyodor kamensky clearwater marianne yvonne heemskerk netherlands wavell wayne hinds kingston upton sinclair lanny budd wwe vince mcmahon thomas mellon judge table relation instances extracted system appear freebase", "", "algorithm uses freebase bollacker et al large semantic database provide distant supervision relation extraction", "freebase contains million instances relations million entities", "intuition distant supervision sentence contains pair entities participate known freebase relation likely express relation way", "since many sentences containing given entity pair extract large numbers potentially noisy features combined logistic regression classifier", "thus whereas supervised training paradigm uses small labeled corpus relation instances training data algorithm use much larger amounts data text relations instances", "use million wikipedia articles million instances relations connecting entities", "addition combining vast numbers features large classifier helps obviate problems bad features", "algorithm supervised database rather labeled text suffer problems overfitting domaindependence plague supervised systems", "supervision database means unlike unsupervised approaches output classifier uses canonical names relations", "paradigm offers natural way integrating data multiple sentences decide relation holds entities", "algorithm use large amounts unlabeled data pair entities occur multiple times test set", "pair entities aggregate features many different sentences pair appeared single feature vector allowing us provide classifier information resulting accurate labels", "table shows examples relation instances extracted system", "use system investigate value syntactic versus lexi cal word sequence features relation extraction", "syntactic features known improve performance supervised ie least using clean handlabeled ace data zhou et al zhou et al know whether syntactic features improve performance unsupervised distantly supervised ie"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04761904375283478, "p": 0.03225806451612903, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017444814913212602, "p": 0.02857142857142857, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016568588454494517, "p": 0.05263157894736842, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017230127070972504, "p": 0.034482758620689655, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["modern models relation extraction tasks like ace based supervised learning relations small handlabeled corpora", "investigate alternative paradigm require labeled corpora avoiding domain dependence ace style algorithms allowing use corpora size", "experiments use freebase large semantic database several thousand relations provide distant supervision", "pair entities appears freebase relation find sentences containing entities large unlabeled corpus extract textual features train relation classifier", "algorithm combines advantages supervised ie combining noisy pattern features probabilistic classifier unsupervised ie extracting large numbers relations large corpora domain", "model able extract instances relations precision", "analyze feature performance showing syntactic parse features particularly helpful relations ambiguous lexically distant expression"]}, "P09-1114": {"introduction": ["previous research bootstrapping unsupervised ie used simple lexical features thereby avoiding computational expense parsing brin agichtein gravano etzioni et al systems used unsupervised ie compared performance types feature", "relation extraction task detecting characterizing semantic relations entities free text", "recent work relation extraction shown supervised machine learning coupled intelligent feature engineering kernel design provides stateoftheart solutions problem culotta sorensen zhou et al bunescu mooney qian et al", "supervised learning heavily relies sufficient amount labeled data training always available practice due laborintensive nature human annotation", "problem especially serious relation ex traction types relations extracted highly dependent application domain", "example working financial domain interested employment relation moving terrorism domain interested ethnic ideology affiliation relation thus create training data new relation type", "old training data really useless", "inspired recent work transfer learning domain adaptation paper study leverage labeled data old relation types help extraction new relation type weaklysupervised setting seed instances new relation type available", "transfer learning proposed decade ago thrun caruana application natural language processing still relatively new territory blitzer et al daume iii jiang zhai 2007a arnold et al dredze crammer application relation extraction still unexplored", "idea performing transfer learning motivated observation different relation types share certain common syntactic structures possibly transferred old types new type", "therefore propose use general multitask learning framework classification models number related tasks forced share common model component trained together", "treating classification different relation types related tasks learning framework naturally model common syntactic structures different relation types principled manner", "allows us introduce human guidance separating common model component typespecific components", "framework naturally transfers knowledge learned old relation types new relation type helps improve recall relation extractor", "exploit ad proceedings 47th annual meeting acl 4th ijcnlp afnlp pages suntec singapore august", "qc acl afnlp ditional human knowledge entity type constraints relation arguments usually derived definition relation type", "imposing constraints improves precision final relation extractor"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407672862, "p": 0.03571428571428571, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407672862, "p": 0.03571428571428571, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["creating labeled training data relation extraction expensive", "paper study relation extraction special weaklysupervised setting seed instances target relation type want extract large amount labeled instances relation types", "observing different relation types share certain common structures propose use multitask learning method coupled human guidance address weaklysupervised relation extraction problem", "proposed framework models commonality different relation types shared weight vector enables knowledge learned auxiliary relation types transferred target relation type allows easy control tradeoff precision recall", "empirical evaluation ace data set shows proposed method substantially improves baseline methods"]}, "P10-1115": {"introduction": ["empirical evaluation ace data set shows proposed method largely outperforms baseline methods improving average f1 measure seed instances new relation type used", "robust unsupervised way perform shallow latent semantic analysis topics text probabilistic topic models hofmann 1999a blei et al 2003b recently attracted much attention", "common idea behind models following", "topic represented multinomial word distribution words characterizing topic generally higher probabilities words", "hypothesize existence multiple topics text define generative model based hypothesized topics", "fitting model text data obtain estimate word distributions corresponding latent topics well topic distributions text", "intuitively learned word distributions capture clusters words cooccur probabilistically", "although many topic models proposed shown useful see section detailed discussion related work share common deficiency designed work monolingual text data would work well extracting crosslingual latent topics ie topics shared text data different natural languages", "deficiency comes fact models rely cooccurrences words forming topical cluster words different language generally cooccur", "thus existing models extract topics text language cannot extract common topics shared multiple languages", "paper propose novel topic model called probabilistic crosslingual latent semantic analysis pclsa model used mine shared latent topics unaligned text data different languages", "pclsa extends probabilistic latent semantic analysis plsa model regularizing likelihood function soft constraints defined based bilingual dictionary", "dictionarybased constraints key bridge gap different languages would force captured cooccurrences words language pclsa synchronized related words languages would similar probabilities", "pclsa estimated efficiently using general expectation maximization gem algorithm", "topic extraction algorithm pclsa would take pair unaligned document sets different languages bilingual dictionary input output set aligned word distributions languages characterize shared topics languages", "addition outputs topic cov proceedings 48th annual meeting association computational linguistics pages uppsala sweden july", "qc association computational linguistics erage distribution language indicate relative coverage different shared topics language", "best knowledge previous work attempted solve topic extraction problem generate output", "closest existing work muto model proposed boydgraber blei jointlda model published recently jagaralamudi daume iii", "used bilingual dictionary bridge language gap topic model", "goals work different models mainly focus mining crosslingual topics matching word pairs discovering correspondence vocabulary level", "therefore topics extracted using model cannot indicate common topic covered differently languages words word pair share probability common topic", "work focuses discovering correspondence topic level", "model since add soft constraint word pairs dictionary probabilities common topics generally different naturally capturing shows different variations common topic different languages", "use crosslingual news data set review data set evaluate pclsa", "propose crosscollection likelihood measure quantitatively evaluate quality mined topics"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809137188224, "p": 0.06451612903225806, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0947830435021853, "p": 0.1111111111111111, "r": 0.08695652173913043}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.18181817681818196, "p": 0.18181818181818182, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04534444140398479, "p": 0.18181818181818182, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02377273381650473, "p": 0.05263157894736842, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0220600295227433, "p": 0.16666666666666666, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}], "abstract": ["probabilistic latent topic models recently enjoyed much success extracting analyzing latent topics text unsupervised way", "common deficiency existing topic models though would work well extracting crosslingual latent topics simply words different languages generally cooccur", "paper propose way incorporate bilingual dictionary probabilistic topic model apply topic models extract shared latent topics text data different languages", "specifically propose new topic model called probabilistic crosslingual latent semantic analysis pclsa extends probabilistic latent semantic analysis plsa model regularizing likelihood function soft constraints defined based bilingual dictionary", "qualitative quantitative experimental results show pclsa model effectively extract crosslingual latent topics multilingual text data"]}, "P10-2025": {"introduction": ["experimental results show pclsa model effectively extract crosslingual latent topics multilingual text data outperforms baseline approach using standard plsa text data language", "word alignment essential step phrase syntax based statistical machine translation smt", "inference problem word correspondences different languages given parallel sentence pairs", "accurate word alignment induce high quality phrase detection translation probability leads significant improvement smt performance", "many word alignment approaches based generative models proposed learn bilingual sentences unsupervised manner vo gel et al och ney fraser marcu", "way improve word alignment quality add linguistic knowledge derived monolingual corpus", "monolingual knowledge makes easier determine corresponding words correctly", "instance functional words language tend correspond functional words another language deng gao syntactic dependency words language help alignment process et al", "shown grammatical information works constraint word alignment models improves word alignment quality", "large number monolingual lexical semantic resources wordnet miller constructed fifty languages sagot fiser", "include word level relations synonyms hypernyms hyponyms", "synonym information particularly helpful word alignment expect synonym correspond word different language", "paper explore method using synonym information effectively improve word alignment quality", "general synonym relations defined terms word sense terms word form", "words synonym relations usually context domain dependent", "instance head chief synonyms contexts referring working environment head forefront synonyms contexts referring physical positions", "difficult imagine context chief forefront synonyms", "therefore easy imagine simply replacing occurrences chief forefront head sometimes harm word alignment accuracy model either context senses words", "propose novel method incorporates synonyms monolingual resources bilingual word alignment model", "formulate synonym pair generative model topic variable use model regularization term bilingual word alignment model", "topic variable synonym model helpful disambiguating meanings synonyms", "extend hmbitam hmmbased word alignment model latent topic novel synonym pair generative model", "applied proposed method englishfrench word alignment task successfully improved word proceedings acl conference short papers pages uppsala sweden july", "qc association computational linguistics translation probability kth topic", "tii state tran sition probability first order markov process", "fig", "shows graphical model hmbitam"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present novel framework word alignment incorporates synonym knowledge collected monolingual linguistic resources bilingual probabilistic model", "synonym information helpful word alignment expect synonym correspond word different language", "design generative model word alignment uses synonym information regularization term", "experimental results show proposed method significantly improves word alignment quality"]}, "P10-2066": {"introduction": ["total likelihood bilingual sentence pairse obtained marginalizing la tent variables figure graphical model hmbitam alignment quality", "entity set expansion problem defined follows given set seed entities particular class set candidate entities eg extracted text corpus wish determine entities belong words expand set based given seeds", "clearly classification problem requires arriving binary decision entity belonging", "practice problem often solved ranking problem ie ranking entities based likelihoods belonging classic method solving problem based distributional similarity pantel et al lee", "approach works comparing similarity surrounding word distributions candidate entity seed entities ranking candidate entities using similarity scores", "machine learning class semi supervised learning algorithms learns positive unlabeled examples pu learning short", "key characteristic pu learning negative training example available learning", "class algorithms less known natural language processing nlp community compared semi supervised learning models algorithms", "pu learning twoclass classification model", "stated follows liu et al given set positive examples particular class set unlabeled examples containing hidden positive negative cases classifier built using classifying data future test cases", "results either binary decisions whether test case belongs positive class ranking based likely test case belongs positive class represented clearly set expansion problem mapped pu learning exactly respectively", "paper shows pu learning method called sem liu et al outperforms distributional similarity considerably based results corpora", "experiments involved extracting named entities eg product organization names type class given seeds", "additionally compared sem recent method called bayesian sets ghahramani heller designed specifically set expansion", "perform well pu learning", "explain pu learning performs better methods section", "believe finding interest nlp community", "proceedings acl conference short papers pages uppsala sweden july", "qc association computational linguistics another approach used web environment entity set expansion", "exploits web page structures identify lists items using wrapper induction techniques", "idea items list often type", "approach used google sets google boowa", "wang cohen"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705444636694, "p": 0.08695652173913043, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07267923621906426, "p": 0.07407407407407407, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.07142857142807142, "p": 0.07142857142857142, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0363372093023574, "p": 0.25, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["distributional similarity classic technique entity set expansion system given set seed entities particular class asked expand set using corpus obtain entities class represented seeds", "paper shows machine learning model called positive unlabeled learning pu learning model set expansion problem better", "based test results corpora show pu learning technique outperformed distributional similarity significantly"]}, "P101121_p07": {"introduction": ["relies web page structures applicable general free texts", "couple reasons automatic evaluation machine translation mt systems difficult task mostly difficult define translation good bad", "given translations better worse", "main reason ambiguities natural languages usually correct translation source sentences ambiguities choice synonyms well order words", "difficulty task multitude automatic mt evaluation measures defined last couple years", "measures become well established example bleu ter others medium small significance", "expect context nists metrics matr evaluation measures added pool evaluation measures", "previous papers proposed measures seemed theoretical interest practical use certainly emphasis important linguistic effects investigated systematically far effects play role difference quality different mt systems", "proposed evaluation measures seemed focus specific properties features previously generated translations trained optimized need lead evaluation measures basically classifiers dividing previously good previously poor systems easy difficult source sentences", "measures property used tune typical statistical mt system sometimes observed mt system learns play might even learn produce translations show good features without actually good translations", "example rosti et al", "report effect", "say new measures share problems need mt evaluation measures go beyond lexical comparison quite opposite", "issues motivation us start established evaluation measures known properties especially regard tuning alter selected points improve correlation human judgment", "paper organized follows section describe modifications bleu score following lin och leusch et al", "", "present simple variant wer section called cder allows block transposition similar ter following leusch et al", "", "measure efficiently calculated exactly without resort shift heuristics greedy search ter", "tradeoff measure measures basically recall precision", "overcome bias later propose linear combination measure per section", "section describe another variant ter exactly calculated polynomial time time restricting possible shifts itg constrains", "method follows leusch et al", "", "call measure vwer", "section introduce simple methods following leusch et al", "improve edit operationbased measures like per wer ter cderinvwer taking account lexical difference words substitution operation"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.011267482944399047, "p": 0.0625, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.011129921499768153, "p": 0.09090909090909091, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01108577489064369, "p": 0.1111111111111111, "r": 0.01098901098901099}}], "abstract": ["present modifications well established automatic machine translation evaluation measures improve correlation measures human evaluation", "following lin och present improved version bleu score uses smoothed geometric mean combining different ngram precisions", "use segment boundary markers increase weight words near segment boundaries bleu score", "second mt evaluation measure variant wer allows block movements demand complete disjoint coverage source sentence", "might problematic mt systems tuned score later investigate linear combination measure per", "finally describe edit distance similar ter allows block reordering", "measure uses full search constraint block operations must bracketed", "describe measure using bracketing transduction grammar sketch polynomialtime algorithm calculation", "modify werlike measures use worddependent substitution costs instead fixed ones model similarity words", "experimental comparison measures show new measures correlate significantly better human judgment original measures"]}, "P101194": {"introduction": ["experimental evaluation proposed evaluation measures section conclude paper section", "typically morphological analysis returns base form lemma associates possible pos partofspeech labels together grammatical information known word form", "analytical languages simple approach taken enough list word forms catch morphological processes", "english example regular verb usually distinct forms irregular ones forms", "hand highly inflectional languages like czech finnish present difficulty simple approaches expansion dictionary least order magnitude greater1", "specialised finitestate compilers implemented allow use specific operations combining base forms affixes applying rules morphophonological variations", "descriptions morphological analysers languages found", "basically major types wordforming processes inflection derivation compounding", "inflection refers systematic modification stem means prefixes suffixes", "inflected forms express morphological distinctions like case number change meaning pos", "contrast process derivation usually causes change meaning often change pos", "compounding deals process merging several word bases form new word", "effective implementation spellchecker czech based finite state au", "tomata suggests necessarily mean application take advantage simple listing word forms highly inflecting languages", "czech belongs family inflectional languages characterised fact morpheme typically ending carries values several grammatical categories together example ending nouns typically expresses value grammatical category case number gender", "feature requires special treatment czech words text processing systems", "end developed universal morphological analyser performs morphological analysis based dividing words czech texts smallest relevant components call segments notion segment roughly corresponds linguistic concept morpheme denotes smallest meaningful unit language", "presented morphological analyser consists major parts formal description morphological processes via morphological patterns assignment czech stems relevant patterns morphological analysis algorithm", "description czech formal morphology represented system inflectional patterns sets endings includes lists segments correct combinations", "assignment czech stems patterns contained czech machine dictionary", "finally algorithm morphological analysis using information splits word appropriate segments", "morphological analyser used lemmatisation morphological tagging czech texts large corpora well generating correct word forms spelling checker"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05687956169059384, "p": 0.2, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}], "abstract": ["new czech morphological analyser ajka based algorithmic description czech formal morphology", "first present important wordforming processes czech inflection derivation", "brief description data structures used storing morphological information well discussion efficient storage lexical items stem bases czech words included", "finally bring interesting features designed implemented system ajka together current statistic data"]}, "P11-1053": {"introduction": ["applied problems arise area processing czech texts eg creating stop lists building indexes used information retrieval systems", "relation extraction important information extraction task natural language processing nlp many practical applications", "goal relation extraction detect characterize semantic relations pairs entities text", "example relation extraction system needs able extract employment relation entities us soldier us phrase us soldier", "current supervised approaches tackling problem general fall categories feature based kernel based", "given entity pair sentence containing pair approaches usually start multiple level analyses sentence tokenization partial full syntactic parsing dependency parsing", "feature based method explicitly extracts variety lexical syntactic semantic features statistical learning either generative discriminative miller et al kambhatla boschee et al grishman et al zhou et al jiang zhai", "contrast kernel based method explicitly extract features designs kernel functions structured sentence representations sequence dependency parse tree capture similarities different relation instances zelenko et al bunescu mooney 2005a bunescu mooney 2005b zhao grishman zhang et al zhou et al qian et al", "lines work depend effective features either explicitly implicitly", "performance supervised relation extraction system usually degraded sparsity lexical features", "example unless example us soldier previously seen training data would difficult feature based kernel based systems detect whether employment relation", "syntactic feature phrase us soldier simply nounnoun compound quite general words crucial extracting relation", "motivates work use word clusters additional features relation extraction", "assumption even word soldier never seen annotated employment relation instances words share cluster membership soldier president ambassador observed employment instances", "absence lexical features compensated proceedings 49th annual meeting association computational linguistics pages portland oregon june", "qc association computational linguistics cluster features", "moreover word clusters implicitly correspond different relation classes", "example cluster president related employment relation us president cluster businessman related affiliation relation us businessman", "main contributions paper explore clusterbased features systematic way propose several statistical methods selecting effective clusters", "study impact size training data cluster features analyze performance improvements extensive experimental study", "rest paper organized follows section presents related work section provides background relation extraction task word clustering algorithm", "section describes detail stateoftheart supervised baseline system", "section describes cluster based features cluster selection methods", "though boschee et al", "chan roth used word clusters relation extraction shared limitation approaches choosing clusters", "example boschee et al", "chose clusters different granularities chan roth simply used single threshold cutting word hierarchy", "moreover boschee et al", "augmented predicate typically verb noun importance relation definition word clusters chan roth performed lexical feature consisting single word", "paper systematically explore effectiveness adding word clusters different lexical features", "background", "relation extraction"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.15999999507200013, "p": 0.14285714285714285, "r": 0.18181818181818182}, "rouge-l": {"f": 0.059361814827767624, "p": 0.125, "r": 0.05405405405405406}}], "abstract": ["present simple semisupervised relation extraction system largescale word clustering", "focus systematically exploring effectiveness different clusterbased features", "propose several statistical methods selecting clusters appropriate level granularity", "training different sizes data semisupervised approach consistently outperformed stateoftheart supervised baseline system"]}, "P11-1056": {"introduction": ["well defined relation extraction tasks present experimental results section automatic content extraction ace program conclude section", "relation extraction defined task identifying given set semantic binary relations text", "instance given span text", "seattle zoo", "would like extract relation seattle zoo locatedat seattle", "frequently studied last years supervised learning task learning spans text annotated set semantic relations interest", "approaches assumed relations arguments given input chan roth jiang zhai jiang zhou et al therefore offer partial solution problem", "conceptually rather simple approach spans texts treated uniformly mapped several relation types interest", "approaches require large amount manually annotated training data achieve good performance making difficult expand set target relations", "moreover show approaches become brittle relations arguments given rather need identified data", "paper build observation exists second dimension relation extraction problem orthogonal relation type dimension relation types expressed several constrained syntacticosemantic structures", "show identifying text span syntacticosemantic structure dimension first leveraged process yield improved performance", "moreover working second dimension provides robustness real problem identifying arguments along relations", "example seattle zoo entity mention seattle modifies noun zoo", "thus mentions seattle seattle zoo involved later call premodifier relation several syntacticosemantic structures identify section", "highlight relation types expressed several syntacticosemantic structures premodifiers possessive preposition formulaic verbal", "turns structures relatively constrained difficult identify", "suggests novel algorithmic approach starts first identifying structures identifying semantic type relation", "approach provide significantly improved perfor proceedings 49th annual meeting association computational linguistics pages portland oregon june", "qc association computational linguistics mance carries additional advantages", "first leveraging syntacticosemantic structure especially beneficial presence small amounts data", "second important fact exploiting syntacticosemantic dimension provides several new options dealing full problem incorporating argument identification problem", "explore possibilities making use constrained structures way aid identification relations arguments", "show already provides significant gain discuss possibilities explored", "contributions paper summarized highlight relation types expressed several syntacticosemantic structures show relatively constrained difficult identify", "consequently working first structural dimension leveraged process improve performance", "show large number training examples exploiting syntacticosemantic structures crucial performance", "show leverage constrained structures improve relations arguments given", "constrained structures allow us jointly entertain argument didates relations built arguments", "specifically show considering argument candidates otherwise would discarded provided exist syntacticosemantic structures reduce error propagation along standard pipeline architecture joint inference process leads improved performance", "next section describe relation extraction framework leverages syntactico semantic structures", "present structures section", "describe mention entity typing system section features system section", "present experiments section perform analysis section concluding section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01848933815567852, "p": 0.01694915254237288, "r": 0.021739130434782608}}], "abstract": ["paper observe exists second dimension relation extraction problem orthogonal relation type dimension", "show second dimensional structures relatively constrained difficult identify", "propose novel algorithmic approach starts first identifying structures identifying semantic type relation", "real problem relation arguments need identified exploiting structures allows reducing pipelined propagated errors", "show framework provides significant improvement performance"]}, "P11-1082": {"introduction": ["premodifier possessive preposition formulaic gold mentions training data mtrain dg mi mj mtrain mtrain mi sentence mj rebase classifier trained dg ds mi mj dg structure inference mi mj using patterns mi mj annotated structure ds ds mi mj done res classifier trained ds output rebase res figure training regular baseline classifier rebase classifier leveraging syntactico semantic structures res", "noun phrase np coreference resolution task determining nps text dialogue refer realworld entity", "difficulty task stems part reliance world knowledge charniak", "exemplify consider following text fragment", "martha stewart hoping people dont run", "celebrity indicted charges stemming", "world knowledge martha stewart celebrity would helpful establishing coreference relation nps", "argue employing heuristics subject preference syntactic parallelism prefers resolving np candidate antecedent grammatical role example would allow us correctly resolve celebrity mitkov marcu ng coreference annotated data eg bengtson roth", "sources world knowledge shown improve coreference resolution improvements typically obtained incorporating world knowledge features baseline resolver composed rather weak coreference model ie mentionpair model small set features ie features adopted soon et als knowledgelean approach", "result questions naturally arise", "first world knowledge still offer benefits used combination richer set features", "second since automatically extracted world knowledge typically noisy ponzetto poesio recentlydeveloped coreference models noise tolerant mentionpair model profit noisily extracted world knowledge", "finally different world knowl proceedings 49th annual meeting association computational linguistics pages portland oregon june", "qc association computational linguistics edge sources shown useful applied isolation coreference system offer complementary benefits therefore improve resolver applied combination", "seek answers questions conducting systematic evaluation different world knowledge sources learningbased coreference resolution", "specifically derive world knowledge encyclopedic sources investigated coreference resolution including framenet baker et al yago suchanek et al addition coreferenceannotated data unannotated data incorporate knowledge features richer baseline feature set previously employed rahman ng evaluate utility using coreference models traditional mentionpair model soon et al recently developed clusterranking model rahman ng", "evaluation corpus contains documents coreferenceannotated using ace annotation scheme well ontonotes annotation scheme hovy et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "P11-1125": {"introduction": ["evaluating sets coreference annotations set documents determine whether usefulness world knowledge sources coreference resolution dependent underlying annotation scheme used annotate documents", "system combination techniques take advantages consensus multiple systems widely used fields speech recognition fiscus mangu et al parsing henderson brill", "stateoftheart system combination methods mt based confusion networks compact graphbased structures representing multiple hypotheses bangalore et al", "confusion networks constructed based string similarity information", "first skeleton backbone sentence selected", "hypotheses aligned skeleton forming lattice arc representing alternative word candidates", "alignment method either modelbased matusov et al et al statistical word aligner used compute hypothesis alignment editbased jayaraman lavie sim et al alignment measured evaluation metric translation error rate ter snover et al", "new translation hypothesis generated selecting best path network", "present novel method system combination exploits syntactic similarity system outputs", "instead constructing stringbased confusion network generate packed forest billot lang mi et al encodes exponentially many parse trees polynomial space", "packed forest confusion forest constructed merging mt outputs regard syntactic consensus", "employ grammarbased method generate confusion forest first system outputs parsed", "second set rules extracted parse trees", "third packed forest generated using variant earleys algorithm earley starting unique root symbol", "new hypotheses selected searching best derivation forest", "grammar set rules limited found parse trees", "spurious ambiguity generation step reduced encoding tree local contextual information nonterminal symbol parent sibling labels using state representation earleys algorithm", "proceedings 49th annual meeting association computational linguistics pages portland oregon june", "qc association computational linguistics experiments carried system combination task fifth workshop statistical machine translation wmt10 directions czech french german spanishto english callisonburch et al found comparable performance conventional confusion network based system combination language pairs statistically significant improve saw forest walked blue forest saw green trees forest found pairwise alignment using first starred hypothesis skeleton", "ments others", "first review stateoftheart method system combination framework based confusion networks", "introduce saw walked blue forest green trees found novel system combination method based con confusion network fusion forest present related work consensus translations", "experiments presentedin section followed discussion conclu sion"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016642975598294377, "p": 0.05, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.033333330338889154, "p": 0.02040816326530612, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01661769956491162, "p": 0.01818181818181818, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016860424846354412, "p": 0.043478260869565216, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["stateoftheart system combination method machine translation mt based confusion networks constructed aligning hypotheses regard word similarities", "introduce novel system combination framework hypotheses encoded confusion forest packed forest representing alternative trees", "forest generated using syntactic consensus parsed hypotheses first mt outputs parsed", "second context free grammar learned extracting set rules constitute parse trees", "third packed forest generated starting root symbol extracted grammar nonterminal rewriting", "new hypothesis produced searching best derivation forest", "experimental results wmt10 system combination shared task yield comparable performance conventional confusion network based method smaller space"]}, "P11-2032": {"introduction": ["saw walked blue green forest trees found", "word alignment crucial early step training statistical machine translation smt systems estimated alignments used constraining set candidates phrasegrammar extraction koehn et al chiang galley et al", "stateoftheart word alignment models ibm models brown et al hmm vogel et al jointlytrained symmetric hmm liang et al contain large number parameters eg word translation probabilities need estimated addition desired hidden alignment variables", "common method inference models expectationmaximization em dempster et al approximation em exact em intractable", "maxi mization eg maximum likelihood ml maximum posteriori map technique em generally prone local optima overfitting", "essence alignment distribution obtained via em takes account likely point parameter space consider contributions points", "problems standard em estimation ibm model pointed moore number heuristic changes estimation procedure smoothing parameter estimates shown reduce alignment error rate effects translation performance reported", "zhao xing note parameter estimation use variational em suffers data sparsity use symmetric dirichlet priors find map solution", "bayesian inference approach paper recently applied several unsupervised learning problems nlp goldwater griffiths johnson et al well tasks smt synchronous grammar induction blunsom et al learning phrase alignments directly denero et al", "word alignment learning problem addressed jointly segmentation learning xu et al", "nguyen et al", "chung gildea", "former works place nonparametric priors known cache models parameters utilize gibbs sampling", "alignment inference neither works exactly bayesian since alignments updated running giza xu et al local maximization nguyen et al", "hand proceedings 49th annual meeting association computational linguisticsshortpapers pages portland oregon june", "qc association computational linguistics chung gildea apply sparse dirichlet prior multinomial parameters prevent fitting", "use variational bayes inference investigate effect bayesian inference word alignment isolation", "recently zhao gildea proposed fertility extensions ibm model hmm place prior parameters inference method actually stochastic em known monte carlo em ml technique sampling used fj associated hidden alignment variable aj whose value ranges word positions corresponding source sentence", "set alignments sentence corpus denoted athe model parameters consist vf ta ble word translation probabilities tef", "joint distribution model1 variables given following generative model3 approximate expected counts estep", "even though report substantial reductions align ep aep ment error rate translation bleu scores improve", "approach paper fully bayesian 1j j1 eaj fj alignment probabilities inferred integrating possible parameter values assuming intuitive sparse prior", "develop gibbs sampler alignments ibm model proposed bayesian setting treat random variable prior", "find suitable prior rewrite vfwhich relevant stateoftheart smt sys tems since model used bootstrapping parameter settings em training higher 1j e1 ef nef vf eorder alignment models many stateofthe tef nef 4art smt systems use model translation probabilities features loglinear model", "eval e1 uate inferred alignments terms endto end translation performance show results variety input data illustrate general applicability proposed technique"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028685822514372142, "p": 0.03125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.18181817681818196, "p": 0.18181818181818182, "r": 0.18181818181818182}, "rouge-l": {"f": 0.05732533087112803, "p": 0.18181818181818182, "r": 0.05405405405405406}}], "abstract": ["work compare translation performance word alignments obtained via bayesian inference obtained via expectationmaximization em", "propose gibbs sampler fully bayesian inference ibm model integrating possible parameter values finding alignment distribution", "show bayesian inference outperforms em tested language pairs domains data set sizes bleu points", "show proposed method effectively addresses wellknown rare word problem emestimated models time induces much smaller dictionary bilingual wordpairs"]}, "P11-3012": {"introduction": ["knowledge first work directly investigate effects bayesian alignment inference translation performance", "summarize accurately determine sentiment answer questions document often necessary able determine relationships entities discussed document partof memberof", "simple sentiment example example bought new car yesterday", "love powerful engine", "determining sentiment author expressing car requires knowing engine part car positive sentiment expressed engine attributed car", "paper examine preliminary results applying relation extraction system jd power associates jdpa sentiment corpus kessler et al", "system uses lexical features prior work classify relations examine system works different subsets jdpa sentiment corpus breaking source documents professionally written reviews blog reviews social networking reviews"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02982825295707428, "p": 0.037037037037037035, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["relation extraction documents allows detection entities discussed document related another eg part", "paper presents analysis relation extraction system based prior work applied jd power associates sentiment corpus examine system works documents range social media", "results examined different subsets jdpa corpus showing system performs much worse documents certain sources", "proposed explanation features used appropriate text strong editorial standards informal writing style blogs"]}, "P12-1001": {"introduction": ["document types represent quite different writing styles see significant difference relation extraction system performs documents different sources", "weight optimization important step building machine translation mt systems", "discrimi native optimization methods mert och mira crammer et al pro hopkins downhillsimplex nelder mead influential improving mt systems recent years", "methods effective tune system maximize automatic evaluation metric bleu serve surrogate objective translation quality", "know single metric bleu enough", "ideally want tune towards automatic metric perfect correlation human judgments translation quality", "nara institute science technology naist many alternatives proposed perfect evaluation metric remains elusive", "result many mt evaluation campaigns report multiple evaluation metrics callisonburch et al paul", "different evaluation metrics focus different aspects translation quality", "example bleu papineni et al focuses wordbased ngram precision meteor lavie agarwal allows stemsynonym matching incorporates recall", "ter snover et al allows arbitrary chunk movements permutation metrics like ribes isozaki et al birch et al measure deviation word order", "syntax owczarzak et al semantics pado et al help", "arguably metrics correspond intuitions good translation", "current approach optimizing mt towards single metric runs risk sacrificing metrics", "really claim system good high bleu low meteor", "similarly highmeteor lowbleu system desirable", "goal propose multiobjective optimization method avoids overfitting single metric", "want build mt system well respect many aspects translation quality", "general cannot expect improve multiple metrics jointly inherent trade offs", "therefore need define notion pareto optimality pareto characterizes tradeoff rigorous way distinguishes set equally good solutions", "describe pareto optimality detail later roughly speaking proceedings 50th annual meeting association computational linguistics pages jeju republic korea july", "qc association computational linguistics hypothesis paretooptimal exist hypothesis better metrics", "contribution paper twofold introduce pmo paretobased multi objective optimization general approach learning multiple metrics", "existing single objective methods easily extended multiobjective using pmo", "show pmo outperforms alternative singleobjective optimization linearly combined metrics multiobjective space metric1 especially obtains stronger results metrics difficult tune individually", "following first explain theory pareto optimality section use build proposed pmo approach section", "experiments nist chineseenglish pubmed englishjapanese translation using bleu ter ribes presented section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.23529411307958487, "p": 0.3333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04432887314574739, "p": 0.2857142857142857, "r": 0.043478260869565216}}], "abstract": ["introduce approach optimize machine translation mt system multiple metrics simultaneously", "different metrics eg bleu ter focus different aspects translation quality multiobjective approach leverages diverse aspects improve overall quality", "approach based theory pareto optimality", "simple implement top existing singleobjective optimization methods eg mert pro outperforms ad hoc alternatives based linearcombination metrics", "discuss issue metric tunability show pareto approach effective incorporating new metrics mt evaluation mt optimization"]}, "P12-1048": {"introduction": ["conclude discussing related work section opportunitieslimitations section", "recent years statistical machine translationsmt rapidly developing novel translation models proposed put practice koehn et al och ney galley et al liu et al chiang chiang", "similar natural language processingnlp tasks smt systems often suffer domain adaptation problem practical applications", "simple reason underlying statistical models always tend closely part work done first authors internship baidu", "approximate empirical distributions training data typically consist bilingual sentences monolingual target language sentences", "translated texts training data come domain smt systems achieve good performance otherwise translation quality degrades dramatically", "therefore significant importance develop translation systems effectively transferred domain another example newswire weblog", "according adaptation emphases domain adaptation smt classified translation model adaptation language model adaptation", "focus adapt translation model trained largescale outofdomain bilingual corpus domainspecific translation task leaving others future work", "aspect previous methods divided categories paid attention collecting sentence pairs information retrieval technology hildebrand et al synthesized parallel sentences ueffing et al wu et al bertoldi federico schwenk senellart exploited full potential existing parallel corpus mixturemodeling foster kuhn civera juan lv et al framework", "approaches focused studies bilingual corpus synthesis exploitation ignoring monolingual corpora therefore limiting potential translation quality improvement", "paper propose novel adaptation method adapt translation model domain specific translation task utilizing indomain proceedings 50th annual meeting association computational linguistics pages jeju republic korea july", "qc association computational linguistics monolingual corpora", "approach inspired recent studies zhao xing zhao xing tam et al gong zhou ruiz federico shown particular translation always appears specific topical contexts topical context information great effect translation selection", "example bank often occurs sentences related economy topic translated yinhang occurs sentences related geography topic translated hean", "therefore cooccurrence frequency phrases specific context used constrain translation candidates phrases", "monolingual corpus bank occurs often sentences related economy topic ones related geography topic likely bank translated yinhang hean", "outofdomain bilingual corpus first incorporate topic information translation probability estimation aiming quantify effect topical context information translation selection", "rescore phrase pairs according phrase topic wordtopic posterior distributions additional indomain monolingual corpora", "compared previous works method takes advantage indomain monolingual corpora outofdomain bilingual corpus incorporate topic information translation model thus breaking corpus barrier translation quality improvement", "experimental effect performance smt system", "phrase probability measures cooccurrence frequency phrase pair lexical probability used validate quality phrase pair checking well words translated", "according definition proposed koehn et al given source sentence f1", "fj", "fj target sentence ei e1", "ei", "ei word alignment subset cartesian product word position", "", "phrase pair said consistent och ney alignment must least word inside phrase aligned word inside phrase words inside phrase aligned word outside phrase", "consistent phrase pairs extracted training corpus phrase probabilities estimated relative frequencies och ney countf countf countf indicates often phrase pair occurs training corpus", "obtain corresponding lexical weight first estimate lexical translation probability distri bution wef relative frequency train ing corpus results nist data set demonstrate effectiveness method", "reminder paper organized follows section provides brief description translation probability estimation", "section introduces adaptation method incorporates topic information translation model section cou nt wef countf retaining alignment phrase pair corresponding lexical weight calculated describes discusses experimental results section briefly summarizes recent related work translation model adaptation"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.04878048387864398, "p": 0.03333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027735009836184408, "p": 0.02857142857142857, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.1739130384877128, "p": 0.16666666666666666, "r": 0.18181818181818182}, "rouge-l": {"f": 0.057769038391880946, "p": 0.16666666666666666, "r": 0.05405405405405406}}], "abstract": ["adapt translation model trained data domain another previous works paid attention studies parallel corpus ignoring indomain monolingual corpora obtained easily", "paper propose novel approach translation model adaptation utilizing indomain monolingual topic information instead indomain bilingual corpora incorporates topic information translation probability estimation", "method establishes relationship outofdomain bilingual corpus indomain monolingual corpora via topic mapping phrasetopic distribution probability estimation indomain monolingual corpora", "experimental result nist chineseenglish translation task shows approach significantly outperforms baseline system"]}, "P12-1079": {"introduction": ["finally end pw ef i1 jj weifj jia conclusion future work section", "topic model hofmann blei et al popular technique discovering underlying topic structure documents", "exploit topic information statistical machine translation smt researchers proposed various topicspecific lexicon translation models zhao xing zhao xing tam et al improve translation quality", "topicspecific lexicon translation models focus wordlevel translations", "models first estimate word translation probabilities conditioned topics adapt lexical weights phrases corresponding author probabilities", "stateofthe art smt systems translate sentences using sequences synchronous rules phrases instead translating word word", "since synchronous rule rarely factorized individual words believe reasonable incorporate topic model directly rule level rather word level", "consequently propose topic similarity model hierarchical phrasebased translation chiang synchronous rule associated topic distribution", "particular given document translated calculate topic similarity rule document based topic distributions", "augment hierarchical phrasebased system integrating proposed topic similarity model new feature section", "discuss section similarity generic rule given source document computed topic similarity model often low", "dont want penalize generic rules", "therefore propose topic sensitivity model rewards generic rules complement topic similarity model", "estimate topic distribution rule based source target side topic models section", "order calculate sim ilarities targetside topic distributions rules sourceside topic distributions given documents decoding project proceedings 50th annual meeting association computational linguistics pages jeju republic korea july", "qc association computational linguistics 30a opera tional capability x1 grands x1 x1 give x1 x1 x2 held talks x1 x2 figure synchronous rules topic distributions", "subgraph shows rule topic distribution xaxis means topic index yaxis means topic probability", "notably rule rule shares source chinese string different topic distributions due different english translations", "targetside topic distributions rules space sourceside topic model oneto many projection section", "experiments chineseenglish translation tasks section show method outperforms baseline hierarchial phrasebased system ble points", "result points higher times faster previous topicspecific lexicon translation method"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["previous work using topic model statistical machine translation smt explore topic information word level", "smt advanced wordbased paradigm phraserulebased paradigm", "therefore propose topic similarity model exploit topic information synchronous rule level hierarchical phrasebased translation", "associate synchronous rule topic distribution select desirable rules according similarity topic distributions given documents", "show model significantly improves translation performance baseline nist chinesetoenglish translation experiments", "model achieves better performance faster speed previous approaches work word level"]}, "P12-2050": {"introduction": ["show sourceside targetside topic distributions improve translation quality improvements complementary", "goal lightweight semantic annotation text particularly scenarios limited resources expertise presents several requirements arepresentation simplicity adaptability new lan guages topics genres coverage", "paper describes coarse lexical semantic annotationof arabic wikipedia articles subject con straints", "traditional lexical semantic representations either narrow scope like named entities1 make reference fullfledged lexiconontology insufficiently cover languagedomainof interest require prohibitive expertise ef fort apply2 therefore turn supersense tags ssts coarse lexical semantic classes fornouns verbs originating wordnet", "previ ously served groupings english lexicon 1some ontologies like sekine et al", "bbn identifinder bikel et al include large selection classes tend especially relevant proper names", "2eg wordnet fellbaum sense annotation effortreported passonneau et al", "found considerable inter annotator variability lexemes framenet baker etal limited coverage even english prop bank kingsbury palmer capture semanticrelationships lexemes", "note omega ontology philpot et al used finegrained cross lingual annotation hovy et al dorr et al", "communication group xcj ad act time guinness book world records considers university alkaraouine fez morocco established year ad oldest university world figure sentence article islamic goldenage supersense tagging anno tators", "arabic shown lefttoright", "entries repurposed target labels direct human annotation", "part earliest versions wordnet supersense categories originally lexicographer classes intended partition english noun verb senses broad groupings semanticfields miller fellbaum", "cently task automatic supersense tagging emerged english ciaramita johnson curran ciaramita altun paa reichartz well italian picca et al picca et al attardi et al chinese qiu et al languages wordnetsmapped english wordnet3 principle lieve supersenses ought apply nouns verbsin language need depend avail ability semantic lexicon4 work focuson noun ssts summarized figure ap plied arabic sentence figure", "ssts refine relate lexical items capture lexical polysemy handeg3note work supersense tagging used text fine grained sense annotations coarsened ssts", "4the nounverb distinction might prove problematic languages", "qjk considers jj guinness aj book jaj ap forrecords thestandard ag", "university alkaraouine ai fez morocco ag", "oldest university artifact location theworld adjak established year ij location proceedings 50th annual meeting association computational linguistics pages jeju republic korea july", "c2012 association computational linguistics crusades damascus ibn tolun mosque imam hussein shrine islamic golden age islamic history ummayad mosque 434s 16185t 5859m atom enrico fermi light nuclear power periodic table physics muhammad alrazi 777s 18559t 6477m summer olympics christiano ronaldo football fifa world cup portugal football team raul gonzales real madrid 390s 13716t 5149m", "computer computer software internet linux richard stallman solaris window system 618s 16992t 5754m table snapshot supersenseannotated data", "article titles translated domain total counts sentences tokens supersense mentions", "overall sentences tokens mentions tokensmention average", "counts exclude sentences marked problematic mentions marked", "disambiguating person vs possession noun principaland generalize lexemes othereg principal teacher student persons", "lumping property might expected give much latitude annotators yetwe find practice possible elicit reason able interannotator agreement even languageother english", "encapsulate interpreta tion tags set brief guidelines aims usable anyone read understand text target language annotators prior expertise linguistics linguistic annotation", "finally note ad hoc categorization schemes unlike ssts developed purposes ranging question answering li roth animacy hierarchy representation corpus linguistics zaenen et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06779660713588063, "p": 0.041666666666666664, "r": 0.18181818181818182}, "rouge-l": {"f": 0.036886483702723245, "p": 0.034482758620689655, "r": 0.10526315789473684}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05121475200160483, "p": 0.05, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04677809313506047, "p": 0.043478260869565216, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["lightweight semantic annotation textcalls simple representation ideally without requiring semantic lexicon achieve good coverage language domainin paper repurpose wordnets super sense tags annotation developing specificguidelines nominal expressions applying arabic wikipedia articles topical domains", "resulting corpus high coverage completed quickly reasonable interannotator agreement"]}, "P13-1110": {"introduction": ["believe interpretation ssts adopted serveas single starting point diverse resource en gineering efforts applications especially finegrained sense annotation feasible", "desire incorporate highdimensional sparse feature representations statistical machine translation smt models driven recent research away minimum error rate training mert och toward discriminative methods optimize features", "examples include minimum risk smith eisner pairwise ranking pro hopkins rampion gimpel smith variations margininfused relaxation algorithm mira watanabe et al chiang et al cherry foster", "objective function optimization method vary optimizer broadly described learning linear model parameter vector used score alternative translation hypotheses", "every smt system machine learning general goal learning find model generalizes well ie yield good translations previously unseen sentences", "dimension feature space increases generalization becomes increasingly difficult", "since small portion sparse features observed relatively small fixed set instances tuning prone overfit training data", "alternative approach solving problem estimating discriminative feature weights directly training bi text tillmann zhang blunsom et al simianer et al usually substantially larger tuning set complementary goal better generalization given fixed size tuning set", "order achieve goal need carefully choose objective optimize perform parameter estimation objective", "focus largemargin methods svm joachims passiveaggressive algorithms mira", "intuitively seek separating distance geometric space hypotheses least large cost incurred selecting incorrect", "criterion performs well practice finding linear separator highdimensional feature spaces tsochantaridis et al crammer et al", "recent advances machine learning shown generalization ability learners improved utilizing second order information second order perceptron cesabianchi et al gaussian margin machines crammer et al 2009b confidence weighted learning dredze crammer arow crammer et al 2009a chiang relative margin machines rmm shivaswamy jebara 2009b", "latter rmm introduced effective less computationally expensive way incorporate spread data second order information proceedings 51st annual meeting association computational linguistics pages sofia bulgaria august", "qc association computational linguistics distance hypotheses projected onto line defined weight vector unfortunately advances machine learning easy apply structured prediction problems smt latter often involve latent variables surrogate references resulting loss functions well explored machine learning mcallester keshet gimpel smith", "although shivaswamy jebara extended rmm handle sequential structured prediction shivaswamy jebara even previously mert shown advantageous"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03008067881492231, "p": 0.05263157894736842, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.040816323048729994, "p": 0.02631578947368421, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02589552824552727, "p": 0.025, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.2857142807256236, "p": 0.3, "r": 0.2727272727272727}, "rouge-l": {"f": 0.08531934253584968, "p": 0.3, "r": 0.08108108108108109}}], "abstract": ["recent advances largemargin learning shown better generalization achieved incorporating higher order information optimization spread data", "solutions impractical complex structured prediction problems statistical machine translation", "present online gradientbased algorithm relative margin maximization bounds spread projected data maximizing margin", "evaluate optimizer chineseenglish arabicenglish translation tasks small large feature sets show learner able achieve significant improvements bleu ter average stateoftheart optimizers large feature set"]}, "P13-1147": {"introduction": ["finally discuss spread key issues rm conclude discussion future work", "relation extraction task extracting semantic relationships entities text eg detect employment relationship person larry page company google following text snippet google ceo larry page holds press announcement headquarters new york", "recent studies relation extraction shown supervised approaches based either feature kernel methods achieve stateoftheart accuracy zelenko et al culotta sorensen first author affiliated department computer science information engineering university trento povo italy design models experiments writing paper", "zhang et al zhou et al zhang et al bunescu nguyen et al chan roth sun et al", "clear drawback supervised methods need training data slow delivery commercial applications new domains labeled data expensive obtain often mismatch training data data system applied", "approaches cope domain changes essential", "problem domain adaptation da transfer learning tl", "technically domain adaptation addresses problem learning assumption independent identically distributed iid samples violated", "domain adaptation studied extensively last couple years various nlp tasks eg shared tasks organized domain adaptation dependency parsing nivre et al petrov mcdonald", "results mixed thus still active research area", "best knowledge almost work adapting relation extraction systems new domains1 prior studies related tasks multitask transfer learning xu et al jiang distant supervision mintz et al clearly related different former problem transfer knowledge old new relation types distant supervision tries learn new relations unlabeled text exploiting weaksupervision form knowledge resource eg freebase", "assume relation types shift underlying besides unpublished manuscript student project clear data used", "httptinyurlcom bn2hdwk proceedings 51st annual meeting association computational linguistics pages sofia bulgaria august", "qc association computational linguistics data distribution", "weak supervision promising approach improve relation extraction system especially increase coverage terms types relations covered", "paper examine related issue changes underlying data distribution keeping relations fixed", "even weakly supervised system expected perform well applied kind text domaingenre thus ideally believe combining domain adaptation weak supervision way go future", "study first step towards", "focus unsupervised domain adaptation ie labeled target data", "moreover consider particular domain adaptation setting single system da ie learning single system able cope different related domains", "studies da far focused building specialized system every specific target domain eg blitzer et al", "", "contrast goal build single system robustly handle several domains line setup recent shared task parsing web petrov mcdonald", "participants asked build single system robustly parse domains reviews weblogs answers emails newsgroups rather build several domainspecific systems", "consider shift considered domain adaptation past adapt source specific target considered somewhat different recent view da became widespread since", "latter assumes target domains isare really known advance", "setup domain adaptation problem boils us terms indicating employment relation person location", "rather matching surface string words lexical similarity enables soft matches similar words convolution tree kernels", "empirical evaluation automatic content extraction ace data evaluate impact convolution tree kernels embedding lexical semantic similarities", "latter derived ways brown word clustering brown et al latent semantic analysis lsa", "first show system aligns well state art ace benchmark", "test system ace data exploits kernels structures similarities domain adaptation"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018438320997158954, "p": 0.14285714285714285, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.07692307358727825, "p": 0.04878048780487805, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07464908480230002, "p": 0.07692307692307693, "r": 0.07272727272727272}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019822282980281738, "p": 0.045454545454545456, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["relation extraction task extracting semantic relationships entities text", "recent studies relation extraction mostly supervised", "clear drawback supervised methods need training data labeled data expensive obtain often mismatch training data data system applied", "problem domain adaptation", "paper propose combine term generalization approaches word clustering latent semantic analysis lsa ii structured kernels improve adaptability relation extractors new text genresdomains", "empirical evaluation ace domains shows suitable combination syntax lexical generalization promising domain adaptation"]}, "P13-2002": {"introduction": ["results show combining huge space tree fragments generalized lexical level provides effective model adapting systems new domains", "wordbased translation models intended model translation process found new uses identifying word correspondences sentence pairs", "word alignments crucial training component machine translation systems", "furthermore useful nlp applications entailment identification", "simplest models use lexical information alone", "seminal model brown et al proved powerful performing nearly well complicated models phrasal systems koehn et al", "minor improvements initialization moore important toutanova galley quite competitive", "subsequent ibm models include detailed information context", "models incorporate positional model based absolute position word models use relative position model instead english word tends align french word nearby french word aligned previous english word", "models incorporate notion fertility number french words align english word", "although latter models covered broad range phenomena estimation techniques map inference challenging", "authors originally recommended heuristic procedures based local search", "methods work reasonably well computationally inefficient guarantees", "thus many researchers switched hmm model vogel et al variants parameters", "captures positional information ibm models framework admits exact parameter estimation inference though objective function concave local maxima concern", "modeling fertility challenging hmm framework violates markov assumption", "hmm jump model considers prior state fertility requires looking whole state space", "therefore standard forwardbackward viterbi algorithms apply", "recent work zhao gildea described extension hmm fertility model using mcmc techniques parameter estimation", "efficient means map inference necessary many applications machine translation", "paper introduces method exact map inference fertility hmm using dual decomposition", "resulting model leads substantial improvements alignment quality", "proceedings 51st annual meeting association computational linguistics pages sofia bulgaria august"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.040353008201822346, "p": 0.07407407407407407, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.2222222174691359, "p": 0.2857142857142857, "r": 0.18181818181818182}, "rouge-l": {"f": 0.036876641994291845, "p": 0.2857142857142857, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.15999999507200013, "p": 0.14285714285714285, "r": 0.18181818181818182}, "rouge-l": {"f": 0.038091521354862184, "p": 0.14285714285714285, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["notion fertility word alignment number words emitted single state useful difficult model", "initial attempts modeling fertility used heuristic search methods", "recent approaches instead use principled approximate inference techniques gibbs sampling parameter estimation", "practice need single best alignment difficult find using gibbs", "building recent advances dual decomposition paper introduces exact algorithm finding single best alignment fertility hmm", "finding best alignment appears important model leads substantial improvement alignment quality"]}, "P13-2015": {"introduction": ["qc association computational linguistics", "coreference resolvers typically evaluated collections news articles cover wide range topics ace ace03 ace04 ace05 ontonotes pradhan et al data sets", "many nlp applications involve text analysis specialized domains clinical medicine gooch roudsari glinos legal text analysis bouayadagha et al biological literature batistanavarro ananiadou castan et al", "learningbased coreference resolvers easily retrained specialized domain given annotated training texts domain", "found retraining offtheshelf coreference resolver domain specific texts showed little benefit", "surprising result led us question nature feature sets used noun phrase np coreference resolvers", "nearly features employed recent systems fall categories string match word overlap syntactic properties eg appositives predicate nominals parse features etc semantic matching eg gender agreement wordnet similarity named entity classes etc", "conspicuously absent systems lexical features allow classifier consider specific words making coreference decision", "researchers experimented lexical features achieved mixed results evaluations broadcoverage corpora bengston roth bjo rkelund nugues rahman ng 2011a", "hypothesized lexicalized features substantial impact domainspecific settings", "lexical features capture domain specific knowledge subtle semantic distinctions important domain", "example based resolutions found domainspecific training sets lexicalized features captured knowledge tomcat coreferent plane uaw coreferent union anthrax coreferent diagnosis", "capturing types domainspecific information often impossible using generalpurpose resources", "example wordnet defines tomcat animal contain entry uaw categorizes anthrax diagnosis differently1 paper evaluate impact lexicalized features domains management succession muc6 data vehicle launches muc7 data disease outbreaks promed texts terrorism muc4 data", "incorporate lexical ized feature sets different coreference architectures reconcile stoyanov et al pairwise coreference classifier sieve raghunathan et al rulebased system"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0395299145297755, "p": 0.05, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["coreference resolvers rely heavily string matching syntactic properties semantic attributes words lack ability make decisions based individual words", "paper explore benefits lexicalized features setting domainspecific coreference resolution", "show adding lexicalized features offtheshelf coreference resolvers yields significant performance gains domainspecific data sets types coreference resolution architectures"]}, "P13-2122": {"introduction": ["results show lexicalized features significantly improve performance domains types coreference architectures", "conversational spoken language translation cslt systems facilitate communication subjects speak language", "current systems typically used achieve specific task eg vehicle checkpoint search medical diagnosis etc", "taskdriven disclaimer paper based upon work supported darpa bolt program", "views expressed authors reflect official policy position department defense us government", "distribution statement approved public release distribution unlimited conversations typically revolve around set central topics evident beginning interaction", "conversation progresses gradual accumulation contextual information used infer topics discussion deploy contextually appropriate translation phrase pairs", "example word drugs predominantly translate spanish medicamentos medicines medical scenario whereas translation drogas illegal drugs predominate law enforcement scenario", "cslt systems take highlevel global context account instead translate utterance isolation", "often results contextually inappropriate translations particularly problematic conversational speech usually exhibits short spontaneous often ambiguous utterances", "paper describe novel topicbased adaptation technique phrasebased statistical machine translation smt spoken conversations", "begin building monolingual latent dirichlet allocation lda topic model training conversations conversation corresponds document lda paradigm", "runtime model used infer topic distribution evolving test conversation including current utterance", "translation phrase pairs originate training conversations whose topic distribution similar current conversation given preference single similarity feature augments standard phrasebased smt loglinear model", "topic distribution test conversation updated incrementally new utterance available history grows", "approach demonstrate significant improvements baseline phrasebased smt system measured bleu ter nist scores englishtoiraqi cslt task", "proceedings 51st annual meeting association computational linguistics pages sofia bulgaria august"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018851537450315963, "p": 0.08333333333333333, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["describe translation model adaptation approach conversational spoken language translation cslt encourages use contextually appropriate translation options relevant training conversations", "approach employs monolingual lda topic model derive similarity measure test conversation set training conversations used bias translation choices towards current context", "significant novelty adaptation technique incremental nature continuously update topic distribution evolving test conversation new utterances become available", "thus approach wellsuited causal constraint spoken conversations", "englishtoiraqi cslt task proposed approach gives significant improvements baseline system measured bleu ter nist", "interestingly incremental approach outperforms nonincremental oracle upfront knowledge whole conversation"]}, "P160500_w06": {"introduction": ["qc association computational linguistics", "automatic extraction hyponymy entities specific field often difficult step accomplish developing ontology knowledge bases construction knowledge graphs regarded crucial part developing highlevel natural language applications", "many algorithms developed achieve hyponymy automatically including dictionarybased pattern based statistical machine learning", "dictionarybased method used acquire hyponymy usually depends field words synonyms approximate synonyms manmade dictionary", "instance nakaya obtained english domain concepts hyponymy wordnet", "moreover li et al used geographical names dictionary acquisition toponym ontology concept relations chinese", "coverage wordnet hownet still limited general field weak coverage domain specific terms named entities", "patternbased methods mainly use linguistics natural language processing technologies consider patterns rules summarized according inherent regular language", "example chinese sentence scenic spots lijiang contains lijiang old town yulong jokul lugu lake etc lijiang lijiang old town yulong jokul lugu lake entities ei used represent entity makes e1 lijiang e2 lijiang old town e3 yulong jokul e4 lugu lake pattern e1 contains e2 e3 e4 obtained e1 hypernym e2 e3 e4 context", "several researchers studied subject regards english language", "hearst raised lexicalsyntactic patterns avoided use precoding method knowledge automatic recognition hyponymy large text corpus tuan et al put forward build term classification hierarchy model based combination pattern matching statistical methods bansal et al used automatic acquisition pattern belief propagation algorithm obtain high precision hypernym arbitrary noun phrases", "addition article published original", "english language various studies conducted chinese", "example wu et al established universal grammar pattern library hyponymy relationship according partwhole relationship different kinds forms view isa pattern tang et al demonstrated hyponym concepts using syntactic parsing results tian et al proposed combination bootstrap ping method chinese doubly anchored pattern hypernym", "patternbased method highly accurate specific sentences compared english chinese language complicated sentence structure semantic difficult use technique generalization", "statistical machine learning method mainly based corpus linguistic knowledge statistical language model uses machinelearning algorithm obtain concept hyponymy relationship", "field english fan et al considered task matrix factorization problem extract relation", "chinese xia et al proposed method graph clustering extract hypo nymy fu et al joined variety features svm support vector machine based rbf radial basis function kernel", "statistical machine learning considered main method hyponymy extraction due advantages reducing manual annotation extracting information automatically", "methods described generally extract entity hyponymy relations simple sentence", "complex sentences necessary cooperate syntactic patterns entity recognition well approaches improve overall performances especially chinese", "addition patterns extracted using bootstrapping method"], "introduction_label": [{"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02942848246114071, "p": 0.08, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.036363633163636645, "p": 0.022727272727272728, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01527794735476414, "p": 0.020833333333333332, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.042553187904029274, "p": 0.027777777777777776, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015320453167827461, "p": 0.023255813953488372, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0143731875638612, "p": 0.05263157894736842, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014154213133862024, "p": 0.06666666666666667, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013783517841510365, "p": 0.16666666666666666, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["extracting entity hyponymy chinese complex sentences highly difficult process", "paper proposes novel hybrid approach combines parsing supervised learning semisupervised learning", "first conditional random fields crf model employed obtain candidate domain named entity", "pattern matching used acquire candidate hyponymy", "next predicate symbol features syntactic analysis semantic roles introduced crf features template identify hyponymy entity pairs", "finally analysis parallel relationship entities sentences entity pairs simple sentences conducted obtain hyponymy entity pairs chinese complex sentences", "experimental results show proposed method reduces manual work required crf markers improved overall performance comparison baseline methods", "keywords chinese complex sentences hyponymy entity identification crf pattern matching syntactic analysis doi 103103s0146411616050035"]}, "P39_p07": {"introduction": ["current paper proceeds follows section describes methods framework section discusses process hyponymy acquisition section reports results section presents conclusions", "hypergraph demonstrated huang chi ang compact datastructure en code exponential number hypotheses gener ated regular phrasebased machine translation mt system eg koehn et al", "syntax based mt system eg chiang", "hypergraph represents large set transla tions quite possible desired transla tions eg reference translations con tained hypergraph due pruning inherent deficiency translation model", "case often required find translations hy pergraph similar desired transla tions similarity computed via automatic metric bleu papineni et al", "maximally similar translations called oracle best translations process extracting oracle extraction", "oracle extraction nontrivial task computing similarity hypothesis requires information scattered many items hypergraph exponentially large number hypotheses makes bruteforce linear search intractable", "therefore efficient algorithms exploit structure hypergraph required", "present efficient oracle extraction algo rithm involves key ideas", "firstly view oracle extraction bottomup model scoring process hypergraph model trained reference translations", "sim ilar algorithm proposed lattice dreyer et al", "", "algorithm requires maintaining separate dynamic programming state distinguished sequence state words number sequences huge mak ing search slow", "secondly therefore present novel lookahead technique called equiv alent oraclestate maintenance merge multiple states equivalent similarity computation", "experiments show equivalent oracle state maintenance technique significantly speeds times oracle extraction", "efficient oracle extraction least im portant applications machine translation", "discriminative training discriminative train ing objective tune model parameters eg weights perceptron model conditional random field reference translations preferred competitors", "reference translations reachable translation system case oraclebest hypotheses substituted training", "proceedings naacl hlt short papers pages boulder colorado june", "association computational linguistics system combination typical system combi nation task eg rosti et al", "compo nent system produces set translations grafted form confusion network", "confusion network rescored often employ ing additional language models select fi nal translation", "measuring goodness hypothesis confusion network requires score component system", "translations confusion network reachable component systems case systems score similar reachable translation serves good approximation", "multisource translation multisource translation task och ney input given multiple source languages"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["hypergraphs used several syntax inspired methods machine translation compactly encode exponentially many trans lation hypotheses", "hypotheses closest given reference translations therefore cannot found via brute force particularly pop ular measures closeness bleu", "develop dynamic program extracting called oraclebest hypothesis hyper graph viewing problem finding likely hypothesis ngram language model trained refer ence translations", "identify move massive redundancies dynamic program state due sparsity ngrams present reference translations resulting efficient program", "present run time statistics program demon strate successful application hypothe ses thus found targets discriminative training translation system components"]}, "P41718_w06": {"introduction": ["leads situation analogous system combination except component translation system corresponds specific source language", "aim create publicly available russian paraphrase corpus could applied information extraction ie text summarization ts compression", "believe corpus helpful paraphrase identication generation russian focus sentential paraphrases", "indeed sentential corpus impose specic methods paraphrase identication generation researcher", "corpus representative enough serve dataset experiments extraction word phrase syntactic level para phrases", "paraphrase restatement text conveys meaning another form", "natural language processing nlp tasks paraphrase identication genera tion shown helpful ie question answering machine trans lation ts text simplication etc paraphrase identication used detect plagiarism remove redundancies ts ie paraphrase generation expand queries information retrieval question answering patterns ie", "paraphrase generation useful text normalization textual entailment recognition tasks", "far denition paraphrase concerned generally implies message expressed dierent words prescribe portion text replaced paraphrasing", "neither state whether common knowledge springer international publishing switzerland braslavski et al", "eds russir ccis pp", "", "doi used judging similarity messages", "consequence ambiguity researchers believe paraphrases absolute semantic equivalence others allow bidirectional textual entailment messages convey roughly meaning", "let us consider example corpus bt oe poa tele2 ae ee", "vtb might sell shares tele2 nearest weeks bt aocpoa poay tele2", "vtb announced sale tele2 although clear sentences describe event rst additional details indication time fact shares going sold", "human judger hisher knowledge world might consider sentences paraphrases", "intend teach machine identify semantically equivalent paraphrases threshold paraphrases higher", "hand second sentence considered summarization rst therefore types paraphrases used automatic ts", "research intend construct paraphrase corpus ie ts", "believe former task requires semantically equivalent precise paraphrases latter demands roughly similar ones socalled loose paraphrases like example", "thus important us distinguish precise paraphrases pp loose paraphrases lp constructing paraphrase corpus", "today already number available paraphrase resources microsoft para phrase corpus wellknown", "wide number metrics paraphrase identication english evaluated corpus", "russian publicly available paraphrase resources known us exception dataset published ganitkevich et al part para phrase database project", "latter includes paraphrases word phrase syntactic levels paraphrase pair annotated set count prob abilitybased features", "corpus used ie ts lacks infor mation context paraphrases", "believe context original sentences provided could improve nlp tasks", "aim constructing sentential corpus", "thus task construct corpus pps lps make helpful paraphrase identication generation ie ts text compression tasks"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01588980230000093, "p": 0.1111111111111111, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper presents crowdsourcing project creation publicly available corpus sentential paraphrases russian", "collected news headlines corpus could applied information extraction text summarization", "collect news headlines dierent agencies real time paraphrase candidates extracted headlines using unsuper vised matrix similarity metric", "provide userfriendly online interface crowdsourced annotation available paraphraserru", "annotated sentence pairs moment included corpus", "annotation process going current version corpus freely available httpparaphraserru", "keywords russian paraphrase corpus lexical similarity metric unsupervised paraphrase extraction crowdsourcing"]}, "P846406_w06": {"introduction": ["research part ongoing crowdsourcing project available para phraserru current results available paraphraserruscorerstat", "almost beginnings artificial intelligence clear automated systems require knowledge reason intelligently multipurpose widedomain robust reasoning amount required nontrivial", "experience especially expert systems 1970s illustrated hard acquire enough right knowledge difficult formalize knowledge ways suitable supporting reasoning", "naturally dream arose enable systems read text learn", "dream never realized", "fact research knowledge representation reasoning krr natural language processing nlp progressed areas diverged point today less entirely separate unrelated conferences journals research paradigms", "years ago research groups funded vulcan inc participated audacious experiment called project halo manually convert information contained chapter high school textbook chemistry knowledge representation statements knowledge representation system take standard high school advanced placement ap exam", "surprisingly systems passed albeit relatively low level performance", "project engendered wide interest see friedland et al", "past year darpa funded groups us conduct pilot studies investigate feasibility building fully learning reading lbr systems", "largest project mbius consortium researchers numerous institutions", "goal petr sojka ivan kopec ek karel pala eds tsd lnai pp", "", "oc springerverlag berlin heidelberg design general framework lbr systems future advise darpa wisdom funding new program area", "typical questions include feasible fully automated lbr", "different phasescomponentssteps lbr", "current levels capability component technologies major bottlenecks failure points", "kind research would best dream lbr", "sorts results could expect years", "remaining projects proceed independently report back mbius", "smaller 9month efforts focuses specific aspects general lbr problem project jointly boeing sri led peter clark boeing focuses mismatch english sentences equivalent knowledge representations propositions methodology building manually representations carefully selected extract pages chemistry textbook", "project northwestern university led ken forbus concentrates processes selfguided inference occurs new information read", "called introspection rumination processes work parallel reading serve source expectations questions background checking", "project focuses selected sentences chemistry textbook thoughts arise", "project cyc corp led michael witbrock addresses problem learning meaning new unknown words context", "starting knowledge already inside large ontology reasoning system cyc researchers develop methods apply inferences order build likely interpretations sentence hypothesize meaning unknown word", "fourth project subject paper", "located isi investigating much done combining traditional statistical nlp methods kinds krr absolutely required points process", "parsed whole chemistry textbook developed methods convert parses shallow logical form investigating types semantics added support reasoning required question answering"], "introduction_label": [{"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03987906708892196, "p": 0.05555555555555555, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.07975813417789403, "p": 0.1111111111111111, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03848295671005285, "p": 0.043478260869565216, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.06063432835753497, "p": 0.05555555555555555, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030990173846653226, "p": 0.02857142857142857, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03946333181507231, "p": 0.07692307692307693, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03848295671005285, "p": 0.043478260869565216, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["long dream build computer systems learn automatically reading text", "dream generally considered infeasible surprising developments us past years led funding several shortterm investigations whether much best current practices natural language processing knowledge representation reasoning combined actually enable dream", "paper briefly describes efforts learning reading project isi converted high school textbook chemistry shallow logical form investigating semantic features plausibly added support kinds inference required answering standard high school text questions"]}, "P87-94": {"introduction": ["paper briefly outline architecture general aspects isis lbr project finish august namely months time writing paper", "word alignment dened procedure detecting corresponding words bilingual sentence pair", "notorious criticisms word alignment inconsistence word alignment model phrase based translation model", "paper intention avoid mentioning inherent weakness word alignment would say far know word alignment fundamental component smt systems", "phrase higher level translation knowledge extracted based word alignment called twostage approach", "even approaches socalled direct phrase alignment rarely abandon word alignment thoroughly", "computation complexity phrase alignment word alignment usually used constrain inference1 denero proposes relative pure joint phrase model still uses word alignment initialization smoothing shows least dependency word alignment2 neubig uses bayesian methods inversion transduction grammar joint phrase alignment3 base distribution dirichlet process prior constructed word alignment model", "therefore word alignment well worth concern", "hope induce better word alignment nojihigashi kusatsu shiga japan", "li ikeda utilizing stateoftheart learning technology establish better baseline word level alignment models", "bayesian inference approach adopt paper broadly applied various learning latent structure", "goldwater points theoretical factors contribute superiority bayesian inference7 first integrating parameter values leads greater robustness decision", "problems trouble em algorithm overtting", "moore discusses details garbage collector generated11 suggests number heuristic solutions bayesian inference oer principled solution", "second factor integration permits use priors favoring sparse distributions proved consistent nature natural language", "another practical advantage implementation much easier em12 following sections review ibm model section reformulate simpler bayesian form section", "section gives bayesian inference section reports results experiment"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present approximative ibm model word alignment", "dierent widelyused word aligner giza implements ibm models hmm model framework expectation maximum em adopt full bayesian inference integrates possible parameter values rather estimating single parameter value", "empirical results show promising improvements alignment quality well bleu score translation performance baselines", "keywords bayesian inference word alignment statistical machine translation"]}, "P96-1010": {"introduction": ["section compares related research section concludes", "spelling correction become common tech nology often perceived problem progress made", "conventional spelling checkers unix spell concerned spelling errors result words cannot found word list given language", "analysis shown spelling errors result elementary typographical er rors character insertion deletion transposition yield another valid word language peterson", "errors remain undetected tradi tional spelling checkers", "addition typographical errors words easily confused instance homophones peace piece remain undetected", "recent studies actual ob served spelling errors estimated overall errors resulting valid words account anywhere errors depending application kukich", "use term contextsensitive spelling cor rection refer task fixing spelling errors result valid words peace cake", "peace typed piece intended", "task cast lexical disambigua tion given predefined collection confu sion sets peace piece etc circumscribe space spelling errors look", "confusion set means word set could mistakenly typed another word set intended", "task predict given occurrence word confusion sets word set actually intended", "previous work contextsensitive spelling cor rection related lexical disambiguation tasks limitations", "wordtrigram methods mays dam erau mercer require extremely large body text train wordtrigram model even extensive training sets problem sparse data often acute", "addition huge wordtrigram tables need available run time", "word trigrams ineffective capturing long distance properties discourse topic tense", "featurebased approaches bayesian clas sifiers gale church yarowsky deci sion lists yarowsky bayesian hybrids golding varying degrees suc cess problem contextsensitive spelling correction", "report experiments show methods limited effective ness cases theyre predominant distinction made words syntactic", "confusion set train test freq", "base theyre youre begin passed past quiet quite weather whether accept except lead led cite sight site principal principle raise rise affect effect peace piece country county amount number past quite whether except led sight principle nse effect peace country number table performance baseline method confusion sets", "train test give number occurrences word confusion set training test corpora", "freq word confusion set occurred often training corpus", "base percentage correct predictions baseline system test corpus", "paper first introduce method called trigrams uses partofspeech trigrams en code context", "method greatly reduces number parameters compared known methods based word trigrams", "method advantage training done quite manageably con fusion sets new confusion sets added later without additional training", "feature makes trigrams easily expandable system", "empirical evaluation trigram method demonstrates performs well words discriminated different parts speech poorly part speech", "latter case reduced simply guessing whichever word confusion set com mon representative partofspeech class", "consider alternative method bayes bayesian hybrid method golding case words part speech", "confirm experimentally bayes trigrams complementary performance trigrams better words confusion set dif ferent parts speech bayes better part speech", "introduce hybrid method tribayes exploits com plementarity invoking method strongest", "tribayes achieves best accuracy methods consideration situations", "evaluate performance tribayes spect external standard compare grammar checker microsoft word", "tribayes found substantially higher performance", "paper organized follows first present methodology used experiments", "discuss methods mentioned interleaved experimental results", "comparison mi crosoft word presented"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012384679545314518, "p": 0.09090909090909091, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01223768932697917, "p": 0.2, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024769359090600476, "p": 0.18181818181818182, "r": 0.024390243902439025}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012299775313490156, "p": 0.125, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.012526521290086582, "p": 0.06666666666666667, "r": 0.012195121951219513}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper addresses problem cor recting spelling errors result valid though unintended words peace piece quiet quite problem correcting particular word usage errors amount num ber", "cor rections require contextual information handled conventional spelling programs unix spell", "first introduce method called trigrams uses partofspeech trigrams encode context", "method uses small num ber parameters compared previous methods based word trigrams", "ever effectively unable distinguish words part speech", "case alternative featurebased method called bayes per forms better bayes less effective trigrams distinction words depends syntactic constraints", "hybrid method called tribayes troduced combines best pre vious methods", "improvement performance tribayes compo nents verified experimentally", "tribayes compared grammar checker microsoft word found sub stantially higher performance"]}, "P98-1046": {"introduction": ["final section concludes", "paper specifically address questions polysemy respect verbs regular extensions meaning achieved adjunction particular syntactic phrases", "see verb classes key making gen eralizations regular extensions mean ing", "current approaches english classifica tion levin classes wordnet limita tions applicability impede utility general classification schemes", "present refinement levin classes intersec tive sets finegrained clas sification coherent sets syn tactic frames associated semantic compo nents", "preliminary indications membership intersective sets compatible wordnet orig inal levin classes"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "P98-1081": {"introduction": ["begun ex amine related classes portuguese find verbs demonstrate similarly coherent syntactic semantic properties", "paper examine differences modelling different data driven systems performing nlp task exploited yield higher accuracy best individual system", "means experiment involving task morphosyntactic wordclass tagging", "wellknown tagger generators hidden markov model memorybased transformation rules maximum entropy trained corpus data", "comparison outputs combined using several voting strategies second stage classifiers"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "P98-2138": {"introduction": ["combination taggers outperform best component best combination showing lower error rate best individual tagger", "optical character recognition ocr useful wide range applications office automation information retrieval system", "ocr thailand still widely used partly existing thai ocrs quite satisfactory terms accuracy", "cently several research projects focused spelling correction many types errors cluding ocr kukich", "nev ertheless strategy slightly different language language since characteristic language different", "characteristics thai make task error correction different languages explicit word boundary characters written levels ie middle upper lower levels", "order solve prob lem ocr error correction first task usually detect error strings input sen tence", "languages explicit word boundary english word separated others white spaces task comparatively simple", "tok enized string found dictionary could error string unknown word", "languages ex plicit word boundary chinese japanese thai task much complicated", "even without errors ocr difficult determine word boundary languages", "situation gets worse noises intro duced text", "existing approach correcting spelling error languages word boundary assumes substrings input sentence error strings tries correct nagata", "computationally expensive since large portion input sentence correct", "characteristic thai writing system many levels placing thai char acters several characters occupy level", "characters easily con nected characters upper lower level", "connected characters cause diffi culties process character segmentation cause errors thai ocr", "problems specific thai realword error another source er rors difficult correct", "several previous works spelling correction demonstrated figure explicit word delimiter thai upper level tophne middle level baseline lower level featurebased approaches effective solving problem", "paper hybrid method thai ocr error correction proposed", "method com bines partofspeech pos trigram model featurebased model", "first pos tri gram model employed correct nonword well realword errors", "step num ber nonword errors mostly reduced realword errors still remain pos trigram model cannot capture use ful features discriminating candidate words", "featurebased approach using winnow algo rithm applied correct remaining errors", "order overcome expensive com putation cost existing approach pro pose idea reducing scope correc tion using word segmentation algorithm find approximate error strings put sentence", "though word segmentation algorithm cannot give accurate boundary error string many give clues unknown strings error strings", "use information reduce scope correction entire sentence mir row scope"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018759018759073556, "p": 0.09090909090909091, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02005494505503871, "p": 0.04, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["languages explicit word bound ary thai chinese japanese cor recting words text harder english additional ambiguities locating er ror words", "traditional method handles hypothesizing every substrings input sentence could error words trying correct", "paper pro pose idea reducing scope spelling correction focusing dubious areas input sentence", "boundaries dubious areas could obtained approximately ap plying word segmentation algorithm finding word sequences low probability", "gener ate candidate correction words used modified edit distance reflects charac teristic thai ocr errors", "finally partof speech trigram model winnow algorithm combined determine probable correction"]}, "P9852_p00": {"introduction": ["next capture characteristic thai ocr errors defined modi fied edit distance use enumerate plau sible candidates deviate word question kedit distance", "morphology study way words built smaller units called morphemes minimal meaningbearing units language ju rafsky", "example english word kind consists single morpheme root word kind whilst word players consists mor phemes play er", "morphemes kind play stand alone words affixes er must appear bound another morpheme", "applying set morphological rules produce morphemes formation relating words example grammatical category whole word well subcategorisation frame word verb", "process called morphological analysis", "respect value morphological ana lyser would twofold theoretical linguis tic viewpoint useful tool linguistic modelling testing certain analyses", "research part collaborative research project", "funded arc discovery grant dp0877595", "hand practical viewpoint supports many applications eg information retrieval search engines machine translation others", "currently interest developing morphological tools indonesian language", "previous work siregar adriani et al", "discuss development indonesian stemmers recover root affixed word implemented procedurally", "stemmers limited use provide lin guistic information beyond stem", "hartono presents initial version morphologi cal analyser developed pckimmo un fortunately handle reduplication key aspect indonesian morphology", "morpho logical analyser developing describe designed able handle rich semantic lexical grammatical information associated words word formation nlp applications", "section first discuss indonesian mor phology followed brief explanation level morphology section", "sections present work applying twolevel morphol ogy indonesian language"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0397417652978834, "p": 0.05263157894736842, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03699025622525012, "p": 0.16666666666666666, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03774928774937501, "p": 0.125, "r": 0.03571428571428571}}], "abstract": ["paper presents efforts developing indonesian morphological analyser pro vides detailed analysis rich affixation process model indonesian morphology us ing twolevel morphology approach decom posing process set morphotactic morphophonemic rules", "rules modelled network finite state transduc ers implemented using xfst lexc", "approach able handle reduplication nonconcatenative morphological process"]}, "P99-1061": {"introduction": ["finally section presents results evaluations carried developed analyser", "paper describes several generallyapplicable techniques help unification based parser process input efficiently robustly", "well presenting number new methods report significant improvements made existing techniques", "methods preserve correctness sense rule legal rule applications", "particular none techniques involve statistical approximate processing", "claim methods independent concrete parser neutral respect given unificationbased grammar theoryformalism", "gain reasonable efficiency parsing using large integrated grammars several thousands huge lexicon entries", "belief single method achieves goal alone", "instead develop use set cheap filters correct sense", "indicate section combining methods leads speedup parsing time reduction space consumption order magnitude applied mature well engineered unificationbased parsing system", "implemented methods extensions hpsg grammar development environment uszkoreit et al employs sophisticated typed feature formalism krieger schifer krieger schifer advanced agendabased bottomup chart parser kiefer scherf", "specialized runtime version system currently used verbmobil primary deep analysis component", "next sections report transformations applied knowledge base grammarlexicon modifications core formalism unifier type system", "section describe given parser extended filter possible rule applications efficiently performing expensive unification", "section shows compute best partial analyses order gain certain level robustness", "finally present empirical results demonstrate efficiency gains speculate extensions intend work near future", "different sections refer corpora used measure effects methods"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03946333181507231, "p": 0.07692307692307693, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0395299145297755, "p": 0.05, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.16666666170138905, "p": 0.15384615384615385, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0789266636300543, "p": 0.15384615384615385, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes new improved techniques help unificationbased parser process input efficiently robustly", "combination methods result speedup parsing time order magnitude", "methods correct sense none rule legal rule applications"]}, "PEAAI_n09": {"introduction": ["reference corpora english german japanese consist samples", "irony paralinguistic element used guratively express concept semantic meaning different actual initial purpose", "challenging eld computational linguistics natural language processing due high ambiguity difculty detect objectively", "language use vigorous creative predened consensual agreement recognize ironic expression due high subjectivity involved", "last decade irony expression thriving social networks particularly twitter characters restraint status updates perfect good old liners", "public social medium users realize writings read reproduced potentially everyone gaining popularity followers", "publicity contrary facebooks real name policy often direct consequences everyday lives since majority participate anonymously using avatar nickname", "nocensorship state contributes freedom expressing personal thoughts tough taboo unpopular controversial issues part contains political satire", "corresponding author", "tel", "spathis", "email addresses p11charioniogr charalampakis p11spationiogr sdimitriscsdauthgr spathis p11kousioniogr kouslis kermanioniogr kermanidis", "political satire signicant part comedy specializes drawing entertainment politics", "times aims please", "nature offer constructive view used part criticism tends simply pinpoint unexpected different", "high topicality twitter combined ephemerality political news forms state described asecho chamber groupthinking effect virtually enclosed spaces ampli ed repetition colleoni et al", "result occasional user might write something political jump bandwagon without initial conscious aim criticize", "adding politics topic almost everybody familiar makes sense engagement attention side write obama instead obscure book read", "studies focus simultaneous usage twitter tv circumstances like political debate metatalk tweets reveal critical scrutiny agenda debate debate kalsnes et al", "rapidly changing web plethora available text especially social networks unlabeled raw unprocessed", "adding traditional supervised methods quite techniques enable us take huge unstructured data account", "insight previous work subjectivity involved tagging text ironic", "authors took tedious task annotation could agree considered ironic", "result cannot gold standard corpus ironic tweets", "main motivation explore semisupervised techniques since take httpdxdoiorg101016jengappai201601007 elsevier ltd rights reserved", "account train test data", "specic technique chose collective classication type semisupervised learning presents interesting method optimizing classication partiallylabeled data", "considering empirical study tries detect irony corpus greek political tweets training classier using appropriate linguistic features proposed rst time herein irony detection", "goal nd relation ironic tweets refer political parties leaders greece preelection period actual election results", "compare semisupervised results supervised ones previous research", "regarding novelty study rst exploration eld irony detection semisupervised learning application politics", "remainder paper organized follows section present related literature topics irony detection twitter sentiment analysis political expression", "next sections dedicated data preprocessing representation schema set linguistic features affect irony detection", "section describes training procedure evaluation algorithms performance test procedure large unlabeled dataset"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01944752536201737, "p": 0.05555555555555555, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01827095812881336, "p": 0.25, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019822282980281738, "p": 0.045454545454545456, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018851537450315963, "p": 0.08333333333333333, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present work describes classication schema irony detection greek political tweets", "hypothesis states humorous political tweets could predict actual election results", "irony detection concept based subjective perceptions relying human annotator driven labor might best route", "proposed approach relies limited labeled training data thus semisupervised approach followed collectivelearning algorithms take labeled unlabeled data consideration", "compare semi supervised results supervised ones previous research", "hypothesis evaluated via correlation study irony party receives twitter respective actual election results greek parliamentary elections difference results ones preceding elections"]}, "PMTS_n09": {"introduction": ["overview study limitations future research prospects summary empirical study described section", "recent years statistical machine translation smt technology used many online applications concentrating professionally edited enterprise quality online content", "time little research gone adapting work done cngl school computing dcusmt technology translation user generated content web", "translation online chats flournoy callisonburch received attention surprisingly little work translation online user forum data despite growing interest area flournoy rueppel", "paper describe efforts building system address particular application area", "experiments conducted data collected online forums symantec security tools services1 multinational company like symantec primary motivation behind translation user forum data enable access language barriers information forums", "forum posts rich information issues problems tools services provided company often provide solutions problems even traditional customercare help lines even aware", "major challenge developing mt systems user forum data concerns lack proper parallel training material", "forum data monolingual hence cannot used directly train smt systems", "use parallel training data form symantec enterprise translation memories tms different product service domains train smt models", "auxiliary source used portions europarl dataset2 koehn selected according similarity forum data section supplement tm based training data", "symantec tm data part enterprise documentation professionally httpcommunitynortoncom httpwwwstatmtorgeuroparl edited large conforms symantec controlled language guidelines signicantly different nature user forum data loosely moderated use controlled language", "contrast europarl data ofdomain respect forum data", "differences available training test datasets necessitate use adaptation techniques optimal translation", "use mixture model adaptation foster kuhn creating individual models different sources data combining using different weights", "monolingual forum posts used language modelling along target side tm training data", "system trained symantec tm forum data serves baseline system", "experiments conducted englishgerman en de englishfrench enfr language pairs special emphasis translation english", "sake completeness report translation scores directions", "apart using models created concatenation indomain symantec tm outof domain europarl datasets used linear log linear combination frameworks combine individual models", "translation models language models separately combined using methods effect adaptation measured translation output using established automatic evaluation metrics", "experiments reveal current task terms translation quality language model adaptation effective translation model adaptation linear combination performs slightly better loglinear setting", "remainder paper organized follows section briey describes related work relevant context", "section reports tools algorithms used along description datasets used", "section focuses mixture modelling experiments weights learnt different settings"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014261417003088998, "p": 0.058823529411764705, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.13793102977407862, "p": 0.1111111111111111, "r": 0.18181818181818182}, "rouge-l": {"f": 0.02897473623959897, "p": 0.09523809523809523, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013880835210649011, "p": 0.1111111111111111, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013961900663032979, "p": 0.09090909090909091, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01420711093254263, "p": 0.0625, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014487368119843434, "p": 0.047619047619047616, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0286337308693425, "p": 0.1111111111111111, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013961900663032979, "p": 0.09090909090909091, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013919906055406692, "p": 0.1, "r": 0.0136986301369863}}], "abstract": ["paper reports experiments adapting components statistical machine translation smt system task translating online usergenerated forum data symantec", "data monolingual differs available bitext mt training resources number important respects", "reason adaptation techniques important achieve optimal results", "investigate use mixture modelling adapt models specic task", "individual models created different indomain outofdomain data sources combined using linear loglinear weighting methods different components smt system", "results show profound effect language model adaptation translation model adaptation respect translation quality", "surprisingly linear combination outperforms loglinear combination models", "best adapted systems provide statistically signicant improvement absolute bleu points relative absolute bleu points relative baseline system englishgerman englishfrench respectively"]}, "PS15684_w09": {"introduction": ["section presents experiments analysis results followed conclusions future work section", "vast amounts natural language text available web well largescale repositories much redundant", "task paraphrase extraction focuses identication text units convey meaning", "detection similar text complicated due rich variability natural languages", "large scale corpora another factor poses hurdles paraphrase extraction", "performing oneonone matching sentences practically ruled even assumed suitable paraphrase recognition system available", "therefore efcient techniques required identify possibly similar candidates large scale corpora subject processing detect exact matches", "effective paraphrase extraction system benet various natural language processing applications multidocument summarization plagiarism detection question answering document clustering", "signicant aspect work novel twolevel fuzzy clustering technique proposed sentencelevel paraphrase extraction", "similar sentences tend describe similar actions fuzzy agglomerative clustering based verbs corresponding author", "tel", "email addresses ctrpsggmailco chitra anupriya rajkumaryahoocoin rajkumar", "tel", "performed initially", "divisive clustering applied identify subgroups sentences center nouns", "support vector machine svm based paraphrase recognizer used identify paraphrases cluster", "performance paraphrase extraction system evaluated using microsoft research paraphrase corpus msrpc subset microsoft research video description corpus msrvdc", "outline paper follows section contains overview previous work related paraphrase extraction hierarchical clustering", "section describes methodology adopted extracting paraphrases using fuzzy hierarchical clustering approach machine learning based paraphrase recognizer", "section presents results experiments conducted using different corpora"], "introduction_label": [{"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0140065771795307, "p": 0.08333333333333333, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0140065771795307, "p": 0.08333333333333333, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014316865434707999, "p": 0.05555555555555555, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01405368928519407, "p": 0.07692307692307693, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01420711093254263, "p": 0.0625, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013844925538298282, "p": 0.125, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013880835210649011, "p": 0.1111111111111111, "r": 0.0136986301369863}}], "abstract": ["paraphrase extraction involves discovery equivalent text segments large corpora nds application tasks multidocument summarization document clustering", "semantic similarity identication challenging problem compounded large size corpus", "paper stage approach involves clustering followed paraphrase recognition proposed extraction sentencelevel paraphrases text collections", "order handle ambiguity inherent variability natural language fuzzy hierarchical clustering approach combines agglomeration based verbs division nouns used", "sentences resultant cluster processed machinelearning based paraphrase recognizer discover paraphrases", "twostage approach applied microsoft research paraphrase corpus subset microsoft research video description corpus", "performance evaluated existing kmeans clustering approach well cosinesimilarity technique fuzzy cmeans clustering stage system consistently demonstrated better performance", "elsevier bv rights reserved"]}, "PSMPT_n09": {"introduction": ["possible directions future work applications discussed section concludes paper", "stateoftheart hierarchical phrasebased smt model chiang uses bilingual phrase pairs phrasebased smt pbsmt koehn et al starting point learn hierarchial rules using probabilistic synchronous contextfree grammar pscfg", "decoding process hierarchical phrasebased smt hpb model based bottomup chart parsing chiang", "chart parsing decoder known hiero require explicit syntactic representation either side phrases rules", "stateoftheart smt models koehn et al chiang viewed loglinear combinations features och ney usually comprise translational features language model", "translational features typically involved models express dependencies source target phrases dependencies phrases source language ie take account contexts phrases", "word sense disambiguation wsd task intricately related mt typically employs rich context sensitive features determine contextually likely sense polysemous word", "inspired contextrich wsd techniques researchers tried integrate various contextual knowledge sources stateoftheart smt models", "recent years source context modelling successfully employed pbsmt taking various contextual information source phrase account", "contextual features include lexical features words appearing context bearing sense discriminatory information positionspecific neighbouring words gimenez marquez stroppa et al shallow deep syntactic features gimpel smith full sentential context carpuat wu lexical syntactic descriptions form supertags haque et al 2009a grammatical dependency relations haque et al 2009b", "limitation hiero chiang shares pbsmt model koehn et al take account contexts sourcesides rules appear", "words argued rule selection hiero suboptimally modelled", "far small number studies made use sourcelanguage context improving rule selection hiero", "positionspecific neighbouring words partofspeech pos prove effective source contexts hpb model et al", "study involving pbsmt haque et al", "2009b showed translations ambiguous words influenced distant words sentence", "syntactic contexts capture longdistance dependencies words sentence useful means disambiguate translations", "accordingly integration syntactic contexts could lead improved translation quality pbsmt", "instance haque et al", "2009a showed supertags powerful source contexts neighbouring words partofspeech tags disambiguate source phrase pbsmt", "inspired haque et al 2009a present work extend stateoftheart hiero system adopting lexical entries robust efficient supertagging approaches", "grammars approaches consist syntactically rich lexicon small set combinatory operators", "combinatory rules combine syntactically rich lexical entries together form parse trees", "supertaggers assign syntactic structure elementary tree lexical category word sentence", "syntactic structures supertag provide rich complex linguistic information describe pos tag word subcategorisation information hierarchy phrase categories word appears", "remainder paper organized follows", "section discuss related work", "section provides brief overview hpb", "section describe contextinformed features contained baseline hpb model", "section de scribe memorybased classification approach", "section describes experimental setups", "section presents results obtained offers brief qualitative analysis"], "introduction_label": [{"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01482260827022887, "p": 0.037037037037037035, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01410298625953104, "p": 0.07142857142857142, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014316865434707999, "p": 0.05555555555555555, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013737499389592766, "p": 0.25, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013783517841510365, "p": 0.16666666666666666, "r": 0.0136986301369863}}], "abstract": ["statistical machine translation smt models recently begun include source context modeling assumption proper lexical choice translation ambiguous word determined context appears", "various types lexical syntactic features explored effective source context improve phrase selection smt", "present work introduce lexicosyntactic descriptions form supertags sourceside context features stateoftheart hierarchical phrasebased smt hpb model", "features enable us exploit source similarity addition target similarity modelled language model", "experiments kinds supertags employed lexicalized treeadjoining grammar ltag combinatory categorial grammar ccg", "use memorybased classification framework enables efficient estimation features", "despite differences supertagging approaches give similar improvements", "evaluate performance approach englishtodutch translation task report statistically significant improvements bleu scores translation quality adding ccg ltag supertags respectively contextinformed features"]}, "PbaneaCSL": {"introduction": ["section formulate conclusions offer avenues work", "sentiment subjectivity analysis seeks automatically identify opinions beliefs speculations emotions sentiments private states natural text wiebe et al", "quirk et al", "define private state state lend objective external validation words person observed assert god exists believe god exists", "belief sense private", "field natural language processing researchers used term subjectivity analysis denote identifying private states text namely separating objective subjective instances sentiment polarity analysis refines subjective text positive negative neutral", "sentiment subjectivity analysis stemmed prolific area research mainly due fact numerous text processing applications stand gain incorporating sentiment dimensions models including automatic expressive texttospeech synthesis alm et al tracking sentiment timelines online forums news balog et al lloyd et al mining opinions product reviews hu liu", "many natural language processing tasks subjectivity sentiment classification used first phase filtering generate viable data", "research benefited additional layering ranges question paper recommended acceptance prof rk moore", "corresponding author", "tel", "email addresses carmenbaneagmailcom banea radacsuntedu mihalcea wiebecspittedu wiebe", "see front matter elsevier ltd rights reserved", "httpdxdoiorg101016jcsl201303002 answering yu hatzivassiloglou conversation summarization carenini et al text semantic analysis wiebe mihalcea esuli sebastiani lexical substitution su markert", "experiments carried english wiebe mihalcea shown robust subjectivity delineation occurs sense word level", "following finegrained perspective esuli sebastiani andreevskaia bergler proposed methods embed senselevel automatic sentiment annotations objectiveneutral negative positive english wordnet structure miller using relationships synonymy antonymy meronymy etc", "hand noticing scarcity hand crafted senselevel subjectivitypolarity lexica markert su explored ways infer data annotated either word sentence level", "senselevel subjectivity crosslingual subjectivity sentiment analysis received considerable attentions recent years paper explores area lies intersection topics", "knowledge area formally investigated techniques similar applied sentiment subjectivity analysis sentence review level work explores difficult task senselevel subjectivity involves deep semantic aspects language", "manual annotation study performed task crosslingual senselevel subjectivity annotations well methods proposed crosslingual multilingual learning using dictionaries multiple languages novel knowledge", "work seeks answer following questions", "first word senses aligned languages subjectivity content consistent words subjective sense language map subjective sense language similarly objective sense", "second employ multilingual framework automatically discover new subjectiveobjective senses starting limited amount annotated data", "seek answer first question conducting manual annotation study section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.039215682891195994, "p": 0.025, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017280516301179202, "p": 0.021739130434782608, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017120432038195812, "p": 0.037037037037037035, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01641971564131067, "p": 0.058823529411764705, "r": 0.015625}}, {"rouge-1": {"f": 0.17647058385813158, "p": 0.13043478260869565, "r": 0.2727272727272727}, "rouge-l": {"f": 0.03385899814481677, "p": 0.08333333333333333, "r": 0.03125}}, {"rouge-1": {"f": 0.19354838251821027, "p": 0.15, "r": 0.2727272727272727}, "rouge-l": {"f": 0.05015014461792053, "p": 0.14285714285714285, "r": 0.046875}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0157128749490135, "p": 0.2, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["recent research english word sense subjectivity shown subjective aspect entity characteristic better delineated sense level instead traditional word level", "paper seek explore whether senses aligned languages exhibit trait consistently case investigate property leveraged automatic fashion", "first conduct manual annotation study gauge whether subjectivity trait sense robustly transferred language boundaries", "automatic framework introduced able predict subjectivity labeling unseen senses using either crosslingual multilingual training enhanced bootstrapping", "show multilingual model consistently outperforms crosslingual accuracy iterations", "elsevier ltd rights reserved", "keywords sentiment text classification multilingual subjectivity analysis sense level subjectivity"]}, "Pjournal": {"introduction": ["second question propose models see section crosslingual multilingual able simultaneously use information extracted several languages making subjectivity senselevel predictions", "loglinear phrasebased statistical machine translation pbsmt koehn et al probability pek fk target phrase ek given source phrase fk modelled loglinear combination features typically consist finite set translation features language model och ney", "translation features normally used models express dependencies source target phrases phrases tokens source language", "stroppa et al", "observed incorporating sourcelanguage context using neighbouring words partofspeech tags potential improve translation quality", "led whole tranche research provide overview sect", "shown integrating source context modelling pbsmt positively influence weighting selection target phrases thus improve translation quality", "approaches include sourcelanguage context help select appropriate target phrases partly inspired methods used wordsense disambiguation wsd rich contextual features employed determine likely sense polysemous word given context", "contextual features include lexical features words appearing immediate context gimnez mrquez stroppa et al shallow deep syntactic features sentential context gimpel smith full sentential context carpuat wu", "studies syntactic features employed made use partofspeech taggers stroppa et al supertaggers haque et al 2009a shallow deep syntactic parsers gimpel smith haque et al 2009b", "prior work shown exploring local sentential context information form supertags haque et al 2009a syntactic dependencies haque et al 2009b successfully integrated pbsmt model", "provide revised extended account previous research", "add number novel aspects including using semantic roles new contextual features pbsmt adding new language pairs examining scalability research larger amounts training data", "results allow us conclude incorporating sourcelanguage contextual features benefits range different language pairs english source language translating dutch chinese japanese hindi spanish czech target language dutch different types data news articles commentary parliamentary debates patents subtitles according range automatic evaluation measures", "remainder contribution organized follows", "section provides motivation work direction related work discussed sect", "", "section provides brief overview pbsmt acts baseline throughout study", "sect", "describe range contextinformed features add baseline pbsmt model", "section describes memorybased classification approach integrated output memorybased classifier stateoftheart pbsmt system", "sect", "present results obtained", "formulate conclusions sect"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016134462682727335, "p": 0.07692307692307693, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016929499072460552, "p": 0.041666666666666664, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016860424846354412, "p": 0.043478260869565216, "r": 0.015625}}, {"rouge-1": {"f": 0.042553187904029274, "p": 0.027777777777777776, "r": 0.09090909090909091}, "rouge-l": {"f": 0.017449745518204156, "p": 0.024390243902439025, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.031779604599970875, "p": 0.2222222222222222, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713948979603, "p": 0.3333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015657719579972886, "p": 0.3333333333333333, "r": 0.015625}}], "abstract": ["translation features typically used phrasebased statistical machine translation pbsmt model dependencies source target phrases phrases source language", "swathe research demonstrated integrating source context modelling directly loglinear pbsmt positively influence weighting selection target phrases thus improve translation quality", "contribution present revised extended account previous work using range contextual features including lexical features neighbouring words supertags dependency information", "add number novel aspects including use semantic roles new contextual features pbsmt adding new language pairs examining scalability research larger amounts training data", "results mixed feature selections classifier hyperparameters language pairs learning curves observe including contextual features source sentence general produces improvements", "significant improvements involve integration longdistance contextual features dependency relations combination partofspeech tags dutchtoenglish subtitle translation combination dependency parse semantic role information englishtodutch parliamentary debate translation supertag features englishtochinese translation", "haque naskar way cngl school computing dublin city university dublin ireland van den bosch ilk research group tilburg center cognition communication tilburg university tilburg netherlands email antalvdnboschuvtnl keywords statistical machine translation phrasebased statistical machine translation syntax machine translation translation modelling word alignment memorybased classification"]}, "Pmert_n09": {"introduction": ["offer avenues work", "minimum error rate training emerged decade ago och superior training method small numbers linear model parameters machine translation systems improving prior work using maximum likelihood criteria och ney", "technique quickly rose prominence becoming standard many research commercial mt systems", "variants operating lattices macherey et al hypergraphs kumar et al subsequently developed benefit reducing approximation error nbest lists", "primary advantages mert twofold", "directly optimizes evaluation metric consideration eg bleu instead surrogate loss", "secondly offers globally optimal line search", "unfortunately several potential difficulties scaling mert larger numbers features due nonconvex loss function lack regularization", "challenges prompted researchers move away mert favor linearly decomposable approximations evaluation metric chiang et al hopkins cherry foster correspond easier optimization problems naturally incorporate regularization", "particular recent work chiang et al shown adding thousands tens thousands features improve mt quality weights optimized using marginbased approximation", "simulated datasets hopkins found conventional mert struggles find reasonable parameter vectors smooth loss function based pairwise ranking optimization pro performs much better real data pro method appears least good mert small feature sets scales better number features increases", "paper seek preserve advantages mert addressing shortcomings terms regularization search", "idea adding regularization term mert objective function perplexing first common regularizers directly applicable mert", "indeed regularizers scale sensitive mert objective function scaling weight vector neither changes predictions linear model affects error count", "hence mert hedge regularization penalty maximally scaling linear model weights", "first contribution paper analyze various forms regularization susceptible scaling problem", "analyze experiment form regularization scale insensitive", "present new parameterizations regularization apply regularization scalesenstive linear transforms original linear model", "addition introduce efficient methods set reference translations rs r1", "rs", "yields following optimization problem incorporating regularization och 2003s exact line searches", "regularizers methods let us find true optimum regularized arg min ers efs s1 objective function along line", "arg min ers sm sm efs finally address issue searching highdimensional space using gradient expected bleu smith eisner find better s1 m1 search directions line searches", "direction finder addresses serious concerns raised hopkins mert widely failed reach optimum synthetic linear objective function", "replicating hopkins mays experiments confirm existing search algorithms mertincluding coordinate ascent powells algorithm powell random direction sets cer et al 2008perform poorly experimental condition", "using gradientbased direction finder mert problem finding true optimum even 1000dimensional space", "results suggest combination regularized objective function gradientinformed line search algorithm enables mert scale well large number features"], "introduction_label": [{"rouge-1": {"f": 0.10810810392987599, "p": 0.07692307692307693, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03435458436594656, "p": 0.07142857142857142, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332868888906, "p": 0.10526315789473684, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03328595119649386, "p": 0.1, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1111111068672841, "p": 0.08, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03411983411993764, "p": 0.07692307692307693, "r": 0.03125}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01606839679849738, "p": 0.08333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01641971564131067, "p": 0.058823529411764705, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["minimum error rate training mert remains preferred methods tuning linear parameters machine translation systems faces significant issues", "first mert unregularized learner therefore prone overfitting", "second commonly used noisy nonconvex loss function becomes difficult optimize number parameters increases", "address issues study addition regularization term mert objective function", "since standard regularizers inapplicable mert due scale invariance objective function turn regularizers0 modification present methods efficiently integrating search", "improve search large parameter spaces present new direction finding algorithm uses gradient expected bleu orient merts exact line searches", "experiments features show extensions mert yield results comparable pro learner often used large feature sets"]}, "Pmorph_p00": {"introduction": ["experiments features show extensions mert yield results comparable pro hopkins parameter tuning method known effective large feature sets", "morphology domain linguistics studies formation words", "traditional distinguish surface forms analyses called lemmas", "lemma surface form english word bigger typically consists traditional dictionary citation form word together terms convey morphological properties particular form", "example lemma bigger might represented bigadjcomp indicate bigger comparative form adjective big", "alternatively morphological properties might encoded terms attributevalue pairs catadj degrcomp", "challenges modeling naturallanguage morphology", "morphotactics", "words typically composed smaller units stems axes mustbe combined certain order"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["theory realizational morphology presented stump inuential book inflectional morphology describes derivation inected surface forms underlying lexical forms means ordered blocks realization rules", "theory presents rich formalism expressing generalizations phenomena commonly found morphological systems natural languages", "paper demonstrates spite apparent complexity stumps formalism system whole powerful collection regular relations", "consequently stumpstyle description morphology particular language lingala bulgarian compiled nitestate transducer maps underlying lexical representations directly corresponding surface forms forms vice versa yielding single lexical transducer", "illustration present explicit nitestate implementation analysis lingala based stumps description sources"]}, "Ponto_w06": {"introduction": ["languages build words concatena tion languages exhibit nonconcatenative processes interdigitation reduplication", "ontology extraction traditionally relied pattern matching algorithms", "hearst introduced hyponymic extraction using lexicosyntactic patterns", "hearst algorithm system looks instances certain expressions text example infers relations isa isa", "systems usually based regular expressions text postagged text sentences containing apposition bracketing longdistance dependencies uncommon structures coded explicitly", "example sentence extracted wikipedia encyclopaedia firemouth cichlid typical commonly seen pet stores cichlasomatype south american cichlids", "obtaining relationship firemouth cichlid isa cichlid involves identification hyponym hypernym first last noun phrases lengthy sentence", "suggests traditional methods might improved using deeper syntactic semantic analysis", "work presented investigates use semantic model address issues", "robust minimal recursion semantics rmrs provides argumentbased representation sentences", "theoretical idea problematic sentence rmrs output would contain predicate associated identity copula first argument corresponding term firemouth cichlid second argument corresponding cichlids regardless word order modification", "thus obtained rmrs representation given corpus would possible extract ontological relationships semantic structure abstracts morphological syntactic details affect ontological relationship", "pointed pennacchiotti pantel ontology extraction systems far focused generalised isa partof relationships", "work involves extracting general hyponymic relations rmrs applying filter results obtain biological taxonomic relationships", "corpus gathered extracting animal articles wikipedia online encyclopaedia httpwwwwikipediaorg providing semiedited setting added robustness semantics might prove usefulness", "next section paper gives overview relevant prior work", "followed description extraction system based rmrs using hard wired rules", "results discussed light different evaluation methods covering manual automatic recall precision", "brief overview given system still development aim automate pattern extraction process"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018438320997158954, "p": 0.14285714285714285, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019045760677469336, "p": 0.07142857142857142, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018374341951265707, "p": 0.16666666666666666, "r": 0.01818181818181818}}], "abstract": ["investigate extraction ontologies biological text using semantic representation derived robust parser", "use semantic representation avoids problems traditional patternbased approaches complex syntactic constructions longdistance dependencies", "discovery taxonomic relationships explored corpus consisting animalrelated articles online encyclopaedia wikipedia", "semantic representation used robust minimal recursion semantics rmrs", "initial experiments show good results systematising extraction variety hyponymic constructions", "key words ontologies ontology extraction wikipedia semantics"]}, "Pproc2014_n09": {"introduction": ["conclusion presents different avenues future work", "opinion mining sentiment analysis identifies positive negative opinions many kinds texts reviews blogs news articles", "exploited many application areas review mining election analysis information extraction", "previous research focusses explicit opinion expressions recent work addresses type opinion inference arises opinions expressed toward events positive negative effects entities deng et al deng wiebe", "call events effect events2 deng wiebe show sentiments toward wordnet httpwordnetprincetonedu", "term goodforbadfor used previous papers deng et al deng wiebe deng et al since decided effect better term", "entity propagated entities via opinion inference rules", "give following example bill would curb skyrocketing health care costs", "writer expresses explicit negative sentiment skyrocketing toward object health care costs", "event curb negative effect costs since reduced", "reason writer positive toward event negative effect costs toward writer negative", "reason writer positive toward bill since agent positive event", "deng wiebe show inferences exploited significantly improve explicit sentiment analysis systems", "achieve results system developed deng wiebe requires instances effect events corpus manually provided input", "system fully automatic needs able recognize effect events automatically", "paper addresses methods creating lexicons events support work opinion inference", "discovered significant sense ambiguity meaning words often mixtures senses classes effect effect null", "thus develop senselevel rather wordlevel lexicon", "goals investigate whether effect property tends shared semanticallyrelated senses another use method applies word senses senses words given wordlevel lexicon", "thus build graphbased model node wordnet sense edges represent semantic wordnet relations senses", "addition hypothesized glosses contain useful information", "thus develop supervised gloss classifier define hybrid model gives best overall performance", "finally wordnet verb senses incorporated model investigate ability method identify unlabeled senses likely effect senses"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020119488342091166, "p": 0.038461538461538464, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01914580265104028, "p": 0.06666666666666667, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["recently work nlp initiated type opinion inference arises opinions expressed toward events positive negative effects entities effect events", "paper addresses methods creating lexicon events support work opinion inference", "due significant sense ambiguity goal develop senselevel rather wordlevel lexicon", "maximize effectiveness different types information combine graphbased method using wordnet1 relations standard classifier using gloss information", "hybrid gives best results", "provide evidence model effective way guide manual annotation find effect senses seed set"]}, "Pproc9_w11": {"introduction": ["find iteratively labeling topweighted unlabeled senses rerunning model used effective method guiding annotation efforts", "word expression semantically compositional meaning understood literal meaning components", "therefore semanti cally compositional expressions involve eg small island hot water hand seman tically noncompositional expressions eg red tape kick bucket", "notion compositionality closely related idiomacy higher compositionality lower idiomacy vice versa sag et al baldwin kim", "noncompositional expressions often referred multiword expressions mwes", "baldwin kim differentiate following subtypes pavel pecina charles university prague faculty mathematics physics institute formal applied linguistics prague czech republic pecinaufalmffcunicz compositionality lexical syntactic semantic prag matic statistical", "paper concerned semantic compositionality", "compositionality feature word expressions discrete", "instead expressions populate con tinuum extremes idioms free word combinations mccarthy et al bannard et al katz fazly baldwin kim biemann giesbrecht", "typical ex amples expressions extremes zebra crossing blind alley", "research compositionality motivated hypothesis special treatment se mantically noncompositional expressions im prove results various natural language process ing npl tasks shown example acosta et al", "utilized mwes information trieval ir", "besides nlp ap plications benefit knowing degree compositionality expressions machine translation carpuat diab lexicography church hanks word sense disambigua tion finlayson kulkarni partofspeech pos tagging parsing seretan listed rarnisch", "main goal paper present anal ysis previous approaches using wsms de termining semantic compositionality expres sions", "analysis found section", "special attention paid evaluation pro posed models described section", "section presents first intuitive experimental setup results oflsa applied disco task", "sec tion concludes paper", "proceedings 9th workshop multiword expressions mwe pages atlanta georgia june"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["research focuses determining seman tic compositionality word expressions us ing word space models wsms", "discuss previous works employing wsms present differences proposed approaches include types wsms corpora preprocess ing techniques methods determining com positionality evaluation testbeds", "present results approach determining semantic compositionality based comparing distributional vectors expressions components", "vec tors obtained latent semantic analy sis lsa applied ukwac corpus", "results outperform participants distributional semantics composi tionality disco shared task"]}, "Pproc_d09": {"introduction": ["association computational linguistics", "handling word order differences languages main challenges statistical machine translation smt today", "differences natorally handled syntactic level since pertaio entire syntactic constituents", "present syntactically motivated discrimioa tive reordering model", "model exploits reorder ing structure word alignment target sentence unknown", "structure allows us treat reordering problem dependency parsing problem", "use standard datadriven de pendency parser predict reorderings instead de pendencies"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present new discriminative reordeting model statistical machine translation", "model employs standard datadriven depen dency parser predict reordetings based syntactic information", "made possi ble introduction reorderiog structure word alignment structure target word order transposed onto source sentence path", "approach iotegrated io phrasebased system", "exper iments show large iocrease io long distance reorderiogs", "automatic human evalu ations show substantial increases io translation quality english german task"]}, "Pproc_d10": {"introduction": ["integrated phrasebased smt psmt framework koehn et al", "wordbased translation models intended model translation process found new uses iden tifying word correspondences sentence pairs", "word alignments crucial training com ponent machine translation systems", "fur thermore useful nlp applica tions entailment identification", "simplest models use lexical infor mation alone", "seminal model brown et al proved powerful per forming nearly well complicated models phrasal systems koehn et al", "minor improvements initializa tion moore important toutanova galley quite competitive", "subsequent ibm models include detailed information context", "models incorporate positional model based absolute position word models use relative position model instead english word tends align french word nearby french word aligned previous english word", "models incorporate tion offertility number french words align english word", "although latter models covered broad range phenomena estimation techniques map inference challenging", "au thors originally recommended heuristic proce dures based local search", "meth ods work reasonably well computation ally inefficient guarantees", "thus many researchers switched hmm model vogel et al variants parameters", "captures posi tional information ibm models frame work admits exact parameter estimation infer ence though objective function concave local maxima concern", "modeling fertility challenging hmm framework violates markov assump tion", "hmm jump model considers prior state fertility requires looking whole state space", "therefore standard forwardbackward viterbi algorithms apply", "recent work zhao gildea de scribed extension hmm fertility model using mcmc techniques parameter es timation", "efficient means map inference necessary many applications machine translation", "paper introduces method exact map inference fertility hmm using dual de composition", "resulting model leads sub stantial improvements alignment quality", "proceedings 51st annual meeting association computational linguistics pages sofia bulgaria august"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01944752536201737, "p": 0.05555555555555555, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.040353008201822346, "p": 0.07407407407407407, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1999999950500001, "p": 0.2222222222222222, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03717445423213492, "p": 0.2222222222222222, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019733995308583406, "p": 0.047619047619047616, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14814814331961607, "p": 0.125, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03849335077529405, "p": 0.125, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018851537450315963, "p": 0.08333333333333333, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["notion fertility word alignment number words emitted sin gle state useful difficult model", "initial attempts modeling fertility used heuristic search methods", "recent ap proaches instead use principled ap proximate inference techniques gibbs sampling parameter estimation", "practice need single best alignment difficult find us ing gibbs", "building recent advances dual decomposition paper introduces exact algorithm finding sin gle best alignment fertility hmm", "finding best alignment appears impor tant model leads substantial improvement alignment quality"]}, "Pproc_w09": {"introduction": ["association computational linguistics", "wide variety applications natural language processing cast terms texttotext gen eration", "given input form natural lan guage texttotext generation system produces natural language output subject set constraints", "compression systems instance pro duce shorter sentences", "paraphrases ie differ ing textual realizations meaning crucial components texttotext generation sys tems successfully applied tasks multidocument summarization barzilay et barzilay query expansion arr ick tipirneni riezler eta ques tion answering mckeown ravichandran hovy sentence compression cohn la pata zhao et simplification wubben et", "paraphrase collections texttotext generation extracted variety different corpora", "several approaches rely bilingual para", "lei data bannard callisonburch zhao et callisonburch ganitkevitch et others leverage distributional meth ods monolingual text corpora lin pantel bhagat ravichandran", "far ever ouly preliminary studies undertaken combine information sources chan eta", "paper describe extension gan itkevitch et", "2011s bilingual databased ap proach", "augment bilinguallysourced para phrases using features based monolingual distri butional similarity", "specifically show using monolingual distributional similarity features improves paraphrase quality beyond achieve features esti mated bilingual data", "define distributional similarity para phrase patterns contain constituentlevel gaps eg simone jj instance np jj case np", "generalizes distributional similarity contiguous phrases", "compare different types monolingual distributional information show used achieve siguificant improve ments grammaticality", "finally compare method several strong baselines texttotext generation task sentence compression", "method shows stateoftheart results beating purely bilingually sourced paraphrasing system", "first joint conference lexical computational semantics sem pages montreal canada june", "association computational linguistics ijiinsithe term would", "ihre langfrisligen ple worden", "ne seine lang en aufzuqeben", ""], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["previous work paraphrase extraction application relied either parallel datasets distributional similarity met tics large text corpora", "approach combines orthogonal sources formation directly integrates paraphrasing systems loglinear model", "compare different distributional similar ity featuresets show significant improve ments grarnmaticality meaning reten tion example texttotext generation task sentence compression achieving state oftheart quality"]}, "Psem_p07": {"introduction": ["without giving longlenn plans figure 1pivotbased paraphrase extraction con tiguous phrasestwo phrases translating phrase foreign language assumed paraphrases another", "terplus terp1 snover et al automatic evaluation metric machine translation mt scores translation hypothesis foreign language text source translation source text created human translator refer reference translation", "set possible correct translations large possibly infinite reference translation represents single point space", "frequently multiple reference translations typically 4are provided give broader sampling space correct translations", "automatic mt evaluation metrics compare hypothesis set reference translations assign score similarity better score given hypothesis similar references", "terp follows methodology builds upon already existing evaluation metric translation error rate ter snover et al", "addition assigning score hypothesis ter provides alignment hypothesis reference enabling useful beyond general translation evaluation", "ter shown correlate well translation quality several flaws considers exact matches measuring similarity hypothesis reference compute measure similarity single reference", "handicap using single reference addressed constructing lattice reference translationsthis technique used combine output multiple translation systems rosti et al", "terp utilize methodology instead addresses exact matching flaw ter", "addition aligning words hypothesis reference exact matches terp uses stemming synonymy allow matches words", "uses probabilistic phrasal substitutions align phrases hypothesis reference", "phrase substitutions generated considering possible paraphrases reference words", "matching using stems synonyms banerjee lavie well using paraphrases zhou et al kauchak barzilay shown beneficial automatic mt evaluation", "paraphrases shown additionally useful expanding number references used evaluation madnani et al although used fashion terp", "use synonymy stemming paraphrases allows terp better cope limited number reference translations provided", "terp top metrics submitted nist metricsmatr challenge przybocki et al highest average rank test conditions snover et al", "first discuss original ter metric sect", "", "sect", "present details various enhancements ter", "briefly review alignment capability terp along examples sect", "", "finally sect", "show results optimizing terp human judgments adequacy compare established evaluation metrics followed analysis relative benefits new features terp sect", "", "terp named nicknameterpof university maryland college park mascot"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02377273381650473, "p": 0.05263157894736842, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13793102977407862, "p": 0.1111111111111111, "r": 0.18181818181818182}, "rouge-l": {"f": 0.0713182014493086, "p": 0.15789473684210525, "r": 0.06521739130434782}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1538461512426036, "p": 0.5, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021778435239977228, "p": 0.5, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes new evaluation metric terplus terp automatic evaluation machine translation mt", "terp extension translation edit rate ter", "builds success ter evaluation metric alignment tool addresses several weaknesses use paraphrases stemming synonyms well edit costs automatically optimized correlate better various types human judgments", "present correlation study comparing terp bleu meteor ter illustrate terp better evaluate translation adequacy", "keywords machine translation evaluation paraphrasing alignment snover madnani dorr laboratory computational linguistics information processing institute advanced computer studies university maryland college park md usa email snoverumiacsumdedu madnani email nmadnaniumiacsumdedu dorr email bonnieumiacsumdedu schwartz bbn technologies cambridge usa email schwartzbbncom"]}, "Pstat_p00": {"introduction": ["diamondback terrapin", "xerox research centre europe produced morphological analyzer modern standard arabic henceforth arabic beesley", "javaapplet interface added allow testing internet using standard arabic orthography", "analyzergenerator based dictionaries earlier project alpnet beesley buckwalter system extensively redesigned rebuilt using xerox finitestate technology beesley karttunen", "system analyzes orthographical words include full partial diacritics diacritics present automatically constrain ambiguity output", "fully voweled spelling terse english gloss returned analysis", "system intended serve pedagogical aid comprehensionassistance tool component larger naturallanguageprocessing systems"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03987906708892196, "p": 0.05555555555555555, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes finitestate morphological analyzer modern standard arabic words tested internet", "overview system provided including history finitestate technology dictionary coverage", "research system scheduled testing commercial development"]}, "Q13-1015": {"introduction": ["morphological analyses morphological analyzer word figure generic morphological analyzer black box", "mapping natural language meaning representations central challenge nlp", "much recent progress unsupervised distributional semantics meaning word induced based usage large corpora", "approach useful range key applications including question answering relation extraction lin pantel poon domingos yao et al", "semantics automically induced escapes limitation depending relations handbuilt training data knowledge bases ontologies proved limited use capturing huge variety meanings expressed language", "distributional semantics largely developed isolation formal semantics literature", "whilst distributional semantics effective modelling meanings content words nouns verbs less clear applied meanings function words", "semantic operators determiners negation conjunctions modals tense mood aspect plurals ubiquitous natural language crucial high performance many practical applications current distributional models struggle capture even simple examples", "conversely computational models formal semantics shown low recall practical applications stemming reliance ontologies wordnet miller model meanings content words bobrow et al bos markert", "example consider needed answer question like google buy youtube", "following sentences"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019822282980281738, "p": 0.045454545454545456, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["introduce new approach semantics combines benefits distributional formal logical semantics", "distributional models successful modelling meanings content words logical semantics necessary adequately represent many function words", "follow formal semantics mapping language logical representations differ relational constants used induced offline distributional clustering level predicate argument structure", "clustering algorithm highly scalable allowing us run corpora size gigaword", "different senses word disambiguated based induced types", "outperform variety existing approaches widecoverage question answering task demonstrate ability make complex multisentence inferences involving quantifiers fracas suite"]}, "Q13-1024-parscit130908": {"introduction": ["google purchased youtube", "word alignment task identifying word correspondences parallel sentence pairs", "word alignment become vital component statistical machine translation smt systems since required almost stateoftheart smt systems purpose extracting phrase tables even syntactic transformation rules koehn et al galley et al", "past decades generative word alignment models ibm models brown et al hmm model vogel et al widely used primarily trained bilingual sentences unsupervised manner implementation freely available giza toolkit och ney", "word alignment quality generative models still far satisfactory smt systems", "recent years discriminative alignment models incorporating linguistically motivated features become increasingly popular moore taskar et al riesa marcu saers et al riesa et al", "models usually trained manually annotated parallel data", "moving new language pair large amount handaligned data usually unavailable expensive create", "practical way improve largescale word alignment quality introduce syntactic knowledge generative model train model unsupervised manner wu yamada knight lopez resnik denero klein pauls et al", "paper take dependency cohesion fox account assumes phrases dominated disjoint dependency subtrees tend overlap translation", "instead treating dependency cohesion hard constraint lin cherry using feature discriminative models cherry lin 2006b treat dependency cohesion distortion constraint integrate modified hmm word alignment model softly influence probabilities alignment candidates", "propose approximate em algorithm explicit gibbs sampling algorithm train model unsupervised manner", "experiments largescale chineseenglish translation task demonstrate model achieves improvements word alignment quality machine translation quality", "remainder paper organized follows section introduces dependency cohesion transactions association computational linguistics", "action editor chris callisonburch", "submitted published", "c2013 association computational linguistics", "constraint word alignment", "section presents generative model word alignment using dependency cohesion constraint", "section describes algorithms parameter estimation", "discuss analyze experiments section", "section gives related work"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020119488342091166, "p": 0.038461538461538464, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020319525300030738, "p": 0.03225806451612903, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01827095812881336, "p": 0.25, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.3333333285802469, "p": 0.42857142857142855, "r": 0.2727272727272727}, "rouge-l": {"f": 0.055314962991424715, "p": 0.42857142857142855, "r": 0.05454545454545454}}], "abstract": ["dependency cohesion refers observation phrases dominated disjoint dependency subtrees source language generally overlap target language", "verified useful constraint word alignment", "previous work either treats hard constraint uses feature discriminative models ineffective largescale tasks", "paper take dependency cohesion soft constraint integrate generative model largescale word alignment experiments", "propose approximate em algorithm gibbs sampling algorithm estimate model parameters unsupervised manner", "experiments largescale chineseenglish translation tasks demonstrate model achieves improvements alignment quality translation quality"]}, "S07-1032": {"introduction": ["finally conclude paper mention future work section", "gplsi system semevals task coarse grained english allwords consists corpus based supervisedlearning method uses local context information", "system uses base level concepts blc rosch features", "short blc synsets wordnet wn fell baum representative certain hyponymy subhierarchy", "synsets selected blc must accomplish certain conditions explained next section", "blc paper supported european union project qallme fp6 ist033860 spanish government project textmess tin200615265 c0601 know tin200615049c0301 slightly different base concepts eurowordnet1 ewn vossen et al balkanet2 meaning project3 selection criteria method capable define automatically", "type features helps system achieve f1 firstsense baseline systems outperformed f1 best", "wordnet widely criticised sense repository often offers finegrained sense distinctions higher level applications like machine translation question answering", "fact wsd level granularity resisted attempts inferring robust broadcoverage models", "seems many wordsense distinctions subtle captured automatic systems current small volumes wordsense annotated examples", "possibly building classbased classifiers would allow avoid data sparseness problem wordbased approach", "thus research focused deriving different sense groupings overcome fine grained distinctions wn hearst schu tze peters et al mihalcea moldo van agirre et al using predefined sets sensegroupings learning classbased classifiers wsd segond et al ciaramita johnson villarejo et al curran ciaramita altun", "later approaches used original lexico graphical files wn recently called super httpwwwillcuvanleurowordnet httpwwwceidupatrasgrbalkanet httpwwwlsiupces nlpmeaning proceedings 4th international workshop semantic evaluations semeval2007 pages prague june", "qc association computational linguistics senses coarsegrained sense distinctions", "much attention paid learning classbased classifiers available sensegroupings wordnet domains magnini cavaglia sumo labels niles pease eurowordnet base concepts top concept ontology labels atserias et al", "obviously resources relate senses level abstraction using different semantic criteria properties could interest wsd", "possibly combination could improve overall results since offer different semantic perspectives data", "furthermore knowledge date comparative evaluation performed exploring different sensegroupings", "paper organized follows", "section present method deriving fully automatically number base level concepts wn version"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.042553187904029274, "p": 0.027777777777777776, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02242101751227985, "p": 0.023255813953488372, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}], "abstract": ["present corpusbased supervised learning system coarsegrained sense disambiguation", "addition usual features training word sense disambiguation system uses base level concepts automatically obtained wordnet", "base level concepts synsets generalize hyponymy subhierarchy provides extra level abstraction well relevant information context word disambiguated", "experiments proved using type features results significant improvement precision", "system achieved almost f1 fifth place coarsegrained english allwords task using simple set features plus base level concepts annotation"]}, "S10-1090": {"introduction": ["section shows details whole system finally section concluding remarks provided", "empirically demonstrated last senseval semeval exercises assigning appropriate meaning words context resisted attempts successfully addressed", "fact supervised wordbased wsd systems dependent corpora used training testing system escudero et al", "possible reason could use inappropriate level abstraction", "class corresponds particular synset word", "wordnet wn widely criticized sense repository often provides finegrained sense distinctions higher level applications like machine translation question answering", "fact wsd level granularity resisted attempts inferring robust broadcoverage models", "seems many wordsense distinctions subtle captured automatic systems current small volumes wordsense annotated examples", "thus research focused deriving different wordsense groupings overcome finegrained distinctions wn hearst schu tze peters et al mihalcea moldovan agirre lopezdelacalle navigli snow et al", "provide methods grouping senses word thus producing coarser word sense groupings better disambiguation", "contrast research focused using predefined sets sensegroupings learning classbased classifiers wsd segond et al ciaramita johnson villarejo et al curran kohomban lee ciaramita altun", "grouping senses different words explicit comprehensive semantic class", "later approaches used original lexico graphical files wn recently called supersenses coarsegrained sense distinctions", "suspect selecting appropriate level abstraction could levels", "thus use semantic classes modeled basic level concepts1 blc izquierdo et al", "previous research using blc empirically demonstrated automatically derived supervised systems simply model polysemous word classification problem httpadimensiehueswebblc proceedings 5th international workshop semantic evaluation acl pages uppsala sweden july", "qc association computational linguistics set meanings groups senses adequate level abstraction order perform classbased word sense disambiguation wsd izquierdo et al", "show classbased wsd allows successfully incorporate monosemous examples domain text", "fact first maximum", "robustness classbased wsd approach shown system uses sem cor examples sc", "performs without kind blc domain adaptation frequent sense mfs baseline", "paper describes participation semeval2010 task agirre et al", "section semantic classes used selection algorithm used obtain automatically wordnet described", "section technique employed extract monosemous examples background data described"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01998346272741679, "p": 0.041666666666666664, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020225458909306583, "p": 0.03571428571428571, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018851537450315963, "p": 0.08333333333333333, "r": 0.01818181818181818}}], "abstract": ["paper summarizes participation task semeval2 allwords wsd specific domain using supervised classbased word sense disambiguation system", "basically use support vector machines svm learning algorithm set simple features build different models", "model considers different training corpus semcor sc examples monosemous words extracted automatically background data bg sc bg scbg", "system explodes monosemous words appearing members particular wordnet semantic class automatically acquire classbased annotated examples domain text", "use classbased examples gathered domain corpus adapt traditional system trained semcor", "evaluation reveal best results achieved training semcor background examples monosemous words obtaining results first sense baseline fifth best position competition rank"]}, "S12-1011": {"introduction": ["section explains general approach system experiments designed finally section results analysis shown", "developing models meanings words phrases key challenge computational linguistics", "distributed representations useful capturing meaning individual words sato et al maas ng curran", "finding compelling account semantic compositionality utilises representations proven difficult active research topic mitchell lapata baroni zamparelli grefenstette sadrzadeh", "area paper makes contribution", "dominant approaches distributional semantics relied relatively simple frequency counting techniques", "approaches fail generalise much sparser distributions encountered modeling compositional processes provide account selectional preference", "propose probabilistic model semantic tion noun adjective semantics together compositional probabilities", "employ formulation give dual view nounmodifier semantics induced latent variables provide explicit account selectional preference marginal distributions latent variables word implicitly produce distributed representation", "related work selectional preference uses classbased probabilities approximate sparse individual probabilities", "relevant papers include seaghdha evaluates several topic models adapted learning selectional preference using cooccurence baroni zamparelli represent nouns vectors adjectives matrices thus treating functions noun meaning"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02730839757874066, "p": 0.25, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029972592299338248, "p": 0.038461538461538464, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}], "abstract": ["investigate semantic relationship noun adjectival modifiers", "introduce class probabilistic models enable us simultaneously capture semantic similarity nouns modifiers adjectivenoun selectional preference", "combination novel existing evaluations test degree adjectivenoun relationships cat egorised", "analyse effect lexical context relationships efficacy latent semantic representation disambiguating word meaning"]}, "S12-1023": {"introduction": ["inference achieved using cooccurrence dimensionality reduction", "biggest challenges computational semantics fact many words polysemous", "instance lamb refer animal lamb squeezed gap food item sue lamb lunch", "polysemy pervasive human language problem almost applications nlp ranging machine translation word senses translate differently textual entailment lexical entailments sensespecific", "field thus devoted large amount effort representation modeling word senses", "arguably prominent effort word sense disambiguation wsd navigli invitro task whose goal identify set predefined senses used given context", "work wsd tasks related pol ysemy word sense induction sense alternations treated wordspecific", "result model meaning lamb accounts relation animal food senses cannot predict relation holds instances chicken salmon type contexts", "large number studies linguistics cognitive science show evidence regulari ties way words vary meaning apresjan lakoff johnson copestake briscoe pustejovsky gentner et al murphy due general analogical processes regular polysemy metonymy metaphor", "work theoretical linguistics focused regular systematic logical polysemy accounts alternations like animalfood", "sense alternations arise metaphorical use words dark dark glassdark mood metonymy instance using name place representative germany signed treatise", "disregarding evidence empirically inadequate leads wellknown lexical bottleneck current word sense models serious problems achieving high coverage navigli", "believe empirical computational semantics could profit model polysemy1 applicable individual words thus capable capturing general patterns generalizing new work mostly inspired research regular polysemy", "given fuzzy nature regularity meaning variation extend focus attention include types analogical sense construction processes", "first joint conference lexical computational semantics sem pages montreal canada june", "qc association computational linguistics words induced unsupervised fashion corpus data", "longterm goal many unsolved subproblems", "current paper presents contributions towards goal", "first since working relatively unexplored area introduce formal framework encompass different approaches section", "second implement concrete instantiation framework unsupervised centroid attribute model section evaluate new task namely detect set words stantiate given type polysemy sections"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02910122989603423, "p": 0.07692307692307693, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03008067881492231, "p": 0.05263157894736842, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03016579555494728, "p": 0.041666666666666664, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02910122989603423, "p": 0.07692307692307693, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}], "abstract": ["many types polysemy word specific instances general sense alternations animalfood", "despite pervasiveness regular alternations mostly ignored empirical computational semantics", "paper presents general framework grounds sense alternations corpus data generalizes individual words allows prediction alternations new words concrete unsupervised implementation framework centroid attribute model", "evaluate model set ambiguous words demonstrate outperforms baselines"]}, "S12-1040": {"introduction": ["finish conclusions future work section", "nothing home semantics phenomenon negation", "classical theories meaning states affairs divided truth values negation plays central role determine truth value stake given sentence", "negation lies heart deductive inference consistency checking searching contradictions texts prime example natural language understanding", "shouldnt therefore come surprise detecting negation adequately representing scope utmost importance computational semantics", "paper present evaluate system transforms texts logical formulas using cc tools boxer bos context shared task recognising negation english texts morante blanco", "first sketch background basics formalism employ analysis negation section", "section explain detect negation cues scope"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01678934866135234, "p": 0.045454545454545456, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01588980230000093, "p": 0.1111111111111111, "r": 0.015625}}], "abstract": ["use nlp toolchain used construct groningen meaning bank address task detecting negation cue scope defined shared task resolving scope focus negation", "toolchain applies cc tools parsing using formalism combinatory categorial grammar applies boxer produce semantic representations form discourse representation structures drss", "negation cue detection drss converted flat nonrecursive structures called discourse representation graphs drgs", "drgs simplify cue detection means edge labels representing relations", "scope detection done gathering tokens occur scope negated drs", "result system fairly reliable cue detection scope detection", "furthermore provides fairly robust algorithm detecting negated event property scope"]}, "S13-1002": {"introduction": ["finally section present results obtained shared task discuss section", "tasks natural language semantics diverse pose different requirements underlying formalism representing meaning", "tasks require detailed representation structure complex sentences", "tasks require ability recognize nearparaphrases degrees similarity sentences", "tasks require logical inference either exact approximate", "often necessary handle ambiguity vagueness meaning", "finally frequently want able learn relevant knowledge automatically corpus data", "single representation natural language meaning time fulfills requirements", "representations meet criteria", "logicbased representations montague kamp reyle provide expressive flexible formalism express even complex propositions come standardized inference mechanisms", "distributional mod simhamster gerbil gerbil hamster hamsterx gerbilx figure turning distributional similarity weighted inference rule els turney pantel use contextual similarity predict semantic similarity words phrases landauer dumais mitchell lapata model polysemy schu tze erk pado thater et al", "suggests distributional models logic based representations natural language meaning complementary strengths grefenstette sadrzadeh garrette et al encourages developing new techniques combine", "garrette et al", "propose framework combining logic distributional models logical form primary meaning representation", "distributional similarity pairs words converted weighted inference rules added logical form illustrated figure", "finally markov logic networks richardson domingos mlns used perform weighted inference resulting knowledge base", "employed singleword distributional similarity rules evaluated small second joint conference lexical computational semantics sem volume proceedings main conference shared task pages atlanta georgia june", "qc association computational linguistics set short handcrafted test sentences", "paper extend garrette et als approach adapt handle existing semantic tasks recognizing textual entailment rte semantic textual similarity sts", "show single semantic framework using probabilistic logical form markov logic adapted support important tasks", "possible mlns constitute flexible programming language based probabilistic logic domingos lowd easily adapted support multiple types linguistically useful inference", "word short phrase level approach model entailment distributional similarity figure", "occur similar contexts assume describe similar entities thus degree entailment", "sentence level hold stricter logicbased view entailment beneficial even model sentence similarity sts entail ment", "main innovations formalism make possible us work naturally occurring corpus data", "first use expressive distributional inference rules based similarity phrases rather individual words", "comparison existing methods creating textual inference rules lin pantel 2001b szpek tor dagan rules computed fly needed rather precompiled", "second use flexible probabilistic combinations evidence order compute degrees sentence similarity sts help compensate parser errors", "replace deterministic conjunction average combiner encodes causal independence natarajan et al", "show framework able handle sentence similarity sts textual entailment rte making simple adaptations mln switching tasks", "framework achieves reasonable results tasks", "sts obtain correlation full logic system weakened variable binding ensemble model", "rte1 obtain accuracy", "show distributional inference rules benefit tasks flexible probabilistic combinations evidence crucial sts"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07703032415486077, "p": 0.2, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.15999999507200013, "p": 0.14285714285714285, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07936507936515344, "p": 0.14285714285714285, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03886956148273119, "p": 0.09090909090909091, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["combine logical distributional representations natural language meaning transforming distributional similarity judgments weighted inference rules using markov logic networks mlns", "show framework supports judging sentence similarity recognizing textual entailment appropriately adapting mln implementation logical connectives", "show distributional phrase similarity used textual inference rules created fly improves performance"]}, "W00-0202": {"introduction": ["al though approaches could adapted handle rte sts know methods explicitly tested problems", "paper describe parameterized ac tion representation par badler et al 1999that provides conceptual representation differ ent types actions used animate virtual humanagents simulated 3d environment", "ac tions involve changes state changes location kinematic exertion force dynamic", "parsare hierarchical parameterized structures fa cilitate visual verbal expressions badler et al", "order support animation ofthe actions pars make explicit many de tails often underspecified language", "detailed level representation well suitedfor interlingua machine translation applications since animations actions therefore pars control equivalent actions described different lan guages", "representations incorporated system uses parbased animations asa workbench creating accurate conceptual representations map seeral different lan guages well produce faithful animations", "verb classes currently considering light involve explicit physical actions asthose expressed motion verb class contact verb class levin"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019733995308583406, "p": 0.047619047619047616, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019733995308583406, "p": 0.047619047619047616, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present parameterized action representation par provides conceptual representation ofdifferent types actions used animate virtual human agents simulated 3d environment", "actions involve changes state changes location kinematic exertion force dynamic", "parsare hierarchical parameterized structures facilitate visual verbal expressions", "order support animation actions parshave make explicit many details often underspecified language", "detailed level ofrepresentation provides suitable pivot representation generation natural languages ie form interlingua", "show examples certain divergences machine translation solved approach focusing specifically verbframed satelliteframed languages use representation"]}, "W00-0701": {"introduction": ["since employ ing par interlingual representation willshow examples handle certain diver gences machine translation focusing specifically verbframed satelliteframed languages talmy yield equivalent actions representation", "many important natural language inferences viewed problems resolving phonetic syntactic semantics pragmatics ambiguities based properties surrounding context", "generally accepted learning compo nent must central role resolving context sensitive ambiguities significant amount work devoted last years developing learning methods tasks considerable success", "un derstanding learning works domain used support increasingly higher level tasks still lacking", "article summarizes work developing learning theory account major learning approaches used nl", "major statistics based methods used nlp typically developed research supported nsf grants iis9801638 sbr9873450 iis9984168", "bayesian view mind bayesian principle cannot directly explain success robust ness methods since probabilistic assumptions typically hold data", "instead provide explanation using sin gle distribution free inductive principle related pac model learning", "describe unified learning framework show addition explaining success robust ness statistics based methods ap plies machine learning methods rule based memory based methods", "important component view devel oped observation methods use simple knowledge representation", "linear representation new feature space transformation original instance space higher dimensional expres sive space", "methods vary mostly algorithmicly ways derive weights features space", "significant explaining generalization properties methods developing understanding methods extended learn structured knowledge intensive ex amples perhaps hierarchically"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03016579555494728, "p": 0.041666666666666664, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02930876266467362, "p": 0.07142857142857142, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028662665435609407, "p": 0.09090909090909091, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}], "abstract": ["article summarizes work developing learning theory account major learning statistics based approaches used natural language processing", "shows ap proaches explained using single dis tribution free inductive principle related pac model learning", "furthermore make predictions using simple knowl edge representation linear representation common feature space", "signifi cant explaining generalization robustness properties methods understanding methods might ex tended learn structured knowl edge intensive examples part learning centered approach higher level natural lan guage inferences"]}, "W01-0502": {"introduction": ["issues briefly discussed emphasize impor tance studying knowledge representation inference developing learning centered ap proach nl inferences", "large number important natural language inferences viewed problems resolving ambiguity either semantic syntactic based properties surrounding context", "turn viewed classification problems goal select class label collection candidates", "examples include partof speech tagging wordsense disambiguation accent restoration word choice selection machine translation contextsensitive spelling correction word selection speech recognition identifying discourse markers", "machine learning methods become popular technique variety classification problems sort shown significant success", "partial list consists bayesian classifiers gale et al decision lists yarowsky bayesian hybrids golding hmms charniak inductive logic methods zelle mooney memorythis research supported nsf grants iis9801638 iis sbr987345", "based methods zavrel et al linear classifiers roth roth transformation based learning brill", "many classification problems significant source difficulty fact number candidates large words words selection problems possible tags tagging problems etc since general purpose learning algorithms handle multiclass classification problems well see studies address whole problem rather small set candidates typically first selected classifier trained choose", "approach important allows research community develop better learning methods evaluate range applications important realize important stage missing", "could significant classification methods embedded part higher level nlp tasks machine translation information extraction small set candidates classifier handle fixed could hard determine", "work develop general approach study multiclass classifiers", "suggest sequential learning model utilizes almost general purpose classifiers sequentially restrict number competing classes maintaining high probability presence true outcome candidate set", "paradigm sought classifier choose single class label small set labels large set labels", "works sequentially applying simpler classifiers outputs probability distribution candidate labels", "distributions multiplied thresholded resulting classifier sequence needs deal significantly smaller number candidate labels previous classifier", "classifiers sequence selected simple sense typically work part feature space decomposition feature space done achieve statistical independence", "simple classifier used since likely accurate chosen high probability whp sided error therefore presence true label candidate set maintained", "order sequence determined maximize rate decreasing size candidate labels set", "beyond increased accuracy multiclass classification problems scheme improves computation time problems several orders magnitude relative standard schemes", "work describe approach discuss experiment done context partofspeech pos tagging provide theoretical justifications approach", "sec", "provides background approaches multiclass classification machine learning nlp", "sec", "describe sequential model proposed sec", "describe experiment exhibits advantages", "theoretical justifications outlined sec"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.040816323048729994, "p": 0.02631578947368421, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02220030681523745, "p": 0.022727272727272728, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022164436572891397, "p": 0.14285714285714285, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["many classification problems require decisions large number competing classes", "tasks handled well general purpose learning methods usually addressed adhoc fashion", "suggest general approach sequential learning model utilizes classifiers sequentially restrict number competing classes maintaining high probability presence true outcome candidates set", "theoretical computational properties model discussed argue important nlplike domains", "advantages model illustrated experiment part ofspeech tagging"]}, "W01-0505": {"introduction": ["", "aligned bitexts useful derivation ofbilingual lexical resources used chine translation cross languages informationretrieval", "thus lot approaches sug gested find sets corresponding word tokens brown et al berger et al melamed1997 phrase shin et al noun phrase ku piec collocation smadja et al bitext", "works used lexical association measures finding word correspondences gale church1991 fung church", "associ ation measures misled cases word source language frequently cooccurs word target language cases indirect associationapos melamed", "works iterative parameter reestimation apossuppose uk vk indeed mutual translation uk ukki often cooccur text", "vk cooccur expected chance represented indirect association", "techniques based ibm model employed brown et al", "usually incorporated em algorithm brown et al kupiec tillmann ney och et al", "often faced difficulties follows ibm modelbased approaches directly applied alignment especially bitext involving less closely related language pair", "", "needs excessive iteration time parame", "ter estimation high decoding complexitythus systems assumed onetoone correspondence reduce computational complexity", "word sequences trans lated literally word word"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["problem finding lexical alignments givensentence pairs computationally expensive", "furthermore much difficult find lexical alignments korean english since considerably different syntactic structures coverage wordforword correspondences lowthis paper presents method extracting structural features reduce mapping space allowing probable alignments", "describe features improve performance lexical alignment model", "structural features providethe information correspondences partsofspeech pos sequences useful translation", "based maximum entropy concept structural features incrementally selected later embedded lexical alignment modelit turns features help get better lexical alignments korean english offering linguistic knowledge"]}, "W02-0503-parscit130908": {"introduction": ["example incases collocations compound nouns ambiguous words different meaning depen dent context require phraselevel correspondences", "morphology system backbone natural language processing system", "application field survive without good morphology system support", "arabic language features found languages", "many researchers worked area", "alfedaghi alanzi present algorithm generate root pattern given arabic word", "main concept algorithm locate position rootaposs letters pattern examine letters position given word see whether tri graph forms valid arabic root", "alshalabi developed system removes longest possible prefix word letters root must lie somewhere first characters remainder", "generates combinations checks roots file", "alshalabi reduced processing discussed point view verbs nouns", "anne roeck waleed alfares developed clustering algorithm arabic words sharing verbal root", "used rootbased clusters substitute dictionaries indexing information retrieval", "beesley karttunen described new technique constructing finitestate transducers involves reapplying regularexpression compiler output", "implementedthe system algorithm called compile replace", "technique proved useful handling nonconcatenate phenomena demonstrate malay fullstem reduplication arabic stem interdigitations", "verbs arabic language follow clear rules define morphology generate paradigms", "nouns derived roots seem follow similar set welldefined rules", "instead groups showing family resemblances", "believe nouns arabic derived roots governed phonological rules lexical patterns must identified stored noun", "like irregular verbs english forms determined history etymology phonology", "many examples pinker points survival past forms became become overcame overcome modeled came come succumb sound pattern regular past form succumbed", "kinds phenomena especially apparent proper nouns arabic derived indian persian names", "pinker uses examples like well emerging research neurophysiology argue coexistence phonological rules lexical storage english verb patterns", "believe work arabic computational linguistics requires development pattern bank nouns", "paper describes tool built purpose"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021889117043134277, "p": 0.25, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04797508372665193, "p": 0.09523809523809523, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["many papers discussed different aspects arabic verb morphology", "used patterns others used patterns affixes", "discussed arabic noun morphology particularly nouns derived verbs", "paper describe learning system analyze arabic nouns produce morphological information paradigms respect gender number using rule base uses suffix analysis well pattern analysis", "system utilizes userfeedback classify noun identify group belongs"]}, "W02-1005": {"introduction": ["set patterns common nouns arabic soon established newspapers dynamic sources language always contain new proper names expect tool permanent part system even though need less often time goes", "focus tasks paper related problems lexical ambiguity resolution word sense disambiguation wsd context sensitive spelling correction cssc", "word sense disambiguation long history computational task kelly stone field recently supported largescale international system evaluation exercises multiple languages sen seva l1 kilgarriff palmer sen seva l2 edmonds cotton", "general purpose spelling correction longstanding task eg mcilroy traditionally focusing resolving typographical errors transposition deletion find closest valid word dictionary morphological variant typically ignoring context", "kukich observed spelling errors found modern documents either contextinappropriate misuses substitutions valid words principal principle detected traditional spelling cor rectors", "previous work addressed problem cssc machine learning perspective including bayesian decision list models golding winnow golding roth transformationbased learning mangu brill", "generally tasks involve selection relatively small set alternatives per keyword eg sense ids churchbu ild churchin stitu io commonly confused spellings quiet quite dependent local longdistance collocational syntactic patterns resolve set alternatives", "thus tasks share common feature space data representation algorithm infrastructure", "present framework investigating use mixture models conjunction new errorcorrection technique competitive alternatives bayesian models"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030209967620282052, "p": 0.047619047619047616, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030160435101406875, "p": 0.05, "r": 0.02702702702702703}}], "abstract": ["paper investigates several augmented mixture models competitive alternatives standard bayesian models prove suitable word sense disambiguation related classification tasks", "present new classification correction technique successfully addresses problem underestimation infrequent classes training data", "show mixture models boostingfriendly adaboost original correction technique improve results raw model significantly achieving state oftheart performance several standard test sets languages", "substantially different output nave bayes statistical methods investigated models shown effective participants classifier combination"]}, "W02-1108": {"introduction": ["several authors observed fundamental similarities cssc wsd eg berleant roth knowledge previous comparative empirical study tackled problems single unified framework", "linguistic research shown verbs fallinto classes distinctive terms syntac tic semantic properties jackendoff hale keyser levin pinker", "example verbs share meaning component aposmotionapos eg fly walk tend behave similarly terms subcategorization thus grouped linguistically coherent class", "correspondence syntax semantics verbs arguably perfect whole notion verb class somewhat elusiveapos verb classifications constructed provide generalization range syntactic semantic properties verbs", "classifications particularly useful practical nlp point view", "used means reducing redundancy inthe lexicon filling gaps lexical knowl edge", "certain extent enable inferring 1for example verbs characterized byseveral meaning components potential crossclassification", "therefore different equally viable classifi cation schemes constructedthe semantics word basis syn tactic behaviour syntax word basis semantic behaviour", "verb classifications fact usedto support various nlp tasks including chine translation language generation dorr document classification klavans kan lexicography sanfilippo andlexical acquisition word sense disambiguation dorr jones subcate gorization acquisition korhonen 2002b", "verb classification employed widely nlp levinaposs taxonomy verbs classes levin", "levin classes based onthe ability verb occur specific diathe sis alternations ie specific pairs syntacticframes assumed meaning tentive", "classification covers substantial number diathesis alternations occurring english", "exhaustive", "work required extending refining comprehensive resource obtained suitable largescale nlp use dorr jones dorr korhonen 2002bperhaps challenging task ex tend classification new participants", "manual classification verbs semanticclasses yields accurate results time con suming levin dang et al", "fully automatic classification hand fast suffers low accuracy dorr stevenson merlo", "paper propose combining strengths theseapproaches", "present method semi automatic semantic classification verbs exploits automatic techniques allows manual interventionthe method involves classifying verbs seman tically via semantic network wordnet miller", "unlike levinaposs source wordnet comprehensive lexical database", "although classifies verbs purely semantic basis syntactic regularities studied levin extent reflected semantic relatedness asit represented wordnetaposs particular struc ture dorr fellbaum", "methodmakes use partial overlap wordnet levin classes", "involves first classify ing entire wordnet senses semantic classesand classifying individual verbs ba sis wordnet senseswe use method classify verbs num ber wordnet files report experimental evaluation shows method fairlyaccurate used build clas sification suitable practical nlp use", "classification built using method alsobe used benefit linguistic research knowledge novel verb verb class associa tions used test enrich linguistic theory eg", "levin 1993as byproduct method produces map ping wordnet senses levin classes", "mapping comprehensive actas useful supplement wordnet incorporates information diathesis alterna tions semantic verb classes", "currently information absent wordnet", "discuss background work section", "section describe method classifying verbs semantically via wordnet", "details experimental evaluation method supplied section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03021330786371127, "p": 0.043478260869565216, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.19999999535555565, "p": 0.15789473684210525, "r": 0.2727272727272727}, "rouge-l": {"f": 0.09068367563335165, "p": 0.13636363636363635, "r": 0.08108108108108109}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02730839757874066, "p": 0.25, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}], "abstract": ["propose method semiautomatic classification verbs levin classes via semantic network wordnet", "method involvesfirst classifying entire wordnet senses semantic classes classifying individual verbs basis wordnet senses", "report evaluation shows method used build verb classification accurateenough practical nlp use", "wordnetlevin mapping produced byproduct turn used supplement wordnet novel information"]}, "W03-0432": {"introduction": ["section concludes directions future work", "language independent ner requires development metalinguistic model sufficiently broad accommodate languages trained exploit specific features target language", "aim paper investigate combination character level model orthographic tries sentencelevel hidden markov model", "local model uses affix information word surrounds classify word independently relies sentencelevel model determine correct state sequence", "capitalisation oftenused discriminator ner misleading sentenceinitial allcaps text", "choose use model makes assumptions capitalisation scheme indeed character set target language", "solve problem misleading case novel way removing effects sentenceinitial allcaps capitalisation"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present named entity recognition classification system uses probabilistic characterlevel features", "classifications multiple orthographic tries combined hidden markov model framework incorporate internal contextual evidence", "part system perform preprocessing stage capitalisation restored sentenceinitial allcaps words high accuracy", "report fvalues english german datasets"]}, "W04-0705": {"introduction": ["results simpler language model easier recognition named entities remaining strongly language independent", "problem name recognition classification intensively studied since1995 introduced part muc evaluation grishman sundheim", "wide variety machine learning methods applied problem including hidden markov models bikel et al maximum entropy methods borthwick et al chieu ng decision trees sekine et al conditional random fields mccallum li classbased language model sun et al agentbased approach ye et al support vector machines", "performance even best models1 limited amount labeled training data available range features employ", "particular methods classify instance name based information instance alone local context instance typically best results reported chinese named entity recognition met2 test corpus fmeasure different name types ye et al", "words preceding following name", "name seen appears relatively uninformative context becomes hard classify", "propose use global information improve performance name recognition", "name taggers incorporated name cache similar mechanism makes use names previously recognized document", "approach perform coreference analysis use detailed evidence phrases document coreferential name order disambiguate name", "allows us perform richer set corrections name cache", "go step process similar documents containing instances name combine evidence additional instances", "step able demonstrate small consistent improvement named entity recognition", "rest paper organized follows", "section briefly describes baseline name tagger coreference resolver used paper", "section considers methods assessing confidence name tagging decisions", "section examines distribution name errors motivation using coreference information", "section shows coreference features use incorporated statistical name filter", "section describes additional rules using coreference improve name recognition", "section provides flow graph improved system"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02029734970915876, "p": 0.03333333333333333, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present novel method applying results coreference resolution improve name recognition chinese", "consider first methods gauging confidence individual tags assigned statistical name tagger", "names low confidence show names filtered using coreference features improve accuracy", "addition present rules use coreference information correct name tagging errors", "finally show gains magnified clustering documents using crossdocument coreference clusters", "combined methods yield absolute improvement tagger score"]}, "W04-3238": {"introduction": ["section reports discusses experimental results section summarizes conclusions", "task general purpose spelling correction long history eg damerau rieseman hanson mcilroy traditionally focusing resolving typographical errors insertions deletions substitutions transpositions letters result unknown words ie words found trusted lexicon language", "typical word processing spell checkers compute unknown word small set inlexicon alternatives proposed possible corrections relying information inlexiconword frequencies common keyboard mistakes typing instead phoneticcognitive mistakes word level eg use acceptible instead acceptable character level eg misuse instead ph", "spell checkers attempt detect correct word substitution errors refer use inlexicon words inappropriate contexts result typographical mistakes typing coed instead cord cognitive mistakes eg principal principle", "research efforts tackle problem made example heidorn et al", "garside et al", "developed systems rely syntactic patterns detect substitution errors mays et al", "employed word cooccurrence evidence large corpus detect correct errors", "former approaches based impractical assumption possible syntactic uses words ie partofspeech known presented recall precision problems many substitution errors syntactically anomalous many unusual syntactic constructions contain errors", "latter approach limited success assumptions sentence contains misspelled word misspelling result single point change insertion deletion substitution transposition defect rate relative number errors text known", "different body work eg golding golding roth mangu brill focused resolving limited number cognitive substitution errors framework context sensitive spelling correction cssc", "although promising results obtained accuracy scope work limited addressed known sets commonly confused words peace piece", "spell checking search engine queries", "task webquery spelling correction addressed work many similarities traditional spelling correction poses additional challenges", "frequency severity spelling errors search queries significantly greater word processing", "roughly queries sent search engines contain errors", "typically validity query cannot decided lexicon lookup checking gram maticality", "web queries short average less words techniques use multitude features based relatively wide context windows investigated cssc difficult apply", "rather well formed sentences queries consist concept enumeration concepts many times containing legitimate words found traditional lexicon", "defining valid web query represents difficult enterprise", "clearly cannot use static trusted lexicon many new names concepts aznar blog naboo nimh nsync shrek become popular every day would extremely difficult impossible maintain highcoverage lexicon", "addition employing large lexicons result errors surfacing word substitutions difficult detect rather unknown words", "alternative investigated work exploit continuously evolving expertise millions people use web search engines collected search query logs seen histograms queries received search engine", "sense could say validity word inferred frequency people querying similarly wittgensteins observation meaning word use language", "approach caveats", "example would er ple ratio number letters words common number letters share1 used classes distances spelling correction edit distances proposed damerau levenshtein correlation matrix distances cherkassky et al", "study use modified version dameraulev enshtein edit distance presented section", "flaw preceding formulation take account frequency words language", "simple solution problem compute probability words target language maximum likelihood estimates mle large corpus reformulate general spellingcorrection problem follows roneous simply extract webquery logs queries whose frequencies certain value consider valid", "misspelled queries given dist find pw max ldist wv pv britny spears much popular correctly spelled queries bayesian nets amd processors"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03889570552123663, "p": 0.045454545454545456, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039940442955507194, "p": 0.058823529411764705, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03848295671005285, "p": 0.043478260869565216, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["logs user queries internet search engine provide large amount implicit explicit information language", "paper investigate use spelling correction search queries task poses many additional challenges beyond traditional spelling correction problem", "present approach uses iterative transformation input query strings strings correspond likely queries according statistics extracted internet search query logs"]}, "W05-0612": {"introduction": ["challenge try utilize query logs learn queries valid build model valid query probabilities despite fact large percentage logged queries misspelled trivial way determine valid invalid queries", "coreference resolution process determining expressions text refer real world entity", "pronoun resolution important challenging subset coreference resolution system attempts establish coreference pronominal anaphor thirdperson pronoun like preceding noun phrase called antecedent", "following example pronoun resolution system must determine correct antecedent pronouns president entered arena family serenaded mariachi band", "pronoun resolution applications many areas natural language processing particularly field information extraction", "resolving pronoun noun phrase provide new interpretation given sentence giving question answering system example data consider", "approach synthesis linguistic statistical methods", "pronoun list antecedent candidates derived parsed corpus presented expectation maximization em learner", "special cases pleonastic reflexive cataphoric pronouns dealt linguistically list construction", "allows us train resolve thirdperson pronouns large question answering corpus", "learn lexicalized gendernumber language antecedent probability models", "models tied individual words learned sufficient coverage labeled data", "pronouns resolved choosing likely antecedent candidate list according distributions", "resulting resolution accuracy comparable supervised methods", "gain performance improvement initializing em gendernumber model derived special cases training data", "model shown perform reliably"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose unsupervised expectation maximization approach pronoun resolution", "system learns fixed list potential antecedents pronoun", "show unsupervised learning possible context performance system comparable supervised methods", "results indicate probabilistic gendernumber model determined automatically unlabeled text powerful feature task"]}, "W06-0106": {"introduction": ["demonstrate models learned unsupervised method used features supervised pronoun resolution system", "noun phrase coreference resolution process detecting noun phrases nps document determining whether nps refer entity entity defined construct represents abstract identity", "nps refer entity known mentions", "mentions antecedents anaphors", "anaphor expression refers back previous expression discourse", "figure president clinton refersto clinton described ana clinton1 said washington would progressively follow economic aid korea2", "kim daejung3 applauded clinton1s speech", "he1 said president clinton1 reiterated talks he1 would provide solid support korea2 shake economic crisis", "figure excerpt text core ferring noun phrases annotated", "english translation italics", "np coreference resolution important sub task natural language processing nlp applications text summarization information extraction data mining question answering", "task attracted much attention recent years cardie wagstaff harabagiu et al soon et al ng cardie yang et al florian et al zhou et al included subtask muc message understanding conferences ace automatic content extraction competitions", "coreference resolution difficult task various reasons", "firstly list features play role support coreference resolution proceedings fifth sighan workshop chinese language processing pages sydney july", "qc association computational linguistics gender agreement number agreement head noun matches semantic class positional information contextual information appositive abbreviation etc ng cardie found features useful problem", "single feature completely reliable since always exceptions eg number agree ment test returns false army singular matched army members plural despite phrases coreferential", "secondly identifying features automatically accurately hard", "features semantic class come named entity recognition ner systems ontologies gazetteers always accurate especially new terms concerned", "thirdly coreference resolution subsumes pronoun resolution problem already difficult since pronouns carry limited lexical semantic information", "addition aforementioned chinese coreference resolution made difficult due lack morphological orthographic clues", "chinese words contain less exterior information words many indoeuropean languages", "example english number agreement detected word inflections partofspeech pos tags simple rules chinese distinguish whether word singular plural", "proper name abbreviations identified capitalization english chinese use capitalization", "moreover written chinese word boundaries word segmentation crucial problem cannot get true meaning sentence based characters alone", "simple sentence segmented several different ways get different meanings", "characteristic affects performance parts leads irrecoverable errors", "addition chinese coreference data sets available research purposes none freely available result easily obtainable benchmark ing dataset training measuring performance", "building reasonably large coreference corpus laborconsuming task", "knowledge chinese coreference systems previously published work florian et al", "presents statistical framework reports experiment results chinese texts zhou et al", "proposed unified transformation based learning framework chinese entity detection tracking", "consists models detection model locates possibly coreferring nps tracking model links coreference relations", "paper presents research performed chinese noun phrase coreference resolution", "since freely available chinese coreference resources used unsupervised method partially borrows cardie wagstaffs clusteringbased technique features specially designed chinese"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.011505240695979647, "p": 0.043478260869565216, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1904761854875285, "p": 0.2, "r": 0.18181818181818182}, "rouge-l": {"f": 0.022213946732666866, "p": 0.2, "r": 0.02197802197802198}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["coreference resolution process identifying expressions refer entity", "paper presents clustering algorithm unsupervised chinese coreference resolution", "investigate chinese coreference hard demonstrate techniques used coreference resolution phoric reference clinton", "president clinton described antecedent", "clinton president clinton second mentions entity refers former us president bill clinton", "english extended chinese", "proposed system exploits clustering advantages traditional classification methods fact training data required easily extended accommodate additional features", "conduct set experiments investigate noun phrase identification feature selection contribute coreference resolution performance", "system evaluated annotated version tdt3 corpus using muc7 scorer obtains comparable performance", "believe first attempt unsupervised approach chinese noun phrase coreference resolution"]}, "W06-0119": {"introduction": ["addition perform present results experiments designed investigate contribution feature", "highperformance chinese word segmentor cws critical processing stage produce intermediate result later processes search engines text mining word spell checking texttospeech speech recognition etc per lin et al tsai et al tsai bottleneck developing high performance cws comprise high performance chinese unknown word identification uwi", "chinese written without separation words words chinese texts web corpus outofvocabulary tsai et al", "report sighan bakeoff tsai shown highly performance fmeasure achieved bmmbased cws using perfect system dictionary tsai", "perfect system dictionary means word types dictionary extracted training testing gold standard corpus", "conventionally approaches develop cws dictionarybased approach cheng et al especial forward backward maximum matching wong chan linguistic approach based syntaxsemantic knowledge chen et al statistical approach based statistical language model slm sproat shih teahan et al gao et al hybrid approach trying combine benefits dictionarybased linguistic statistical approaches tsai et al chen", "practice statistical approaches widely used effective reasonable performance", "develop uwi approaches statistical approach researchers use common statistical features maximum entropy chieu et al association strength mutual information ambiguous matching multistatistical features unknown word detection extraction linguistic approach major types linguistic rules knowledge morphology syntax semantics used identify unknown words hybrid approach recently important trend uwi follows hybrid approach take advantage merits statistical linguistic approaches", "statistical approaches simple efficient whereas linguistic approaches effective identifying low frequency unknown words chen et al", "develop wsd major types word segmentation ambiguities unknown word problems overlap ambiguity oa", "take string c1c2c3 proceedings fifth sighan workshop chinese language processing pages sydney july", "qc association computational linguistics comprised chinese characters c1 c2 c3 example", "segmentation either c1c2c3 c1c2c3 depending context meaning c1c2c3 called overlapambiguity string oas gen eraluse getfor military use symbol indicates word bound ary", "combination ambiguity ca", "take string c1c2 comprised chinese characters c1 c2 example", "segmentation either c1c2 c1c2 depending context meaning c1c2 called combina tion ambiguity string cas ability besides oa ca problems types word segmentation errors caused unknown word problems", "lack unknown word luw means segmentation error occurred lack unknown word system dictionary error identified word eiw means segmentation error occurred error identified unknown words", "goal paper report approach experiment results backward maximum matchingbased bmmbased cws word support model wsm sighan bakeoff", "tsai wsm shown effectively improve chinese input system", "third bakeoff cws mainly addressed improving performance oaca disambiguation wsm", "show wsm able improve bmmbased cws reported sighan bakeoff fmeasure", "remainder paper arranged follows", "section present details bmmbased cws comprised wsm", "section present scored results cws microsoft research closed track give experiment results analysis"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.044444440750617584, "p": 0.029411764705882353, "r": 0.09090909090909091}, "rouge-l": {"f": 0.021295507185823175, "p": 0.019230769230769232, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.036363633163636645, "p": 0.022727272727272728, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019909930215115428, "p": 0.017857142857142856, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027806102439457355, "p": 0.14285714285714285, "r": 0.02702702702702703}}], "abstract": ["paper describes chinese word segmentor cws third international chinese language processing bakeoff sighan bakeoff", "participate word segmentation task microsoft research msr closed testing track", "cws based backward maximum matching word support model wsm contextualbased chinese unknown word identification", "scored results experimental results shows wsm improve previous cws reported sighan bakeoff fmeasure"]}, "W06-0206": {"introduction": ["finally section give conclusions future research directions", "applying machine learning approaches natural language processing tasks time consuming expensive handlabel large amounts training data necessary good performance", "unlabeled data collected much larger quantities", "therefore natural question whether use unlabeled data build accurate learner given amount labeled data", "problem often referred semisupervised learning", "significantly reduces effort needed develop training set", "shown promise improving performance many tasks name tagging miller et al semantic class extraction lin et al chunking ando zhang coreference resolution bean riloff text classification blum mitchell", "clear semisupervised learning applied improve learner system effectively select unlabeled data size relevance data impact performance", "paper apply semisupervised learning algorithms improve stateoftheart name tagger", "run baseline name tagger large unlabeled corpus bootstrapping test set selftraining automatically generate highconfidence machinelabeled sentences additional training data", "iteratively retrain model increased training data", "first investigated whether improve system simply using lot unlabeled data", "dramatically increasing size corpus unlabeled data get significant improvement compared baseline system", "found adding offtopic unlabeled data sometimes makes performance worse", "tried select relevant documents unlabeled data advance got clear improvements", "obtained significant improvement selftraining boot strapping test data without additional unlabeled data", "therefore contrast claim banko brill concluded applications effective use large unlabeled corpora demands good data selection measures", "propose quantify effective measures select documents sentences paper", "rest paper structured follows", "section briefly describes efforts made previous researchers use semisupervised learning well work banko brill", "section presents baseline name tag ger", "section describes motivation approach section presents details semisupervised learning methods", "section presents discusses experimental results english chinese"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03008067881492231, "p": 0.05263157894736842, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.15999999507200013, "p": 0.14285714285714285, "r": 0.18181818181818182}, "rouge-l": {"f": 0.058617525329242776, "p": 0.14285714285714285, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}], "abstract": ["present semisupervised learning techniques improve stateoftheart multilingual name tagger", "english chinese overall system obtains improvement fmeasure representing relative reduction spurious missing incorrect tags", "conclude simply relying upon large corpora sufficient must pay attention unlabeled data selection", "describe effective measures automatically select documents sentences"]}, "W06-1624": {"introduction": ["section presents conclusions directions future work", "spoken language understanding slu key components spoken dialogue systems", "task identify users goal extract input utterance information needed complete query", "traditionally mainly mainstreams slu researches knowledgebased approaches based robust parsing template matching techniques sneff dowding et al ward issar datadriven approaches generally based stochastic models pieraccini levin miller et al", "approaches drawbacks", "former approach costexpensive develop since grammar development time consuming laboursome requires linguistic skills", "strictly domaindependent hence difficult adapted new domains", "hand although addressing drawbacks associated knowledgebased approaches latter approach often suffers data sparseness problem hence needs fully annotated corpus order reliably estimate accurate model", "recently new variation methods proposed certain trade offs semiautomatically grammar learning approach wang acero hidden vector state hvs model young", "methods require minimally annotated data semantic frames annotated", "paper proposes novel weakly supervised spoken language understanding approach", "slu framework mainly includes successive classifiers topic classifier semantic classifier", "main advantage proposed approach mainly datadriven requires minimally annotated corpus training whilst retaining understanding robustness deepness spoken language", "particular classifiers trained using weakly supervised strategies former trained combination active learning selftraining tur et al latter proceedings conference empirical methods natural language processing emnlp pages sydney july"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.011688886080841752, "p": 0.03571428571428571, "r": 0.01098901098901099}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01108577489064369, "p": 0.1111111111111111, "r": 0.01098901098901099}}], "abstract": ["paper present weakly supervised learning approach spoken language understanding domainspecific dialogue systems", "model task spoken language understanding successive classification problem", "first classifier topic classifier used identify topic input utterance", "restriction recognized target topic second classifier semantic classifier trained extract corresponding slotvalue pairs", "mainly datadriven requires minimally annotated corpus training whilst retaining understanding robustness deepness spoken language", "importantly allows employment weakly supervised strategies training classifiers", "first apply training strategy combining active learning selftraining tur et al topic classifier", "propose practical method bootstrapping topicdependent semantic classifiers small amount labeled sentences", "experiments conducted context chinese public transportation information inquiry domain", "experimental results demonstrate effectiveness proposed slu framework show possibility reduce human labeling efforts significantly"]}, "W06-1634": {"introduction": ["qc association computational linguistics trained using practical bootstrapping technique", "primitive approach information extraction ie manually craft numerous extraction patterns particular applications presently main streams biomedical ie blaschke valencia koike et al", "although ie attempts demonstrated nearpractical performance sets patterns cannot applied different kinds information", "realworld task requires several kinds ie thus manually engineering extraction current afliation fujitsu laboratories ltd faculty informatics kogakuin university patterns tedious timeconsuming process really practical", "techniques based machine learning zhou et al hao et al bunescu mooney expected alleviate problem manually crafted ie", "cases cost manually crafting patterns simply transferred constructing large amount training data requires tedious amount manual labor annotate text", "systematically reduce necessary amount training data divided task constructing extraction patterns subtask general natural language processing techniques solve subtask specic properties according information extracted", "former subtask full parsing ie recognizing syntactic structures sentences latter subtask constructing specic extraction patterns ie nding clue words extract information based obtained syntactic structures", "adopted full parsing various levels parsing believe offers best utility generalize sentences normalized syntactic relations", "divided patterns components improve recall introduced machine learning support vector machine svm learn prediction model using matching results extraction patterns"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02388547125399133, "p": 0.05, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper presents method automatically constructing information extraction patterns predicateargument structures pass obtained full parsing smaller training corpus", "pass represent generalized structures syntactical variants patterns pass expected generalized surface words", "addition patterns divided components improve recall introduce support vector machine learn prediction model using pattern matching results", "paper present experimental results analyze well proteinprotein interactions extracted medline abstracts", "results demonstrated method improved accuracy compared machine learning approach using surface wordpartofspeech patterns"]}, "W06-1667": {"introduction": ["actual ie task extracted pairs interacting protein names biomedical text", "task relation extraction identify various semantic relations name entities text", "prior work automatic relation extraction come kinds supervised learning algorithms miller et al zelenko et al culotta soresen kambhatla zhou et al semisupervised learning algorithms brin agichtein gravano zhang unsupervised learning algorithm hasegawa et al", "methods supervised learning usually preferred large amount la beled training data available", "timeconsuming laborintensive manually tag large amount training data", "semisupervised learning methods put forward minimize corpus annotation requirement", "semisupervised methods employ bootstrapping framework need predefine initial seeds particular relation bootstrap seeds acquire relation", "often quite difficult enumerate class labels initial seeds decide optimal number", "compared supervised semisupervised methods hasegawa et al", "2004s unsupervised approach relation extraction overcome difficulties requirement large amount labeled data enumeration class labels", "hasegawa et al", "2004s method use hierarchical clustering method cluster pairs named entities according similarity context words intervening named entities", "drawback hierarchical clustering required providing cluster number users", "furthermore clustering performed original high dimensional space induce nonconvex clusters hard identified", "paper presents novel application spectral clustering technique unsupervised relation extraction problem", "works calculating eigenvec tors adjacency graphs laplacian recover submanifold data high dimensional space performing cluster number estimation transformed space defined first eigen vectors", "method help us find nonconvex clusters", "need predefine number context clusters prespecify similarity threshold clusters hasegawa et al proceedings conference empirical methods natural language processing emnlp pages sydney july", "qc association computational linguistics 2004s method", "rest paper organized follows", "section formulates unsupervised relation extraction presents apply spectral clustering technique resolve task", "section reports experiments results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020310633213816317, "p": 0.02857142857142857, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.2499999957031251, "p": 0.4, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03663663663665088, "p": 0.4, "r": 0.03636363636363636}}], "abstract": ["present unsupervised learning approach disambiguate various relations name entities use various lexical syntactic features contexts", "works calculating eigen vectors adjacency graphs laplacian recover submanifold data high dimensionality space performing cluster number estimation eigenvectors", "method address difficulties encoutered hasegawa et al", "2004s hierarchical clustering consideration manifold structure data requirement provide cluster number users", "experiment results ace corpora show spectral clustering based approach outperforms hasegawa et al", "2004s hierarchical clustering method plain kmeans clustering method"]}, "W06-1670": {"introduction": ["finally give conclusion work section", "named entity recognition ner studied information extraction ie task", "ner typically focuses detecting instances person location organization names optionally instances miscellaneous time categories", "scalability statistical ner allowed researchers apply successfully large collections newswire text several languages biomedical literature", "newswire ner performance terms fscore upper 80s carreras et al florian et al bioner accuracy ranges low 70s 80s depending dataset used trainingevaluation dingare et al", "shortcoming ner oversimplified onto logical model leaving instances potentially informative categories unidentified", "hence utility named entity information limited", "addition instances detected mainly restricted sequences proper nouns", "word sense disambiguation wsd task deciding intended sense ambiguous words context", "respect ner wsd lies end semantic tagging spectrum since dictionary defines tens thousand specific word senses including ner categories", "wordnet fellbaum possibly used resource wsd defines word senses verbs common proper nouns", "word sense disambiguation level granularity complex task resisted attempts robust broadcoverage solutions", "many distinctions subtle captured automatically magnitude class space several orders larger ners makes hard approach problem sophisticated scal able machine learning methods", "lastly even methods would scale enough manually tagged data word sense level training model", "performance state art wsd systems realistic evaluations comparable first sense baseline cf", "section", "notwithstanding much research benefits disambiguated lexical information language processing still mostly speculativethis paper presents novel approach broad first author yahoo", "research", "tag ger described paper free software downloaded httpwwwloacnritciaramitahtml", "referring wordnet throughout paper", "mean wordnet version", "proceedings conference empirical methods natural language processing emnlp pages sydney july", "qc association computational linguistics nouns supersense nouns denoting supersense nouns denoting act animal artifact attribute body cognition communication event feeling food group location motive acts actions animals manmade objects attributes people objects body parts cognitive processes contents communicative processes contents natural events feelings emotions foods drinks groupings people objects spatial position goals object quantity phenomenon plant possession process person relation shape state substance time tops natural objects manmade quantities units measure natural phenomena plants possession transfer possession natural processes people relations people things ideas dimensional shapes stable states affairs substances time temporal relations abstract terms unique beginners verbs supersense verbs supersense verbs body change cognition communication competition consumption contact creation grooming dressing bodily care size temperature change intensifying thinking judging analyzing doubting telling asking ordering singing fighting athletic activities eating drinking touching hitting tying digging sewing baking painting performing emotion motion perception possession social stative weather feeling walking flying swimming seeing hearing feeling buying selling owning political social activities events spatial relations raining snowing thawing thundering table", "nouns verbs supersense labels short description wordnet documentation", "coverage information extraction word sense disambiguation", "goal simplify disambiguation task nouns verbs level approached tagging problem solved state art methods", "byproduct task includes extends ner", "define tagset based word nets lexicographers classes supersenses ciaramita johnson cf", "table", "size supersense tagset allows us adopt structured learning approach takes local dependencies labels account", "extent cast supersense tagging problem sequence labeling task train discriminative hidden markov model hmm based collins manually annotated semcor corpus miller et al", "experiments evaluate accuracy tagger semcor corpus english words senseval shared task data snyder palmer", "model outperforms remarkably best known baseline first sense heuristic best knowledge first time realistic words evaluation setting", "paper organized follows", "section introduces tagset section discusses related work section learning model", "section reports experimental settings results"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.01428571283775525, "p": 0.007751937984496124, "r": 0.09090909090909091}, "rouge-l": {"f": 0.006408590006161674, "p": 0.005988023952095809, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01934753164270764, "p": 0.058823529411764705, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper approach word sense disambiguation information extraction unified tagging problem", "task consists annotating text tagset defined wordnet super sense classes nouns verbs", "since tagset directly related wordnet synsets tagger returns partial word sense disambiguation", "furthermore since noun tags include standard named entity detection classes person location organization time etc tagger byproduct returns extended named entity information", "cast problem supersense tagging sequential labeling task investigate empirically discriminativelytrained hidden markov model", "experimental evaluation main senseannotated datasets available ie semcor senseval shows considerable improvements best known firstsense baseline"]}, "W06-2709": {"introduction": ["section summarize contribution consider directions research", "recently working data annotated multiple levels different types annotation required rather advanced computer skills cannot expected majority potentially interested users", "present annis linguistic database aims providing infrastructure supporting linguists work multilevel annotations", "describe illustrate current state work sketch next steps", "sec", "present research scenario nis developed show role linguistic database therein sketch major requirements aims fulfill", "describe architecture current functionality discuss way difficult aspects multidimensional annotations treated sec", "", "sec", "illustrate work database exemplary approaches"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02730839757874066, "p": 0.25, "r": 0.02702702702702703}}], "abstract": ["present annis linguistic database aims facilitating process exploiting richly annotated language data naive users", "describe role database research project project requirements special focus aspects multilevel annotation", "illustrate usability database illustrative examples", "address current challenges next steps"]}, "W06-2910": {"introduction": ["finally sketch next steps", "variety manual semantic verb classifications major frameworks levin classes levin wordnet fellbaum framenet fontenelle", "different frameworks depend different instantiations semantic similarity eg levin relies verb similarity referring syntaxsemantic alternation behaviour wordnet uses synonymy framenet relies situationbased agreement defined fillmores frame semantics fillmore", "alternative resourceintensive manual classifications automatic methods classification clustering applied induce verb classes corpus data eg", "merlo stevenson joanis stevenson korhonen et al stevenson joanis schulte im walde fer rer", "depending types verb classes induced automatic approaches vary choice verbs classificationclustering algorithm", "another central parameter automatic induction semantic verb classes selection verb features", "since target classification determines similarity dissimilarity verbs verb feature selection model similarity interest", "example merlo stevenson classify english verbs alternate intransitive transitive usage assign verb classes according semantic role assignment frames verb features chosen model syntactic frame alternation proportions heuristics semantic role assignment", "largerscale classifications korhonen et al stevenson joanis schulte im walde model verb classes similarity syntaxsemantics interface clear features salient", "verb features need relate behavioural component modelling syntaxsemantics interplay set features potentially influence behaviour large ranging structural syntactic descriptions argument role fillers adverbial adjuncts", "addition clear finegrained features example much information covered lowlevel window cooccurrence vs higherorder syntactic frame fillers", "proceedings 10th conference computational natural language learning conllx pages new york city june", "qc association computational linguistics paper investigate whether human associations verbs help us identify salient verb features semantic verb classes", "collected associations german verbs web experiment hope associations represent useful basis theoryindependent semantic classification german verbs assuming associations model nonrestricted set salient verb meaning aspects", "preparatory step perform unsupervised clustering experiment verbs based verb associations", "validate resulting verb classes henceforth assocclasses demonstrating show considerable overlap standard approaches semantic verb classes ie germanet framenet", "main body work compare associations underlying assocclasses standard corpusbased feature types check many associations find corpusbased features adverbs direct object nouns etc hypothesise associations found instantiations feature set better clustering based feature type", "assess hypothesis applying various corpusbased feature types experiment verbs comparing resulting classes henceforth corpusclasses assocclasses", "basis comparison intend answer question whether human associations help identify salient features induce semantic verb classes ie corpusbased feature types identified basis associations outperform previous clustering results", "applying feature choices germanet framenet address question whether types features salient different types semantic verb classes", "follows paper presents association data section associationbased classes section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028386649728229026, "p": 0.030303030303030304, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper investigates whether human associations verbs collected web experiment help us identify salient verb features semantic verb classes", "assuming associations model aspects verb meaning apply clustering verbs based associations validate resulting verb classes standard approaches semantic verb classes ie germanet framenet", "various clusterings verbs performed basis standard corpusbased types evaluated associationbased clustering well germanet framenet classes", "hypothesise corpus based clusterings better instantiations feature types show overlap verb associations associations therefore help identify salient feature types"]}, "W06-3604": {"introduction": ["section compare associations corpusbased feature types section apply insights induce semantic verb classes", "word prediction intriguing language engineering semiproduct", "arguably archetypical prediction problem natural language processing evenzohar roth", "usually engineering end predict next word sequence fill blankedout word sequence", "could asset higherlevel proofing authoring tools eg able automatically discern confusables thereby detect con fusable errors golding roth evenzohar roth banko brill huang powers", "could alleviate problems low frequency unknown words natural language processing information retrieval replacing likely higherfrequency alternatives carry similar information", "since task word prediction direct interpretation language modeling word prediction system could provide useful information used speech recognition systems", "unique aspect word prediction task compared tasks natural language processing realworld examples abound large amounts", "digitized text used training material word prediction system capable learning examples nowadays gigascale terascale document collections available research purposes", "specific type word prediction confus able prediction ie learn predict limited sets confusable words totwotoo theretheirtheyre golding roth banko brill", "trained confusable predictor occurrences words confusable set applied new occurrence word set prediction based context deviates word actually present workshop computationally hard problemsand joint inference speech language processing pages new york city new york june", "qc association computational linguistics word might confusable error classifiers prediction might correction", "confusable prediction correction strong asset proofing tools", "paper generalize word prediction task predicting word context", "basically task generic language model", "explicit choice particular study allwords prediction encode context words higherlevel linguistic nonterminals investigated related work word prediction wu et al evenzohar roth", "choice leaves open question tasks learned examples nonterminal symbols taken account well", "choice algorithm decisiontree approximation knearestneigbor knn based memorybased learning motivated fact describe later paper particular algorithm scale predicting tens thousands words simultaneously able scale tens millions examples training material predicting words useful rates hundreds thousands words per second", "another motivation choice decisiontree approximation knearest neighbor classification functionally equivalent backoff smoothing zavrel daelemans share performance capacities ngram models backoff smoothing shares scaling abilities models able handle large values article structured follows", "section describe data selected experiments provide overview experimental methodology used throughout experiments including description igtr ee algorithm central study", "section results word prediction experiments presented subsequent section contains experimental results experiments confusables", "briefly relate work earlier work inspired current study section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.019822282980281738, "p": 0.045454545454545456, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018851537450315963, "p": 0.08333333333333333, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03717445423213492, "p": 0.2222222222222222, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["present classificationbased word prediction model based igtr ee decisiontree induction algorithm favorable scaling abilities functional equivalence ngram models back smoothing", "first series experiments train reuters newswire text test either type data general fictional text demonstrate system exhibits loglinear increases prediction accuracy increasing numbers training examples", "trained million words newswire text prediction accuracies range fictional text newswire text", "second series experiments compare allwords prediction confusable prediction ie task specialized predicting limited sets words", "con fusable prediction yields high accuracies example confusable sets genres text", "confusable approach outperforms allwordsprediction approach data difference decreases"]}, "W07-0722": {"introduction": ["results discussed conclusions drawn section", "mixture modelling popular approach density estimation many scientific areas mclachlan peel", "interesting properties mixture modelling capability model multimodal datasets defining soft partitions datasets learning specific probability distributions partition better explains general data generation process", "work supported ec feder spanish mec grant tin200615694co201 consellera dempresa universitat cienciageneralitat valenciana contract gv06252 universidad politecnica de valencia ileta project ministerio de educacio ciencia", "machine translation mt common encounter large parallel corpora devoted heterogeneous topics", "topics usually define sets topicspecific lexicons need translated taking semantic context found", "semantic dependency problem could overcome learning topicdependent translation models capture together semantic context translation process", "recently application mixture modelling smt received increasing attention", "zhao xing fairly sophisticated bayesian topical translation models taking ibm model baseline model presented bilingual topic admixture model formalism", "models capture latent topics document level order reduce semantic ambiguity improve translation coherence", "models proposed provide cases better word alignment translation quality hmm ibm models englishchinese task", "civera juan mixture extension ibm model along specific dynamic programming decoding algorithm proposed", "ibm2 mixture model offers significant gain translation quality conventional ibm model semisynthetic task", "work present mixture extension wellknown hmm alignment model first proposed vogel others refined och ney", "model possesses appealing properties worth mentioning simplicity firstorder word alignment distribution made independent absolute positions proceedings second workshop statistical machine translation pages prague june", "qc association computational linguistics taking advantage localization phenomenon word alignment european languages efficient exact computation estep viterbi alignment using dynamicprogramming approach", "properties made model suitable extensions toutanova et al integration phrasebased model deng byrne past", "mixture hmm alignment models"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024265011818283992, "p": 0.04, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}], "abstract": ["mixture modelling standard technique density estimation use statistical machine translation smt started explored", "main advantages technique capability learn specific probability distributions better fit subsets training dataset", "feature even important smt given difficulties translate polysemic terms whose semantic depends context term appears", "paper describe mixture extension hmm alignment model derivation viterbi alignments feed stateoftheart phrasebased system", "experiments carried europarl news commentary corpora show potential interest limitations mixture modelling"]}, "W07-0802-parscit130908": {"introduction": ["let us suppose px generated using tcomponent mixture hmm alignment models px pt px t1 pt px", "problem", "language technology software localization con sume significant share many companies timeand work", "translating operating system application different languages involves tra ditional approach translating outofcontext strings different languages", "requires languageexpert new language still involve languagerelated problems difficulty translating outofcontext strings tak ing care morphological syntactic variations time", "illustrate example", "mail reader application wants display messages like new message new messages new messages new messagesif translated arabic special mor phological syntactic considerations bemade include inflecting message num ber risalatun messages risalatani messages prasava messages risalatan x100 messages risalatin word messages translated dif ferent words arabic depending numeral counting", "counted nouns extreme example varied case inflection case thesingular dual determined syntac tic function nominative example", "thisis case plurals assume geni tive case diptote thus nightynine genitive plurals aremultiples hundred", "mention noun adjective agreement taken care translating new messages arabicthe aforementioned details responsibility application programmer hav message marker accusative singular eleven proceedings 5th workshop important unresolved matters pages prague czech republic june", "c2007 association computational linguistics ing translators work application costly lead repeated work andor poor results", "solution contributions", "reviewed works dada ranta approach addresses problems language technology similar limited", "applied approach arabic thus developing aresource grammar arabic imple ment rules cover orthography morphologyand syntax", "short approach based de veloping libraries natural language constructs andrules used application programmer knowledgeable specific language", "core programming language grammatical framework gf ranta", "lan guage library called resource grammar khegai ranta comprising linguistic rules reused applications application programming interface api programmers thatare unaware details specific natural language", "programmer uses resource gram mar assuming take care morphological andsyntactic rules", "far implemented significant parts arabic morphology syntax ortho graphic rules provided sample lexicon words based swadesh list hymes", "paper describe part work namely numeral system arabic syntax"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02281353468472712, "p": 0.08333333333333333, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.046489459939925856, "p": 0.13333333333333333, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["numeral system arabic rich morphosyntactic variety suffers lack good computational resource describes reusable way", "implies applications require use rules arabic numeral system either reimplement time implieswasted resources use simplified imprecise rules result low quality applications", "solution devised withgrammatical framework gf use language constructs grammars librariesthat written reused various applications", "paper describe implementation arabic numeralsystem example bigger implementation grammar library arabic", "show users reuse system accessing simple languageindependent api rule"]}, "W08-0329": {"introduction": ["next section elaborate approach programming language implements resource grammars", "confusion network decoding applied combining outputs multiple machine translation systems", "earliest approach bangalore et al used edit distance based multiple string alignment msa durbin et al build confusion networks", "recent approaches used pairwise alignment algorithms based symmetric alignments hmm alignment model matusov et al edit distance alignments allowing shifts rosti et al", "alignment method described paper extends latter incrementally aligning hypotheses msa allowing shifts ter alignment", "confusion networks built around skeleton hypothesis", "skeleton hypothesis defines word order decoding output", "usually 1best hypotheses system considered possible skeletons", "using pairwise hypothesis alignment confusion networks built steps", "first hypotheses aligned skeleton independently", "second confusion networks created union alignments", "incremental hypothesis alignment algorithm combines steps", "words previously aligned hypotheses available even present skeleton hypothesis aligning following hypotheses", "rosti et al confusion networks built around skeletons joined lattice expanded scored language models", "system weights language model weights tuned optimize quality decoding output development set", "paper organized follows", "incremental ter alignment algorithm described section", "experimental evaluation comparing incremental pairwise alignment methods presented section along results wmt08 europarl test sets"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029503220552409816, "p": 0.06666666666666667, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027452833904475744, "p": 0.2, "r": 0.02702702702702703}}], "abstract": ["confusion network decoding successful approach combining outputs multiple machine translation mt systems recent darpa gale nist open mt evaluations", "due varying word order outputs different mt systems hypothesis alignment presents biggest challenge confusion network decoding", "paper describes incremental alignment method build confusion networks based translation edit rate ter algorithm", "new algorithm yields significant bleu score improvements recent alignment methods gale test sets used bbns submission wmt08 shared translation task"]}, "W09-0441": {"introduction": ["conclusions future work presented section", "since introduction bleu metric pa pineni et al statistical mt systems moved away human evaluation performance towards rapid evaluation using automatic metrics", "automatic metrics evaluated ability generate scores mt output correlate well human judgments translation quality", "numerous methods judging mt output humans used including fluency adequacy recently humanmediated translation edit rate hter snover et al", "fluency measures whether translation fluent regardless correct meaning adequacy measures whether translation conveys correct meaning even translation fully fluent", "fluency adequacy frequently measured together discrete point scale average used single score translation quality", "hter complex semiautomatic measure humans score translations directly rather generate new reference translation closer mt output retains fluency meaning original reference", "new targeted reference used reference translation scoring mt output using translation edit rate ter snover et al used automatic metrics bleu meteor banerjee lavie", "difficulties creation targeted references requirement annotator attempt minimize number edits measured ter mt output targeted reference creating reference close possible mt output still adequate fluent", "way true errors mt output counted", "hter shown consistent finer grained individual human annotators fluency adequacy much time consuming taxing human annotators types human judgments making difficult expensive use", "addition hter treats edits equally distinction made serious errors errors names missing subjects minor edits difference verb agreement proceedings fourth workshop statistical machine translation pages athens greece march march", "qc association computational linguistics missing determinator", "different types translation errors vary importance depending type human judgment used evaluate translation", "example errors tense might barely affect adequacy translation might cause translation scored less fluent", "hand deletion content words might lower fluency translation adequacy would suffer", "paper examine differences taking automatic evaluation metric tuning human judgments examining resulting differences parameterization metric", "study introduce new evaluation metric terplus terp1 improves existing translation edit rate ter metric snover et al incorporating morphology synonymy paraphrases well tunable costs different types errors allow easy interpretation differences human judgments", "section summarizes ter metric discusses terp improves", "correlation results human judgments including independent results nist metrics matr evaluation terp consistently top metrics presented section show utility terp evaluation metric", "generation paraphrases well effect varying source paraphrases discussed section"], "introduction_label": [{"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02407764113210427, "p": 0.045454545454545456, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024217344368538975, "p": 0.041666666666666664, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024048381892663364, "p": 0.030303030303030304, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["automatic machine translation mt evaluation metrics traditionally evaluated correlation scores assign mt output human judgments translation performance", "different types human judgments fluency adequacy hter measure varying aspects mt performance captured automatic mt metrics", "explore differences use new tunable mt metric terplus extends translation edit rate evaluation metric tun able parameters incorporation morphology synonymy paraphrases", "terplus shown top metrics nists metrics matr challenge highest average rank terms pearson spear man correlation", "optimizing terplus different types human judgments yields significantly improved correlations meaningful changes weight different types edits demonstrating significant differences types human judgments"]}, "W09-0621": {"introduction": ["section discusses results tuning terp fluency adequacy hter affects weights various edit types", "recent years texttotext generation received increasing attention field natural language generation nlg", "contrast traditional concepttotext systems texttotext generation systems convert source text target text typically source target text share meaning extent", "applications texttotext generation include sum marization knight marcu question answering lin pantel machine translation", "texttotext generation important know words phrases semantically close exchangable contexts", "various resources available capture knowledge word level eg synset knowledge wordnet kind information much harder get phrase level", "therefore paraphrase acquisition considered important technology producing resources texttotext generation", "paraphrase generation already proven valuable question answering lin pantel riezler et al machine translation callisonburch et al evaluation thereof russolassner et al kauchak barzilay zhou et al text simplification explanation", "study described paper make effort collect dutch paraphrases news article headlines unsupervised way used future paraphrase generation", "news article headlines abundant web already grouped news aggregators google news", "services collect multiple articles covering event", "crawling news aggregators effective way collecting related articles straightforwardly used acquisition paraphrases dolan et al nelken shieber"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05994511817304724, "p": 0.1111111111111111, "r": 0.05405405405405406}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["developing datadriven text rewriting algorithm paraphrasing essential monolingual corpus aligned paraphrased sentences", "news article headlines rich source paraphrases tend describe event various different ways easily obtained web", "compare methods aligning headlines construct aligned corpus paraphrases based clustering pairwise similaritybased matching", "show latter performs best task aligning paraphrastic headlines"]}, "W09-0802": {"introduction": ["use method collect large amount aligned paraphrases automatic fashion", "karamel system defining executing multitape finitestate transducers relationships tapes expressed using tree structure", "structure obtained embedded units used analyze tuple strings recognized transducer", "instance units considered example affix form sentence", "system includes language integrated development environment", "language uses extended regular expressions computations contextual rules", "environment provides graphical interface write execute finitestate descriptions", "karamel many applications", "natural language processing used morphological analysis transliteration parsing etc paper dedicated application karamel morphological analysis semitic languages multiple tapes complex structures useful", "descriptions morphology semitic languages use several tiers", "instance mccarthy uses tiers prefixes root template consonant vowel pattern last vocalizationsuch multitiered description im plemented using cascade 2tape machines beesley using multitape transducer tier described tape surface form additional tape", "approach kiraz syriac language kiraz", "karamel designed later solution", "multitape feature interesting describing related dialects whenever great part analysis shared", "separate tape dedicated surface form dialect", "semitic morphology strongly structured roots", "basis analysis identification root", "furthermore several layers affixes distinguished around core containing roots paradigmatic prefixes affixes encoding person gender number clitics pronouns", "structure conveniently defined using karamels units", "following section paper present karamels language theoretical background syntax", "section describe aspects karamel system development environment current state future evolutions", "comes example semitic morphology written karamel description akkadian verbal flexion"], "introduction_label": [{"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.049999996012500325, "p": 0.034482758620689655, "r": 0.09090909090909091}, "rouge-l": {"f": 0.048408284628669786, "p": 0.06451612903225806, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0220600295227433, "p": 0.16666666666666666, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["karamel system finitestate morphology multitape uses typed cartesian product relate tapes structured way", "implements statically compiled feature structures", "language allows use regular expressions generalized restriction rules define multitape transducers", "simultaneous successive application local constraints possible", "system interesting describing rich structured morphologies morphology semitic languages"]}, "W09-1007": {"introduction": ["last section compares karamel systems", "writing texts people often use spelling checkers reduce number spelling mistakes texts", "many spelling checkers concentrate nonword errors", "errors easily identified texts consist character sequences part language", "example english woord part language hence nonword error", "possible correction would word even text contain non word errors guarantee text errorfree", "several types spelling errors words part language used incorrectly context", "note kinds errors much harder recognize information context occur required recognize correct errors", "contrast nonword errors recognized without context", "class errors called confusibles consists words belong language used incorrectly respect local sentential context", "example owns cars contains confusible note word valid token part language used incorrectly context", "considering context correct likely alternative would word confusibles grouped together confusible sets", "confusible sets sets words similar often used incorrectly context", "third alternative particular confusible set", "research presented part larger project focusses contextsensitive spelling mistakes general", "project classes contextsensitive spelling errors tackled", "example addition confusibles class pragmatically incorrect words words incorrectly used document wide context considered well"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.016273788316535746, "p": 0.06666666666666667, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["problem identifying correcting confusibles ie contextsensitive spelling errors text typically tackled using specifically trained machine learning classifiers", "different set con fusibles specific classifier trained tuned", "research investigate generic approach contextsensitive con fusible correction", "instead using specific classifiers use generic classifier based language model", "measures likelihood sentences different possible solutions confusible place", "advantage approach confusible sets handled single model", "preliminary results show performance generic classifier approach slightly worse specific classifier approach"]}, "W10-1730-parscit": {"introduction": ["article concentrate problem confusibles context large sentence", "principle maximum entropy states thatgiven known constraints probability distri bution best represents current state knowledge largest entropymaximum entropy models based princi ple widely used natural language processing eg tagging ratnaparkhi 1996parsing charniak named entity recog nition bender et al", "maximum entropy models following form fi feature function ai weight zx normalizing factor zx exp aifix iin statistical machine translation smt trans lation model tm pts probability string target language translation string source language", "typical approach smt use backward translationmodel pst according bayes rule noisy channel model", "paper deal forward direct model1the idea using maximum entropy con structing forward translation models new", "naturally allows make use various featurespotentially important correct choice targetlanguage expressions", "let us adopt motivat ing example feature berger et al contains first usage maxenttranslation model aware house ap pears next words eg phrases house red house dans might likely french translation incorporating nonlocal features extracted fromthe source sentence standard noisychannel model backward trans lation model available possible", "thisdrawback noisychannel approach typi cally compensated using large targetlanguage ngram models result play arole similar elaborate context sensitive forward translation model", "ever expect would beneficial exploit parallel data monolingual data balance fashion rather extract reduced amount information parallel data compensate large language model target side", "1a backward translation model used pruning training data paper", "pyx zxexp aifix proceedings joint 5th workshop statistical machine translation metricsmatr pages uppsala sweden july", "c2010 association computational linguistics deeper discussion potential advantagesof maximum entropy approach noisy channel approach found foster 2000and och ney another suc cessful applications maxent translation models shown", "loglinear translation models instead mle rich feature sets used ittycheriah roukos gimpel andsmith idea traced back pap ineni et al", "makes approach different previously published works that1", "show maximum entropy trans lation model used dependencyframework use deepsyntactic dependency trees defined prague depen dency treebank hajic et al transfer layer2", "combine maximum entropy transla tion model targetlanguage dependency tree model use treemodified viterbisearch finding optimal lemmas label ing targettree nodes", "rest paper structured follows", "insection give brief overview translation framework tectomt experi ments implemented", "section describehow translation models constructed"], "introduction_label": [{"rouge-1": {"f": 0.040816323048729994, "p": 0.02631578947368421, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023216622514122622, "p": 0.02564102564102564, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023987541863372472, "p": 0.047619047619047616, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["maximum entropy principle used successfully various nlp tasks", "inthis paper propose forward translation model consisting set maximum entropy classifiers separate classifier trained sufficiently frequent sourceside lemma", "way estimates translation probabilitiescan sensitive large number features derived source sentence including nonlocal features features making use sentence syntactic structureetc", "integrated englishtoczech dependencybased translation scenario implemented tectomt framework new translation model significantly outperforms baseline modelmle terms bleu", "performance boosted configurationinspired hidden tree markov models combines maximum entropy translation model targetlanguage dependency tree model"]}, "W10-1750": {"introduction": ["sec tion summarizes experimental results section contains summary", "current automatic similarity measures machine translation mt evaluation operate without exception segment level", "translations analyzed segmentbysegment1 fashion ignoring text structure", "document system scores obtained using aggregate statistics individual segments", "strategy presents main disadvantage ignoring cross sententialdiscursive phenomena", "work suggest widening scope evaluation methods", "defined genuine documentlevel measures able exploit structure text provide informed evaluation scores", "purpose take advantage coincidental facts", "first test beds employed recent mt evaluation campaigns include document structure grouping sentences related event story topic przybocki et al przybocki et al callisonburch et al", "second count automatic linguistic processors provide detailed discourse level representations text curran et al", "discourse representations allow us focus relevant pieces information agent segment typically consists sentences", "location time theme spread text", "counting means discerning events individuals taking part role crucial determine semantic equivalence reference document candidate translation", "moreover discourse analysis document mere concatenation analyses individual sentences", "phenomena go beyond scope sentence explained context whole document", "instance newspaper article facts entities progressively added discourse referred anaphorically later", "following extract development set illustrates importance phenomenon discourse analysis current underlying crises middle east rod larsen mentioned arabisraeli conflict iranian nuclear portfolio well crisis lebanon syria", "stated leads us back crucial values opinions render situation prone moment getting control past days", "subject pronoun works anaphoric pronoun whose antecedent proper noun rod larson", "anaphoric relation established elements identified analyzing text whole thus considering gender agreement third person singular masculine subject pronoun masculine proper noun rod larson", "sentences analyzed separately identification anaphoric relation would feasible due lack connection elements", "discourse representations allow us trace links sentences different facts entities appearing", "therefore providing approach text similar human implies taking account whole text structure instead considering sentence separately", "rest paper organized follows", "section describes evaluation methods linguistic theory upon based", "experimental results reported discussed section", "section presents metric submitted evaluation challenge", "future work outlined section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03699025622525012, "p": 0.16666666666666666, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.036935842946350037, "p": 0.038461538461538464, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.036935842946350037, "p": 0.038461538461538464, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0363372093023574, "p": 0.25, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes joint submission universitat politecnica de catalunya universitat de barcelona metrics matr evaluation challenge collaboration eldaelra", "work aimed widening scope current automatic evaluation measures sentence document level", "preliminary experiments based extension metrics gimenez marquez operating discourse representations presented"]}, "W10-1757": {"introduction": ["additional result documentlevel metrics generated study incorporated iqmt package automatic mt evaluation2", "many natural language processing applications machine translation mt parsing language modeling benefit nbest reranking framework shen et al collins koo roark et al", "advantage nbest reranking abstracts away complexities firstpass decoding allowing researcher try new features learning algorithms fast experimental turnover", "nbest reranking scenario training data consists sets hypotheses ie nbest lists generated firstpass system along labels", "given new nbest list goal rerank best hypothesis appears near top list", "existing research focused training single reranker directly entire data", "approach reasonable data homogenous fails features vary significantly different nbest lists", "particular employs sparse feature sets seldom finds features simultaneously active multiple nbest lists", "case believe advantageous view nbest reranking problem multi task learning problem nbest list corresponds distinct task", "multitask learning subfield machine learning focuses effectively train set different related datasets tasks", "heterogenous nbest list data fits nicely assumption", "contribution work threefold", "introduce idea viewing nbest", "reranking multitask learning problem"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.14285713948979603, "p": 0.3333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.015657719579972886, "p": 0.3333333333333333, "r": 0.015625}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["propose new framework nbest reranking sparse feature sets", "idea reformulate reranking problem multitask learning problem nbest list corresponds distinct task", "motivated observation nbest lists often show significant differences feature distributions", "training single reranker directly heteroge nous data difficult", "proposed metaalgorithm solves challenge using multitask learning regularization discover common feature representations best lists", "metaalgorithm simple implement modular approach allows plugin different learning algorithms existing literature", "proof concept show statistically significant improvements machine translation system involving millions features"]}, "W10-1761": {"introduction": ["view particularly apt general reranking problem sparse feature sets", "hierarchical decoding usually described formally syntactic model without linguistic commitments contrast syntactic decoding constrains rules production linguistically motivated labels", "decoding mechanism hierarchical syntactic systems identical rule extraction similar", "hierarchical syntax statistical machine translation made great progress last years claim represent state art field", "use synchronous context free grammar scfg formalism consisting rewrite rules simultaneously parse input sentence generate output sentence", "common algorithm decoding scfg currently cky cube pruning works hierarchical syntactic systems implemented hiero chiang joshua li et al moses hoang et al rewrite rules hierarchical systems general applicability nonterminals undec orated giving hierarchical system broad coverage", "rules used inappropriate situations without labeled constraints", "general applicability undecorated rules create spurious ambiguity decreases translation performance causing decoder spend time sifting duplicate hypotheses", "syntactic systems makes use linguistically motivated information bias search space expense limiting model coverage", "paper presents work combining hierarchical syntax translation utilizing high coverage hierarchical decoding insights syntactic information bring", "seek balance generality using undeco rated nonterminals specificity labeled nonterminals", "specifically use syntactic labels source language parser label nonterminal production rules", "source span information chunk tags used", "investigate methods combining hierarchical syntactic approach", "first method syntactic translation rules used concurrently hierarchical phrase rules", "ruleset trained independently used concurrently decode sentences", "results method improve", "second method uses translation model containing hierarchical syntactic rules", "moreover individual rule contain decorated syntactic nonterminals undeco rated hierarchicalstyle nonterminals lefthandside nonterminal decorated", "results improvement hierarchical baseline analysis suggest longrange ordering improved"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.039926289926312905, "p": 0.0625, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03801431126979297, "p": 0.041666666666666664, "r": 0.03571428571428571}}], "abstract": ["present new translation model include undecorated hierarchicalstyle phrase rules decorated sourcesyntax rules partially decorated rules", "results show increase translation performance bleu germanenglish translation trained newscommentary corpus using syntactic annotation source language parser", "experimented annotation shallow taggers found increased performance bleu"]}, "W10-4128": {"introduction": ["applied methods using linguistic annotation chunk tagger abney instead parser obtained improvement bleu hierarchical baseline showing gains additional source side annotation obtained simpler tools", "chinese word segmentation cws witnessed prominent progress first sighan bakeoffs", "since xue used characterbased tagging method attracted attention", "previous work peng et al tseng et al low et al illustrated effectiveness using characters tagging units literatures zhang et al zhao kit 2007a zhang clark focus employing lexical words subwords tagging units", "wordbased models capture wordlevel contextual information iv knowledge", "besides many strategies proposed balance iv oov performance wang et al", "crf widely used sequence labeling tasks good performance laffertyet al", "zhao kit 2007b tempt integrate global information local information improve crfbased tagging method cws provides solid foundation strengthening crf learning unsupervised learning outcomes"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11428570997551038, "p": 0.08333333333333333, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03020562225567975, "p": 0.06060606060606061, "r": 0.0273972602739726}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013844925538298282, "p": 0.125, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014601778735040098, "p": 0.043478260869565216, "r": 0.0136986301369863}}], "abstract": ["paper presents chinese word segmentation system cipssighan chinese language processing task", "firstly based conditional random field crf model local features global features characterbased tagging model designed", "secondly hidden markov models hmm used revise substrings low marginal probability crf", "finally confidence measure used regenerate result simple rules deal strings letters numbers", "well known characterbased approach outstanding capability discovering outofvocabulary oov word external information word lost", "hmm makes use word information increase invocabulary iv recall", "participate simplified chinese word segmentation closed open test corpora belong different domains", "system achieves better performance"]}, "W10-4223": {"introduction": ["order increase accuracy tagging using crf adopt strategy marginal probability characters lower threshold modified component based hmm trigged combining confidence measure results regenerated", "texttotext generation increasingly studied subfield natural language processing", "contrast typical natural language generation paradigm converting concepts text text totext generation source text converted target text approximates meaning source text", "texttotext generation extends varied tasks summarization knight marcu questionanswering lin pan tel machine translation paraphrase generation", "sentential paraphrase generation spg process transforming source sentence target sentence language differs form source sentence approximates meaning", "paraphrasing often used subtask complex nlp applications allow variation text strings presented input example generate paraphrases questions original form cannot answered lin pantel riezler et al generate paraphrases sentences failed translate callisonburch et al", "paraphrasing used evaluation machine translation system output russolassner et al kauchak barzilay zhou et al", "adding certain constraints paraphrasing allows additional useful applications", "constraint specified paraphrase shorter input text paraphrasing used sentence compression knight marcu barzilay lee well text simplification question answering subtitle generation daelemans et al", "regard spg monolingual machine translation task source target languages quirk et al", "problems dealt make approach work namely obtaining sufficient amount examples proper evaluation methodology", "callisonburch et al", "argue automatic evaluation paraphrasing problematic", "essence spg generate sentence structurally different source", "automatic evaluation metrics related fields machine translation operate notion similarity paraphrasing centers around achieving dissimilarity", "besides evaluation issue another problem data driven mt account paraphrasing work large collection data required", "case would pairs sentences paraphrases", "far paraphrasing data sets sufficient size mostly lacking"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024217344368538975, "p": 0.041666666666666664, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper investigate automatic generation evaluation sentential paraphrases", "describe method generating sentential paraphrases using large aligned monolingual corpus news headlines acquired automatically google news standard phrasebased machine translation pbmt framework", "output system compared word substitution baseline", "human judges prefer pbmt paraphrasing system word substitution system", "demonstrate bleu correlates well human judgements provided generated paraphrased sentence sufficiently different source sentence"]}, "W11-0311": {"introduction": ["argue headlines aggregated google news offer attractive avenue", "often methods opinion sentiment subjectivity analysis rely lexicons subjective opinioncarrying words eg turney whitelaw et al riloff wiebe yu hatzivassiloglou kim hovy bloom et al andreevskaia bergler agarwal et al", "examples words following bold disease every team gone", "converting smf headache", "concert left cold", "guy pain", "even manually developed subjectivity lexicons significant degrees subjectivity sense ambiguity su markert gyamfi et al", "many clues lexicons subjective objective senses", "ambiguity leads errors opinion sentiment analysis objective instances represent false hits subjectivity clues", "example following sentence contains keywords used objective senses early symptoms disease include severe headaches red eyes fevers cold chills body pain vomiting", "recently akkaya et al introduced task subjectivity word sense disambiguation swsd automatically determine word instances corpus used subjective senses used objective senses", "developed supervised system swsd exploited swsd output improve performance multiple contextual opinion analysis tasks", "although reported results promising obvious shortcomings", "first able apply swsd contextual opinion analysis small scale due shortage annotated data", "experiments show swsd improves contextual opinion analysis small amount opinionannotated data coverage system", "questions arise feasible obtain greater amounts needed data swsd performance improvements contextual opinion analysis hold proceedings fifteenth conference computational natural language learning pages portland oregon usa june", "qc association computational linguistics larger scale", "second annotations akkaya et al piggybacked senseval sense tagged data finegrained word sense annotations created trained annotators", "concern swsd performance improvements contextual opinion analysis achieved using finegrained expert annotations availability limited", "third akkaya et al uses manual rules apply swsd contextual opinion analysis", "although rules advantage transparently show effects swsd somewhat ad hoc", "likely optimal holding back potential swsd improve contextual opinion analysis", "address shortcomings paper investigate feasibility obtaining substantial amount annotated data whether performance improvements contextual opinion analysis realized larger scale whether improvements realized subjectivity sense tagged data built expert full inventory sense annotations"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02310151878506479, "p": 0.07142857142857142, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["subjectivity word sense disambiguation swsd automatically determining word instances corpus used subjective senses used objective senses", "swsd shown improve performance contextual opinion analysis small scale using manually developed integration rules", "paper scale integration swsd contextual opinion analysis still obtain improvements performance successfully gathering data annotated nonexpert annotators", "improving method integrating swsd contextual opinion analysis even greater benefits swsd achieved previous work", "thus firmly demonstrate potential swsd improve contextual opinion analysis"]}, "W11-0910": {"introduction": ["addition explore better methods applying swsd contextual opinion analysis", "automatic semantic analysis successful taking supervised learning approach data labeled sense tags semantic roles eg see mrquez et al", "underlying recent successes lexical resources propbank palmer et al verbnet kipper et al framenet baker et al fillmore et al encode relational semantics numerous lexical items especially verbs", "authors speakers use verbs productively previously unseen ways semantic analysis systems must limited direct extrapolation previously seen usages licensed static lexical resources cf", "pustejovsky jezek", "achieve accurate semantic analyses must augment resources knowledge extensibility verbs", "central verb extensibility process semantic syntactic coercion", "coercion allows verb used atypical contexts extend relational semantics thereby enabling expression novel concept simply fluid expression complex concept", "example consider strictly intransitive action verb blink", "verb instead used construction object blinked snow lashes leading interpretation verb object causally affected changes location caused motion construction goldberg", "type constructional coercion common language underlies much extensibility verb usages", "understanding coercive processes thus significant impact represent knowledge verbs lexical resource", "importantly constructional coercion allor nothing process word must semantically syntactically compatible respects context order use extended context restrictions compatibility hardandfast rules langacker kay fillmore goldberg goldberg appear", "gradience compatibility plays important role coercion suggesting probabilistic approach necessary encoding knowledge constructional coercion verb lexicon cf", "lapata lascarides", "hypothesis due gradient process productivity existing verb lexicons adequately capture actual patterns use extensible constructions", "paper focus caused motion cm construction initial test case", "first annotate classes extensive verb lexicon verbnet whether cm construction allowed none verbs class noting additionally whether typical coerced usage", "find many classes allow construction least verbs include cm frame definition indicating significant shortcoming relational knowledge encoded lexicon", "next develop probabilistic measures determining degree class likely admit cm construction", "test measures corpus data manually annotated use cm construction", "finally present preliminary work automatic techniques calculating proposed measures unsupervised way avoid need expensive manual annotation"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02253498210224221, "p": 0.1, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0226722207020277, "p": 0.09090909090909091, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02365074441697788, "p": 0.05555555555555555, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.14814814331961607, "p": 0.125, "r": 0.18181818181818182}, "rouge-l": {"f": 0.04677025001981951, "p": 0.125, "r": 0.043478260869565216}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0233851250099607, "p": 0.0625, "r": 0.021739130434782608}}], "abstract": ["take first steps towards augmenting lexical resource verbnet probabilistic information coercive constructions", "focus caused motion example construction occurring verbs typical usage must interpreted extending event semantics coercion occurs productively adds substantially relational semantics verb", "annotation find verbnet fails accurately capture usages construction", "use unsupervised methods estimate probabilistic measures corpus data predicting usage construction verb classes lexicon evaluate verbnet", "discuss methods form basis enhancements verbnet supporting accurate analysis relational semantics verb productive usages"]}, "W11-1101": {"introduction": ["work forms preliminary steps toward empirically augmenting verbnets predictive capabilities concerning event semantics verbs coercible constructions", "entity relation detection erd aims finding relations pairs named entities nes text", "availability annotated corpora nist doddington et al introduction shared tasks eg", "farkas et al carreras marquez spurred large amount research field recent times", "researchers used supervised semisupervised approaches hasegawa et al mintz et al jiang explored rich features kambhatla kernel design culotta sorensen zhou et al bunescu mooney qian et al inference algorithms chan roth detect predefined relations nes", "work explore latent semantics text help detecting entity relations", "adapt latent dirichlet allocation lda approach solve erd task", "specifically present erd system based maximum entropy discriminant latent dirichlet allocation medlda", "medlda zhu et al extension latent dirichlet allocation lda combines capability capturing latent semantics discriminative capabilities svm", "number challenges employing lda framework erd", "latent dirichlet allocation supervised extensions labeled lda llda ramage et al supervised lda slda blei mcauliffe powerful generative models capture underlying semantics texts", "trouble discovering marginal classes easily employing rich feature sets important erd", "overcome first drawback employing medlda framework integrates maximum likelihood estimation mle maximum margin estimation mme", "specifically combination slda support vector machines svms", "order employ rich heterogeneous features introduce separate exponential family distribution feature similar et al medlda model", "formulate relation detection task topic model framework follows", "pairs ne mentions1 text considered adopting terminology used automatic context extraction ace program nist specific ne instances called mentions", "minidocument", "minidocument relation type analogous response variable supervised topic model", "topic model infers topic relation type distribution mini documents", "supervised topic model discovers latent topic representation minidocuments response parameter distribution", "topic representation discovered observed response variables training", "testing topic distribution minidocument form prediction relation types", "carry experiments measure effectiveness approach compare svm based lldabased models well previous work using corpora", "measure analyze effectiveness incorporating different features model relative models", "approach exhibits better overall precision recall fmeasure baseline systems", "find medldabased approach shows consistent capability incorporation improvement due variety heterogeneous features", "rest paper organized follows", "describe proposed model section features explore work section", "section describes data experiments results analyses"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01858722711608743, "p": 0.1111111111111111, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.23076922588757406, "p": 0.2, "r": 0.2727272727272727}, "rouge-l": {"f": 0.057437407952954864, "p": 0.2, "r": 0.05454545454545454}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018509530400842942, "p": 0.125, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018374341951265707, "p": 0.16666666666666666, "r": 0.01818181818181818}}], "abstract": ["paper proposes novel application supervised topic model entity relation detection erd", "adapt maximum entropy discriminant latent dirichlet allocation medlda mixed membership relation detection", "erd task reformulated fit topic modeling framework", "approach combines benefits maximumlikelihood estimation mle maxmargin estimation mme mixed membership formulation enables system incorporate heterogeneous features", "incorporate different features system perform experiments ace corpus", "approach achieves better overall performance precision recall fmeasure metrics compared svmbased lldabased models"]}, "W11-1604": {"introduction": ["discuss related work section concluding section", "challenging properties natural language semantic content typically expressed many different surface forms", "ability deal paraphrases holds great potential improving coverage nlp systems substantial body research addressing recognition extraction generation paraphrases emerged androutsopoulos malakasiotis madnani dorr", "paraphrase generation regarded translation task source target language", "paraphrase generation machine translation mt instances texttotext generation involves transforming text another obeying certain restrictions", "restrictions generated text must grammatically wellformed semanticallytranslationally equivalent source text", "addionally paraphrase generation requires output differ input certain degree", "similarity paraphrase generation mt suggests methods tools originally developed mt could exploited paraphrase generation", "popular approach arguably successful far statistical phrase based machine translation pbmt learns phrase translation rules aligned bilingual text corpora och et al vogel et al zens et al koehn et al", "prior work explored use pbmt paraphrase generation quirk et al bannard callisonburch mad nani et al callisonburch zhao et al wubben et al since many researchers believe pbmt reached performance ceiling ongoing research looks structural approaches statistical mt marcu wong och ney khalilov fonollosa", "syntax based mt attempts extract translation rules terms syntactic constituents subtrees rather arbitrary phrases presupposing syntactic structures source target languages", "syntactic information might lead better results area grammatical wellformedness unlike phrase based mt uses contiguous ngrams syntax enables modeling longdistance translation patterns", "verdict whether approach leads significant performance gain still similar line reasoning would suggest syntaxbased paraphrasing offer similar advantages phrasebased paraphrasing", "considering fact success pbmt partly attributed abundance large parallel corpora workshop monolingual texttotext generation pages proceedings 49th annual meeting association computational linguistics pages portland oregon june", "oc association computational linguistics sufficiently large parallel corpora still lacking paraphrase generation using linguistically motivated methods might prove beneficial paraphrase generation", "time automatic syntactic analysis introduces errors parse trees syntactic parser perfect", "likewise automatic alignment syntactic phrases prone errors", "main contribution paper systematic comparison phrasebased syntaxbased paraphrase generation using offtheshelf statistical machine translation smt decoder namely moses koehn et al wordalignment tool giza och ney", "training data derives new large scale 21m tokens paraphrase corpus dutch recently released", "paper organized follows", "section reviews paraphrase corpus provides training test data", "next section describes paraphrase generation methods experimental setup", "results presented section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.042553187904029274, "p": 0.027777777777777776, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024330631565134743, "p": 0.022727272727272728, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.060606056161616496, "p": 0.045454545454545456, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030227891877784848, "p": 0.045454545454545456, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030209967620282052, "p": 0.047619047619047616, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029972592299338248, "p": 0.038461538461538464, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paraphrase generation regarded machine translation source target language", "use moses statistical machine translation toolkit paraphrasing comparing phrasebased syntaxbased approaches", "data derived recently released large scale 21m tokens paraphrase corpus dutch", "preliminary results indicate phrasebased approach performs better terms nist scores produces paraphrases greater distance source"]}, "W11-1815": {"introduction": ["section discuss findings formulate conclusions", "given set web pages information extraction goal bacteria biotope bb task precisely identify bacteria locations relate", "type predicted locations selected types", "host hostpart locations related partof relation", "teams participated challenge", "bb task example ureaplasma parvum mycoplasma pathogenic biology challenges kim et al geographical locations zhou et al", "locations include natural environments hosts well food medical locations", "order deal heterogeneity propose framework based term analysis test corpus shallow mapping terms bacteria biotope bb terminoontology", "mapping derives type location terms filters nonlocation terms", "large external dictionaries host names ie ncbi taxonomy geographical names ie agrovoc thesaurus complete lexical resources", "high frequency bacteria anaphora ambiguous antecedent candidates corpus difficulty", "alvis system implements anaphora resolution algorithm takes consideration anaphoric distance position antecedent sentence", "alvis predicts bacteria names relation locations help handmade patterns based linguistic analysis lexical resources", "methods predicting typing locations section bacteria section first described", "section details method relating"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.023521012430543487, "p": 0.058823529411764705, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022957210171587188, "p": 0.07692307692307693, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes system inra bibliome research group applied bacteria biotope bb task bionlp shared tasks", "bacteria geographical locations host entities processed patternbased approach domain lexical resources", "extraction environment locations propose framework based semantic analysis supported ontology biotope domain", "domainspecific rules developed dealing bacteria anaphora", "official results show alvis system achieves best performance participating systems"]}, "W11-2139": {"introduction": ["section comments experimental results", "describe germanenglish translation system submitted shared translation task sixth workshop machine translation wmt11 ark research group carnegie mellon university1 core translation system hierarchical phrasebased machine translation system chiang extended several ways described paper", "innovations focus modeling", "since german english word orders diverge considerably particularly nonmatrix clauses focused feature engineering improve modeling longdistance relationships poorly captured standard hierarchical phrase based translation models", "developed features assess goodness source httpwwwarkcscmuedu language parse tree translation grammar rather linguistic grammar", "train feature weights made use novel twophase training algorithm incorporates probabilistic training objective standard minimum error training och", "segmentation features supplemented 7gram classbased language model directly models longdistance relationships", "together features provide modest improvement baseline suggest interesting directions future work", "work parse modeling involved required substantial changes training pipeline modeling enhancements quite simple example improving outofvocabulary words handled", "propose simple change show provides small consistent gain", "training side improvements baseline system", "first inspired work madnani showed training optimize bleu papineni et al overfitting reduced supplementing single humangenerated reference translation additional computergenerated references", "generated supplementary pseudoreferences development set translated many languages using mt output secondary spanishenglish translation system", "second following foster kuhn used secondary development set select many optimization runs improved generalization", "largely sought techniques require languagespecific resources eg treebanks pos proceedings 6th workshop statistical machine translation pages edinburgh scotland uk july", "qc association computational linguistics annotations morphological analyzers", "exception compound segmentation model used preprocessing trained corpus manually segmented german", "aside manually annotated data used suspect many improvements described language pairs"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058852258852275095, "p": 0.09090909090909091, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.053975258056447026, "p": 0.05555555555555555, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04975186104159399, "p": 0.047619047619047616, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05726011334742372, "p": 0.06666666666666667, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.054054049875822095, "p": 0.038461538461538464, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03974176529728453, "p": 0.03571428571428571, "r": 0.05263157894736842}}], "abstract": ["paper describes germanenglish translation system developed ark research group carnegie mellon university sixth workshop machine translation wmt11", "present results several modeling training improvements core hierarchical phrasebased translation system including feature engineering improve modeling derivation structure translations better handing oovs using development set translations languages create additional pseudo references training"]}, "W11-2408": {"introduction": ["despite avoiding languagespecific resources using training data provided workshop extensive manual evaluation determined outputs produced significantly higher quality statistical rulebased systems made use languagespecific resources callisonburch et al", "interested ultimately enabling inference system reason forward facts well backward goals using lexical knowledge together world knowledge", "creating appropriate collections general world knowledge support reasoning long goal researchers artificial intelligence", "efforts information extraction eg banko et al", "focused learning base facts specific entities barack obama president work knowledge extraction eg van durme schubert found generalizations president make speech", "latter provides basis possibilistic forward inference barack obama probably makes speech least occasionally meaning sharpened gordon schubert resources dont provide basis saying might expect happen instance someone crashes car", "driver car crash might injured car damaged matter common sense rarely stated directly", "found sentences expectation disconfirmed sally crashed car tree wasnt hurt exploring use lexicosyntactic discourse patterns indicating disconfirmed expectations well peoples goals joe apologized repeatedly hoping forgiven"], "introduction_label": [{"rouge-1": {"f": 0.22222221739369008, "p": 0.1875, "r": 0.2727272727272727}, "rouge-l": {"f": 0.22086306489938157, "p": 0.23529411764705882, "r": 0.21052631578947367}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.058524734982232, "p": 0.07692307692307693, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1290322534859523, "p": 0.1, "r": 0.18181818181818182}, "rouge-l": {"f": 0.09950372208377761, "p": 0.09523809523809523, "r": 0.10526315789473684}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.055555551311728714, "p": 0.04, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04243912420639128, "p": 0.038461538461538464, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.05521576622455992, "p": 0.058823529411764705, "r": 0.05263157894736842}}], "abstract": ["reasoning ordinary human situations activities requires availability diverse types knowledge including expectations probable results actions lexical entailments many predicates", "describe initial work acquire collection conditional ifthen knowledge exploiting presuppositional discourse patterns ones involving hoping abstracting matched material general rules"]}, "W12-0304": {"introduction": ["resulting rules expressed point natural language first step toward obtaining classes general conditional knowledge typically obtained methods", "paper discusses series early experiments methodology detection correction grammatical errors based cooccurrence statistics using extensive corpus ngrams google books compiled michel et al", "start complementary assumptions hand books published accurately say usually go different phases revision correction high standards thus large proportion texts used reference corpus inferring grammar rules language", "hand hypothesise sufficiently large corpus high percentage information rules extracted word ngrams", "thus although still many grammatical errors cannot detected method another important group identified corrected successfully see section", "grammatical errors difficult complex type language errors grammar made extensive number rules exceptions", "furthermore grammar observed actual texts panorama becomes far complicated number exceptions grows variety complexity syntactical structures increase extent predicted theoretical studies grammar", "grammar errors extremely important majority cannot considered performancebased meaning text therefore success failure communication compromised", "knowledge grammar book dictionary provided solution problems person writes tries follow grammar rules language", "doubts arise writing process always clearly associated lexical unit writer able detect association makes difficult find solution using reference book", "recent years advances made automatic detection grammar mistakes see section", "effective rulebased methods reported cost time consuming task inherent lack flexibility", "contrast statistical methods easier faster implement well flexible adaptable", "experiment describe following sections first part extensive study", "probably logical step follow order continue study hybrid approach based proceedings eacl workshop computational linguistics writing pages avignon france april", "qc association computational linguistics statistics rules", "hence paper aims contribute statistical approach applied grammar checking", "google books ngram corpus database ngrams sequences words records frequency distribution unit year onwards", "bulk corpus starts year took starting point material used compile reference corpus", "idea using database grammar checker analyse input text detect sequence words cannot found ngram database contains ngrams frequency equal greater eventually replace unit text makes frequent ngram", "specifically conduct types operations accepting text spotting possible errors inflecting lemma appropriate form given context fillingin blanks text selecting number options probable word form given context", "order evaluate algorithm applied solve exercises spanish grammar book tested detection errors corpus real errors made second language learners", "paper organised follows first offer brief description related work explain methodology experiments"], "introduction_label": [{"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03848295671005285, "p": 0.043478260869565216, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03968253968261376, "p": 0.07142857142857142, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0645161244536944, "p": 0.05, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0395299145297755, "p": 0.05, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.038137648251937765, "p": 0.1111111111111111, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05714285283265339, "p": 0.041666666666666664, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03633961810930086, "p": 0.037037037037037035, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03918918918928951, "p": 0.08333333333333333, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.14285713808673486, "p": 0.11764705882352941, "r": 0.18181818181818182}, "rouge-l": {"f": 0.07988088591102555, "p": 0.11764705882352941, "r": 0.07142857142857142}}], "abstract": ["research explore possibility using large ngram corpus google books derive lexical transition probabilities frequency word ngrams use check suggest corrections target text without need grammar rules", "conduct several experiments spanish although conclusions reach languages since procedure corpusdriven", "paper reports experiments involving different types grammar errors conducted test different grammarchecking procedures namely spotting possible errors deciding different lexical possibilities fillingin blanks text"]}, "W12-3205": {"introduction": ["next section show evaluation results comparison microsoft word grammar checker finally draw conclusions discuss lines future work", "research natural language processing nlp long benefitted fact text often treated simply bag words bag sentences", "always position often matters eg wellknown first sentences news report usually comprise best ex tractive summary", "order often matters eg different events conveyed depending clauses sentences ordered", "said magic words genie appeared", "genie appeared said magic words", "adjacency often matters eg attributed material span sequence adjacent sentences contrasts visible sentence juxtaposition", "context always matters eg languages achieve economy minimal expressions convey intended meaning understood context", "position order adjacency context intrinsic features discourse research discourse processing attempts solve challenges posed contextbound expressions discourse structures give rise linearized position order adjacency", "challenges language technology lt researchers care discourse rather discourse enable lt overcome known obstacles better performance", "consider automated summarization machine translation humans regularly judge output quality terms include referential clarity coherence", "systems improve paying attention discourse ie linguistic features level grams single sentences", "fact predict soon cheap ie nonmanual methods found reliably assessing features example using proxies like suggested pitler et al supplant least complement todays common metrics bleu rouge say little matters human text understanding callisonburch et al", "consider work automated text simplification way human editors simplify text reexpressing long complex sentence discourse sequence simple sentences", "researchers able automate understanding various ways information conveyed discourse", "examples lt applications already benefitting recognizing applying discourselevel information include automated assessment student essays burstein chodorow2010 summarization thione et al infor proceedings acl2012 special workshop rediscovering years discoveries pages jeju republic korea july", "qc association computational linguistics eales et al maslennikov chua recently statistical machine translation foster et al", "described detail webber et al"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": []}, "W12-3311": {"introduction": ["aim occasion acls 50th annual meeting briefly describe evolution computational approaches discourse structure reflect field currently stands new challenges faces trying deliver promised benefit language technology", "multiword expressions mwes range linguistic constructions idioms pay arm leg fixed phrases rock roll noun compounds dry ice", "unique widely accepted definition term multiword expression", "arbitrary recurrent word combination smadja syntactic semantic unit whose exact unambiguous meaning connotation cannot derived directly meaning connotation components choueka simply idiosyncratic interpretation crosses word boundaries spaces sag et al", "mwes lie fuzzy zone lexicon syntax thus constituting real challenge nlp systems", "addition pervasive occurring frequently everyday language well specialised communications", "common properties mwes are1 binary yesno flags values continuum going flexible word combinations prototypical fixed expressions", "src paid poor parents visit mt jai pay mes pauvres parents une visite ref jai rendu visite mes pauvres parents src students pay arm leg park campus mt les tudiants paient un bras et une jambe pour se garer sur le campus ref les tudiants paient les yeux de la tte pour se garer sur le campus src shares translationinvariance homogeneity properties central moment mt il partage la traductioninvariance et proprits dhomognit avec le moment central ref il partage les proprits dinvariance par translation et dhomognit avec le moment central table examples smt errors due mwes", "arbitrariness sometimes valid constructions acceptable people use", "smadja illustrates presenting different ways referring dow jones index used", "institutionalisation mwes recurrent correspond conventional ways saying things", "jackendoff estimates compose half entries speakers lexicon sag et al", "point underestimate consider domainspecific mwes", "limited semantic variability mwes undergo semantic compositionality rules ordinary word combinations", "expressed terms noncompositionality meaning whole expression often cannot directly inferred meaning parts composing ii nonsubstitutability possible replace part mwe related synonymequivalent word construction iii wordfor word translation", "proceedings student research workshop pages jeju republic korea july", "qc association computational linguistics limited syntactic variability standard grammatical rules apply mwes", "expressed terms lexicalisation cannot list mwes lexicon undergeneration include grammar overgeneration ii extragrammaticality mwes unpredictable seem weird second language learner knows general rules2 heterogeneity mwes hard define encompass large amount phenomena", "thus nlp applications cannot use unified approach need rely typology3 paper adopt definition calzolari et al", "define mwes different related phenomena described sequence4 words acts single unit level linguistic analysis", "generic intentionally vague definition narrowed according application needs", "example statistical machine translation mt system5 used examples shown table mwe sequence words translated unit generates errors ungrammatical unnatural verbal constructions sentence awkward literal translations idioms sentence problems lexical choice word order specialised texts sentence", "examples illustrate importance correctly dealing mwes mt applications generally mwes speed help remove ambiguities many current nlp applications example lexicography church hanks used lexicographic environment evaluation scenario comparing manual intuitive research automatic association ratio proposed", "word sense disambiguation mwes tend less polysemous simple words", "finlayson kulkarni exemplify word world senses wordnet record world record", "pos tagging parsing recent work parsing pos tagging indicates mwes help remove syntactic ambiguities seretan"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.022403507877481244, "p": 0.1111111111111111, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08695651674858251, "p": 0.08333333333333333, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02324472997001195, "p": 0.06666666666666667, "r": 0.021739130434782608}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper presents open flexible methodological framework automatic acquisition multiword expressions mwes monolingual textual corpora", "research motivated importance mwes nlp applications", "briefly presenting modules framework paper reports extrinsic evaluation results considering applications computeraided lexicography statistical machine translation", "applications benefit automatic mwe acquisition expressions acquired automatically corpora speed improve quality", "promising results previous ongoing experiments encourage investigation optimal way integrate mwe treatment many applications"]}, "W13-2101": {"introduction": ["information retrieval mwes like pop star indexed unit accuracy system improves multiword queries acosta et al", "surface realization task producing fluent text kind formal abstract representation meaning reiter dale", "obvious output natural language generation component namely text little agreement input formalism evans et al", "since opendomain semantic parsers able produce formal semantic representations nowadays bos butler yoshimoto would natural see generation reversed process consider semantic representations input surface realization component", "idea using large text corpora annotated formal semantic representations robust generation presented recently basile bos wanner et al", "need formal semantic representations basis nlg expressed already much earlier power derives semantic networks enriched scope information knowledge representations content planning", "paper take step towards goal generating text deep semantic representations consider issue aligning representations surface strings capture meaning", "first describe basic idea aligning semantic representations logical forms surface strings formalismindependent way section", "apply method wellknown widelyused semantic formalism namely discourse representation theory drt first demonstrating represent discourse representation structures drss graphs section showing resulting discourse representation graphs drgs equivalent drss convenient fulfill word level alignment section", "finally section present method generates partial surface strings discourse referent occurring semantic representation text composes complete surface form"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06896551253269949, "p": 0.05555555555555555, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.06666666202222254, "p": 0.05263157894736842, "r": 0.09090909090909091}, "rouge-l": {"f": 0.030209967620282052, "p": 0.047619047619047616, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0299725590865641, "p": 0.05555555555555555, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["statistical natural language generation abstract meaning representations presupposes large corpora consisting textmeaning pairs", "even though corpora exist nowadays could constructed using robust semantic parsing simple alignment text meaning representation coarse developing robust statistical nlg systems", "reformatting semantic representations graphs finegrained alignment obtained", "given precise alignment word level complete surface form meaning representations deduced using simple declarative rule"]}, "W13-2708": {"introduction": ["think would first important step surface realization formal semantic representations", "paper give examples nlp methods tools used provide support complex tasks political sciences", "many concepts political science complex faceted tend come different linguistic realizations often complex ones many concepts directly identifiable means small set individual lexical items require interpretation", "many researchers political sciences either work qualitatively small amounts data precise results due rather unspecific search well semantically invalid ambigious search words", "hand large amounts eg news texts available longer periods time eg tendencies time derived", "corpora currently working contain ca", "articles british irish german austrian newspapers well unexplored material french", "figure depicts simple example quantitative analysis1 example shows often terms friedensmissionpeace operation auslandseinsatzforeign intervention used last decades newspaper texts interventions wars", "longterm goal project provide similar analysis complex concepts", "example complex concept evocation collective identities political contexts indirect news", "examples collective identities europeans french catholics", "objective work going discuss paper provide nlp methods tools assisting political scientists exploration large data sets view detailed qualitative analysis text instances quantitative overview trends time level corpora", "examples discussed possibly multiple collective identities", "typical context identities tend report communication direct indirect speech", "examples contexts given 1they interpret instancewise die europaer wu rden die lu cke fu llen terested quantitative trends use comparatively simple tools keywordbased search europeans would gap fill corpora text classification basis terms latter approach lead im figure shows screenshot webbased prototype", "proceedings 7th workshop language technology cultural heritage social sciences humanities pages sofia bulgaria august", "qc association computational linguistics figure screenshot webbased system shows simple quantitative analysis frequency terms news articles time", "90s term friedensmission peace operation predominant reverse tendency observed since auslandseinsatz foreign intervention frequently used", "sagte ru", "lated infrastructural standards use said ru", "clarin communit section exemplify europeans would fill gap ru said tool support meant semiautomatic automatic tools propose candidates need validated refused political scientists", "combine chain corpus processing tools classifierbased tools eg topic classifiers commentaryreport classifiers etc make tools interoperable ensure flexible data exchange multiple usage scenarios embed tool collection web service based user interface", "remainder paper structured follows", "section present outline architecture tool collection motivate architecture", "section presents examples implemented modules corpus processing search retrieval instances complex concepts", "show tools intended use methods case studies steps necessary identifying evocation able separate reports comments strategies identifying indirect speech"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999548828142, "p": 0.09523809523809523, "r": 0.18181818181818182}, "rouge-l": {"f": 0.03964456596045945, "p": 0.09090909090909091, "r": 0.03636363636363636}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018670649738658215, "p": 0.1, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.05128204723208447, "p": 0.03571428571428571, "r": 0.09090909090909091}, "rouge-l": {"f": 0.020225458909306583, "p": 0.03571428571428571, "r": 0.01818181818181818}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.018374341951265707, "p": 0.16666666666666666, "r": 0.01818181818181818}}], "abstract": ["develop pipeline consisting various text processing tools designed assist political scientists finding specific complex concepts large amounts text", "main focus interaction political scientists natural language processing groups ensure beneficial assistance political scientists new application challenges nlp", "particular importance find common language different disciplines", "therefore use interactive webinterface easily usable nonexperts", "interfaces active learning algorithm complemented nlp pipeline provide rich feature selection", "political scientists thus enabled use intuitions find custom concepts"]}, "W13-3208": {"introduction": ["section devoted conclusion discussion future work", "", "study follows biemann giesbrecht attempted find list expressions composition ality assumption meaning expression determined meaning constituents combination hold", "results promising appreciated interested wsms compositionality andor relevant evaluation methods", "introduction", "understanding wsm agreement sahlgren word space model computational model word meaning utilizes distributional patterns words collected large text data represent semantic similarity words terms spatial proximity", "many types wsms built different algorithms", "wsms based harris distributional hypothesis harris assumes words similar extent share similar linguistic contexts", "wsm viewed set words associated vectors representing contexts words occur", "similar vectors imply semantic similarity words vice versa", "consequently wsms provide means find words semantically similar given word", "capability wsms exploited many natural language processing nlp applications listed eg turney pantel", "study follows biemann giesbrecht attempted find list non compositional expressions whose meaning fully determined meaning constituents combination", "task turned frustratingly hard johannsen et al", "biemanns idea motivation non compositional expressions could treated single units many nlp applications information retrieval acosta et al machine translation carpuat diab", "extend motivation stating wsms could benefit set noncompositional expressions", "specifically wsms could treat semantically noncompositional expressions single units", "example consider kick bucket hot dog zebra crossing", "treating expressions single units might improve quality wsms since neighboring words expressions related constituents kick bucket dog zebra instead whole expressions", "recent works including lin baldwin et al", "biemann giesbrecht johannsen et al", "reddy et al", "2011a krcmar et al", "krcmar et al", "show applicability wsms determining compositionality word expressions", "proposed methods exploit various types wsms combined various measures determining compositionality applied various datasets", "first leads nondirectly comparable results second many combinations proceedings workshop continuous vector space models compositionality pages sofia bulgaria august", "qc association computational linguistics wsms measures never applied task", "main contribution novelty study lies systematic research several basic advanced wsms combined far best knowledge proposed wsmbased measures determining semantic compositionality", "explored wsms described detail section include vector space model noted log cij denoted sqrt", "purpose local weighting lower importance highly occurring words document", "global function weights every value row value calculated row typically none denoted inverse document frequency denoted df function referred entropy ent", "df calculated logndocsdf ent latent semantic analysis hyperspace analogue pi log pi log ndocs language correlated occurrence analogue lexical semantics random indexing", "measures including substitutability endocentricity compositionality neighborsincommon based described detail section", "section describes experiments performed manually annotated datasets distributional semantics compositionality dataset disco dataset built reddy et al", "2011a"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.06249999548828158, "p": 0.047619047619047616, "r": 0.09090909090909091}, "rouge-l": {"f": 0.04975186104159399, "p": 0.047619047619047616, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper presents comparative study different types word space models wsms combined different compositionality measures applied task automatically determining semantic compositionality word expressions", "many combinations wsms mea"]}, "W13-3209": {"introduction": ["section summarizes results section concludes paper", "distributional models semantics characterize meanings words function words cooccur firth", "models mathematically instantiated sets vectors high dimensional vector spaces applied tasks thesaurus extraction grefenstette curran wordsense discrimination schu tze automated essay marking landauer dumais", "past years research shifted using distributional methods modelling semantics words using modelling semantics larger linguistic units phrases entire sentences", "move word sentence yielded models applied tasks paraphrase detection mitchell lapata mitchell lapata grefenstette sadrzadeh blacoe lapata sentiment analysis socher et al hermann blunsom semantic relation classification ibid", "efforts approach problem modelling phrase meaning vector composition using linear algebraic vector operations mitchell lapata mitchell lapata zanzotto et al matrix tensorbased approaches baroni zamparelli coecke et al grefenstette et al kartsaklis et al use recursive autoencoding socher et al hermann blunsom neuralnetworks socher et al", "noncompositional front erk pado keep word vectors separate using syntactic information sentences disambiguate words context likewise turney treats compositional aspect phrases sentences matter similarity measure composition rather vector composition", "compositional distributional approaches often portray attempts reconcile empirical aspects distributional semantics structured aspects formal semantics", "fact principally coopt syntaxsensitivity formal semantics mostly eschewing logical aspects", "expressing effect logical operations high dimensional distributional semantic models different task boolean logic", "example whereas predicates red seen predicate calculi functions mapping elementsof set mred domain elements compositional distributional mod els give meaning red vectorlike representation devise combination operation noun representations obtain representation adjectivenoun pair", "logical view negation predicate therefore yields new truthfunction mapping elements ofthe complement mred main elements effect negation logical operations distributional models sharp expect representation red remain close objects domain discourse ie colours sufficiently different representation red manner", "exactly textual logic proceedings workshop continuous vector space models compositionality pages sofia bulgaria august", "qc association computational linguistics would best represented continuous vector space model remains open problem", "paper propose possible formulation continuous vector space based representation semantics", "use formulation basis providing account logical operations distributional models", "particular focus case negation might work higher dimensional distributional models", "formulation separates domain value functional representation way allow negation handled naturally", "explain linguistic modelrelated impacts mode representation discuss approach could generalised semantic functions", "section provide overview work relating presented paper covering integration logical elements distributional models integration distributional elements logical models", "section introduce argue tripartite representation distributional semantics discuss issues relating providing linguistically sensible notion negation representations", "section present matrixvector models similar socher et al", "good candidate expressing tripartite representation", "argue elimination nonlinearities models thus show negation cannot adequately captured", "section present short analysis limitation matrix vector models regard task modelling nonboolean logical operations present improved model bypassing limitations section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.07948353059585987, "p": 0.10526315789473684, "r": 0.07142857142857142}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.046511624099513565, "p": 0.03125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.024310911407012647, "p": 0.021739130434782608, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.05263157483379533, "p": 0.037037037037037035, "r": 0.09090909090909091}, "rouge-l": {"f": 0.035066140134551615, "p": 0.034482758620689655, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03851516207748191, "p": 0.1, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.08333332836805586, "p": 0.07692307692307693, "r": 0.09090909090909091}, "rouge-l": {"f": 0.03987906708892196, "p": 0.05555555555555555, "r": 0.03571428571428571}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.24999999503472223, "p": 0.23076923076923078, "r": 0.2727272727272727}, "rouge-l": {"f": 0.11838999544503627, "p": 0.23076923076923078, "r": 0.10714285714285714}}], "abstract": ["increasing empirical success distributional models compositional semantics timely consider types textual logic models capable capturing", "paper address shortcomings ability current models capture logical operations negation", "solution propose tripartite formulation continuous vector space representation semantics subsequently use representation develop formal compositional notion negation models"]}, "W13-3306": {"introduction": ["finally section conclude suggesting future work extend build upon theoretical foundations presented paper", "recently research statistical machine translation smt renewed interest fact variety linguistic phenomena needs information longerrange context", "current statistical translation models decoding algorithms operate sentence andor phrase level considering already translated context previous sentences", "local distance many cases restrictive correctly model lexical cohesion referential expressions noun phrases pronouns discourse markers relate sentences translated", "discourse relations sentences often conveyed explicit discourse connectives dc although since", "dcs play significant role coherence readability text", "likewise wrong connective used translation target text fully incomprehensible conveying meaning established discourse relations source text", "english types explicit connectives annotated penn discourse treebank pdtb see section signaling discourse relations temporality contrast spans text", "depending set relations used relations combinations thereof", "discourse relations present implicitly inferred context without explicit marker present", "although annotation implicit dcs exists well deal explicit dcs paper", "dcs difficult translate mainly english connective signal different discourse relations different contexts target language either different connectives according source relations signaled uses different lexical syntactical constructs place english connective", "paper present mt experiments english en czech cz large amount manually annotated dcs", "corpus parallel prague czechenglish dependency treebank pcedt section directly usable mt experiments entire discourse annotation en paralleled human cz translation", "means build evaluate cz reference translation system learns en gold standard discourse relations", "distortion wrongly labeled connectives given related work section automatic classifiers used label connectives certain error rate", "furthermore use sense labels types en connectives whereas related work focused highly ambiguous connectives especially problematic translation", "paper starts illustrating difficult translations involving connectives section discusses related work section", "resources data used introduced section", "mt experiments explained section proceedings workshop discourse machine translation discomt pages sofia bulgaria august", "qc association computational linguistics automatic evaluation given section", "provide detailed manual evaluation error analysis cz translations generated smt systems section"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07692307204142042, "p": 0.06666666666666667, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02968090741393358, "p": 0.0625, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.029838390382700104, "p": 0.058823529411764705, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.09090908590909119, "p": 0.09090909090909091, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028884519195989302, "p": 0.08333333333333333, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}], "abstract": ["paper presents machine translation experiments english czech large amount manually annotated discourse connectives", "goldstandard discourse relation annotation leads better translation performance ranges ambiguous english connectives helps find correct syntactical constructs czech less ambiguous connectives", "automatic scoring confirms stability newly built discourse aware translation systems", "error analysis human translation evaluation point cases annotation less helpful"]}, "W15-0909": {"introduction": ["future work described section concludes paper", "explicit identification multiword expressions mwes sag et al", "baldwin kim shown useful various nlp applications ramisch recent work shown automatic prediction degree compositionality mwes utility applications including information retrieval ir acosta et al", "machine translation mt weller et al", "carpuat diab venkatapathy joshi", "instance acosta et al", "showed considering noncompositional mwes single unit effectiveness document ranking ir system improves carpuat diab showed adding compositionality scores moses smt system koehn et al could improve translation quality", "paper presents first attempt use mwe compositionality scores evaluation mt system outputs", "basic intuition underlying work sensitise relative reward associated partial mismatches mt outputs reference translations based compositionality", "example mt output white tower rewarded partial overlap ivory tower reference translation tower naturally interpreted compositionally mt output noncompositionally reference translation", "hand partial mismatch traffic signal traffic light rewarded usage traffic highly compositional cases"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0588235250346024, "p": 0.043478260869565216, "r": 0.09090909090909091}, "rouge-l": {"f": 0.0438534068664247, "p": 0.04, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07407406924554216, "p": 0.0625, "r": 0.09090909090909091}, "rouge-l": {"f": 0.056321314467967, "p": 0.0625, "r": 0.05263157894736842}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper present first attempt integrate predicted compositionality scores multiword expressions automatic machine translation evaluation integrating compositionality scores english noun compounds tesla machine translation evaluation metric", "attempt marginally successful speculate whether largerscale attempt likely greater impact"]}, "W96-0108": {"introduction": ["ask question better judge quality translations means automatically estimating relative compositionality mwes focusing compound nouns tesla machine translation metric liu et al", "word errors present problems various text speechbased applications optical char acter recognition ocr voiceinput computer interfaces", "particular though current ocr technology quite refined robust sources old books poorquality nthgeneration photocopies faxes still difficult process cause many ocr errors", "ocr truly useful wide range applications office automation information retrieval systems ocr reliability must improved", "method automatic correction ocr errors would clearly beneficial", "essentially types word errors nonword errors realword errors", "non word error occurs word source text interpreted ocr string correspond valid word given word list dictionary", "realword error occurs sourcetext word interpreted string actually occur dictionary identical sourcetext word", "example source text john found man rendered john fomd man ocr device fomd nonword error realword error", "general nonword errors never correspond dictionary entries include wildly incorrect strings well misrecognized alphanumeric sequences bn234 8n234", "nonword errors might become real word errors size word list dictionary increases", "example word ruel1 might count nonword error sourcetext word rut small dictionary used reference count realword error unabridged dictionary used", "nonword errors might corrected without considering context error occurs realword error corrected taking context account", "problems worderror detection correction studied several decades", "good survey area found kukich", "traditional wordcorrection techniques concentrate nonword error correction consider context error appears", "recently statistical language models slms featurebased methods used contextsensitive spellingerror correction", "example atwell elliittm used partofspeech pos tagging method detect realword errors text", "mays colleagues exploited word trigrams detect correct nonword realword errors artificially generated sentences", "church gale used bayesian classifier method improve performance nonword error correction", "golding applied hybrid bayesian method realword error correction golding schabes combined pos trigram bayesian methods purpose", "goal work described investigate effectiveness efficiency slm based methods applied problem ocr error correction", "since posbased methods effective distinguishing candidates pos tags since methods based wordtrigram models involve extensive training data require huge wordtrigram tables available run time used wordbigram slm first step investigation", "paper describe system uses wordbigram slm technique correct ocr errors", "system takes advantage information multiple sources including letter grams character confusion probabilities word bigram probabilities effect contextbased word error correction", "correct nonword well realword errors"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.07142856665816359, "p": 0.058823529411764705, "r": 0.09090909090909091}, "rouge-l": {"f": 0.014261417003088998, "p": 0.058823529411764705, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013758473770508641, "p": 0.2, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0799999950720003, "p": 0.07142857142857142, "r": 0.09090909090909091}, "rouge-l": {"f": 0.01410298625953104, "p": 0.07142857142857142, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.12499999570312517, "p": 0.2, "r": 0.09090909090909091}, "rouge-l": {"f": 0.013758473770508641, "p": 0.2, "r": 0.0136986301369863}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}], "abstract": ["paper describes automatic contextsensitive worderror correction system based statistical language modeling slm applied optical character recognition ocr post processing", "system exploits information multiple sources including letter ngrams character confusion probabilities wordbigram probabilities", "letter ngrams used index words lexicon", "given sentence corrected system decomposes string sentence letter ngrams retrieves word candidates lexicon comparing string ngrams lexiconentry ngrams", "retrieved candidates ranked conditional probability matches string given character confusion probabilities", "finally wordbigram model viterbi algorithm used determine best scoring word sequence sentence", "system correct nonword errors well realword errors achieves error reduction rate real ocr text", "addition system learn character confusion probabilities specific ocr environment use selfcalibration achieve better performance"]}, "W98-1234": {"introduction": ["addition system learn character confusion probability table specific ocr environment use achieve better performance", "alan turing brilliant british mathematician played important role development partofspeech verbs nouns adjectives adverbs", "sets divided semanticals categories eg synonymous nouns", "wordnet completely described url httpwwwspeechcscmuedulcompspeechsectionl lexicavwoidnethtml", "", "architecture", "mimic parts human thought created different principal modules spelling correction disambiguation words generation comments simulating human typing", "computers developed test would serve indicator intelligence machines", "lot researchers posed loebner prize first formal instantiation turing test", "participate competition conceived program attempts simulate responses human", "well begin describe wordnet includes classification english words", "afterwards well present architecture system programming moment", "section well briefly explain every module", "next well give gecnomemraetinotnof", "spelling correction", "", "example interaction program human", "section well show different processes generating response input user"], "introduction_label": [{"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.09523809024943337, "p": 0.1, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028439780845337992, "p": 0.1, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.1052631530193908, "p": 0.125, "r": 0.09090909090909091}, "rouge-l": {"f": 0.028007426952079435, "p": 0.125, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.11111110635802489, "p": 0.14285714285714285, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027806102439457355, "p": 0.14285714285714285, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.11764705425605555, "p": 0.16666666666666666, "r": 0.09090909090909091}, "rouge-l": {"f": 0.027619965008197564, "p": 0.16666666666666666, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.13333332942222234, "p": 0.25, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02730839757874066, "p": 0.25, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0}, "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}, {"rouge-1": {"f": 0.09999999505000023, "p": 0.1111111111111111, "r": 0.09090909090909091}, "rouge-l": {"f": 0.02821999922158948, "p": 0.1111111111111111, "r": 0.02702702702702703}}, {"rouge-1": {"f": 0.8695652124007562, "p": 0.8333333333333334, "r": 0.9090909090909091}, "rouge-l": {"f": 0.28884519195901404, "p": 0.8333333333333334, "r": 0.2702702702702703}}], "abstract": ["paper describes differents methods tricks connection program entered loebner prize competition happen sunday january powerhouse museum sydney", "course isnt exhaustive possible techniques aim give main ideas", "well speak main modules program spelling correction different uses wordnet generation comments", "module used spelling correction developed basis works brill brill marcus golding golding schabes powers"]}}