{"A00-1020": {"title": ["Multilingual Coreference Resolution"], "abstract": ["In this paper we present a new multi lingual datadriven method for coreference resolution as implemented in the SWIZZLE system", "The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts outperformed coreference resolution in each of the indi vidual languages"], "introduction": ["The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing", "Most of the research has focused on bilingual terminology identification either as par allel multiwords forms eg the Champollion sys tem Smadja et al1996 technical terminology eg the Termight system Dagan and Church 1994 or broadcoverage translation lexicons eg the SABLE system Resnik and Melamed 1997", "In addition the Multilingual Entity Task MET from the TIP STER program1 httpwwwnlpirnistgovjrelated projectsjtipsterjmethtm challenged the partici pants in the Message Understanding Conference MUC to extract named entities across several for eign language corpora such as Chinese Japanese and Spanish", "In this paper we present a new application of aligned multilingual texts", "Since coreference reso lution is a pervasive discourse phenomenon causing performance impediments in current IE systems we considered a corpus of aligned English and Roma nian texts to identify coreferring expressions", "Our task focused on the same kind of coreference as considered in the past MUC competitions namely 1The TIPSTER Text Program was a DARPAled government effort to advance the state of the art in text processing technologies", "Steven J Maiorano IPO Washington DC 20505 maiorano caiscom the identity coreference", "Identity coreference links nouns pronouns and noun phrases including proper names to their corresponding antecedents", "We created our bilingual collection by translating the MUC6 and MUC7 coreference training texts into Romanian using native speakers", "The train ing data set for Romanian coreference used wher ever possible the same coreference identifiers as the English data and incorporated additional tags as needed", "Our claim is that by adding the wealth of coreferential features provided by multilingual data new powerful heuristics for coreference resolu tion can be developed that outperform monolingual coreference resolution systems", "For both languages we resolved coreference by using SWIZZLE our implementation of a bilingual coreference resolver", "SWIZZLE is a multilingual en hancement of COCKTAIL Harabagiu and Maiorano 1999 a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information2  When COCKTAIL was applied separately on the English and the Ro manian texts coreferring links were identified for each English and Romanian document respectively", "When aligned referential expressions corefer with nonaligned anaphors SWIZZLE derived new heuris tics for coreference", "Our experiments show that SWIZZLE outperformed COCKTAIL on both English and Romanian test documents", "The rest of the paper is organized as follows", "Sec tion 2 presents COCKTAIL a monolingual coreference resolution system used separately on both the En glish and Romanian texts", "Section 3 details the datadriven approach used in SWIZZLE and presents some of its resources", "Section 4 reports and discusses the experimental results", "Section 5 summarizes the 2 The name of COCKTAIL is a pun on CogNIAC be cause COCKTAIL combines a larger number of heuristics than those reported in Baldwin 1997", "SWIZZLE more over adds new heuristics discovered from the bilingual aligned corpus", "conclusions"]}, "A00-1024": {"title": ["Categorizing Unknown Words Using Decision Trees to Identify"], "abstract": ["This paper introduces a system for categorizing un known words", "The system is based on a multi component architecture where each component is re sponsible for identifying one class of unknown words", "The focus of this paper is the components that iden tify names and spelling errors", "Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word", "The sys tem is evaluated using data from live closed captions  a genre replete with a wide variety of unknown words"], "introduction": ["In any real world use a Natural Language Process ing NLP system will encounter words that are not in its lexicon what we term unknown words", "Un known words are problematic because a NLP system will perform well only if it recognizes the words that it is meant to analyze or translate the more words a system does not recognize the more the systems per formance will degrade", "Even when unknown words are infrequent they can have a disproportionate ef fect on system quality", "For example Min 1996 found that while only 06 of words in 300 emails were misspelled this meant that 12 of the sen tences contained an error discussed in Min and Wilson 1998", "Words may be unknown for many reasons the word may be a proper name a misspelling an ab breviation a number a morphological variant of a known word eg receared  or missing from the dictionary", "The first step in dealing with unknown words is to identify the class of the unknown word whether it is a misspelling a proper name an ab breviation etc Once this is known the proper ac tion can be taken misspellings can be corrected ab breviations can be expanded and so on as deemed necessary by the particular text processing applica tion", "In this paper we introduce a system for cat egorizing unknown words", "The system is based on a multi component architecture where each compo nent is responsible for identifying one category of unknown words", "The main focus of this paper is the components that identify names and spelling errors", "Both components use a decision tree architecture to combine multiple types of evidence about the un known word", "Results from the two components are combined using a weighted voting procedure", "The system is evaluated using data from live closed cap tions  a genre replete with a wide variety of un known words", "This paper is organized as follows", "In section 2 we outline the overall architecture of the unknown word categorizer", "The name identifier and the mis spelling identifier are introduced in section 3", "Perfor mance and evaluation issues are discussed in section 4", "Section 5 considers portability issues", "Section 6", "compares the current system with relevant preced ing research", "Concluding comments can be found in section 6"]}, "A00-2019": {"title": ["An Unsupervised Method for Detecting Grammatical Errors"], "abstract": ["We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora", "The system was developed and tested using essaylength responses to prompts on the Test of English as a Foreign Language TOEFL", "The error recognition system ALEK performs with about 80 precision and 20 recall"], "introduction": ["A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence Miller and Gildea 1987", "Much information about usage can be obtained from quite a limited context Choueka and Lusignan 1985 found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it", "Statisticallybased computer programs have been able to do the same with a high level of accuracy Kilgarriff and Palmer 2000", "The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word", "We have developed a statistical system ALEK Assessing Lexical Knowledge that uses statistical analysis for this purpose", "A major objective of this research is to avoid the laborious and costly process of collecting errors or negative evidence for each word that we wish to evaluate", "Instead we train ALEK on a general corpus of English and on edited text containing example uses of the target word", "The system identifies inappropriate usage based on differences between the words local context cues in an essay and the models of context it has derived from the corpora of wellformed sentences", "Claudia Leacock Educational Testing Service Rosedale Road Princeton NJ cleacocketsorg A requirement for ALEK has been that all steps in the process be automated beyond choosing the words to be tested and assessing the results", "Once a target word is chosen preprocessing building a model ofthe words appropriate usage and identifying usage errors in essays is performed without manual intervention", "ALEK has been developed using the Test of English as a Foreign Language TOEFL administered by the Educational Testing Service", "TOEFL is taken by foreign students who are applying to US undergraduate and graduatelevel programs", "1 Background", "Approaches to detecting errors by nonnative writers typically produce grammars that look for specific expected error types Schneider and McCoy 1998 Park Palmer and Washburn 1997", "Under this approach essays written by ESL students are collected and examined for errors", "Parsers are then adapted to identify those error types that were found in the essay collection", "We take a different approach initially viewing error detection as an extension of the word sense disambiguation WSD problem", "Corpusbased WSD systems identify the intended sense of a polysemous word by 1 collecting a set of example sentences for each of its various senses and 2 extracting salient contextual cues from these sets to 3 build a statistical model for each sense", "They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model eg Leacock Chodorow and Miller 1998 Yarowsky 1993", "Golding 1995 showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and theyre", "He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context", "However most grammatical errors are not the result of simple word confusions", "This complicates the task of building a model of incorrect usage", "One approach we considered was to proceed without such a model represent appropriate word usage across senses in a single model and compare a novel example to that model", "The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck", "All occurrences ofthe word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage", "Inappropriate usage would be signaled by contextual cues that do not occur in training", "Unfortunately this approach was not effective for error detection", "An example of a word usage error is often very similar to the model of appropriate usage", "An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element", "The problem of error detection does not entail finding similarities to appropriate usage rather it requires identifYing one element among the contextual cues that simply does not fit"]}, "A00-2034": {"title": ["Using  Semantic Preferences to  Identify Verbal  Participation  in"], "abstract": ["We propose a method for identifying diathesis alter nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms", "The method uses selectional pref erences acquired as probability distributions over WordNet", "Preferences for the target slots are com pared using a measure of distributional similarity", "The method is evaluated on the causative and cona tive alternations but is generally applicable and does not require a priori knowledge specific to the alternation"], "introduction": ["Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically", "The syntactic changes are sometimes accompanied by slight changes in the meaning of the verb", "An ex ample of the causative alternation is given in 1 be low", "In this alternation the object of the transitive variant can also appear as the subject of the intransi tive variant", "In the conative alternation the transi tive form alternates with a prepositional phrase con struction involving either at or on", "An example of the conative alternation is given in 2", "1", "The boy broke the window ", "The window", "broke"]}, "A97-1025": {"title": [""], "abstract": ["Contextual spelling errors are defined as the use of an incorrect though valid word in a particular sentence or context", "Tra ditional spelling checkers flag misspelled words but they do not typically attempt to identify words that are used incorrectly in a sentence", "We explore the use of Latent Se mantic Analysis for correcting these incor rectly used words and the results are com pared to earlier work based on a Bayesian classifier"], "introduction": ["Spelling checkers are now available for all major word processing systems", "However these spelling checkers only catch errors that result in misspelled words", "If an error results in a different but incor rect word it will go undetected", "For example quite may easily be mistyped as quiet", "Another type of er ror occurs when a writer simply doesnt know which word of a set of homophones 1 or near homophones is the proper one for a particular context", "For ex ample the usage of affect and effect is commonly confused", "Though the cause is different for the two types of errors we can treat them similarly by examining the contexts in which they appear", "Consequently no effort is made to distinguish between the two er ror types and both are called contextual spelling er rors", "Kukich 1992a 1992b reports that 40 to 45 of observed spelling errors are contextual er rors", "Sets of words which are frequently misused or mistyped for one another are identified as confusion sets", "Thus from our earlier examples quiet quite and affect effect are two separate confusion sets", "In this paper we introduce Latent Semantic Anal ysis LSA as a method for correcting contextual spelling errors for a given collection of confusion sets", "1Homophones are words that sound the same but are spelled differently", "LSA was originally developed as a model for infor mation retrieval Dumais et al 1988 Deerwester et al 1990 but it has proven useful in other tasks too", "Some examples include an expert Expert lo cator Streeter and Lochbaum 1988 and a confer ence proceedings indexer Foltz 1995 which per forms better than a simple keywordbased index", "Recently LSA has been proposed as a theory of se mantic learning Landauer and Dumais In press", "Our motivation in using LSA was to test its effec tiveness at predicting words based on a given sen tence and to compare it to a Bayesian classifier", "LSA makes predictions by building a highdimensional semantic space which is used to compare the sim ilarity of the words from a confusion set to a given context", "The experimental results from LSA predic tion are then compared to both a baseline predic tor and a hybrid predictor based on trigrams and a Bayesian classifier"]}, "C00-1081": {"title": ["A Stochastic Parser Based on a Structural Word Prediction Model"], "abstract": ["In this paper we present a stochastic language model using dependency", "This model considers a sentence as a word sequence and predicts each wordfrom left to right", "The history at each step of prediction is a sequence of partial parse trees covering the preceding words", "First our model predicts the partial parse trees which have a dependency relation with the next word among them and then predictsthe next word from only the trees which have a dependency relation with the next word", "Our model is a generative stochastic model thus this can be used not only as a parser but also as a language modelof a speech recognizer", "In our experiment we prepared about 1000 syntactically annotated Japanese sentences extracted from a nancial newspaper and estimated the parameters of our model", "We built aparser based on our model and tested it on approximately 100 sentences of the same newspaper", "The accuracy of the dependency relation was 899 thehighest accuracy level obtained by Japanese stochastic parsers"], "introduction": ["The stochastic language modeling imported from the speech recognition area is one of the successful methodologies of natural language processing", "In fact all language models for speech recognition are as far as we know based on an ngram model and most practical partofspeech POS taggers are alsobased on a word or POS ngram model or its exten sion Church 1988 Cutting et al 1992 Merialdo1994 Dermatas and Kokkinakis 1995", "POS tagging is the rst step of natural language process ing and stochastic taggers have solved this problem with satisfying accuracy for many applications", "The next step is parsing or that is to say discovering the structure of a given sentence", "Recently many parsers based on the stochastic approach have been proposed", "Although their reported accuracies arehigh they are not accurate enough for many appli cations at this stage and more attempts have to be made to improve them further", "One of the major applications of a parser is toparse the spoken text recognized by a speech rec ognizer", "This attempt is clearly aiming at spokenlanguage understanding", "If we consider how to com bine a parser and a speech recognizer it is better if the parser is based on a generative stochastic modelas required for the language model of a speech rec ognizer", "Here generative means that the sum of probabilities over all possible sentences is equal to or less than 1", "If the language model is generative it allows a seamless combination of the parser and the speech recognizer", "This means that the speech recognizer has the stochastic parser as its languagemodel and benets richer information than a normal ngram model", "Even though such a combination is not possible in practices  the recognizer out puts N best sentences with their probabilities and the parser taking them as input parses all of them and outputs the sentence with its parse tree thathas the highest probability of all possible combina tions", "As a result a parser based on a generativestochastic language model may help a speech rec ognizer to select the most syntactically reasonable sentence among candidates", "Therefore it is better if the language model of a parser is generativeIn this paper taking Japanese as the object lan guage we propose a generative stochastic language model and a parser based on it", "This model treats a sentence as a word sequence and predicts each wordfrom left to right", "The history at each step of predic tion is a sequence of partial parse trees covering the preceding words", "To predict a word our model rst predicts which of the partial parse trees at this stage have dependency relation with the word and then predicts the word from the selected partial parsetrees", "In Japanese each word depends on a subse quent word that is to say each dependency relationis left to right it is not necessary to predict the di rection of each dependency relation", "So in order to extend our model to other languages the model may have to predict the direction of each dependency", "Webuilt a parser based on this model whose parameters are estimated from 1072 sentences in a nan cial newspaper and tested it on 119 sentences fromthe same newspaper", "The accuracy of the depen dency relation was 899 the highest obtained by any Japanese stochastic parsers"]}, "C00-2118": {"title": ["Autotnatic Lexical Acquisition  Based on  Statistical Distributions"], "abstract": ["We automatically classify verbs into lexical se rnantic classes based on distributions of indiet tors or verb alternations extracted from a very large annotated corpus", "Ve address a problem which is particularly difficult because the verb classes although semantically diflerr nt how sim ilar surface syntactic behavior", "Five prantmatical feattJres arc suflicient to reduce error rate by more thw 50 over dance we achieve almost 70 accuracy in a task whose baseline performance is 11 and whosexpertbased upptr bound we cal culated at 8G51L vVe conclude that corpudriven extraction of grannuatical featurcs is a promiing methodology for fin grained verb classification"], "introduction": ["Detailcd information about verbs is critical to a broad range of NLI and IR tasks yet its Jnau ual determination for large numbers of verbs is difficult and resourc intensive", "Hmearch 011 tit automatic acquisition of verbbas d krtowledg has succeded in pleaning syntactic proprtis of v rbs such as subcategorization frames from on line resources lhmt 1993 Briscoe and Carroll 1997 Dorr 1997 Manning 1993", "Recently researchers have investigated statistical corpus based methods for lexical semantic classification from syntactic properties of verb usage one and Ivlclcc 199G Lapata aud Brew 1999 Schulte i111 Walde 199H Stevenson and Ivlerlo 1999 Steven son ct al 1999 McCarthy 2000", "Corpusbased approaches to lexical semantic classification in particular have drawn on Levins hypothesis Levin 1993 that verbs can be classi fied according to the diathesis alternations alter nations in he syntactic expressions or arguments in which they participatefor example whether a  fhis research was partly sponsored hy US N W prauts070211 aml  li5J8J22 Swiss NSF fellowship 8210 1650 Information Sciences Cmmcil of Hutgers University and HCS U  of Pennsylvania", "This research was cml clncted while the first author was at Hutgers University", "Paola Merlo LATL  Department of Linguistics University of Geneva", "2 rue de Caudollc 1211 Gcncvc tlSuisse", "merlolettresunigech verb occurs in the dativeprepositional phrase al ternation in English", "One diagnostic for diathesis alternations is the subcategorization alternatives of a verb", "However some classes exhibit the same subcaegorization possibilities but difler in their argument structures ie the content of the the tnatic roles assigned to the arguments of the verb", "This type of situation constitutes a particularly difficult case for corpusbased classiltcation meth ods", "Jn this paper we apply corpusbased lexica", "acquisition tucthodology to distinguish classes of verbs which allow the sante su bcatcgorizations but difrer in thematic roles", "Ve first assume that one can atttontatically restrict the choice of classes to those that participate in the relevant su bcaic gorizations d", "Lapata", "and Brew ID99", "Our proposal is then to use statistics over diathesis alternants as a way to furthcr distinguish those verbs w h iclt allow tlw sante su bcat gorizations achivinp fin graincd classification within that St", "Our work focuses 011 dekrtnining the bestS  lllantic class for a verb lypr the set of usages of a verb across a document or corpusrather than for a single verb lokrn in a single local context", "In this way we can exploit the broad behavior of the verb across the corpus to determine its most likely class overall", "VVe investigate the proposed approach in an in depth case study of the three major classes of op tionally intransitive verbs in English utwrgative unaccusativc and objectdrop", "Nlore specifically according to Levins classification  lA vin 1993 the unergatives arc manner of motion verbs such as iump and march the unaccusatives arc verbs of change of state such as open and explode the objectdrop verbs are unexpressed object alterna tion verbs such as played and painlcd", "These classes all support both transitive and intransi tive subcategorizations but arc distinguished by the pattern of thematic role assignments to sub jed and object position", "Ve au tom atically clas sify these verbs on the basis of statistical ap proximations to syntactic indicators of the under lying argument structures using numerical fea tures collected from a large syntactically anno tated tagged or parsed corpus", "We apply ma chine learning techniques to determine whether the frequency distributions of the features in dividually or in combination support automatic classification of the verbs", "To preview our re sults we demonstrate that combining only fivc numerical indicators is sufficient to reduce the er ror rate in this classification task by more than 50 over chance", "Specifically we achieve almost 70 accuracy in a task whose baseline chance performance is 31 and whose expertbased up per bound is calculated at 865", "We conclude that a distributionbased method for lexical se mantic verb classification is a promising avenue of research"]}, "C00-2148": {"title": ["Integrating compositional sentantics into a verb lexicon"], "abstract": ["We present a classbased approach to building a verb lexicon that makes explicit the close asso ciation between syntax and semantics for Levin classes", "We have used Lexicalized Tree Adjoin ing Grammars to capture the syntax associated with each verb class and have augmented the trees to in clude sclcctional restrictions", "In addition semantic predicates arc associated with each tree which al low for a compositional interpretation"], "introduction": ["The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within wellclefinccl subdomains", "Despite many different lexicon clcvclopmcnt ap proaches Melcuk 1988 Copestakc and Sanfil ippo 1993 Lowe et al 1997 the field has yet to develop a clear consensus on guidelines for a computational lexicon", "One of the most controver sial areas in building such a lexicon is polysemy how senses can be computationally distinguished and characterized", "We address this problem by em ploying compositional semantics and the adjunction of syntactic phrases to support regular verb sense extensions", "This differs from the Lexical Concep tual Structure LCS approach exemplified by Voss 1996 which requires a separate LCS representa tion for each possible sense extension", "In this pa per we describe the construction of VcrbNet a verb lexicon with explicitly stated syntactic and seman tic information for individual lexical items using Levin verb classes Levin 1993 to systematically construct lexical entries", "We use Lexicalizecl Tree Adjoining Grammar LTAG Joshi 1987 Schabes 1990 to capture the syntax for each verb class and associate semantic predicates with each tree", "Although similar ideas have been explored for verb sense extension Pustejovsky 1995 Goldberg 1995 our approach of applying LTAG to the prob lem of composing and extending verb senses is novel", "LTAGs have an extended domain of local ity that captures the arguments of a verb in a local manner", "The association of semantic predicates to a tree yields a complete semantics for the verb", "More over the operation of adjunction in LTAGs provides a mechanism for extending verb senses"]}, "C02-1027": {"title": ["Shallow language processing architecture  for Bulgarian"], "abstract": ["This paper describes LINGUA  an architecture for text processing in Bulgarian", "First the preprocessing modules for tokenisation sentence splitting paragraph segmentation part ofspeech tagging clause chunking and noun phrase extraction are outlined", "Next the paper proceeds to describe in more detail the anaphora resolution module", "Evaluation results are reported for each processing task"], "introduction": ["The state of the art of todays full parsing and knowledgebased automatic analysis still falls short of providing a reliable processing framework for robust realworld applications such as automatic abstracting or information extraction", "The problem is especially acute for languages which do not benefit from a wide range of processing programs such as Bulgarian", "There have been various projects which address different aspects of the automatic analysis in Bulgarian such as morphological analysis Krushkov 1997 Simov et al 1992 morphological disambiguation Simov et al 1992 and parsing Avgustinova et al 1989 but no previous work has pursued the development of a knowledgepoor robust processing environment with a high level of component integrity", "This paper reports the development and implementation of a robust architecture for language processing in Bulgarian referred to as LINGUA which includes modules for POS tagging sentence splitting clause segmentation parsing and anaphora resolution", "Our text processing framework builds on the basis of considerably shallower linguistic analysis of the input thus trading off depth of interpretation for breadth of coverage and workable robust solution", "LINGUA uses knowledge poor heuristi cally based algorithms for language analysis in this way getting round the lack of resources for Bulgarian"]}, "C02-1033": {"title": [""], "abstract": ["We present in this paper a method for achieving in an integrated way two tasks of topic analysis segmentation and link detection", "This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches", "We report an evaluation of our method for segmentation on two corpora one in French and one in English and we propose an evaluation measure that specifically suits that kind of systems"], "introduction": ["Topic analysis which aims at identifying the topics of a text delimiting their extend and finding the relations between the resulting segments has recently raised an important interest", "The largest part of it was dedicated to topic segmentation also called linear text segmentation and to the TDT Topic Detection and Tracking initiative Fiscus et al 1999 which addresses all the tasks we have mentioned but from a domaindependent viewpoint and not necessarily in an integrated way", "Systems that implement this work can be categorized according to what kind of knowledge they use", "Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts word distribution as in Hearst 1997 Choi 2000 and Utiyama and Isahara 2001 or linguistic cues as in Passonneau and Litman 1997", "They can be applied without restriction about domains but have low results when a text doesnt characterize its topical structure by surface clues", "Some systems exploit domainindependent knowl edge about lexical cohesion a network of words built from a dictionary in Kozima 1993 a large set of collocations collected from a corpus in Fer ret 1998 Kaufmann 1999 and Choi 2001", "To some extend this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains", "The last main type of systems relies on knowledge about the topics they may encounter in the texts they process", "This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts", "The work of Bigi Bigi et al 1998 stands in the same perspective but focuses on much larger topics than TDT", "These systems have a limited scope due to their topic representations but they are also more precise for the same reason", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination Jobbins and Evett 1998 combined word recurrence collocations and a thesaurus Beeferman et al 1999 relied on both collocations and linguistic cues", "The topic analysis we propose implements such a hybrid approach it relies on a general language resource a collocation network but exploits it together with word recurrence in texts", "Moreover it simultaneously achieves topic segmentation and link detection ie determining whether two segments discuss the same topic", "We detail in this paper the implementation of this analysis by the TOPICOLL system we reportevaluations of its capabilities concerning segmen tation for two languages French and English and finally we propose an evaluation measure that integrates both segmentation and link detection"]}, "C02-1039": {"title": [""], "abstract": ["We describe an algorithm for Word Sense Disambiguation WSD that relies on a lazy learner improved with automatic feature selection", "The algorithm was implemented in a system that achieves excellent performance on the set ofdata released during the SENSEVAL2 competition", "We present the results obtained and discuss the performance of various features in the context of supervised learning algorithms for WSD"], "introduction": ["The task of Word Sense Disambiguation consists in assigning the most appropriate meaning to a polysemous word within a given con text", "A large range of applications includingmachine translation knowledge acquisition in formation retrieval information extraction andothers require knowledge about word mean ings and therefore WSD algorithms represent anecessary step in all these applications", "Start ing with SENSEVAL1 in 1999 WSD has received growing attention from the Natural LanguageProcessing community and motivates a continuously increasing number of researchers to de velop WSD systems and devote time for finding solutions to this challenging problem", "The SENSEVAL1 competitions provided a goodenvironment for the development of super vised WSD systems making freely available large amounts of sense tagged data", "During SENSEVAL1 in 1999 data for 35 words wasmade available adding up to about 20000 examples tagged with respect to the Hector dic tionary", "The size of the tagged corpus increasedwith SENSEVAL2 in 2001 when 13000 additional examples were released for 73 polyse lhttpwwwitribtonacukeventssensevalmous words", "This time the semantic annota tions were performed with respect to WordNet", "The experiments and results reported in thispaper pertain to the SENSEVAL2 data", "How ever similar experiments were performed on the SENSEVAL1 data with comparable resultsMost of the efforts in the WSD field were concentrated so far towards supervised learning algorithms and these are the methods that usu ally achieve the best performance at the cost of low recall", "Each sense tagged occurrence of a particular word is transformed into a featurevector suitable for an automatic learning pro cess", "Two main decisions need to be made in the design of such a system the set of features to be used and the learning algorithm", "Commonly used features include surrounding words and their part of speechBruce and Wiebe 1999context keywords Ng and Lee 1996 or context bigrams Pedersen 2001 various syntac tic properties Fellbaum et al 2001 etc As for the learning methodology a large range ofalgorithms have been employed including neu ral networks Leacock et al 1998 decision trees Pedersen 2001 decision lists Yarowsky 2000 memory based learning Veenstra et al2000 and others", "An experimental comparison of seven learning algorithms used to disambiguate the meaning of the word line is pre sented in Mooney 1996", "We investigate in this paper the use of a lazy learner namely instance based learning to solve the semantic ambiguity of words in contextThe main advantage of instance based learn ers is the fact that they consider every single training example when making a classification decision", "This characteristic proves particularly useful for NLP problems where training datais usually expensive and exceptions are impor tant", "On the other side lazy learners including instance based learners have the disadvantage of being easily misled by irrelevant features", "Inthe algorithm described in this paper this draw back is solved by improving the learner with a scheme for automatic feature selection", "The methodology presented here is integralpart of a larger system that has the capabil ity of performing both supervised and opentext WSD Mihalcea 2002", "For reasons of clarity and space we focus in this paper only on the description of the supervised component", "To our knowledge instance based learning with per word automatic feature selection is a new approach in the WSD field and we show that it leads to very good results", "Previous work has considered the application of instance based learning with automatic feature selection for the problem of pronoun resolution Cardie 1996", "In WSD the work that is closest to ours was reported by Bruce and Wiebe 1999 where decomposable probabilistic models are used in combination with eager Naive Bayes algorithms"]}, "C04-1074": {"title": ["Optimizing Algorithms for Pronoun Resolution"], "abstract": ["The paper aims at a deeper understanding of several wellknown algorithms and proposes ways to optimize them", "It describes and discusses factors and strategies of factor interaction used in the algorithms", "The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information Negra Skut et al 1997", "A common format for pronoun resolution algorithms with several open parameters is proposed and the parameter settings optimal on the evaluation data are given"], "introduction": ["In recent years a variety of approaches to pronoun resolution have been proposed", "Some of them are based on centering theory Strube 1998 Strube and Hahn 1999 Tetreault 2001 others on Machine Learning Aone and Bennett 1995 Ge et al 1998 Soon et al 2001 Ng and Cardie 2002 Yang et al 2003", "They supplement older heuristic approaches Hobbs 1978 Lappin and Leass 1994", "Unfortunately most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible", "Appreciation of the new insights is quite hard", "Evaluation differs not only with regard to size and genre of corpora but also along the following lines", "Scope of application Some approaches only deal with personal and possessive pronouns centering and heuristic while others consider coreference links in general Soon et al 2001 Ng and Cardie 2002 Yang et al 2003", "A drawback of this latter view is that it mixes problems on different lev els of difficulty", "It remains unclear how much of the success is due to the virtues of the approach and how much is due to the distribution of hard and easy problems in the corpus", "In this paper we will only deal with coreferential pronouns ie possessive demonstrative and third person pronouns", "My thanks go to Melvin Wurster for help in annotation and to Ciprian Gerstenberger for discussion", "Quality of linguistic input Some proposals were evaluated on hand annotated Strube and Hahn 1999 or tree bank input Ge et al 1998 Tetreault 2001", "Other proposals provide a more realistic picture in that they work as a backend to a parser Lappin and Leass 1994 or noun chunker Mitkov 1998 Soon et al 2001 Ng and Cardie 2002", "In evaluation of applications presupposing parsing it is helpful to separate errors due to parsing from intrinsic errors", "On the other hand one would also like to gauge the endtoend performance of a system", "Thus we will provide performance figures for both ideal handannotated input and realistic automatically generated input", "Language Most approaches were evaluated on English where large resources are available both in terms of preannotated data MUC6 and MUC7 data and lexical information WordNet", "This paper deals with German", "Arguably the free wordorder of German arguably leads to a clearer distinction between grammatical function surface order and information status Strube and Hahn 1999", "The paper is organized as follows", "Section 2 describes the evaluation corpus", "Section 3 describes several factors relevant to pronoun resolution", "It assesses these factors against the corpus measuring their precision and restrictiveness", "Section 4 describes and evaluates six algorithms on the basis of these factors", "It also captures the algorithms as para metric systems and proposes parameter settings optimal on the evaluation data", "Section 5 concludes"]}, "C04-1075": {"title": ["A HighPerformance Coreference Resolution System"], "abstract": ["This paper presents a constraintbased multi agent strategy to coreference resolution of general noun phrases in unrestricted English text", "For a given anaphor and all the preceding referring expressions as the antecedent candidates a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge", "Then according to the type of the anaphor a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge", "Finally a simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates based on the proximity principle", "One interesting observation is that the most recent antecedent of an anaphor in the coreferential chain is sometimes indirectly linked to the anaphor via some other antecedents in the chain", "In this case we find that the most recent antecedent always contains little information to directly determine the coreference relationship with the anaphor", "Therefore for a given anaphor the corresponding special constraint agent can always safely filter out these less informative antecedent candidates", "In this way rather than finding the most recent antecedent for an anaphor our system tries to find the most direct and informative antecedent", "Evaluation shows that our system achieves Precision  Recall  Fmeasures of 847 658  739 and 828  557  665 on MUC6 and MUC7 English coreference tasks respectively", "This means that our system achieves significantly better precision rates by about 8 percent over the bestreported systems while keeping recall rates"], "introduction": ["Coreference accounts for cohesion in texts", "Especially a coreference denotes an identity of reference and holds between two expressions which can be named entities definite noun phrases pronouns and so on", "Coreference resolution is the process of determining whether two referring expressions refer to the same entity in the world", "The ability to link referring expressions both within and across the sentence is critical to discourse and language understanding in general", "For example coreference resolution is a key task in natural language interfaces machine translation text summarization information extraction and question answering", "In particular information extraction systems like those built in the DARPA Message Understanding Conferences MUC have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC6 1995 and MUC7 1998", "There is a long tradition of work on coreference resolution within computational linguistics", "Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge Carter 1987 Rich and LuperFoy 1988 Carbonell and Brown 1988", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledgepoor strategies Dagan and Itai 1990 Lappin and Leass 1994 Mitkov 1998 Soon Ng and Lim 2001 Ng and Cardie 2002 which was further motivated by the emergence of cheaper and more reliable corpus based NLP tools such as partofspeech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", "Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements ccommand constraints semantic consistency syntactic parallelism semantic parallelism salience proximity etc These factors can be either constraints which discard invalid ones from the set of possible candidates such as gender and number agreements ccommand constraints semantic consistency or preferences which gives more preference to certain candidates and less to others such as syntactic parallelism semantic parallelism salience proximity", "While a number of approaches use a similar set of factors the computational strategies the way antecedents are determined ie the algorithm and formula for assigning antecedents may differ ie from simple cooccurrence rules Dagan and Itai 1990 to decision trees Soon Ng and Lim 2001 Ng and Cardie 2002 to pattern induced rules Ng and Cardie 2002 to centering algorithms Grosz and Sidner 1986 Brennan Friedman and Pollard 1987 Strube 1998 Tetreault 2001", "This paper proposes a simple constraintbased multiagent system to coreference resolution of general noun phrases in unrestricted English text", "For a given anaphor and all the preceding referring expressions as the antecedent candidates a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge", "Then according to the type of the anaphor a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge", "Finally a simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates based on the proximity principle", "One interesting observation is that the most recent antecedent of an anaphor in the coreferential chain is sometimes indirectly linked to the anaphor via some other antecedents in the chain", "In this case we find that the most recent antecedent always contains little information to directly determine the coreference relationship with the anaphor", "Therefore for a given anaphor the corresponding special constraint agent can always safely filter out these less informative antecedent candidates", "In this way rather than finding the most recent antecedent for an anaphor our system tries to find the most direct and informative antecedent", "In this paper we focus on the task of determining coreference relations as defined in MUC6 1995 and MUC7 1998", "In order to evaluate the performance of our approach on coreference resolution we utilize the annotated corpus and the scoring programs from MUC6 and MUC7", "For MUC6 30 dryrun documents annotated with coreference information are used as the training data", "There are also 30 annotated training documents from MUC7", "The total size of 30 training documents is close 12400 words for MUC6 and 19000 for MUC7", "For testing we utilize the 30 standard test documents from MUC 6 and the 20 standard test documents from MUC7", "The layout of this paper is as follows in Section 2 we briefly describe the preprocessing determination of referring expressions", "In Section 3 we differentiate coreference types and discuss how to restrict possible types of direct and informative antecedent candidates according to anaphor types", "In Section 4 we describe the constraintbased multiagent system", "In Section 5 we evaluate the multiagent algorithm", "Finally we present our conclusions"]}, "C04-1131": {"title": ["Word sense disambiguation criteria a systematic study"], "abstract": ["This article describes the results of a systematic in depth study of the criteria used for word sense disambiguation", "Our study is based on 60 target words 20 nouns 20 adjectives and 20 verbs", "Our results are not always in line with some practices in the field", "For example we show that omitting non content words decreases performance and that bigrams yield better results than unigrams"], "introduction": ["The task of word sense disambiguation WSD is to identify the correct sense of a word in context", "WSD is usually performed by matching information from the context in which the word occurs with disambiguation knowledge source", "Our approach uses supervised machinelearning techniques to automatically acquire such disambiguation knowledge from sensetagged corpora", "At present this type of approach is widely used and seems to yield the best results Kilgarriff Rosenzweig 2000 Ng 1997b", "Information conveyed by context words morphological form is enriched with further annotations partofspeech tag lemma etc Each individual piece of information is called a feature", "A good feature should capture an important source of knowledge that is critical in determining the sense of the word to be disambiguated", "The choice of the appropriate set of features is an important issue for WSD Bruce Wiebe Perdersen 1996 Ng Zelle 1997 Pedersen 2001", "Thus this paper describes the results of a systematic and indepth study of homogenous criteria ie set of features that can be used for WSD"]}, "C04-1143": {"title": ["FASIL Email Summarisation System"], "abstract": ["Email summarisation presents a unique set of requirements that are different from general text summarisation", "This work describes the implementation of an email summarisation system for use in a voicebased Virtual Personal Assistant developed for the EU FASiL Project", "Evaluation results from the first integrated version of the project are presented"], "introduction": ["Email is one of the most ubiquitous applications used on a daily basis by millions of people worldwide traditionally accessed over a fixed terminal or laptop computer", "In the past years there has been an increasing demand for email access over mobile phones", "Our work has focused on creating an email summarisation service that provides quality summaries adaptively and quickly enough to cater for the tight constrains imposed by a real time texttospeech system", "This work has been done as part of the European Union FASiL project which aims to aims to construct a conversationally intelligent Virtual Personal Assistant VPA designed to manage the users personal and business information through a voicebased interface accessible over mobile phones", "As the quality of life and productivity is to improved in an increasingly information dominated society people need access to information anywhere anytime", "The Adaptive Information Management AIM service in the FASiL VPA seeks to automatically prioritise and present information that is most pertinent to the mobile users and adapt to different user preferences", "The AIM service is comprised of three main parts an email sum mariser email categoriser calendar schedulingPIM interaction and an adaptive prioritisation service that optimizes the sequence in which information is presented keeping the overall duration of the voicebased dialogue to a minimum"]}, "C08-1002": {"title": ["A Supervised Algorithm for Verb Disambiguation into VerbNet Classes"], "abstract": ["VerbNet VN is a major largescale English verb lexicon", "Mapping verb instances to their VN classes has been proven useful for several NLP tasks", "However verbs are polysemous with respect to their VN classes", "We introduce a novel supervised learning model for mapping verb instances to VN classes using rich syntactic features and class membership constraints", "We evaluate the algorithm in both indomain and corpus adaptation scenarios", "In both cases we use the manually tagged Sem link WSJ corpus as training data", "For in domain testing on Semlink WSJ data we achieve 959 accuracy 351 error reduction ER over a strong baseline", "For adaptation we test on the GENIA corpus and achieve 724 accuracy with 107 ER", "This is the first largescale experimentation with automatic algorithms for this task"], "introduction": ["The organization of verbs into classes whose members exhibit similar syntactic and semantic behavior has been discussed extensively in the linguistics literature see eg", "Levin and Rappaport Hovav 2005 Levin 1993", "Such an organization helps in avoiding lexicon representation redundancy and enables generalizations across similar verbs", "It can also be of great practical use eg in compensating NLP statistical models for data sparseness", "Indeed Levins seminal work had motivated Qc 2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "much research aimed at automatic discovery of verb classes see Section 2", "VerbNet VN Kipper et al 2000 Kipper Schuler 2005 is a large scale publicly available domain independent verb lexicon that builds on Levin classes and extends them with new verbs new classes and additional information such as semantic roles and selectional restrictions", "VN classes were proven beneficial for Semantic Role Labeling SRL Swier and Stevenson 2005 Semantic Parsing Shi and Mihalcea 2005 and building conceptual graphs Hensman and Dun nion 2004", "Levininspired classes have been used in several NLP tasks such as Machine Translation Dorr 1997 and Document Classification Klavans and Kan 1998", "Many applications that use VN need to map verb instances onto their VN classes", "However verbs are polysemous with respect to VN classes", "Sem link Loper et al 2007 is a dataset that maps each verb instance in the WSJ Penn Treebank to its VN class", "The mapping has been created using a combination of automatic and manual methods", "Yi et al", "2007 have used Semlink to improve SRL", "In this paper we present the first largescale experimentation with a supervised machine learning classification algorithm for disambiguating verb instances to their VN classes", "We use rich syntactic features extracted from a treebankstyle parse tree and utilize a learning algorithm capable of imposing class membership constraints thus taking advantage of the nature of our task", "We use Semlink as the training set", "We evaluate our algorithm in both indomain and corpus adaptation scenarios", "In the former we test on the WSJ using Semlink again obtaining 959 accuracy with 351 error reduction ER over a strong baseline most frequent 9 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 916 Manchester August 2008 class when using a modern statistical parser", "In the corpus adaptation scenario we disambiguate verbs in sentences taken from outside the training domain", "Since the manual annotation of new corpora is costly and since VN is designed to be a domain independent resource adaptation results are important to the usability in NLP in practice", "We manually annotated 400 sentences from GENIA Kim et al 2003 a medical domain corpus1  Testing on these we achieved 724 accuracy with 107 ER", "Our adaptation scenario is complete in the sense that the parser we use was also trained on a different corpus WSJ", "We also report experiments done using goldstandard manually created parses", "The most relevant previous works addressing verb instance class classification are Lapata and Brew 2004 Li and Brew 2007 Girju et al 2005", "The former two do not use VerbNet and their experiments were narrower than ours so we cannot compare to their results", "The latter mapped to VN but used a preliminary highly restricted setup where most instances were monosemous", "For completeness we compared our method to theirs2  achieving similar results", "We review related work in Section 2 and discuss the task in Section 3", "Section 4 introduces the model Section 5 describes the experimental setup and Section 6 presents our results"]}, "C08-1014": {"title": ["Regenerating Hypotheses for Statistical Machine Translation"], "abstract": ["This paper studies three techniques that improve the quality of Nbest hypotheses through additional regeneration process", "Unlike the multisystem consensus approach where multiple translation systems are used our improvement is achieved through the expansion of the N best hypotheses from a single system", "We explore three different methods to implement the regeneration process re decoding ngram expansion and confusion networkbased regeneration", "Experiments on ChinesetoEnglish NIST and IWSLT tasks show that all three methods obtain consistent improvements", "Moreover the combination of the three strategies achieves further improvements and outperforms the baseline by 081 BLEUscore on IWSLT06 057 on NIST03 061 on NIST05 test set respectively"], "introduction": ["Stateoftheart Statistical Machine Translation SMT systems usually adopt a twopass search strategy Och 2003 Koehn et al 2003 as shown in Figure 1", "In the first pass a decoding algorithm is applied to generate an Nbest list of translation hypotheses while in the second pass the final translation is selected by rescoring and reranking the Nbest translations through additional feature functions", "The fundamental assumption behind using a second pass is that the generated Nbest list may contain better transla  2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbync sa30", "Some rights reserved", "tions than the best choice found by the decoder", "Therefore the performance of a twopass SMT system can be improved from two aspects ie scoring models and the quality of the Nbest hypotheses", "Rescoring pass improves the performance of machine translation by enhancing the scoring models with more global sophisticated and dis criminative feature functions", "The idea for applying two passes instead of one is that some global feature functions cannot be easily decomposed into local scores and computed during decoding", "Furthermore rescoring allows some feature functions such as word and ngram posterior probabilities to be estimated on the Nbest list Ueffing 2003 Chen et al 2005 Zens and Ney 2006", "In this twopass method translation performance hinges on the Nbest hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance", "This technique is called system combination Bangalore et al 2001 Matusov et al 2006 Sim et al 2007 Rosti et al 2007a Rosti et al 2007b", "We have instead chosen to regenerate new hypotheses from the original Nbest list a technique which we call regeneration", "Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2", "Given the original Nbest list Nbest1 generated by the decoder this regeneration pass creates new translation hypotheses from this list to form another Nbest list Nbest2", "These two Nbest lists are then combined and given to the rescoring pass to derive the best translation", "We implement three methods to regenerate new hypotheses redecoding ngram expansion and confusion network", "Redecoding Rosti et al 2007a based regeneration redecodes the source sentence using original LM as well as new trans 105 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 105112 Manchester August 2008 Figure 1 Structure of a typical twopass machine translation system", "Nbest translations are generated by the decoder and the 1best translation is returned after rescored with additional feature functions", "didates from the other MT systems"]}, "C08-1088": {"title": ["Exploiting Constituent Dependencies for Tree Kernelbased Semantic"], "abstract": ["This paper proposes a new approach to dynamically determine the tree span for tree kernelbased semantic relation extraction", "It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities while removing the noisy information from the syntactic parse tree eventually leading to a dynamic syntactic parse tree", "This paper also explores entity features and their combined features in a unified parse and semantic tree which integrates both structured syntactic parse information and entityrelated semantic information", "Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans and the composite kernel combining this tree kernel with a linear stateoftheart featurebased kernel achieves the so far best performance"], "introduction": ["Information extraction is one of the key tasks in natural language processing", "It attempts to identify relevant information from a large amount of natural language text documents", "Of three sub tasks defined by the ACE program1 this paper focuses exclusively on Relation Detection and Characterization RDC task which detects and classifies semantic relationships between predefined types of entities in the ACE corpus", "For  2008", "Licensed under the Creative Commons Attribution NoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "1 httpwwwldcupenneduProjectsACE example the sentence Microsoft Corp is based in Redmond WA conveys the relation GPEAFFBased between Microsoft Corp ORG and Redmond GPE", "Due to limited accuracy in stateoftheart syntactic and semantic parsing reliably extracting semantic relationships between named entities in natural language documents is still a difficult unresolved problem", "In the literature featurebased methods have dominated the research in semantic relation extraction", "Featuredbased methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features such as lexical knowledge entityrelated information syntactic parse trees and deep semantic information", "However detailed research Zhou et al 2005 shows that its difficult to extract new effective features to further improve the extraction accuracy", "Therefore researchers turn to kernelbased methods which avoids the burden of feature engineering through computing the similarity of two discrete objects eg parse trees directly", "From prior work Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005 to current research Zhang et al 2006 Zhou et al 2007 kernel methods have been showing more and more potential in relation extraction", "The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances", "While kernel methods using the dependency tree Culotta and Sorensen 2004 and the shortest dependency path Bunescu andMooney 2005 suffer from low recall perform ance convolution tree kernels Zhang et al 2006 Zhou et al 2007 over syntactic parse trees achieve comparable or even better performance than featurebased methods", "However there still exist two problems regarding currently widely used tree spans", "Zhang et al", "2006 discover that the Shortest Path 697 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 697704 Manchester August 2008 enclosed Tree SPT achieves the best performance", "Zhou et al", "2007 further extend it to ContextSensitive Shortest Pathenclosed Tree CS SPT which dynamically includes necessary predicatelinked path information", "One problem with both SPT and CSSPT is that they may still contain unnecessary information", "The other problem is that a considerable number of useful contextsensitive information is also missing from SPTCSSPT although CSSPT includes some contextual information relating to predicate linked path", "This paper proposes a new approach to dynamically determine the tree span for relation extraction by exploiting constituent dependencies to remove the noisy information as well as keep the necessary information in the parse tree", "Our motivation is to integrate dependency information which has been proven very useful to relation extraction with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction", "Moreover we also explore interesting combined entity features for relation extraction via a unified parse and semantic tree", "The other sections in this paper are organized as follows", "Previous work is first reviewed in Section 2", "Then Section 3 proposes a dynamic syntactic parse tree while the entityrelated semantic tree is described in Section 4", "Evaluation on the ACE RDC corpus is given in Section 5", "Finally we conclude our work in Section 6"]}, "C08-2009": {"title": ["Underspecified Modelling  of Complex Discourse Constraints"], "abstract": ["We introduce a new type of discourse constraints for the interaction of discourse relations with the configuration of discourse segments", "We examine corpusextracted examples as soft constraints", "We show how to use Regular Tree Gramamrs to process such constraints and how the representation of some constraints depends on the expressive power of this formalism"], "introduction": ["Discourse structures cannot always be described completely either because they are ambiguous Stede 2004 or because a discourse parser fails to analyse them completely", "In either case underspecification formalisms UFs can be used to represent partial information on discourse structure", "UFs are used in semantics to model structural ambiguity without disjunctive enumeration of the readings van Deemter and Peters 1996", "Underspecified descriptions of discourse must handle two kinds of incomplete information on the configuration of discourse segments how they combine into larger units and on the discourse relations that bring about this configuration Our corpus studies on the RST Discourse Treebank Carlson et al 2002 showed interdependencies between relations and configuration a phenomenon first noted by CorstonOliver 1998", "These interdependencies can be formulated as constraints that contribute to the disambiguation of underspecified descriptions of discourse structure", "Eg in discourse segments constituted by the relation Condition the premiss tends to be a dis Qc 2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "course atom or at least maximally short1 Similarly there is evidence for an interdependency constraint for the relation Purpose1 2", "In most cases Purpose1 has a discourse atom as its nucleus", "The corpus evaluation furthermore shows that those patterns never occur exclusively but only as tendencies", "Realised as soft constraints such tendencies can help to sort the set of readings according to the established preferences which allows to focus on the best reading or the nbest readings", "This is of high value for an UFbased approach to discourse structure which must cope with extremely high numbers of readings", "To model interdependency constraints we will use Regular Tree Grammars RTGs Comon et al 2007", "RTGs can straightforwardly be extended to weighted Regular Tree Grammars wRTGs which can represent both soft and hard constraints", "Apart from our corpusextracted examples we also consider a hard interdependency constraint similar to the Right Frontier Constraint", "We show that we can integrate this attachment constraint with our formalism and how its representation depends on the expressiveness of RTGs"]}, "C10-1018": {"title": ["Exploiting Background Knowledge for Relation Extraction"], "abstract": ["Relation extraction is the task of recognizing semantic relations among entities", "Given a particular sentence supervised approaches to Relation Extraction employed feature or kernel functions which usually have a single sentence in their scope", "The overall aim of this paper is to propose methods for using knowledge and resources that are external to the target sentence as a way to improve relation extraction", "We demonstrate this by exploiting background knowledge such as relationships among the target relations as well as by considering how target relations relate to some existing knowledge resources", "Our methods are general and we suggest that some of them could be applied to other NLP tasks"], "introduction": ["Relation extraction RE is the task of detecting and characterizing semantic relations expressed between entities in text", "For instance given the sentence Cone a Kansas City native was originally signed by the Royals and broke into the majors with the team one of the relations we might want to extract is the employment relation between the pair of entity mentions Cone and Royals", "RE is important for many NLP applications such as building an ontology of entities biomedical information extraction and question answering", "Prior work have employed diverse approaches towards resolving the task", "One approach is to build supervised RE systems using sentences annotated with entity mentions and predefined target relations", "When given a new sentence the RE system has to detect and disambiguate the presence of any predefined relations that might exist between each of the mention pairs in the sentence", "In building these systems researchers used a wide variety of features Kambhatla 2004 Zhou et al 2005 Jiang and Zhai 2007", "Some of the common features used to analyze the target sentence include the words appearing in the sentence their partof speech POS tags the syntactic parse of the sentence and the dependency path between the pair of mentions", "In a related line of work researchers have also proposed various kernel functions based on different structured representations eg dependency or syntactic tree parses of the target sentences Bunescu and Mooney 2005 Zhou et al 2007 Zelenko et al 2003 Zhang et al 2006", "Additionally researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia Weld et al 2008 and databases Mintz et al 2009 or attempted open information extraction IE Banko et al 2007 to extract all possible relations", "In this work we focus on supervised RE", "In prior work the feature and kernel functions employed are usually restricted to being defined on the various representations eg lexical or structural of the target sentences", "However in recognizing relations humans are not thus constrained and rely on an abundance of implicit world knowledge or background information", "What quantifies as world or background knowledge is rarely explored in the RE literature and we do not attempt to provide complete nor precise definitions in this paper", "However we show that by considering the relationship between our relations of interest as 152 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 152160 Beijing August 2010 well as how they relate to some existing knowledge resources we improve the performance of RE", "Specifically the contributions of this paper are the following  When our relations of interest are clustered or organized in a hierarchical ontology we show how to use this information to improve performance", "By defining appropriate con straints between the predictions of relations at different levels of the hierarchy we obtain globally coherent predictions as well as improved performance", " Coreference is a generic relationship that might exists among entity mentions and we show how to exploit this information by assuming that coreferring mentions have no other interesting relations", "We capture this intuition by using coreference information to constraint the predictions of a RE system", " When characterizing the relationship between a pair of mentions one can use a large encyclopedia such as Wikipedia to infer more knowledge about the two mentions", "In this work after probabilistically mapping mentions to their respective Wikipedia pages we check whether the mentions are related", "Another generic relationship that might exists between a pair of mentions is whether they have a parentchild relation and we use this as additional information", " The sparsity of features especially lexical features is a common problem for supervised systems", "In this work we show that one can make fruitful use of unlabeled data by using word clusters automatically gathered from unlabeled texts as a way of generalizing the lexical features", " We combine the various relational predictions and background knowledge through a global inference procedure which we formalize via an Integer Linear Programming ILP framework as a constraint optimization problem Roth and Yih 2007", "This allows us to easily incorporate various constraints that encode the background knowledge", "Roth and Yih 2004 develop a relation extraction approach that exploits constraints among entity types and the relations allowed among them", "We extend this view significantly within a similar computational framework to exploit relations among target relations background information and world knowledge as a way to improve relation extraction and make globally coherent predictions", "In the rest of this paper we first describe the features used in our basic RE system in Section 2", "We then describe how we make use of background knowledge in Section 3", "In Section 4 we show our experimental results and perform analysis in Section 5", "In Section 6 we discuss related work before concluding in Section 7"]}, "C10-1064": {"title": ["A Crosslingual Annotation Projection Approach"], "abstract": ["While extensive studies on relation extraction have been conducted in the last decade statistical systems based on supervised learning are still limited because they require large amounts of training data to achieve high performance", "In this paper we develop a crosslingual annotation projection method that leverages parallel corpora to bootstrap a relation detector without significant annotation efforts for a resourcepoor language", "In order to make our method more reliable we introduce three simple projection noise reduction methods", "The merit of our method is demonstrated through a novel Korean relation detection task"], "introduction": ["Relation extraction aims to identify semantic relations of entities in a document", "Many relation extraction studies have followed the Relation Detection and Characterization RDC task organized by the Automatic Content Extraction project Doddington et al 2004 to make multilingual corpora of English Chinese and Arabic", "Although these datasets encourage the development and evaluation of statistical relation extractors for such languages there would be a scarcity of labeled training samples when learning a new system for another language such as Korean", "Since manual annotation of entities and their relations for such resourcepoor languages is very expensive we would like to consider instead a weaklysupervised learning technique in order to learn the relation extractor without significant annotation efforts", "To do this we propose to leverage parallel corpora to project the relation annotation on the source language eg English to the target eg Korean", "While many supervised machine learning approaches have been successfully applied to the RDC task Kambhatla 2004 Zhou et al 2005 Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005 Zhang et al 2006 few have focused on weaklysupervised relation extraction", "For example Zhang 2004 and Chen et al 2006 utilized weaklysupervised learning techniques for relation extraction but they did not consider weak supervision in the context of crosslingual relation extraction", "Our key hypothesis on the use of parallel corpora for learning the relation extraction system is referred to as crosslingual annotation projection", "Early studies of crosslingual annotation projection were accomplished for lexicallybased tasks for example partofspeech tagging Yarowsky and Ngai 2001 namedentity tagging Yarowsky et al 2001 and verb classification Merlo et al 2002", "Recently there has been increasing interest in applications of annotation projection such as dependency parsing Hwa et al 2005 mention detection Zitouni and Florian 2008 and semantic role labeling Pado and Lapata 2009", "However to the best of our knowledge no work has reported on the RDC task", "In this paper we apply a crosslingual annotation projection approach to binary relation detection a task of identifying the relation between two entities", "A simple projection method propagates the relations in source language sentences to 564 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 564571 Beijing August 2010 wordaligned target sentences and a target relation detector can bootstrap from projected annotation", "However this automatic annotation is unreliable because of misclassification of source text and word alignment errors so it causes a critical fallingoff in annotation projection quality", "To alleviate this problem we present three noise reduction strategies a heuristic filtering an alignment correction with dictionary and an instance selection based on assessment and combine these to yield a better result", "We provide a quantitive evaluation of our method on a new Korean RDC dataset", "In our experiment we leverage an EnglishKorean parallel corpus collected from the Web and demonstrate that the annotation projection approach and noise reduction method are beneficial to build an initial Korean relation detection system", "For example the combined model of three noise reduction methods achieves F1scores of 369 598 precision and 267 recall favorably comparing with the 305 shown by the supervised baseline1 The remainder of this paper is structured as follows", "In Section 2 we describe our crosslingual annotation projection approach to relation detection task", "Then we present the noise reduction methods in Section 3", "Our experiment on the proposed Korean RDC evaluation set is shown in Section 4 and Section 5 and we conclude this paper in Section 6"]}, "C10-1081": {"title": ["Semantic Role Features for Machine Translation"], "abstract": ["We propose semantic role features for a TreetoString transducer to model the reorderingdeletion of sourceside semantic roles", "These semantic features as well as the TreetoString templates are trained based on a conditional loglinear model and are shown to significantly outperform systems trained based on MaxLikelihood and EM", "We also show significant improvement in sentence fluency by using the semantic role features in the loglinear model based on manual evaluation"], "introduction": ["Syntaxbased statistical machine translation SSMT has achieved significant progress during recent years Galley et al 2006 May and Knight 2007 Liu et al 2006 Huang et al 2006 showing that deep linguistic knowledge if used properly can improve MT performance", "Semanticsbased SMT as a natural extension to SSMT has begun to receive more attention from researchers Liu and Gildea 2008 Wu and Fung 2009", "Semantic structures have two major advantages over syntactic structures in terms of helping machine translation", "First of all semantic roles tend to agree better between two languages than syntactic constituents Fung et al 2006", "This property motivates the approach of using the consistency of semantic roles to select MT outputs Wu and Fung 2009", "Secondly the set of semantic roles of a predicate models the skeleton of a sentence which is crucial to the readability of MT output", "By skeleton we mean the main structure of a sentence including the verbs and their arguments", "In spite of the theoretical potential of the semantic roles there has not been much success in using them to improve SMT systems", "Liu and Gildea 2008 proposed a semantic role based TreetoString TTS transducer by adding semantic roles to the TTS templates", "Their approach did not differentiate the semantic roles of different predicates and did not always improve the TTS transducers performance", "Wu and Fung 2009 took the output of a phrasebased SMT system Moses Koehn et al 2007 and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence", "This approach shows the positive effect of applying semantic role constraints but it requires retagging semantic roles for every permuted MT output and does not scale well to longer sentences", "This paper explores ways of tightly integrating semantic role features SRFs into an MT system rather than using them in postprocessing or n best reranking", "Semantic role labeling SRL systems usually use sentencewide features Xue and Palmer 2004 Pradhan et al 2004 Toutanova et al 2005 thus it is difficult to compute target side semantic roles incrementally during decoding", "Noticing that the source side semantic roles are easy to compute we apply a compromise approach where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences", "Since this approach does not perform true SRL on the target string it cannot fully evaluate whether the source and target semantic structures are consistent", "However the approach does capture the semanticlevel reordering of the sentences", "We assume here that the MT system is capable of providing word alignment or equivalent information during decoding which is generally true for current statistical MT systems", "Specifically two types of semantic role features are proposed in this paper a semantic role reordering feature designed to capture the skeleton level permutation and a semantic role deletion fea 716 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 716724 Beijing August 2010 ture designed to penalize missing semantic roles in the target sentence", "To use these features during decoding we need to keep track of the semantic role sequences SRS for partial translations which can be generated based on the sourceside semantic role sequence and the corresponding word alignments", "Since the SRL system and the MT system are separate a translation rule eg a phrase pair in phrasebased SMT could cover two partial sourceside semantic roles", "In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs", "Dealing with this problem will increase the complexity of the decoding algorithm", "Fortunately Treeto String transducer based MT systems Liu et al 2006 Huang et al 2006 can avoid this problem by using the same syntax tree for both SRL and MT Such an arrangement guarantees that a TTS template either covers parts of one sourceside semantic role or a few complete semantic roles", "This advantage motivates us to use a TTS transducer as the MT system with which to demonstrate the use of the proposed semantic role features", "Since it is hard to design a generative model to combine both the semantic role features and the TTS templates we use a loglinear model to estimate the feature weights by maximizing the conditional probabilities of the target strings given the source syntax trees", "The loglinear model with latent variables has been discussed by Blunsom et al", "2008 we apply this technique to combine the TTS templates and the semantic role features", "The remainder of the paper is organized as follows Section 2 describes the semantic role features proposed for machine translation Section 3 describes how semantic role features are used and trained in a TTS transducer Section 4 presents the experimental results and Section 5 gives the conclusion"]}, "C10-2052": {"title": ["Whats in a Preposition"], "abstract": ["Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments", "We explore this idea for prepositions an often overlooked word class", "We examine the parameters that must be considered in preposition disambiguation namely context features and granularity", "Doing so delivers an increased performance that significantly improves over two stateof theart systems and shows potential for improving other word sense disambiguation tasks", "We report accuracies of 918 and 848 for coarse and finegrained preposition sense disambiguation respectively"], "introduction": ["Ambiguity is one of the central topics in NLP", "A substantial amount of work has been devoted to disambiguating prepositional attachment words and names", "Prepositions as with most other word types are ambiguous", "For example the word in can assume both temporal in May and spatial in the US meanings as well as others less easily classifiable in that vein", "Prepositions typically have more senses than nouns or verbs Litkowski and Hargraves 2005 making them difficult to disambiguate", "Preposition sense disambiguation PSD has many potential uses", "For example due to the relational nature of prepositions disambiguating their senses can help with allword sense disambiguation", "In machine translation different senses of the same English preposition often correspond to different translations in the foreign language", "Thus disambiguating prepositions correctly may help improve translation quality1 Coarsegrained PSD can also be valuable for information extraction where the sense acts as a label", "In a recent study Hwang et al", "2010 identified preposition related features among them the coarsegrained PP labels used here as the most informative feature in identifying causedmotion constructions", "Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing one of the most frequent sources of parse errors", "Several papers have successfully addressed PSD with a variety of different approaches Rudzicz and Mokhov 2003 OHara and Wiebe 2003 Ye and Baldwin 2007 OHara and Wiebe 2009 Tratz and Hovy 2009", "However while it is often possible to increase accuracy by using a different classifier andor more features adding more features creates two problems a it can lead to overfitting and b while possibly improving accuracy it is not always clear where this improvement comes from and which features are actually informative", "While parameter studies exist for general word sense disambiguation WSD tasks Yarowsky and Florian 2002 and PSD accuracy has been steadily increasing there has been no exploration of the parameters of prepositions to guide engineering decisions", "We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative", "We explore the different options for context and feature se 1 See Chan et al 2007 for the relevance of word sense disambiguation and Chiang et al 2009 for the role of prepositions in MT 454 Coling 2010 Poster Volume pages 454462 Beijing August 2010 lection the influence of different preprocessing methods and different levels of sense granularity", "Using the resulting parameters in a Maximum Entropy classifier we are able to improve significantly over existing results", "The general outline we present can potentially be extended to other word classes and improve WSD in general"]}, "C10-2087": {"title": ["DependencyDriven Featurebased Learning for Extracting"], "abstract": ["Recent kernelbased PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information but at the expense of computational complexity", "This paper incorporates dependency information as well as other lexical and syntactic knowledge in a featurebased framework", "Our motivation is that considering the large amount of biomedical literature being archived daily featurebased methods with comparable performance are more suitable for practical applications", "Additionally we explore the difference of lexical characteristics between biomedical and newswire domains", "Experimental evaluation on the AIMed corpus shows that our system achieves comparable performance of 547 in F1Score with other stateoftheart PPI extraction systems yet the best performance among all the featurebased ones"], "introduction": ["In recent years automatically extracting biomedical information has been the subject of significant research efforts due to the rapid growth in biomedical development and discovery", "A wide concern is how to characterize protein interaction partners since it is crucial to understand not only the functional role of individual proteins but also the organization of the entire biological process", "However manual collection of relevant ProteinProtein Interaction PPI information from thousands of research papers published every day is so timeconsuming that automatic extraction approaches with the help of Natural Language Processing NLP techniques become necessary", "Various machine learning approaches for relation extraction have been applied to the biomedical domain which can be classified into two categories featurebased methods Mitsumori et al 2006 Giuliano et al 2006 Stre et al 2007 and kernelbased methods Bunescu et al 2005 Erkan et al 2007 Airola et al 2008 Kim et al 2010", "Provided a largescale manually annotated corpus the task of PPI extraction can be formulated as a classification problem", "Typically for featuredbased learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names", "Early studies identify the existence of protein interactions by using bagofwords features usually unigram or bigram around the protein names as well as various kinds of shallow linguistic information such as POS tag lemma and orthographical features", "However these systems do not achieve promising results since they disregard any syntactic or semantic information altogether which are very useful for the task of relation extraction in the newswire domain Zhao and Grishman 2005 Zhou et al 2005", "Furthermore featurebased methods fail to effectively capture the structural information which is essential to  Corresponding author 757 Coling 2010 Poster Volume pages 757765 Beijing August 2010 With the wide application of kernelbased methods to many NLP tasks various kernels such as subsequence kernels Bunescu and Mooney 2005 and tree kernels Li et al 2008 are also applied to PPI detection", "Particularly dependencybased kernels such as edit distance kernels Erkan et al 2007 and graph kernels Airola et al 2008 Kim et al 2010 show some promising results for PPI extraction", "This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories Culotta and Sorensen 2004", "In order to appreciate the advantages of both featurebased methods and kernelbased methods composite kernels Miyao et al 2008 Miwa et al 2009a Miwa et al 2009b are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction", "However one critical challenge for kernelbased methods is their computation complexity which prevents them from being widely deployed in realworld applications regarding the large amount of biomedical literature being archived everyday", "Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernelbased methods one may naturally ask the question Can the essential dependency information be maximally exploited in featuredbased PPI extraction so as to enhance the performance without loss of efficiency If the answer is Yes then How This paper addresses these problems focusing on the application of dependency information to featurebased PPI extraction", "Starting from a baseline system in which common lexical and syntactic features are incorporated using Support Vector Machines SVM we further augment the baseline with various features related to dependency information including predicates in the dependency tree", "Moreover in order to reveal the linguistic difference between distinct domains we also compare the effects of various features on PPI extraction from biomedical texts with those on relation extraction from newswire narratives", "Evaluation on the AIMed and other PPI cor The rest of the paper is organized as follows", "A featurebased PPI extraction baseline system is given in Section 2 while Section 3 describes our dependencydriven method", "We report our experiments in Section 4 and compare our work with the related ones in Section 5", "Section 6 concludes this paper and gives some future directions"]}, "C10-2101": {"title": ["Semantic Classification of Automatically Acquired Nouns"], "abstract": ["In this paper we present a twostage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them", "We first acquire unknown morphemes only making a morphology level distinction and then apply semantic classification to acquired nouns", "One advantage of this approach is that at the second stage we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition we can rely on automatic parsing", "Japanese semantic classification poses an interesting challenge proper nouns need to be distinguished from common nouns", "It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them", "We explore lexicosyntactic clues that are extracted from automatically parsed text and investigate their effects"], "introduction": ["A dictionary plays an important role in Japanese morphological analysis or the joint task of segmentation and partofspeech POS tagging Kurohashi et al 1994 Asahara and Mat sumoto 2000 Kudo et al 2004", "Like Chinese and Thai Japanese does not delimit words by whitespace", "This makes the first step of natural language processing more ambiguous than simple POS tagging", "Accordingly morphemes in a predefined dictionary compactly represent our knowledge about both segmentation and POS", "One obvious problem with the dictionarybased approach is caused by unknown morphemes or morphemes not defined in the dictionary", "Even though historically extensive human resources were used to build highcoverage dictionaries Yokoi 1995 texts other than newspaper articles in particular web pages contain a large number of unknown morphemes", "These unknown morphemes often cause segmentation errors", "For example morphological analyzer JU MAN 601 wrongly segments the phrase   saQporo eki Sapporo Station where   saQporo is an unknown morpheme as follows  sa nouncommon difference  Q UNK  po UNK  ro nouncommon sumac and  eki nouncommon station where UNK refers to unknown morphemes automatically identified by the analyzer", "Such an erroneous sequence has disastrous effects on applications of morphological analysis", "For example it can hardly be identified as a LOCATION in named entity recognition", "One solution to the unknown morpheme problem is unknown morpheme acquisition Mori and Nagao 1996 Murawaki and Kurohashi 2008", "It is the task of automatically augmenting the dictionary by acquiring unknown morphemes from text", "In the above example the goal is to acquire the morpheme  saQporo with the POS tag nounlocation name However unknown morpheme acquisition usually adopts a coarser POS tagset that only represents the morphology level distinction among noun verb and adjective", "This means that  saQporo is acquired as just a noun and that the semantic label location name remains to be assigned", "The reason only the morphology level distinction is made is 1 httpnlpkueekyotouacjp nlresourcejumanehtml 876 Coling 2010 Poster Volume pages 876884 Beijing August 2010 that the semantic level distinction cannot easily be captured with morphological clues that are exploited in unknown morpheme acquisition", "In this paper we investigate the remaining problem and introduce the new task of semantic classification that is to be applied to automatically acquired nouns", "In this task we can exploit syntactic clues in addition to morphological ones because as a result of acquisition we can now rely on automatic parsing", "For exam ple since text containing  saQporo noununclassified is correctly segmented we can extract not only the phrase saQporo station but the tree fragment  go to saQporo and we can determine its semantic label", "Japanese semantic classification poses an interesting challenge proper nouns need to be distinguished from common nouns", "Like Chinese and Thai Japanese has no orthographic distinction between common and proper nouns as there is no such thing as capitalization", "In addition there seems no morphosyntactic ie grammatical distinction between them", "In this paper we explore lexicosyntactic clues that can be extracted from automatically parsed text", "We train a classification model on manually registered nouns and apply it to automatically acquired nouns", "We then investigate the effects of lexicosyntactic clues"]}, "C10-2104": {"title": ["Kernelbased Reranking for NamedEntity Extraction"], "abstract": ["We present novel kernels based on structured and unstructured features for reranking the Nbest hypotheses of conditional random fields CRFs applied to entity extraction", "The former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples", "The experiments on two standard corpora in two languages ie the Italian EVALITA 2009 and the English CoNLL 2003 datasets show a large improvement on CRFs in Fmeasure ie from 8034 to 8433 and from 8486 to 8816 respectively", "Our analysis reveals that both kernels provide a comparable improvement over the CRFs baseline", "Additionally their combination improves CRFs much more than the sum of the individual contributions suggesting an interesting kernel synergy"], "introduction": ["Reranking is a promising computational framework which has drawn special attention in the Natural Language Processing NLP community", "Basically this method first employs a probabilistic model to generate a list of topn candidates and then reranks this nbest list with additional features", "One appeal of this approach is its flexibility of incorporating arbitrary features into a model", "These features help in discriminating good from bad hypotheses and consequently their automatic learning", "Various algorithms have been applied for reranking in NLP applications Huang 2008 Shen et al 2004 Collins 2002b Collins and Koo 2000 including parsing name tagging and machine translation", "This work has exploited the disciminative property as one of the key criterion of the reranking algorithm", "Reranking appears extremely interesting if coupled with kernel methods Dinarelli et al 2009 Moschitti 2004 Collins and Duffy 2001 as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies", "Indeed while featurebased learning algorithms involve only the dotproduct between feature vectors kernel methods allow for a higher generalization by replacing the dot product with a function between pairs of linguistic objects", "Such functions are a kind of similarity measure satisfying certain properties", "An example is the tree kernel Collins and Duffy 2001 where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees", "Similarly sequence kernels Lodhi et al 2002 count the number of common subsequences shared by two input strings", "Namedentities NEs are essential for defining the semantics of a document", "NEs are objects that can be referred by names Chinchor and Robinson 1998 such as people organizations and locations", "The research on NER has been promoted by the Message Understanding Conferences MUCs 19871998 the shared task of the Conference on Natural Language Learning CoNLL 20022003 and the Automatic Content Extraction program ACE 20022005", "In the literature there exist various learning approaches to extract namedentities from text", "A NER sys 901 Coling 2010 Poster Volume pages 901909 Beijing August 2010 tem often builds some generativediscriminative model then either uses only one classifier Car reras et al 2002 or combines many classifiers using some heuristics Florian et al 2003", "To the best of our knowledge reranking has not been applied to NER except for the reranking algorithms defined in Collins 2002b Collins 2002a which only targeted the entity detection and not entity classification task", "Besides since kernel methods offer a natural way to exploit linguistic properties applying kernels for NE reranking is worthwhile", "In this paper we describe how kernel methods can be applied for reranking ie detection and classification of namedentities in standard corpora for Italian and English", "The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences", "For this purpose we apply tree kernels to a tree structure encoding NE tags of a sentence and combined them with a polynomial kernel which efficiently exploits global features", "Our main contribution is to show that a tree kernels can be used to define general features not merely syntactic and b using appropriate algorithms and features reranking can be very effective for namedentity recognition", "Our study demonstrates that the composite kernel is very effective for reranking namedentity sequences", "Without the need of producing and heuristically combining learning models like previous work on NER the composite kernel not only captures most of the flat features but also efficiently exploits structured features", "More interestingly this kernel yields significant improvement when applied to two corpora of two different languages", "The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the stateoftheart"]}, "C16-1060": {"title": ["A Bayesian model for joint word alignment and partofspeech transfer"], "abstract": ["Current methods for word alignment require considerable amounts of parallel text to deliver accurate results a requirement which is met only for a small minority of the worlds approximately 7000 languages", "We show that by jointly performing word alignment and annotation transfer in a novel Bayesian model alignment accuracy can be improved for language pairs where annotations are available for only one of the languagesa finding which could facilitate the study and processing of a vast number of lowresource languages", "We also present an evaluation where our method is used to perform singlesource and multisource partofspeech transfer with 22 translations of the same text in four different languages", "This allows us to quantify the considerable variation in accuracy depending on the specific source texts used even with different translations into the same language"], "introduction": ["Word alignment is the problem of identifying translationally equivalent words across the languages of a parallel text", "It has found widespread use for enabling applications such as statistical machine translation Brown et al 1993 Koehn et al 2003 annotation transfer Yarowsky et al 2001 word sense disambiguation Diab and Resnik 2002 and lexicon extraction Wu and Xia 1994", "Although many types of algorithms have been explored the main line of research through the last couple of decades has been based on the generative IBM models introduced by Brown et al", "1993", "What these models have in common is that they are unsupervised asymmetric models assuming one of the languages in a bitext the source language generates the corresponding text in the other language the target language word by word", "Most often a variant of the ExpectationMaximization algorithm Dempster et al 1977 has been used for inference in these models but recently there has been some work using Bayesian alignment models using Gibbs sampling for inference DeNero et al 2008 Mermer and Saraclar 2011 Gal and Blunsom 2013", "The incorporation of Bayesian priors into these models has been shown to improve accuracy since they provide a flexible way of biasing the model towards empirical observations about language most importantly that a given word type tends to have a very limited number of translations", "While the basic word alignment models use only lexical cooccurrence and word order lexical data tends to be sparse and a number of authors have explored the usefulness of other information sources", "Toutanova et al", "2002 showed that Part of Speech PoS tags can be integrated into the IBM models to improve word alignment accuracy and others have reported similar results for dependency Cherry and Lin 2003 Wang and Zong 2013 and phrasestructure Yamada and Knight 2001 parse trees and for lemmatized texts Bojar and Prokopov 2006", "In addition to the studies just mentioned that showed how various types of linguistic annotation can be used to guide word alignment there has been research showing that the reverse also holds wordaligned parallel texts can be used to transfer annotations and models from languages where those resources exist to languages where they do not", "Pioneering work by Yarowsky et al", "2001 explored tasks such as PoS This work is licenced under a Creative Commons Attribution 40 International License", "License details http creativecommonsorglicensesby40 620 Proceedings of COLING 2016 the 26th International Conference on Computational Linguistics Technical Papers pages 620629 Osaka Japan December 1117 2016", "tagging shallow parsing and lemmatization which was followed by eg dependency parsing Hwa et al 2005", "The present work combines these previous lines of work by exploring joint models of word alignment and annotation transfer of PoS tags within a Bayesian framework", "The source code of our implementation is available at httpwwwlingsusespacos"]}, "C92-1059": {"title": ["A TREATMENT OF NEGATIVE DESCRIPTIONS OF"], "abstract": ["A formal treatment of typed feature structuresTFSs is developed to augment TFSs so that negative descriptions of them can be treated", "Negative descriptions of TFSs can make linguistic descriptionscompact and thus easy to understand", "Negative descriptions can be classified into three primitive negative descriptions 1 negations of type symbols 2 negations of feature existences and 3 negations of featureaddress value agreements", "The formalizationproposed in this paper is based on AitKaciaposs complex terms", "The first description is treated by extending type symbol lattices to include complement typesymbols", "The second and third are treated by augmenting terns structures with structures representingthese negations", "Algorithms for augmentedWS unification have been developed using graph unificationand programs using these algorithms have been written in Common Lisp"], "introduction": ["In unificationbased or informationbased linguisticframeworks the most important objects are struc tures called aposfeature structuresapos FSs which are usedto describe linguistic objects and phenomena", "A fea ture structure is either atomic or complex an atomic FS is denoted by an atomic symbol a complex FS consists of a set of featurevalue pairs each of which describes an aspect of an object", "Partial information on an object is merged by applying the unification operation to FSs", "Research on unificationbased linguistic theories has been accompanied by research on FSs themselves", "Several extensions on FSs or on feature descriptions and formal treatments of the extensions have been proposed", "Disjunctive and negative descriptions on FSs help make the linguistic descriptions simple compact andthus easy to understand", "For disjunctive feature descriptions Kay14 introduces them into FUG Functional Unification Grammar and gives the procedural semantics", "Karttunen111 also proposes proce dural treatments of disjunctions in conjunction with relatively simple negations", "Rounds and Kasper19 13 propose a logicbased formalismfeature logicwhich uses automata to model FSs and can treat disjunctive feature descriptions and they obtain impor tant results", "For negative descriptions of FSs one of the mostfundamental properties of FSs the partiality of in formation they carry makes its insufficient to adopt relatively simple treatments", "Classical interpretation of negation for example does not allow evaluation of negations to be freely interleaved with unification", "Moshier and Rounds171 propose a formal framework which treats negative feature descriptions on the basis of intuitionistic logic", "However their formalism has trouble treating double negations", "Dawar5 proposes a formal treatment based on threevalued logic", "In order to treat feature domains of complex FSsand to treat taxonomic hierarchies of symbolic feature values type or sort hierarchies have been in troduced allowing definition of typed or sorted featurestructures Ms", "A TIaposS consists of a type symbol from a lattice and a set of featurevalue pairs", "A TFS can be seen as a generalized concept of bothatomic and complex FSs", "Pollard and Sag I8 intro duce sorts into II PSG Headdriven Phrase Structtire Grammar and use sorted Pis to describe linguistic objects", "AHKacill proposes an algebraic framework usingthe sittypes and ctypes one of promising formalizations of 1TSs based on lattice theory", "This formalization was originally aimed at formalizing and in tegrating various kinds of knowledge representation frameworks in Al In this approach types are defined as equivalence classes of complex term structures", "Asubsumption relation is defined on these term structures", "The join and meet operations on them correspond to the generalization and an opera tions on TFSs respectively", "This approach essentially adopts apostypeassetapos semantics", "Subtype relationships on type correspond to subsumption relationships on denotations of types", "Based on this frau aapos work an extension to Prolog LOGIN 2 has been developed", "Smolka20 proposes a feature logic with subsoilsIn this approach negative descriptions can be decoin posed into three kinds of primitive negations namely negations of sorts or complement sorts which denotethe complements of sets that positive counterparts de note negations of feature existences and negationsof featureaddress agreement or featureaddress dis agreement", "Smolka extends feature descriptions buta featurestructure interpretation of an extended de scription does not include negative in fort nation and corresponds to a simple TIaposSSome WSbased natural language processing systems have been developed7 24 12 15 8 22", "Car penter and Pollard 4 propose an interface to build type lattices", "Formalizations of extended FSs and of extended featuredescriptions described above are classified into two classes 1 extensions of FSs themselves and 2 extensions not of Pis themselves hut of featuredescriptions", "Previous attempts to introducetype hierarchies fall into the former class while previous treatments of disjunctive and negative descrip tions mainly fall into the latter", "Acres us COLINGt92 NANTES 2328 AmYr 1992 380 PROC", "OF COLING92 NANTES AuG 2328 1992 This paper proposes an extension to AithaciapossVapos type that incorporates three kinds of the primitive negative descriptions described below into the 0type", "AitKaciaposs 0type formalization uses ter in structures", "In this paper both these type structures and the type symbol lattice on which term structures are definedare extended to treat negative descriptions", "Nega tions of type symbols are treated by extending type symbol lattices and negations of feature existwicesand featureaddress disagreements are treated by ex tending term structures", "This extension can be seen as intuitionistic", "The extension is classified into class 1 aboveBased on this papers formalization unification al gorithms have been developed using graph unificationtechniques23 16", "Programs based on these algo rithms have been implemented in Common Lisp", "bodyText sectionHeader confidence0650502 genericHeadermethod"]}, "D07-1012": {"title": ["A Comparative Evaluation of Deep and Shallow Approaches to the"], "abstract": ["This paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically well formed or illformed", "The deep processing approach uses the XLE LFG parser and English grammar two versions are presented one which uses the XLE directly to perform the classification and another one which uses a decision tree trained on features consisting of the XLEs output statistics", "The shallow processing approach predicts gram maticality based on ngram frequency statistics we present two versions one which uses frequency thresholds and one which uses a decision tree trained on the frequencies of the rarest ngrams in the input sentence", "We find that the use of a decision tree improves on the basic approach only for the deep parserbased approach", "We also show that combining both the shallow and deep decision tree features is effective", "Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences", "The ungrammatical test set is generated automatically by inserting grammatical errors into wellformed BNC sentences"], "introduction": ["This paper is concerned with the task of predicting whether a sentence contains a grammatical error", "An accurate method for carrying out automatic Also affiliated to IBM CAS Dublin", "grammaticality judgements has uses in the areas of computerassisted language learning and grammar checking", "Comparative evaluation of existing error detection approaches has been hampered by a lack of large and commonly used evaluation error corpora", "We attempt to overcome this by automatically creating a large error corpus containing four different types of frequently occurring grammatical errors", "We use this corpus to evaluate the performance of two approaches to the task of automatic error detection", "One approach uses lowlevel detection techniques based on POS ngrams", "The other approach is a novel parserbased method which employs deep linguistic processing to discriminate grammatical input from ungrammatical", "For both approaches we implement a basic solution and then attempt to improve upon this solution using a decision tree classifier", "We show that combining both methods improves upon the individual methods", "Ngrambased approaches to the problem of error detection have been proposed and implemented in various forms by Atwell1987 Bigert and Knutsson 2002 and Chodorow and Leacock 2000 amongst others", "Existing approaches are hard to compare since they are evaluated on different test sets which vary in size and error density", "Furthermore most of these approaches concentrate on one type of grammatical error only namely contextsensitive or real word spelling errors", "We implement a vanilla n grambased approach which is tested on a very large test set containing four different types of error", "The idea behind the parserbased approach to error detection is to use a broadcoverage handcrafted precision grammar to detect ungrammatical sen 112 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "112121 Prague June 2007", "Qc 2007 Association for Computational Linguistics tences", "This approach exploits the fact that a precision grammar is designed in the traditional generative grammar sense Chomsky 1957 to distinguish grammatical sentences from ungrammatical sentences", "This is in contrast to treebankbased grammars which tend to massively overgenerate and do not generally aim to discriminate between the two", "In order for our approach to work the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences and for this reason we choose the XLE Maxwell and Kaplan 1996 an efficient and robust parsing system for Lexical Functional Grammar LFG Kaplan and Bresnan 1982 and the ParGram English grammar Butt et al 2002 for our experiments", "This system employs robustness techniques some borrowed from Optimality Theory OT Prince and Smolen sky 1993 to parse extragrammatical input Frank et al 1998 but crucially still distinguishes between optimal and suboptimal solutions", "The evaluation corpus is a subset of an ungrammatical version of the British National Corpus BNC a 100 million word balanced corpus of British English Burnard 2000", "This corpus is obtained by automatically inserting grammatical errors into the original BNC sentences based on an analysis of a manually compiled real error corpus", "This paper makes the following contributions to the task of automatic error detection 1", "A novel deep processing XLEbased approach"]}, "D07-1018": {"title": ["Modelling Polysemy in Adjective Classes by MultiLabel Classification"], "abstract": ["This paper assesses the role of multilabel classification in modelling polysemy for language acquisition tasks", "We focus on the acquisition of semantic classes for Catalan adjectives and show that polysemy acquisition naturally suits architectures used for multi label classification", "Furthermore we explore the performance of information drawn from different levels of linguistic description using feature sets based on morphology syntax semantics and ngram distribution", "Finally we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence a simple majority voting ensemble classifier improves the accuracy from 625 best single classifier to 84"], "introduction": ["This paper reports on a series of experiments to explore the automatic acquisition of semantic classes for Catalan adjectives", "The most important challenge of the classification task is to model the assignment of polysemous lexical instances to multiple semantic classes combining a a stateoftheart Machine Learning architecture for Multilabel Classification Schapire and Singer 2000 Ghamrawi and McCallum 2005 and an Ensemble Classifier Dietterich 2002 with b the definition of features at various levels of linguistic description", "A proper treatment of polysemy is essential in the area of lexical acquisition since polysemy repre sents a pervasive phenomenon in natural language", "However previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem cf", "Merlo and Stevenson 2001 and Stevenson and Joanis 2003 for English semantic verb classes or Schulte im Walde 2006 for German semantic verb classes", "There are a few exceptions to this tradition such as Pereira et al", "1993 Rooth et al", "1999 Korhonen et al", "2003 who used soft clustering methods for multiple assignment to verb semantic classes", "Our work addresses the lack of methodology in modelling a polysemous classification", "We implement a multilabel classification architecture to handle polysemy", "This paper concentrates on the classification of Catalan adjectives but the general nature of the architecture should allow related tasks to profit from our insights", "As target classification for the experiments a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic classes", "We deliberately decided in favour of a smallscale broad classification", "So far there is little work on the semantic classification of adjectives as opposed to verbal semantic classification", "The semantic classification we propose is a first step in characterising adjectival meaning and can be refined and extended in subsequent work", "The experiments also provide a thorough comparison of feature sets based on different levels of linguistic description morphology syntax semantics", "A set of features is defined for each level of description and its performance is assessed within the series of experiments", "An ensemble classifier comple 171 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "171180 Prague June 2007", "Qc 2007 Association for Computational Linguistics ments the classification architecture by optimising the combination of these different types of linguistic evidence", "Our task is motivated by the fact that adjectives play an important role in sentential semantics they are crucial in determining the reference of NPs and in defining properties of entities", "Even using only three different classes the information acquired could be applied to eg identify referents in a given context in Dialog or Question Answering systems and to induce properties of objects within Information Extraction tasks", "Furthermore with the semantic classes corresponding to broad sense representations they can be exploited for Word Sense Disambiguation", "The remainder of this paper is organised as follows", "Section 2 provides background on Catalan adjectives and Section 3 presents the Gold Standard classification", "Section 4 introduces the methodology of the multilabel classification experiments Section 5 discusses the results and the improved ensemble classifier is presented in Section 6"]}, "D07-1076": {"title": ["Tree Kernelbased Relation Extraction"], "abstract": ["This paper proposes a tree kernel with context sensitive structured parse tree information for relation extraction", "It resolves two critical problems in previous tree kernels for relation extraction in two ways", "First it automatically determines a dynamic contextsensitive tree span for relation extraction by extending the widely used Shortest Pathenclosed Tree SPT to include necessary context information outside SPT", "Second it proposes a contextsensitive convolution tree kernel which enumerates both contextfree and context sensitive subtrees by consid ering their ancestor node paths as their contexts", "Moreover this paper evaluates the complementary nature between our tree kernel and a stateoftheart linear kernel", "Evaluation on the ACE RDC corpora shows that our dynamic contextsensitive tree span is much more s uitable for relation extraction than SPT and our tree kernel outperforms the stateoftheart Collins and Duffys convolution tree kernel", "It also shows that our tree kernel achieves much better performance than the stateoftheart linear kernels  Finally it shows that featurebased and tree kernelbased methods much complement each other and the composite kernel can well integrate both flat and structured features"], "introduction": ["Relation extraction is to find various predefined semantic relations between pairs of entities in text", "The research in relation extraction has been pr omoted by the Message Understanding Conferences MUCs MUC 19871998 and the NIST Automatic Content Extraction ACE program ACE 20022005", "According to the ACE Program an entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities", "For example the sentence Bill Gates is the chairman and chief software architect of Microsoft Corporation c onveys the ACEstyle relation EMPLOYMENTexec between the entities Bill Gates person name and Microsoft Corporation organization name", "Extraction of semantic relations between entities can be very useful in many applic a tions such as question answering eg to answer the query Who is the president of the United States and information retrieval eg to expand the query George W Bushwith the pres ident of the United Statesvia his relationship with the United States", "Many researches have been done in relation extraction", "Among them featurebased methods Kambhatla 2004 Zhou et al 2005 achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information Zhou et al 2005 which is critical for further performance improvement in relation extraction", "As an alternative to featurebased methods tree kernelbased methods provide an elegant solution to explore implic itly structured features by directly computing the simila rity between two trees", "Although earlier researches Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005a only achieve success on simple tasks and fail on complex tasks such as the ACE RDC task tree kernelbased methods achieve much progress recently", "As the stateoftheart Zhang et al 2006 applied the c onvolution tree kernel Collins and Duffy 2001 and achieved comparable performance with a stateofthe art linear kernel Zhou et al 2005 on the 5 relation types in the ACE RDC 2003 corpus", "However there are two problems in Collins and Duffys convolution tree kernel for relation extrac tion", "The first is that the subtrees enumerated in the tree kernel computation are contextfree", "That is each subtree enumerated in the tree kernel computation 728 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "728736 Prague June 2007", "Qc 2007 Association for Computational Linguistics does not consider the context information outside the subtree", "The second is to dec ide a proper tree span in relation extraction", "Zhang et al 2006 explored five tree spans in relation extraction and it was a bit surprising to find that the Shortest Pathenclosed Tree SPT ie the subtree enclosed by the shortest path linking two involved entities in the parse tree performed best", "This is contrast to our intuition", "For example got married is critical to determine the relationship between John and Maryin the sentence John and Mary got married  as shown in Figure 1e", "It is obvious that the information contained in SPT John and Marry is not enough to determine their relationshipThis paper proposes a contextsensitive convolu tion tree kernel for relation extraction to resolve the above two problems", "It first automatically determines a dynamic contextsensitive tree span for relation extraction by extending the Shortest Pathenclosed Tree SPT to include necessary context information outside SPT", "Then it proposes a context sensitive convolution tree kernel whic h not only enumerates context free subtrees but also context sensitive subtrees by considering their ancestor node paths as their contexts", "Moreover this paper evaluates the complementary nature of different linear kernels and tree kernels via a composite kernel", "The layout of this paper is as follows", "In Section 2 we review related work in more details", "Then the dynamic contextsensitive tree span and the context sensitive convolution tree ker nel are proposed in Section 3 while Section 4 shows the experimental results", "Finally we conclude our work in Sec tion 5"]}, "D09-1025": {"title": ["Entity Extraction via Ensemble Semantics"], "abstract": ["Combining information extraction systems yields significantly higher quality resources than each system in isolation", "In this paper we generalize such a mixing of sources and features in a framework called Ensemble Semantics", "We show very large gains in entity extraction by combining stateoftheart distributional and pattern based systems with a large set of features from a webcrawl query logs and Wikipedia", "Experimental results on a web scale extraction of actors athletes and musicians show significantly higher mean average precision scores 29 gain compared with the current state of the art"], "introduction": ["Mounting evidence shows that combining information sources and information extraction algorithms leads to improvements in several tasks such as fact extraction Pasca et al 2006 open domain IE Talukdar et al 2008 and entailment rule acquisition Mirkin et al 2006", "In this paper we show large gains in entity extraction by combining stateoftheart distributional and pattern based systems with a large set of features from a 600 million document webcrawl one year of query logs and a snapshot of Wikipedia", "Further we generalize such a mixing of sources and features in a framework called Ensemble Semantics", "Distributional and patternbased extraction algorithms capture aspects of paradigmatic and syntagmatic dimensions of semantics respectively and are believed to be quite complementary", "Pasca et al", "2006 showed that filtering facts extracted by a patternbased system according to their arguments distributional similarity with seed facts yielded large precision gains", "Mirkin et al", "2006 showed similar gains on the task of acquiring lexical entailment rules by exploring a supervised combination of distributional and patternbased algorithms using an MLbased SVM classifier", "This paper builds on the above work by studying the impact of various sources of features external to distributional and patternbased algorithms on the task of entity extraction", "Mirkin et als results are corroborated on this task and large and significant gains over this baseline are obtained by incorporating 402 features from a webcrawl query logs and Wikipedia", "We extracted candidate entities for the classes Actors Athletes and Musicians from a webcrawl using a variant of Pasca et als 2006 patternbased engine and Pantel et als 2009 distributional extraction system", "A gradient boosted decision tree is used to learn a regression function over the feature space for ranking the candidate entities", "Experimental results show 29 gains 19 nominal in mean average precision over Mirkin et als method and 34 gains 22 nominal in mean average precision over an unsupervised baseline similar to Pasca et als method", "Below we summarize the contributions of this paper  We explore the hypothesis that although dis tributional and patternbased algorithms are complementary they do not exhaust the semantic space other sources of evidence can be leveraged to better combine them  We model the mixing of knowledge sourcesand features in a novel and general informa tion extraction framework called Ensemble Semantics and  Experiments over an entity extraction taskshow that our model achieves large and sig nificant gains over stateoftheart extractors", "A detailed analysis of feature correlations and interactions shows that query log and we bcrawl features yield the highest gains but easily accessible Wikipedia features also improve over current stateoftheart systems", "238 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 238247 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP Figure 1 The Ensemble Semantics framework for information extraction", "The remainder of this paper is organized as follows", "In the next section we present our Ensemble Semantics framework and outline how various information extraction systems can be cast into the framework", "Section 3 then presents our entity extraction system as an instance of Ensemble Semantics comparing and contrasting it with previous information extraction systems", "Our experimental methodology and analysis is described in Section 4 and shows empirical evidence that our extractor significantly outperforms prior art", "Finally Section 5 concludes with a discussion and future work"]}, "D09-1115": {"title": ["Latticebased System Combination for Statistical Machine Translation"], "abstract": ["Current system combination methods usually use confusion networks to find consensus translations among different systems", "Requiring onetoone mappings between the words in candidate translations confusion networks have difficulty in handling more general situations in which several words are connected to another several words", "Instead we propose a latticebased system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations", "Experiments show that our approach achieves significant improvements over the stateof theart baseline system on ChinesetoEnglish translation test sets"], "introduction": ["System combination aims to find consensus translations among different machine translation systems", "It has been proven that such consensus translations are usually better than the output of individual systems Frederking and Nirenburg 1994", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al 2001 Matusov et al 2006 Sim et al 2007 Rosti et al 2007a Rosti et al 2007b Rosti et al 2008 He et al 2008 which show stateoftheart performance in benchmarks", "A confusion network consists of a sequence of sets of candidate words", "Each candidate word is associated with a score", "The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score", "To construct a confusion network one first need to choose one of the hypotheses ie candidate translations as the backbone also called skeleton in the literature and then decide the word alignments of other hypotheses to the backbone", "Hypothesis alignment plays a crucial role in confusion networkbased system combination because it has a direct effect on selecting consensus translations", "However a confusion network is restricted in such a way that only 1to1 mappings are allowed in hypothesis alignment", "This is not the fact even for word alignments between the same languages", "It is more common that several words are connected to another several words", "For example be capable of and be able to have the same meaning", "Although confusionnetworkbased approaches resort to inserting null words to alleviate this problem they face the risk of producing degenerate translations such as be capable to and be able of", "In this paper we propose a new system combination method based on lattices", "As a more general form of confusion network a lattice is capable of describing arbitrary mappings in hypothesis alignment", "In a lattice each edge is associated with a sequence of words rather than a single word", "Therefore we select phrases instead of words in each candidate set and minimize the chance to produce unexpected translations such as be capable to", "We compared our approach with the stateoftheart confusionnetworkbased system He et al 2008 and achieved a significant absolute improvement of 123 BLEU points on the NIST 2005 Chineseto", "English test set and 093 BLEU point on the NIST 2008 ChinesetoEnglish test set", "1105 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 11051113 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP He feels like apples He prefer apples He feels like apples He is fond of apples a unidirectional alignments He feels like apples He prefer apples He feels like apples He is fond of apples b bidirectional alignments lations", "Note that the phrase is fond of is attached to an edge", "Now it is unlikely to obtain a translation like He is like of apples", "A lattice G  V E is a directed acyclic graph formally a weighted finite state automation FSA where V is the set of nodes and E is the set of edges", "The nodes in a lattice are usually labeled according  prefer of He feels like  apples is fond c confusion network  prefer he feels like apples is fond of d lattice Figure 1 Comparison of a confusion network and a lattice"]}, "D09-1138": {"title": ["Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes"], "abstract": ["The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes", "We intend for the probabilistic model to provide a probability distribution of verbclass associations over known and unknown verbs including pol ysemous words", "In our approach training instances are obtained from an existing lexicon andor from an annotated corpus while the features which represent syntactic frames semantic similarity and selectional preferences are extracted from unannotated corpora", "Our model is evaluated in typelevel verb classification tasks we measure the prediction accuracy of VerbNet classes for unknown verbs and also measure the dissimilarity between the learned and observed probability distributions", "We empirically compare several settings for model learning while we vary the use of features source corpora for feature extraction and disambiguated corpora", "In the task of verb classification into all VerbNet classes our best model achieved a 1069 error reduction in the classification accuracy over the previously proposed model"], "introduction": ["Lexicons are invaluable resources for semantic processing", "In many cases lexicons are necessary to restrict a set of semantic classes to be assigned to a word", "In fact a considerable number of works on semantic processing implicitly or explicitly presupposes the availability of a lexicon such as in word sense disambiguation WSD McCarthy et al 2004 and in tokenlevel verb class disambiguation Lapata and Brew 2004 Girju et al 2005 Li and Brew 2007 Abend et al 2008", "In other words those methods are heavily dependent on the availability of a semantic lexicon", "Therefore recent research efforts have invested in developing semantic resources such as WordNet Fellbaum 1998 FrameNet Baker et al 1998 and VerbNet Kipper et al 2000 KipperSchuler 2005 which greatly advanced research in semantic processing", "However the construction of such resources is expensive and it is unrealistic to presuppose the availability of fullcoverage lexicons this is the case because unknown words always appear in real texts and wordsemantics associations may vary Abend et al 2008", "This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon", "We target the automatic classification of arbitrary verbs including polysemous verbs into all VerbNet classes further we target the estimation of a probabilistic model which represents the saliences of verbclass associations for polysemous verbs", "In our approach an existing lexicon andor an annotated corpus are used as the training data", "Since VerbNet classes are designed to represent the distinctions in the syntactic frames that verbs can take features representing the statistics of syntactic frames are extracted from the unannotated corpora", "Additionally as the classes represent semantic commonalities semantically inspired features like distributionally similar words are used", "These features can be considered as a generalized representation of verbs and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words", "Our model is evaluated in two tasks of type level verb classification one is the classification of monosemous verbs into a small subset of the classes which was studied in some previous works Joanis and Stevenson 2003 Joanis et al 2008", "The other task is the classification of all verbs into the full set of VerbNet classes which has not yet 1328 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 13281337 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP been attempted", "In the experiments training instances are obtained from VerbNet andor Sem Link Loper et al 2007 while features are extracted from the British National Corpus or from Wall Street Journal", "We empirically compare several settings for model learning by varying the set of features the source domain and the size of a corpus for feature extraction and the use of the tokenlevel statistics obtained from a manually disambiguated corpus", "We also provide the analysis of the remaining errors which will lead us to further improve the supervised learning of a probabilistic semantic lexicon", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al 1999 Stevenson and Merlo 1999 Merlo and Stevenson 2001 Stevenson and Joanis 2003 Joanis and Stevenson 2003 Joanis et al 2008", "However their focus has been limited to a small subset of verb classes and a limited number of monosemous verbs", "The main contributions of the present work are i to provide empirical results for the automatic classification of all verbs including polysemous ones into all VerbNet classes and ii to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes"]}, "D09-1149": {"title": ["SemiSupervised Learning for Semantic Relation Classification using"], "abstract": ["This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrappingbased semisupervised learning for semantic relation classification", "First the training data is partitioned into several strata according to relation typessubtypes then relation instances are randomly sampled from each stratum to form the initial seed set", "We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance", "Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself", "This suggests that a proper sampling strategy is critical in semisupervised learning"], "introduction": ["With the dramatic increase in the amount of textual information available in digital archives and the WWW there has been growing interest in techniques for automatically extracting information from text documents", "Information Extraction IE is such a technology that IE systems are expected to identify relevant information usually of predefined types from text documents in a certain domain and put them in a structured format", "According to the scope of the NIST AutomaticContent Extraction ACE program ACE 2000 2007 current research in IE has three main objectives Entity Detection and Tracking EDT Relation Detection and Characterization RDC and Event Detection and Characterization EDC", "This paper focuses on the ACE RDC subtask where many machine learning methods have been proposed including supervised methods Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 Zhang et al 2006 Qian et al 2008 semisupervised methods Brin 1998 Agichtein and Gravano 2000 Zhang 2004 Chen et al 2006 Zhou et al 2008 and unsupervised methods Hasegawa et al 2004 Zhang et al 2005  Current work on semantic relation extraction task mainly uses supervised learning methods since it achieves relatively better performance", "However this method requires a large amount of manually labeled relation instances which is both timeconsuming and laborious", "In the contrast unsupervised methods do not need definitions of relation types and handtagged data but it is difficult to evaluate their performance since there are no criteria for evaluation", "Therefore semisupervised learning has received more and more attention as it can balance the advantages and disadvantages between supervised and unsupervised methods", "With the plenitude of unlabeled natural language data at hand semisupervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance", "Specifically a bootstrapping algorithm chooses the unlabeled instances with the highest probability of being correctly labeled and use them to augment labeled training data iteratively", "Although previous work Yarowsky 1995 Blum and Mitchell 1998 Abney 2000 Zhang 2004 has tackled the bootstrapping approach from both the theoretical and practical point of view many key problems still remain unresolved such as the selection of initial seed set", "Since the size of the initial seed set is usually small eg 1437 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 14371445 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP 100 instances the imbalance of relation types or manifold structure cluster structure in it will severely weaken the strength of bootstrapping", "Therefore it is critical for a bootstrapping approach to select the most appropriate initial seed set", "However current systems Zhang 2004 Chen et al 2006 use a randomly sampling strategy which fails to explore the affinity nature among the training instances", "Alternatively Zhou et al", "2008 bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM", "Nevertheless the initial labeled data is still randomly generated only to ensure that there are at least 5 instances for every relation subtype", "This paper presents a new approach to selecting the initial seed set based on stratified sampling strategy in the bootstrapping procedure for semisupervised semantic relation classification", "The motivation behind the stratified sampling is that every relation type should be as much as possible represented in the initial seed set thus leading to more instances with diverse structures being added to the labeled set", "In addition we also explore different strategies to augment reliably classified instances to the labeled data iteratively and attempt to find a stoppage criterion for the iteration procedure to greatly decrease the training time other than using up all the unlabeled set", "The rest of this paper is organized as follows", "First Section 2 reviews related work on semi supervised relation extraction", "Then we present an underlying supervised learner in Section 3", "Section 4 details various key aspects of the bootstrapping procedure including the stratified sampling strategy", "Experimental results are reported in Section 5", "Finally we conclude our work in Section 6"]}, "D10-1005": {"title": ["  Holistic Sentiment Analysis Across Languages"], "abstract": [], "introduction": ["As its name suggests MLSLDA is an extension of Latent Dirichlet allocation LDA Blei et al 2003 a modeling approach that takes a corpus of unannotated documents as input and produces two outputs a set of topics and assignments of documents to topics", "Both the topics and the assignments are probabilistic a topic is represented as a probability distribution over words in the corpus and each document is assigned a probability distribution over all the topics", "Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentiment bearing words and because topic distributions are a parsimonious way to represent a document1 LDA has been used to discover latent structure in text eg for discourse segmentation Purver et al 2006 and authorship RosenZvi et al 2004", "MLSLDA extends the approach by ensuring that this latent structure  the underlying topics  is consistent across languages", "We discuss multilingual topic modeling in Section 11 and in Section 12 we show how this enables supervised regression regardless of a documents language", "11 Capturing Semantic Correlations", "Topic models posit a straightforward generative process that creates an observed corpus", "For each document d some distribution d over unobserved topics is chosen", "Then for each word position in the document a topic z is selected", "Finally the word for that position is generated by selecting from the topic indexed by z", "Recall that in LDA a topic is a distribution over words", "In monolingual topic models the topic distribution is usually drawn from a Dirichlet distribution", "Using Dirichlet distributions makes it easy to specify sparse priors and it also simplifies posterior inference because Dirichlet distributions are conjugate to multinomial distributions", "However drawing topics from Dirichlet distributions will not suffice if our vocabulary includes multiple languages", "If we are working with English German and Chinese at the same time a Dirichlet prior has no way to fa vor distributions z such that pgoodz pgutz and 1 The latter property has also made LDA popular for information retrieval Wei and Croft 2006", "phaoz all tend to be high at the same time or low at the same time", "More generally the structure of our model must encourage topics to be consistent across languages and Dirichlet distributions cannot encode correlations between elements", "One possible solution to this problem is to use the multivariate normal distribution which can produce correlated multinomials Blei and Lafferty 2005 in place of the Dirichlet distribution", "This has been done successfully in multilingual settings Cohen and Smith 2009", "However such models complicate inference by not being conjugate", "Instead we appeal to treebased extensions of the Dirichlet distribution which has been used to induce correlation in semantic ontologies BoydGraber et al 2007 and to encode clustering constraints An drzejewski et al 2009", "The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree", "For con creteness in this section we will use WordNet Miller 1990 as the representation of this multilingual semantic bridge since it is well known offers convenient and intuitive terminology and demonstrates the full flexibility of our approach", "However the model we describe generalizes to any treestructured representation of multilingual knowledge we discuss some alternatives in Section 3", "WordNet organizes a vocabulary into a rooted directed acyclic graph of nodes called synsets short for synonym sets A synset is a child of another synset if it satisfies a hyponomy relationship each child is a more specific instantiation of its parent concept thus hyponomy is often called an isa relationship", "For example a dog is a canine is an animal is a living thing etc As an approximation it is not unreasonable to assume that WordNets structure of meaning is language independent ie the concept encoded by a synset can be realized using terms in different languages that share the same meaning", "In practice this organization has been used to create many alignments of international WordNets to the original English WordNet Ordan and Wintner 2007 Sagot and Fiser 2008 Isahara et al 2008", "Using the structure of WordNet we can now describe a generative process that produces a distribution over a multilingual vocabulary which encourages correlations between words with similar mean ings regardless of what language each word is in", "For each synset h we create a multilingual word distribution for that synset as follows 1", "Draw transition probabilities h  Dir h "]}, "D10-1034": {"title": ["Clusteringbased Stratified Seed Sampling for SemiSupervised Relation"], "abstract": ["Seed sampling is critical in semisupervised learning", "This paper proposes a clustering based stratified seed sampling approach to semisupervised learning", "First various clustering algorithms are explored to partition the unlabeled instances into different strata with each stratum represented by a center", "Then diversitymotivated intrastratum sampling is adopted to choose the center and additional instances from each stratum to form the unlabeled seed set for an oracle to annotate", "Finally the labeled seed set is fed into a bootstrapping procedure as the initial labeled data", "We systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ACE RDC Relation Detection and Classification task", "In particular we compare various clustering algorithms on the stratified bootstrapping performance", "Experimental results on the ACE RDC 2004 corpus show that our clustering based stratified bootstrapping approach achieves the best F1score of 759 on the sub task of semantic relation classification approaching the one with golden clustering"], "introduction": ["Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text", "Many machine learning approaches have been proposed to attack this problem including supervised Miller et al 2000 Zelenko et al 2003 Culotta and Soresen 2004 Kambhatla 2004 Zhao and Grishman 2005 Zhou et al 2005 Zhang et al 2006 Zhou and Zhang 2007 Zhou et al 2007 Qian et al 2008 Zhou et al 2010 semi supervised Brin 1998 Agichtein and Gravano 2000 Zhang 2004 Chen et al 2006 Qian et al 2009 Zhou et al 2009 and unsupervised methods Hasegawa et al 2004 Zhang et al 2005 Chen et al 2005", "Current work on relation extraction mainly adopts supervised learning methods since they achieve much better performance", "However theynormally require a large number of manually la beled relation instances whose acquisition is both time consuming and labor intensive", "In contrast unsupervised methods do not need any manually labeled instances", "Nevertheless it is difficult to assess their performance due to the lack of evaluation criteria", "As something between them semi supervised learning has received more and more attention recently", "With the plenitude of unlabeled natural language text at hand semisupervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance", "For example Abney 2002 proposes a bootstrapping algorithm which chooses the unlabeled instances with the highest probability of being correctly labeled and add them in turn into the labeled training data iteratively", "This paper focuses on bootstrappingbased semi supervised learning in relation extraction", "Since the performance of bootstrapping depends much on the quality and quantity of the seed set and researchers tend to employ as few seeds as possible eg 100 instances to save time and labor the quality of the seed set plays a critical role in bootstrapping", "Furthermore the imbalance of different classes and 346 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 346355 MIT Massachusetts USA 911 October 2010", "Qc 2010 Association for Computational Linguistics the inherent structural complexity of instances will severely weaken the strength of bootstrapping and semisupervised learning as well", "Therefore it is critical for a bootstrapping procedure to select an appropriate seed set which should be representative and diverse", "However most of current semi supervised relation extraction systems Zhang 2004 Chen et al 2006 use a random seed sampling strategy which fails to fully exploit the affinity nature in the training data to derive the seed set", "Alternatively Zhou et al", "2009 bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM and feed these instances into semisupervised relation extraction", "However their seed set is sequentially generated only to ensure that there are at least 5 instances for each relation class", "Our previous work Qian et al 2009 attempts to solve this problem via a simple stratified sampling strategy for selecting the seed set", "Experimentation on the ACE RDC 2004 corpus shows that the stratified sampling strategy achieves promising results for semisupervised learning", "Nevertheless the success of the strategy relies on the assumption that the true distribution of all relation types is already known which is impractical for real NLP applications", "This paper presents a clusteringbased stratified seed sampling approach for semisupervised relation extraction without the assumption on the true distribution of different relation types", "The motivations behind our approach are that the unlabeled data can be partitioned into a number of strata using a clustering algorithm and that representative and diverse seeds can be derived from such strata in the framework of stratified sampling Neyman 1934 for an oracle to annotate", "Particularly we employ a diversitymotivated intrastratum sampling scheme to pick a center and additional instances as seeds from each stratum", "Experimental results show the effectiveness of the clustering based stratified seed sampling for semisupervised relation classification", "The rest of this paper is organized as follows", "First an overview of the related work is given in Section 2", "Then Section 3 introduces the stratifiedbootstrapping framework including an intra stratum sampling scheme while Section 4 describes various clustering algorithms", "The experimental results on the ACE RDC 2004 corpus are reported in Section 5", "Finally we conclude our work and indicate some future directions in Section 6"]}, "D10-1100": {"title": ["Automatic Detection and Classification of Social Events"], "abstract": ["In this paper we introduce the new task of social event extraction from text", "We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact", "We annotate part of Automatic Content Extraction ACE data and perform experiments using Support Vector Machines with Kernel methods", "We use a combination of structures derived from phrase structure trees and dependency trees", "A characteristic of our events which distinguishes them from ACE events is that the participating entities can be spread far across the parse trees", "We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks", "We also use three data sampling approaches to solve the problem of data skewness", "Sampling methods improve the F1measure for the task of relation detection by over 20 absolute over the baseline"], "introduction": ["This paper introduces a novel natural language processing NLP task social event extraction", "We are interested in this task because it contributes to our overall research goal which is to extract a social network from written text", "The extracted social network can be used for various applications such as summarization questionanswering or the detection of main characters in a story", "For example we manually extracted the social network of characters in Alice in Wonderland and ran standard social network analysis algorithms on the network", "The most influential characters in the story were correctly detected", "Moreover characters occurring in a scene together were given same social roles and positions", "Social network extraction has recently been applied to literary theory Elson et al 2010 and has the potential to help organize novels that are becoming machine readable", "We take a social network to be a network consisting of individual human beings and groups of human beings who are connected to each other by the virtue of participating in social events", "We define social events to be events that occur between people where at least one person is aware of the other and of the event taking place", "For example in the sentence John talks to Mary entities John and Mary are aware of each other and the talking event", "In the sentence John thinks Mary is great only John is aware of Mary and the event is the thinking event", "In the sentence Rabbit ran by Alice there is no evidence about the cognitive states of Rabbit and Alice because the Rabbit could have run by Alice without any one of them noticing each other", "A text can describe a social network in two ways explicitly by stating the type of relationship between two individuals eg husbandwife or implicitly by describing an event which creates or perpetuates a social relationship eg John talked to Mary", "We will call these types of events social events", "We define two types of social events interaction in which both parties are aware of the social event eg a conversation and observation in which only one party is aware of the interaction eg thinking about or 1024 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 10241034 MIT Massachusetts USA 911 October 2010", "Qc 2010 Association for Computational Linguistics spying on someone", "Note that the notion of cognitive state is crucial to our definition", "This paper is the first attempt to detect and classify social events present in text", "Our task is different from related tasks notably from the Automated Content Extraction ACE relation and event extraction tasks because the events are different they are a class of events defined through the effect on participants cognitive state and the linguistic realization is different", "Mentions of entities1 engaged in a social event are often quite distant from each other in the sentence unlike in ACE relations where about 70 of relations are local in our social event annotation only 25 of the events are local", "In fact the average number of words between entities participating in any social event is 9", "We use tree kernel methods on structures derived from phrase structure trees and dependency trees in conjunction with Support Vector Machines SVMs to solve our tasks", "For the design of structures and type of kernel we take motivation from a system proposed by Nguyen et al", "2009 which is a state oftheart system for relation extraction", "Data skew ness turns out to be a big challenge for the task of relation detection since there are many more pairs of entities without a relation as compared to pairs of entities that have a relation", "In this paper we discuss three data sampling techniques that deal with this skewness and allow us to gain over 20 in F1 measure over our baseline system", "Moreover we introduce a new sequence kernel that outperforms previously proposed sequence kernels for the task of social event detection and plays a role to achieve the best performing system for the task of social event detection and classification", "The paper is structured as follows", "In Section 2 we compare our work to existing work notably the ACE extraction literature", "In Section 3 we present our task in detail and explain how we annotated our corpus", "We also show why this is a novel task and how it is different from the ACE extraction tasks", "We then discuss kernel methods and the structures we use and introduce our new structure in Section 4", "In Section 5 we present the sampling methods used for experiments", "In Section 6 we present our exper 1 An entity mention is a reference of an entity in text", "Also we use entities and people interchangeably since the only entities we are interested in are people or groups of people", "iments and results for social event detection and social event classification tasks", "We conclude in Section 7 and mention our future direction of research"]}, "D11-1044": {"title": ["QuasiSynchronous Phrase Dependency Grammars"], "abstract": ["We present a quasisynchronous dependency grammar Smith and Eisner 2006 for machine translation in which the leaves of the tree are phrases rather than words as in previous work Gimpel and Smith 2009", "This formulation allows us to combine structural components of phrasebased and syntaxbased MT in a single model", "We describe a method of extracting phrase dependencies from parallel text using a targetside dependency parser", "For decoding we describe a coarsetofine approach based on lattice dependency parsing of phrase lattices", "We demonstrate performance improvements for ChineseEnglish and UrduEnglish translation over a phrasebased baseline", "We also investigate the use of unsupervised dependency parsers reporting encouraging preliminary results"], "introduction": ["Two approaches currently dominate statistical machine translation MT research", "Phrasebased models Koehn et al 2003 excel at capturing local reordering phenomena and memorizing multiword translations", "Models that employ syntax or syntax like representations Chiang 2005 Galley et al 2006 Zollmann and Venugopal 2006 Huang et al 2006 handle longdistance reordering better than phrasebased systems Auli et al 2009 but often require constraints on the formalism or rule extraction method in order to achieve computational tractability", "As a result certain instances of syntactic divergence are more naturally handled by phrasebased systems DeNeefe et al 2007", "In this paper we present a new way of combining the advantages of phrasebased and syntaxbased MT We propose a model in which phrases are organized into a tree structure inspired by dependency syntax", "Instead of standard dependency trees in which words are vertices our trees have phrases as vertices", "We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side and use them to compute targetside tree features", "We define additional stringtotree features and if a sourceside dependency parser is available treetotree features to capture properties of how phrase dependencies interact with reordering", "To leverage standard phrasebased features alongside our novel features we require a formalism that supports flexible feature combination and efficient decoding", "Quasisynchronous grammar QG provides this backbone Smith and Eisner 2006 we describe a coarsetofine approach for decoding within this framework advancing substantially over earlier QG machine translation systems Gimpel and Smith 2009", "The decoder involves generating a phrase lattice Ueffing et al 2002 in a coarse pass using a phrasebased model followed by lattice dependency parsing of the phrase lattice", "This approach allows us to feasibly explore the combined search space of segmentations phrase alignments and target phrase dependency trees", "Our experiments demonstrate an average improvement of 065 BLEU in ChineseEnglish translation across three test sets and an improvement of 075 BLEU in UrduEnglish translation over a phrasebased baseline", "We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers reporting promising results using a supervised Chinese parser and a stateoftheart unsupervised English parser provides our best results giving an averaged gain of 079 BLEU over the baseline", "We also discuss how our model improves translation quality and discuss future possibilities for combining approaches to ma 474 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 474485 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics chine translation using our framework"]}, "D11-1076": {"title": ["Relation Acquisition using Word Classes and Partial Patterns"], "abstract": ["This paper proposes a semisupervised relation acquisition method that does not rely on extraction patterns eg X causes Y for causal relations but instead learns a combination of indirect evidence for the target relation  semantic word classes and partial patterns", "This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus  in extreme cases patterns that occur only once in the entire corpus", "Such patterns are beyond the reach of current pattern based methods", "We show that our method performs on par with stateoftheart pattern based methods and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns", "This ability to acquire long tail instances is crucial for risk management and innovation where an exhaustive database of highlevel semantic relations like causation is of vital importance"], "introduction": ["Pattern based relation acquisition methods rely on lexicosyntactic patterns Hearst 1992 for extracting relation instances", "These are templates of natural language expressions such as X causes Y  that signal an instance of some semantic relation ie causality", "Pattern based methods Agichtein and Gravano 2000 Pantel and Pennacchiotti 2006b Pasca et al 2006 De Saeger et al 2009 learn many  This work was done when all authors were at the National Institute of Information and Communications Technology", "such patterns to extract new instances word pairs from the corpus", "However since extraction patterns are learned using statistical methods that require a certain frequency of observations pattern based methods fail to capture relations from complex expressions in which the pattern connecting the two words is rarely observed", "Consider the following sentence Curing hypertension alleviates the deterioration speed of the renal function thereby lowering the risk of causing intracranial bleeding Humans can infer that this sentence expresses a causal relation between the underlined noun phrases", "But the actual pattern connecting them ie Curing X alleviates the deterioration speed of the renal function thereby lowering the risk of causing Y  is rarely observed more than once even in a 108 page Web corpus", "In the sense that the term pattern implies a recurring event this expression contains no pattern for detecting the causal relation between hypertension and intracranial bleeding", "This is what we mean by long tail instances  words that cooccur infrequently and only in sparse extraction contexts", "Yet an important application of relation extraction is mining the Web for socalled unknown unknowns  in the words of D Rumsfeld things we dont know we dont know Torisawa et al 2010", "In knowledge discovery applications like risk management and innovation the usefulness of relation extraction lies in its ability to find many unexpected remedies for diseases causes of social problems and so on", "To give an example our relation extrac 825 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 825835 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics tion system found a blog post mentioning Japanese automaker Toyota as a hidden cause of Japans deflation", "Several months later the same connection was made in an article published in an authoritative economic magazine", "We propose a semisupervised relation extraction method that does not rely on direct pattern evidence connecting the two words in a sentence", "We argue that the role of binary patterns can be replaced by a combination of two types of indirect evidence semantic class information about the target relation and partial patterns which are fragments or sub patterns of binary patterns", "The intuition is this if a sentence like the example sentence above contains some word X belonging to the class of medical conditions and another word Y from the class of traumas and X matches the partial pattern ", "causing X there is a decent chance that this sentence expresses a causal relation between X and Y  We show that just using this combination of indirect evidence we can pick up semantic relations with roughly 50 precision regardless of the complexity or frequency of the expression in which the words cooccur", "Furthermore by combining this idea with a straightforward machine learning approach the overall performance of our method is on par with stateoftheart pattern based methods", "However our method manages to extract a large number of instances from sentences that contain no pattern that can be learned by pattern induction methods", "Our method is a twostage system", "Figure 1 presents an overview", "In Stage 1 we apply a state oftheart pattern based relation extractor to a Web corpus to obtain an initial batch of relation instances", "In Stage 2 a supervised classifier is built from various components obtained from the output of Stage 1", "Given the output of Stage 1 and access to a Web corpus the Stage 2 extractor is completely selfsufficient and the whole method requires no supervision other than a handful of seed patterns to start the first stage extractor", "The whole procedure is therefore minimally supervised", "Semantic word classes and partial patterns play a crucial role throughout all steps of the process", "We evaluate our method on three relation acquisition tasks causation prevention and material relations using a 600 million Japanese Web page cor Figure 1 Proposed method data flow", "pus Shinzato et al 2008 and show that our system can successfully acquire relations from both frequent and infrequent patterns", "Our system extracted 100000 causal relations with 846 precision 50000 prevention relations with 584 precision and 25000 material relations with 761 precision", "In the extreme case we acquired several thousand word pairs cooccurring only in patterns that appear once in the entire corpus", "We call such patterns single occurrence SO patterns", "Word pairs that cooccur only with SO patterns represent the theoretical limiting case of relations that cannot be acquired using existing pattern based methods", "In this sense our method can be seen as complementary with pattern based approaches and merging our methods output with that of a pattern based method may be beneficial"]}, "D11-1081": {"title": ["Fast Generation of Translation Forest"], "abstract": ["Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features it is hard to scale up to large data due to decoding complexity", "We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment", "Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation", "With millions of features trained on 519K sentences in 003 second per sentence our system achieves significant improvement by 084 BLEU over the baseline system on the NIST ChineseEnglish test sets"], "introduction": ["Discriminative model Och and Ney 2002 can easily incorporate nonindependent and overlapping features and has been dominating the research field of statistical machine translation SMT in the last decade", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al 2006 Tillmann and Zhang 2006 Watanabe et al 2007 Blunsom et al 2008 Chiang et al 2009", "However the training of the large number of features was always restricted in fairly small data sets", "Some systems limit the number of training examples while others use short sentences to maintain efficiency", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 2007 Chiang et al 2009", "Obviously using much more data can alleviate such problem", "Furthermore large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT", "Despite these advantages to the best of our knowledge no previous discriminative training paradigms scale up to use a large amount of training data", "The main obstacle comes from the complexity of packed forests or nbest lists generation which requires to search through all possible translations of each training example which is computationally prohibitive in practice for SMT", "To make normalization efficient contrastive estimation Smith and Eisner 2005 Poon et al 2009 introduce neighborhood for unsupervised loglinear model and has presented positive results in various tasks", "Motivated by these work we use a translation forest Section 3 which contains both reference derivations that potentially yield the reference translation and also neighboring nonreference derivations that fail to produce the reference translation1However the complexity of generating this translation forest is up to On6 because we still need bi parsing to create the reference derivations", "Consequently we propose a method to fast generate a subset of the forest", "The key idea Section 4 is to initialize a reference derivation tree with maximum score by the help of word alignment and then traverse the tree to generate the subset forest in linear time", "Besides the efficiency improvement such a forest allows us to train the model without resort 1 Exactly there are no reference derivations since derivation is a latent variable in SMT", "We call them reference derivation just for convenience", "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 880888 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics 04 1 2 hyper edge rule 01 5 0 1 24 3 2 3 4 34 6 4 e1 r1 X  X1 bei X2 X1 was X2 e2 r2 X  qiangshou bei X1 t h e g u n m a n w a s X 1  e3 r3 X  jingfang X1 X1 by the police e4 r4 X  jingfang X1 police X1  e5 r5 X  qiangshou the gunman e6 r6 X  jibi shot dead Figure 1 A translation forest which is the running example throughout this paper", "The reference translation is the gunman was killed by the police", "1 Solid hyperedges denote a reference derivation tree t1 which exactly yields the reference translation", "2 Replacing e3 in t1 with e4 results a competing nonreference derivation t2  which fails to swap the order of X34 ", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference Liang et al 2006 Watanabe et al 2007 Chiang et al 2009 which is nontrivial for SMT and needs to be determined experimentally", "Given such forests we globally learn a loglinear model using stochastic gradient descend Section 5", "Overall both the generation of forests and the training algorithm are scalable enabling us to train millions of features on largescale data", "To show the effect of our framework we globally quence of SCFG rules ri", "Translation forest Miet al 2008 Li and Eisner 2009 is a compact repre sentation of all the derivations for a given sentence under an SCFG see Figure 1", "A tree t in the forest corresponds to a derivation", "In our paper tree means the same as derivation", "More formally a forest is a pair V E where V is the set of nodes E is the set of hyperedge", "For a given source sentence f  f n Each node v  V is in the form  which denotes the recognitiontrain millions of word level context features moti vated by word sense disambiguation Chan et al 2007 together with the features used in traditional SMT system Section 6", "Training on 519K sentence pairs in 003 seconds per sentence we achieve significantly improvement over the traditional pipeline by 084 BLEU"]}, "D11-1084": {"title": ["Cachebased Documentlevel Statistical Machine Translation"], "abstract": ["Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time ignoring documentlevel information", "In this paper we propose a cachebased approach to documentlevel translation", "Since caches mainly depend on relevant data to supervise subsequent decisions it is critical to fill the caches with highlyrelevant data of a reasonable size", "In this paper we present three kinds of caches to store relevant documentlevel information 1 a dynamic cache which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document 2 a static cache which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs ie source documents similar to the test document and their corresponding target documents in the training parallel corpus 3 a topic cache which stores the targetside topic words related with the test document in the sourceside", "In particular three new features are designed to explore various kinds of documentlevel information in above three kinds of caches", "Evaluation shows the effectiveness of our cachebased approach to documentlevel translation with the performance improvement of 081 in BLUE score over Moses", "Especially detailed analysis and discussion are presented to give new insights to documentlevel translation"], "introduction": ["During last decade tremendous work has been done to improve the quality of statistical machine  Corresponding author", "translation SMT systems", "However there is still a huge performance gap between the stateofthe art SMT systems and human translators", "Bond 2002 suggested nine ways to improve machine translation by imitating the best practices of human translators Nida 1964 with parsing the entire document before translation as the first priority", "However most SMT systems still treat parallel corpora as a list of independent sentencepairs and ignore documentlevel information", "Documentlevel information can and should be used to help documentlevel machine translation", "At least the topic of a document can help choose specific translation candidates since when taken out of the context from their document some words phrases and even sentences may be rather ambiguous and thus difficult to understand", "Another advantage of documentlevel machine translation is its ability in keeping a consistent translation", "However documentlevel translation has drawn little attention from the SMT research community", "The reasons are manifold", "First of all most of parallel corpora lack the annotation of document boundaries Tam 2007", "Secondly although it is easy to incorporate a new feature into the classical loglinear model Och 2003 it is difficult to capture documentlevel information and model it via some simple features", "Thirdly reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts", "This makes the evaluation of documentlevel SMT systems extremely difficult", "Tiedemann 2010 showed that the repetition and consistency are very important when modeling natural language and translation", "He proposed to employ cachebased language and translation models in a phrasebased SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 909919 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics adaptation", "Especially the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences", "One problem with the dynamic cache is that those initial sentences in a test document may not benefit from the dynamic cache", "Another problem is that the dynamic cache may be prone to noise and cause error propagation", "This explains why the dynamic cache fails to much improve the performance", "This paper proposes a cachebased approach for documentlevel SMT using a static cache and a dynamic cache", "While such a approach applies to both phrasebased and syntaxbased SMT this paper focuses on phrasebased SMT", "In particular the static cache is employed to store relevant bilingual phrase pairs extracted from similar bilingual document pairs ie source documents similar to the test document and their target counterparts in the training parallel corpus while the dynamic cache is employed to store bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document", "In this way our cachebased approach can provide useful data at the beginning of the translation process via the static cache", "As the translation process continues the dynamic cache grows and contributes more and more to the translation of subsequent sentences", "Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple a human translator often collects similar bilingual document pairs to help translation", "If there are translation pairs of sentencesphraseswords in similar bilingual document pairs this makes the translation much easier", "Given a test document our approach imitates this procedure by first retrieving similar bilingual document pairs from the training parallel corpus which has often been applied in IRbased adaptation of SMT systems Zhao et al2004 Hildebrand et al2005 Lu et al2007 and then extracting bilingual phrase pairs from similar bilingual document pairs to store them in a static cacheHowever such a cachebased approach may in troduce many noisyunnecessary bilingual phrase pairs in both the static and dynamic caches", "In order to resolve this problem this paper employs a topic model to weaken those noisyunnecessary bilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the targetside text of similar bilingual document pairs", "Just like a human translator even with a big bilingual dictionary is often confused when he meets a source phrase which corresponds to several possible translations", "In this case some topic words can help reduce the perplexity", "In this paper the topic words are stored in a topic cache", "In some sense it has the similar effect of employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model", "The rest of this paper is organized as follows", "Section 2 reviews the related work", "Section 3 presents our cachebased approach to document level SMT", "Section 4 presents the experimental results", "Session 5 gives new insights on cache based documentlevel translation", "Finally we conclude this paper in Section 6"]}, "D11-1095": {"title": ["Hierarchical Verb Clustering Using Graph Factorization"], "abstract": ["Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data although many manually built classifications are taxonomic in nature", "Also Natural Language Processing NLP applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification", "We introduce a new clustering method called Hierarchical Graph Factorization Clustering HGFC and extend it so that it is optimal for the task", "Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet and that it yields stateoftheart performance also on a flat test set", "We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification"], "introduction": ["A variety of verb classifications have been built to support NLP tasks", "These include syntactic and semantic classifications as well as ones which integrate aspects of both Grishman et al 1994 Miller 1995 Baker et al 1998 Palmer et al 2005 Kipper 2005 Hovy et al 2006", "Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness", "One such classification is the taxonomy of English verbs proposed by Levin 1993 which is based on shared morphosyntactic 1023 and semantic properties of verbs", "Levins taxonomy or its extended version in VerbNet Kipper 2005 has proved helpful for various NLP application tasks including eg parsing word sense disambiguation semantic role labeling information extraction questionanswering and machine translation Swier and Stevenson 2004 Dang 2004 Shi and Mihalcea 2005 Zapirain et al 2008", "Because verbs change their meaning and be haviour across domains it is important to be able to tune existing classifications as well to build novel ones in a costeffective manner when required", "In recent years a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose Schulte im Walde 2006 Joanis et al 2008 Sun et al 2008 Li and Brew 2008 Korhonen et al 2008 O Seaghdha and Copestake 2008 Vlachos et al 2009", "The best of such approaches have yielded promising results", "However they have mostly focussed on acquiring and evaluating flat classifications", "Levins classification is not flat but taxonomic in nature which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification", "In this paper we experiment with hierarchical Levinstyle clustering", "We adopt as our baseline method a wellknown hierarchical method  agglomerative clustering AGG  which has been previously used to acquire flat Levinstyle classifications Stevenson and Joanis 2003 as well as hierarchical verb classifications not based on Levin Fer rer 2004 Schulte im Walde 2008", "The method has also been popular in the related task of noun clus Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 10231033 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics tering Ushioda 1996 Matsuo et al 2006 Bassiou and Kotropoulos 2011", "We introduce then a new method called Hierarchical Graph Factorization Clustering HGFC Yu et al 2006", "This graphbased probabilistic clustering algorithm has some clear advantages over AGG eg it delays the decision on a verbs cluster membership at any level until a full graph is available minimising the problem of error propagation and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons Yu et al 2006", "The method has been applied to the identification of social network communities Lin et al 2008 but has not been used to the best of our knowledge in NLP before", "We modify HGFC with a new tree extraction algorithm which ensures a more consistent result and we propose two novel extensions to it", "The first is a method for automatically determining the tree structure ie number of clusters to be produced for each level of the hierarchy", "This avoids the need to predetermine the number of clusters manually", "The second is addition of soft constraints to guide the clustering performance Vlachos et al 2009", "This is useful for situations where a partial eg a flat verb classification is available and the goal is to extend it", "Adopting a set of lexical and syntactic features which have performed well in previous works we compare the performance of the two methods on test sets extracted from Levin and VerbNet", "When evaluated on a flat clustering task HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set Sun and Korhonen 2009", "When evaluated on a hierarchical task HGFC performs considerably better than AGG at all levels of gold standard classification", "The constrained version of HGFC performs the best as expected demonstrating the usefulness of soft constraints for extending partial classifications", "Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards", "The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members classes and levels of hierarchy"]}, "D11-1119": {"title": ["Exploiting Syntactic and Distributional  Information  for"], "abstract": ["We propose a novel way of incorporating dependency parse and word cooccurrence information into a stateoftheart webscale n gram model for spelling correction", "The syntactic and distributional information provides extra evidence in addition to that provided by a webscale ngram corpus and especially helps with data sparsity problems", "Experimental results show that introducing syntactic features into ngram based models significantly reduces errors by up to 124 over the current stateoftheart", "The word cooccurrence information shows potential but only improves overall accuracy slightly"], "introduction": ["The function of contextsensitive text correction is to identify wordchoice errors in text Bergsma et al 2009", "It can be viewed as a lexical disambiguation task Lapata and Keller 2005 where a system selects from a predefined confusion word set such as affect effect or complement compliment and provides the most appropriate word choice given the context", "Typically one determines if a word has been used correctly based on lexical syntactic and semantic information from the context of the word", "One of the top performing models of spelling correction Bergsma et al 2010 is based on webscale ngram counts which reflect both syntax and meaning", "However even with a largescale ngram corpus data sparsity can hurt performance in two ways", " This work was done when the first author was an intern for Educational Testing Service", "First ngram based methods require exact word and order matches", "If there is a low frequency word in the context such as a persons name there will be little if any evidence in the ngram data to support the usage", "Second if the target confusable word is rare there will not be enough ngram support or training data to render a confident decision", "Because of the data sparsity problem language modeling is not always sufficient to capture the meaning of the sentence and the correct usage of the word", "Take a sentence from The New York Times NYT for example This fellows won a war the dean of the capitals press corps David Broder announced on Meet the Press after complimenting the president on the great sense of authority and command he exhibited in a flight suit Unfortunately neither the phrase complementing the president nor complimenting the president exists in the webscale Google Ngram corpus Brants and Franz 2006", "The ngram models decide solely based on the frequency of the bigrams after compleimenting and compleimenting the which are common usages for both words", "The real question is whether we are more likely to compliment or complement a person the president", "Several clues could help us answer that question", "A dependency parser can identify the word president as the subject of compliment or complement which also may be the case in some of the training data", "Lexical cooccurrence Edmonds 1997 and semantic word relatedness measurements such as Random Indexing Sahlgren 2006 could provide evidence that compliment is more likely to cooccur with president than complement", "Fur 1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 12911300 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics thermore some important clues can be quite distant from the target word eg outside the 9word context window Bergsma et al", "2010 and Carlson 2007 used", "Consider another sentence in the NYT corpus GM says the addition of OnStar which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision complements the Vues top fivestar safety rating for the driver and front passenger in both front and side impact crash tests The dependency parser finds the object of complement is rating which is outside the 9word window", "We propose enhancing stateoftheart webscale ngram models for spelling correction with syntactic structures and distributional information", "For our work we build on a baseline system that combines ngram and lexical features Bergsma et al 2010", "Specifically this paper makes the following contributions 1", "We show that the baseline system can be", "improved by augmenting it with dependency parse features"]}, "D11-1125": {"title": ["Tuning as Ranking"], "abstract": ["We offer a simple effective and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking Herbrich et al 1999", "Unlike the popular MERT algorithm Och 2003 our pairwise ranking optimization PRO method is not limited to a handful of parameters and can easily handle systems with thousands of features", "Moreover unlike recent approaches built upon the MIRA algorithm of Crammer and Singer 2003 Watanabe et al 2007 Chi ang et al 2008b PRO is easy to implement", "It uses offtheshelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours", "We establish PROs scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrasebased and syntaxbased systems in a variety of language pairs using large scale data scenarios"], "introduction": ["The MERT algorithm Och 2003 is currently the most popular way to tune the parameters of a statistical machine translation MT system", "MERT is wellunderstood easy to implement and runs quickly but can behave erratically and does not scale beyond a handful of features", "This lack of scalability is a significant weakness as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation", "Several researchers have attempted to address this weakness", "Recently Watanabe et al", "2007 and Chiang et al", "2008b have developed tuning methods using the MIRA algorithm Crammer and Singer 2003 as a nucleus", "The MIRA technique of Chiang et al has been shown to perform well on largescale tasks with hundreds or thousands of features 2009", "However the technique is complex and architecturally quite different from MERT", "Tellingly in the entire proceedings of ACL 2010 Hajic et al 2010 only one paper describing a statistical MT system cited the use of MIRA for tuning Chiang 2010 while 15 used MERT1 Here we propose a simpler approach to tuning that scales similarly to highdimensional feature spaces", "We cast tuning as a ranking problem Chen et al 2009 where the explicit goal is to learn to correctly rank candidate translations", "Specifically we follow the pairwise approach to ranking Herbrich et al 1999 Freund et al 2003 Burges et al 2005 Cao et al 2007 in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs", "Of primary concern to us is the ease of adoption of our proposed technique", "Because of this we adhere as closely as possible to the established MERT architecture and use freely available machine learning software", "The end result is a technique that scales and performs just as well as MIRAbased tuning but which can be implemented in a couple of hours by anyone with an existing MERT implementation", "Mindful that many wouldbe enhancements to the 1 The remainder either did not specify their tuning method", "though a number of these used the Moses toolkit Koehn et al 2007 which uses MERT for tuning or in one case set weights by hand", "stateoftheart are false positives that only show improvement in a narrowly defined setting or with limited data we validate our claims on both syntax and phrasebased systems using multiple language pairs and large data sets", "We describe tuning in abstract and somewhat formal terms in Section 2 describe the MERT algorithm in the context of those terms and illustrate its scalability issues via a synthetic experiment in Section 3 introduce our pairwise ranking optimization method in Section 4 present numerous largescale MT experiments to validate our claims in Section 5 discuss some related work in Section 6 and conclude in Section 7"]}, "D12-1016": {"title": ["Aligning  Predicates across Monolingual Comparable Texts"], "abstract": ["Generating coherent discourse is an important aspect in natural language generation", "Our aim is to learn factors that constitute coherent discourse from data with a focus on how to realize predicateargument structures in a model that exceeds the sentence level", "We present an important subtask for this overall goal in which we align predicates across comparable texts admitting partial argument structure correspondence", "The contribution of this work is twofold We first construct a large corpus resource of comparable texts including an evaluation set with manual predicate alignments", "Secondly we present a novel approach for aligning predicates across comparable texts using graphbased clustering with Mincuts", "Our method significantly outperforms other alignment techniques when applied to this novel alignment task by a margin of at least 65 percentage points in F1 score"], "introduction": ["Discourse coherence is an important aspect in natural language generation NLG applications", "A number of theories have investigated coherence inducing factors", "A prominent example is Centering Theory Grosz et al 1995 which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse", "A datadriven model based on this theory is the entitybased approach by Barzilay and Lap ata 2008 which models coherence phenomena by observing sentencetosentence transitions of entity occurrences", "Barzilay and Lapata show that their approach can discriminate between a coherent and a noncoherent set of ordered sentences", "However their model is not able to generate alternative entity realizations by itself", "Furthermore the entitybased approach only investigates realization patterns for individual entities in discourse in terms of core grammatical functions", "It does not investigate the interplay between entity transitions and realization patterns for full fledged semantic structures", "This interplay however is an important factor for a semanticsbased generative model of discourse coherence", "The main hypothesis of our work is that we can automatically learn contextspecific realization patterns for predicate argument structures PAS from a semantically parsed corpus of comparable text pairs", "Our assumption builds on the success of previous research where comparable and parallel texts have been exploited for a range of related learning tasks eg unsupervised discourse segmentation Barzilay and Lee 2004 and bootstrapping semantic analyzers Titov and Kozhevnikov 2010", "For our purposes we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events and hence involve the same set of underlying event participants", "By aligning predicates in such texts we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments", "These include the specific forms of argument realization as a pronoun or a specific type of referential expression as studied in prior work in NLG Belz et al 2009 inter alia", "The specific setup we examine however allows us to further investi 171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 171182 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics gate the factors that govern the nonrealization of an argument position as a special form of coherence inducing element in discourse", "Example 1 extracted from our corpus of aligned textsillustrates this point Both texts report on the same event of locating victims in an avalanche", "While 1a explicitly talks about the location of this event the role remains implicit in the second sentence of 1b given that it can be recovered from the preceding sentence", "In fact realization of this argument role would impede the fluency of discourse by being overly repetitive", "1 a   ", "The official said that no bodiesArg1 had been recovered from the avalanchesArg2 which occurred late Friday in the Central Asian country near the Afghan border some 300 kilometers 185 miles southeast of the capital Dushanbe", "b Three other victims were trapped in an avalanche in the village of Khichikh", "None of the victims bodiesArg1 have been found  Argmloc  This phenomenon clearly relates to the problem of discourselinking of implicit roles a very challenging task in discourse processing1 In our work we consider this problem from a contentbased generation perspective concentrating on the discourse factors that allow for the omission of a role", "Thus our aim is to identify comparable predications across aligned texts and to study the discourse coherence factors that determine the realization patterns of arguments in the respective discourses", "This can be achieved by considering the full set of arguments that can be recovered from the aligned predications", "This paper focuses on the first of these tasks henceforth called predicate alignment2 In line with datadriven approaches in NLP we automatically align predicates in a suitable corpus of paired texts", "The induced alignments will i serve to identify events described in both comparable texts and ii provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse", "We investigate a graphbased clustering method for induc 1 See the recent SemEval 2010 task Linking Events and their Participants in Discourse Ruppenhofer et al 2010"]}, "D12-1074": {"title": ["Improving NLP through Marginalization of Hidden Syntactic Structure"], "abstract": ["Many NLP tasks make predictions that are inherently coupled to syntactic relations but for many languages the resources required to provide such syntactic annotations are unavailable", "For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models and what structures in the syntactic trees are most relevant to the current task", "We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task", "Our method couples latent syntactic representations constrained to form valid dependency graphs or constituency parses with the prediction task via specialized factors in a Markov random field", "At both training and test time we marginalize over this hidden structure learning the optimal latent representations for the problem", "Results show that this approach provides significant gains over a syntactically uninformed baseline outperforming models that observe syntax on an English relation extraction task and performing comparably to them in semantic role labeling"], "introduction": ["Many NLP tasks are inherently tied to syntax and stateoftheart solutions to these tasks often rely on syntactic annotations as either a source for useful features Zhang et al 2006 path features in relation extraction or as a scaffolding upon which a more narrow specialized classification can occur as often done in semantic role labeling", "This decou pling of the end task from its intermediate representation is sometimes known as the twostage approach Chang et al 2010 and comes with several drawbacks", "Most notably this decomposition prohibits the learning method from utilizing the labels from the end task when predicting the intermediate representation a structure which must have some correlation to the end task to provide any benefit", "Relying on intermediate representations that are specifically syntactic in nature introduces its own unique set of problems", "Large amounts of syntactically annotated data is difficult to obtain costly to produce and often tied to a particular domain that may vary greatly from that of the desired end task", "Additionally current systems often utilize only a small amount of the annotation for any particular task", "For instance performing named entity recognition NER jointly with constituent parsing has been shown to improve performance on both tasks but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases Finkel and Manning 2009", "By instead discovering a latent representation jointly with the end task we address all of these concerns alleviating the need for any syntactic annotations while simultaneously attempting to learn a latent syntax relevant to both the particular domain and structure of the end task", "We phrase the joint model as factor graph and marginalize over the hidden structure of the intermediate representation at both training and test time to optimize performance on the end task", "Inference is done via loopy belief propagation making this framework trivially extensible to most graph structures", "Computation over latent syntactic rep 810 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 810820 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics resentations is made tractable with the use of special combinatorial factors which implement unlabeled variants of common dynamicprogramming parsing algorithms constraining the hidden representation to realize valid dependency graphs or constituency trees", "We apply this strategy to two common NLP tasks coupling a model for the end task prediction with latent and general syntactic representations via specialized logical factors which learn associations between latent and observed structure", "In comparisons with identical models which observe gold syntactic annotations derived from offtheshelf parsers or provided with the corpora we find that our hidden marginalization method is comparable in both tasks and almost every language tested sometimes significantly outperforming models which observe the true syntax", "The following sections serves as a preliminary introducing an inventory of factors and variables for constructing factor graph representations of syntacticallycoupled NLP tasks", "Section 3 explores the benefits of this method on relation extraction RE where we compare the use dependency and constituency structure as latent representations", "We then turn to a more established semantic role label ing SRL task 4 where we evaluate across a wide range of languages"]}, "D13-1060": {"title": ["Identifying Phrasal Verbs Using Many Bilingual Corpora"], "abstract": ["We address the problem of identifying multiword expressions in a language focusing on English phrasal verbs", "Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages", "Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel rankingoriented boosting algorithm produces a comprehensive set of English phrasal verbs achieving performance comparable to a humancurated set"], "introduction": ["A multiword expression MWE or noncompositional compound is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words", "These idiosyncratic phrases are prevalent in the lexicon of a language Jackendoff 1993 estimates that their number is on the same order of magnitude as that of single words and Sag et al", "2002 suggest that they are much more common though quantifying them is challenging Church 2011", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al 2003 Ren et al 2009 Pal et al 2010 information retrieval Xu et al 2010 Acosta et al 2011 and syntactic parsing Sag et al 2002", "Awareness of MWEs has empirically proven useful in a number of domains Finlayson and Kulkarni 2011 for example use MWEs to attain a significant perfor We focus on a particular subset of MWEs English phrasal verbs", "A phrasal verb consists of a head verb followed by one or more particles such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words Baldwin and Villavicencio 2002 Dixon 1982 Bannard et al 20031 Examples of phrasal verbs include count on rely look after tend or take off remove the meanings of which do not involve counting looking or taking", "In contrast there are verbs followed by particles that are not phrasal verbs because their meaning is compositional such as walk towards sit behind or paint on", "We identify phrasal verbs by using frequency statistics calculated from parallel corpora consisting of bilingual pairs of documents such that one is a translation of the other with one document in English", "We leverage the observation that a verb will translate in an atypical way when occurring as the head of a phrasal verb", "For example the word look in the context of look after will tend to translate differently from how look translates generally", "In order to characterize this difference we calculate a frequency distribution over translations of look then compare it to the distribution of translations of look when followed by the word after", "We expect that idiomatic phrasal verbs will tend to have unexpected translation of their head verbs measured by the KullbackLeibler divergence between those distributions", "Our polyglot ranking approach is motivated by the hypothesis that using many parallel corpora of different languages will help determine the degree of semantic idiomaticity of a phrase", "In order to com mance improvement in word sense disambiguation Venkatapathy and Joshi 2006 use features associated with MWEs to improve word alignment", "Research conducted during an internship at Google", "1 Nomenclature varies the term verbparticle construction", "is also used to denote what we call phrasal verbs further the term phrasal verb is sometimes used to denote a broader class of constructions", "bine evidence from multiple languages we develop a novel boosting algorithm tailored to the task of ranking multiword expressions by their degree of id iomaticity", "We train and evaluate on disjoint subsets of the phrasal verbs in English Wiktionary2", "In our 3 1experiments the set of phrasal verbs identified au tomatically by our method achieves heldout recall that nears the performance of the phrasal verbs in WordNet 30 a humancurated set", "Our approach strongly outperforms a monolingual system and continues to improve when incrementally adding translation statistics for 50 different languages"]}, "D13-1141": {"title": ["Bilingual Word Embeddings for PhraseBased Machine Translation"], "abstract": ["We introduce bilingual word embeddings semantic embeddings associated across two languages in the context of neural language models", "We propose a method to learn bilingual embeddings from a large unlabeled corpus while utilizing MT word alignments to constrain translational equivalence", "The new em beddings significantly outperform baselines in word semantic similarity", "A single semantic similarity feature induced with bilingual em beddings adds near half a BLEU point to the results of NIST08 ChineseEnglish machine translation task"], "introduction": ["It is difficult to recognize and quantify semantic similarities across languages", "The FrEn phrasepair un cas de force majeure case of absolute necessity ZhEn phrase pair persist in a stubborn manner are similar in semantics", "If co occurrences of exact word combinations are rare in the training parallel text it can be difficult for classical statistical MT methods to identify this similarity or produce a reasonable translation given the source phrase", "We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages", "As an extension to their monolingual counterpart Turian et al 2010 Huang et al 2012 Bengio et al 2003 bilingual embeddings capture not only semantic information of monolingual words but also semantic relationships across different languages", "This prop erty allows them to define semantic similarity metrics across phrasepairs making them perfect features for machine translation", "To learn bilingual embeddings we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence", "The latter utilizes word alignments a natural subtask in the machine translation pipeline", "Through large scale curriculum training Bengio et al 2009 we obtain bilingual distributed representations which lie in the same feature space", "Embeddings of direct translations overlap and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus", "Consequently we produce for the research community a first set of Mandarin Chinese word embed dings with 100000 words trained on the Chinese Gigaword corpus", "We evaluate these embedding on Chinese word semantic similarity from SemEval 2012 Jin and Wu 2012", "The embeddings significantly outperform prior work and pruned tfidf baselines", "In addition the learned embeddings give rise to 011 F1 improvement in Named Entity Recognition on the OntoNotes dataset Hovy et al 2006 with a neural network model", "We apply the bilingual embeddings in an endto end phrasebased MT system by computing semantic similarities between phrase pairs", "On NIST08 ChineseEnglish translation task we obtain an improvement of 048 BLEU from a competitive baseline 3001 BLEU to 3049 BLEU with the Stanford Phrasal MT system", "1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 13931398 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics"]}, "D13-1161": {"title": ["Scaling Semantic Parsers with Onthefly Ontology Matching"], "abstract": ["We consider the challenge of learning semantic parsers that scale to large opendomain problems such as question answering with Freebase", "In such settings the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology", "For example even simple phrases such as daughter and number of people living in cannot be directly represented in Freebase whose ontology instead encodes facts about gender parenthood and population", "In this paper we introduce a new semantic parsing approach that learns to resolve such ontological mismatches", "The parser is learned from questionanswer pairs uses a probabilistic CCG to build linguistically motivated logical form meaning representations and includes an ontology matching model that adapts the output logical forms for each target ontology", "Experiments demonstrate stateoftheart performance on two benchmark semantic parsing datasets including a nine point accuracy improvement on a recent Freebase QA corpus"], "introduction": ["Semantic parsers map sentences to formal representations of their underlying meaning", "Recently algorithms have been developed to learn such parsers for many applications including question answering QA Kwiatkowski et al 2011 Liang et al 2011 relation extraction Krishnamurthy and Mitchell 2012 robot control Matuszek et al 2012 Krishnamurthy and Kollar 2013 interpreting instruc tions Chen and Mooney 2011 Artzi and Zettlemoyer 2013 and generating programs Kushman and Barzilay 2013", "In each case the parser uses a predefined set of logical constants or an ontology to construct meaning representations", "In practice the choice of ontology significantly impacts learning", "For example consider the following questions Q and candidate meaning representations MR Q1 What is the population of Seattle", "Q2 How many people live in Seattle", "MR1 xpopulationSeattle x MR2 countxpersonx  livex Seattle A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2 these pairings align constants count person etc directly to phrases How many people etc", "Unfortunately few ontologies have sufficient coverage to support both meaning representations for example many QA databases would only include the population relation required for MR1", "Most existing approaches would given this deficiency simply aim to produce MR1 for Q2 thereby introducing significant lexical ambiguity that complicates learning", "Such ontological mismatches become increasingly common as domain and language complexity increases", "In this paper we introduce a semantic parsing approach that supports scalable opendomain ontological reasoning", "The parser first constructs a linguistically motivated domainindependent meaning representation", "For example possibly producing MR1 for Q1 and MR2 for Q2 above", "It then uses a learned ontology matching model to transform this represen 1545 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 15451556 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics x  How many people visit the public library of New York annually l0  xeqx countypeopley  evisity zpublicz  libraryz  of z new york e  annuallye y  xlibrarypublic library systemannual visitsx new york public library a  13554002 x  What works did Mozart dedicate to Joseph Haydn l0  xworksx  ededicatemozart x e  tohaydn e y  xdedicated workx  ededicated bymozart e  dedicationx e  dedicated tohaydn e a   String Quartet No 19 Haydn Quartets String Quartet No 16 String Quartet No 18 String Quartet No 17  Figure 1 Examples of sentences x domainindependent underspecified logical forms l0 fully specified logical forms y and answers a drawn from the Freebase domain", "tation for the target domain", "In our example producing either MR1 MR2 or another more appropriate option depending on the QA database schema", "This two stage approach enables parsing without any domaindependent lexicon that pairs words with logical constants", "Instead word meaning is filled in onthefly through ontology matching enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains", "Figure 1 shows the desired outputs for two example Freebase sentences", "The first parsing stage uses a probabilistic combinatory categorial grammar CCG Steedman 2000 Clark and Curran 2007 to map sentences to new underspecified logicalform meaning representations containing generic logical constants that are not tied to any specific ontology", "This approach enables us to share grammar structure across domains instead of repeatedly relearning different grammars for each target ontology", "The ontologymatching step considers a large number of typeequivalent domainspecific meanings", "It enables us to incorporate a number of cues including the target ontology structure and lexical similarity between the names of the domainindependent and dependent constants to construct the final logical forms", "During learning we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching", "Following a number of recent approaches Clarke et al 2010 Liang et al 2011 we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs", "This approach aligns naturally with our twostage parsing setup where the final logical expression can be directly used to provide answers", "We report performance on two benchmark datasets GeoQuery Zelle and Mooney 1996 and Freebase QA FQ Cai and Yates 2013a", "Geo Query includes a geography database with a small ontology and questions with relatively complex compositional structure", "FQ includes questions to Freebase a large communityauthored database that spans many subdomains", "Experiments demonstrate stateoftheart performance in both cases including a nine point improvement in recall for the FQ test"]}, "E06-1030": {"title": ["Web Text Corpus for Natural Language Processing"], "abstract": ["Web text has been successfully used as training data for many NLP applications", "While most previous work accesses web text through search engine hit counts we created a Web Corpus by downloading web pages to create a topicdiverse collection of 10 billion words of English", "We show that for contextsensitive spelling correction the Web Corpus results are better than using a search engine", "For thesaurus extraction it achieved similar overall results to a corpus of newspaper text", "With many more words available on the web better results can be obtained by collecting much larger web corpora"], "introduction": ["Traditional written corpora for linguistics research are created primarily from printed text such as newspaper articles and books", "With the growth of the World Wide Web as an information resource it is increasingly being used as training data in Natural Language Processing NLP tasks", "There are many advantages to creating a corpus from web data rather than printed text", "All web data is already in electronic form and therefore readable by computers whereas not all printed data is available electronically", "The vast amount of text available on the web is a major advantage with Keller and Lapata 2003 estimating that over 98 billion words were indexed by Google in 2003", "The performance of NLP systems tends to improve with increasing amount of training data", "Banko and Brill 2001 showed that for context sensitive spelling correction increasing the training data size increases the accuracy for up to 1 billion words in their experiments", "To date most NLP tasks that have utilised web data have accessed it through search engines using only the hit counts or examining a limited number of results pages", "The tasks are reduced to determining ngram probabilities which are then estimated by hit counts from search engine queries", "This method only gathers information from the hit counts but does not require the computationally expensive downloading of actual text for analysis", "Unfortunately search engines were not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations Nakov and Hearst 2005", "Volk 2002 proposed a linguistic search engine to extract word relationships more accurately", "We created a 10 billion word topicdiverse Web Corpus by spidering websites from a set of seed URLs", "The seed set is selected from the Open Directory to ensure that a diverse range of topics is included in the corpus", "A process of text cleaning transforms the HTML text into a form useable by most NLP systems  tokenised words one sentence per line", "Text filtering removes unwanted text from the corpus such as nonEnglish sentences and most lines of text that are not grammatical sentences", "We compare the vocabulary of the Web Corpus with newswire", "Our Web Corpus is evaluated on two NLP tasks", "Contextsensitive spelling correction is a disambiguation problem where the correction word in aconfusion set eg their theyre needs to be se lected for a given context", "Thesaurus extraction is a similarity task where synonyms of a target word are extracted from a corpus of unlabelled text", "Our evaluation demonstrates that web text can be used for the same tasks as search engine hit counts and newspaper text", "However there is a much larger quantity of freely available web text to exploit"]}, "E06-2012": {"title": ["Maytag A multistaged approach to identifying"], "abstract": ["We present a novel application of NLP and text mining to the analysis of financial documents", "In particular we describe an implemented prototype Maytag which combines information extraction and subject classification tools in an interactive exploratory framework", "We present experimental results on their performance as tailored to the financial domain and some forwardlooking extensions to the approach that enables users to specify classifications on the fly"], "introduction": ["Our goal is to support the discovery of complex events in text", "By complex events we mean events that might be structured out of multiple occurrences of other events or that might occur over a span of time", "In financial analysis the domain that concerns us here an example of what we mean is the problem of understanding corporate acquisition practices", "To gauge a companys modus operandi in acquiring other companies it isnt enough to know just that an acquisition occurred but it may also be important to understand the degree to which it was debtleveraged or whether it was performed through reciprocal stock exchanges", "In other words complex events are often composed of multiple facets beyond the basic event itself", "One of our concerns is therefore to enable end users to access complex events through a combination of their possible facets", "Another key characteristic of rich domains like financial analysis is that facts and events are subject to interpretation in context", "To a financial analyst it makes a difference whether a multimilliondollar loss occurs in the context of recurring operations a potentially chronic problem or in the context of a onetime event such as a merger or layoff", "A second concern is thus to enable end users to interpret facts and events through automated context assessment", "The route we have taken towards this end is to model the domain of corporate finance through an interactive suite of language processing tools", "Maytag our prototype makes the following novel contribution", "Rather than trying to model complex events monolithically we provide a range of multipurpose information extraction and text classification methods and allow the end user to combine these interactively", "Think of it as Boolean queries where the query terms are not keywords but extracted facts events entities and contextual text classifications"]}, "E09-1045": {"title": ["An Empirical Study on Classbased Word Sense Disambiguation"], "abstract": ["As empirically demonstrated by the last SensEval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed", "One possible reason could be the use of inappropriate set of meanings", "In fact WordNet has been used as a defacto standard repository of meanings", "However to our knowledge the meanings represented by WordNet have been only used for WSD at a very finegrained sense level or at a very coarsegrained class level", "We suspect that selecting the appropriate level of abstraction could be on between both levels", "We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet", "We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform classbased Word Sense Disambiguation allowing accuracy figures over 80"], "introduction": ["Word Sense Disambiguation WSD is an intermediate Natural Language Processing NLP task which consists in assigning the correct semantic interpretation to ambiguous words in context", "One of the most successful approaches in the last years is the supervised learning from examples in which statistical or Machine Learning classification models are induced from semantically annotated corpora Marquez et al 2006", "Generally supervised systems have obtained better results than the unsupervised ones as shown by experimental work and international evaluation exercises such This paper has been supported by the European Union under the projects QALLME FP6 IST033860 and KYOTO FP7 ICT211423 and the Spanish Government under the project TextMess TIN200615265C0601 and KNOW TIN200615049C0301 as Senseval1", "These annotated corpora are usually manually tagged by lexicographers with word senses taken from a particular lexical semantic resource most commonly WordNet2 WN Fell baum 1998", "WN has been widely criticized for being a sense repository that often provides too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broad coverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Possibly building classbased classifiers would allow to avoid the data sparseness problem of the wordbased approach", "Recently using WN as a sense repository the organizers of the English allwords task at SensEval3 reported an interannotation agreement of 725 Snyder and Palmer 2004", "Interestingly this result is difficult to outperform by stateoftheart sensebased WSD systems", "Thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldovan 2001 Agirre and LopezDeLaCalle 2003 Navigli 2006 and Snow et al 2007", "That is they provide methods for grouping senses of the same word thus producing coarser word sense groupings for better disambiguation", "Wikipedia3 has been also recently used to overcome some problems of automatic learning methods excessively finegrained definition of meanings lack of annotated data and strong domain dependence of existing annotated corpora", "In this way Wikipedia provides a new very large source of annotated data constantly expanded Mihalcea 2007", "1 httpwwwsensevalorg 2 httpwordnetprincetonedu 3 httpwwwwikipediaorg Proceedings of the 12th Conference of the European Chapter of the ACL pages 389397 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics In contrast some research have been focused on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 and Ciaramita and Altun 2006", "That is grouping senses of different words into the same explicit and comprehensive semantic class", "Most of the later approaches used the original Lexicographical Files of WN more recently called SuperSenses as very coarsegrained sense distinctions", "However not so much attention has been paid on learning classbased classifiers from other available sensegroupings such as WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 EuroWordNet Base Concepts Vossen et al 1998 Top Concept Ontology labels Alvez et al 2008 or Basic Level Concepts Izquierdo et al 2007", "Obviously these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD", "Possibly their combination could improve the overall results since they offer different semantic perspectives of the data", "Furthermore to our knowledge to date no comparative evaluation has been performed on SensEval data exploring different levels of abstraction", "In fact Villarejo et al 2005 studied the performance of classbased WSD comparing only SuperSenses and SUMO by 10fold crossvalidation on SemCor but they did not provide results for SensEval2 nor SensEval3", "This paper empirically explores on the supervised WSD task the performance of different levels of abstraction provided by WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 and Basic Level Concepts Izquierdo et al 2007", "We refer to this approach as classbased WSD since the classifiers are created at a class level instead of at a sense level", "Classbased WSD clusters senses of different words into the same explicit and comprehensive grouping", "Only those cases belonging to the same semantic class are grouped to train the classifier", "For example the coarser word grouping obtained in Snow et al 2007 only has one remaining sense for church", "Using a set of Base Level Concepts Izquierdo et al 2007 the three senses of church are still represented by faithn3 buildingn1 and religious ceremonyn1", "The contribution of this work is threefold", "We empirically demonstrate that a Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class based WSD b that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c that the classbased approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers", "After this introduction section 2 presents the sensegroupings used in this study", "In section 3 the approach followed to build the classbased system is explained", "Experiments and results are shown in section 4", "Finally some conclusions are drawn in section 5"]}, "E09-1072": {"title": ["Empirical evaluations of animacy annotation"], "abstract": ["This article presents empirical evaluations of aspects of annotation for the linguistic property of animacy in Swedish ranging from manual human annotation automatic classification and finally an external evaluation in the task of syntactic parsing", "We show that a treatment of animacy as a lexical semantic property of noun types enables generalization over distributional properties of these nouns which proves beneficial in automatic classification and furthermore gives significant improvements in terms of parsing accuracy for Swedish compared to a stateofthe art baseline parser with gold standard ani macy information"], "introduction": ["The property of animacy influences linguistic phenomena in a range of different languages such as case marking Aissen 2003 and argument realization Bresnan et al 2005 de Swart et al 2008 and has been shown to constitute an important factor in the production and comprehension of syntactic structure Branigan et al 2008 Weckerly and Kutas 19991 In computational linguistic work animacy has been shown to provide important information in anaphora resolution Orasan and Evans 2007 argument disambiguation DellOrletta et al 2005 and syntactic parsing in general vrelid and Nivre 2007The dimension of animacy roughly distin guishes between entities which are alive and entities which are not however other distinctions 1 Parts of the research reported in this paper has been supported by the Deutsche Forschungsgemeinschaft DFG Son derforschungsbereich 632 project D4", "are also relevant and the animacy dimension is often viewed as a continuum ranging from humans to inanimate objects", "Following Silverstein 1976 several animacy hierarchies have been proposed in typological studies focusing on the linguistic category of animacy ie the distinctions which are relevant for linguistic phenomena", "An example of an animacy hierarchy taken from Aissen 2003 is provided in 1 1 Human  Animate  Inanimate Clearly nonhuman animates like animals are not less animate than humans in a biological sense however humans and animals show differing linguistic behaviourEmpirical studies of animacy require human an notation efforts and in particular a welldefined annotation task", "However annotation studies of animacy differ distinctly in their treatment of ani macy as a type or tokenlevel phenomenon as well as in terms of granularity of categories", "The use of the annotated data as a computational resource furthermore poses requirements on the annotation which do not necessarily agree with more theoretical considerations", "Methods for the induction of animacy information for use in practical applications require the resolution of issues of level of representation as well as granularityThis article addresses these issues through em pirical and experimental evaluation", "We present an indepth study of a manually annotated data set which indicates that animacy may be treated as a lexical semantic property at the type level", "We then evaluate this proposal through supervised machine learning of animacy information and focus on an indepth error analysis of the resulting classifier addressing issues of granularity of the animacy dimension", "Finally the automatically an Proceedings of the 12th Conference of the European Chapter of the ACL pages 630638 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics notated data set is employed in order to train a syntactic parser and we investigate the effect of the an imacy information and contrast the automatically acquired features with gold standard ones", "The rest of the article is structured as follows", "In section 2 we briefly discuss annotation schemes for animacy the annotation strategies and categories proposed there", "We go on to describe annotation for the binary distinction of human reference found in a Swedish dependency treebank in section 3 and we perform an evaluation of the consistency of the human annotation in terms of linguistic level", "In section 4 we present experiments in lexical acquisition of animacy based on morphosyntactic features extracted from a considerably larger corpus", "Section 5 presents experiments with the acquired animacy information applied in the datadriven dependency parsing of Swedish", "Finally section 6 concludes the article and provides some suggestions for future research"]}, "E12-1020": {"title": ["Compensating for Annotation Errors in Training a Relation Extractor"], "abstract": ["The wellstudied supervised Relation Extraction algorithms require training data that is accurate and has good coverage", "To obtain such a gold standard the common practice is to do independent double annotation followed by adjudication", "This takes significantly more human effort than annotation done by a single annotator", "We do a detailed analysis on a snapshot of the ACE 2005 annotation files to understand the differences between singlepass annotation and the more expensive nearly threepass process and then propose an algorithm that learns from the much cheaper singlepass annotation and achieves a performance on a par with the extractor trained on multipass annotated data", "Furthermore we show that given the same amount of human labor the better way to do relation annotation is not to annotate with highcost quality assurance but to annotate more"], "introduction": ["Relation Extraction aims at detecting and categorizing semantic relations between pairs of entities in text", "It is an important NLP task that has many practical applications such as answering factoid questions building knowledge bases and improving web search", "Supervised methods for relation extraction have been studied extensively since rich annotated linguistic resources eg the Automatic Content Extraction1 ACE training corpus were released", "We will give a summary of related methods in section 2", "Those methods rely on accurate and complete annotation", "To obtain high quality annotation the common wisdom is to let 1 httpwwwitlnistgoviadmigtestsace two annotators independently annotate a corpus and then asking a senior annotator to adjudicate the disagreements 2", "This annotation procedure roughly requires 3 passes3 over the same corpus", "Therefore it is very expensive", "The ACE 2005 annotation on relations is conducted in this way", "In this paper we analyzed a snapshot of ACE training data and found that each annotator missed a significant fraction of relation mentions and annotated some spurious ones", "We found that it is possible to separate most missing examples from the vast majority of truenegative unlabeled examples and in contrast most of the relation mentions that are adjudicated as incorrect contain useful expressions for learning a relation extractor", "Based on this observation we propose an algorithm that purifies negative examples and applies transductive inference to utilize missing examples during the training process on the singlepass annotation", "Results show that the extractor trained on singlepass annotation with the proposed algorithm has a performance that is close to an extractor trained on the 3pass annotation", "We further show that the proposed algorithm trained on a singlepass annotation on the complete set of documents has a higher performance than an extractor trained on 3pass annotation on 90 of the documents in the same corpus although the effort of doing a singlepass annotation over the entire set costs less than half that of doing 3 passes over 90 of the documents", "From the perspective of learning a highperformance relation extractor it suggests that a better way to do relation annotation is not to annotate with a highcost quality assurance but to annotate more"]}, "E99-1007": {"title": ["Automatic  Verb Classification Using"], "abstract": ["We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes based on distributional approximations of diathe ses extracted from a very large anno tated corpus", "Distributions of four gram matical features are sufficient to reduce error rate by 50 over chance", "We con clude that corpus data is a usable repos itory of verb class information and that corpusdriven extraction of grammatical features is a promising methodology for automatic lexical acquisition"], "introduction": ["Recent years have witnessed a shift in grammar development methodology from crafting large grammars to annotation of corpora", "Correspond ingly there has been a change from developing rulebased parsers to developing statistical meth ods for inducing grammatical knowledge from an notated corpus data", "The shift has mostly oc curred because building widecoverage grammars is timeconsuming error prone and difficult", "The same can be said for crafting the rich lexical rep resentations that are a central component of lin guistic knowledge and research in automatic lex ical acquisition has sought to address this Dorr and Jones 1996 Dorr 1997 among others", "Yet there have been few attempts to learn fine grained lexical classifications from the statisti cal analysis of distributional data analogously to the induction of syntactic knowledge though see eg Brent 1993 Klavans and Chodorow 1992 Resnik 1992", "In this paper we propose such an approach for the automatic classification of verbs into lexical semantic classes1 We can express the issues raised by this ap proach as follows", "1", "Which linguistic distinctions among lexical", "classes can we expect to find in a corpus"]}, "E99-1031": {"title": ["A Flexible Architecture for Reference Resolution"], "abstract": ["This paper describes an architecture for performing anaphora resolution in a flexible way", "Systems which con form to these guidelines are well encapsulated and portable and can be used to compare anaphora resolu tion techniques for new language un derstanding applications", "Our im plementation of the architecture in a pronoun resolution testing platform demonstrates the flexibility of the ap proach"], "introduction": ["When building natural language understand ing systems choosing the best technique for anaphora resolution is a challenging task", "The system builder must decide whether to adopt an existing technique or design a new approach", "A huge variety of techniques are described in the literature many of them achieving high suc cess rates on their own evaluation texts cf", "Hobbs 1986 Strube 1998 Mitkov 1998", "Each technique makes different assumptions about the data available to reference resolution for ex ample some assume perfect parses others as sume only POStagged input some assume se mantic information is available etc The chances are high that no published technique will ex actly match the data available to a particular sys tems reference resolution component so it may The authors thank James Allen for help on this project as well as the anonymous reviewers for helpful comments on the paper", "This material is based on work supported by USAFRome Labs contract F3060295l0025 ONR grant N0001495l1088 and Columbia Univ grant OPGI307", "not be apparent which method will work best", "Choosing a technique is especially problematic for designers of dialogue systems trying to pre dict how anaphora resolution techniques devel oped for written monologue will perform when adapted for spoken dialogue", "In an ideal world the system designer would implement and com pare many techniques on the input data available in his system", "As a good software engineer he would also ensure that any pronoun resolution code he implements can be ported to future ap plications or different language domains without modification", "The architecture described in this paper was designed to provide just that functionality", "Anaphora resolution code developed within the architecture is encapsulated to ensure portabil ity across parsers language genres and domains", "Using these architectural guidelines a testbed system for comparing pronoun resolution tech niques has been developed at the University of Rochester", "The testbed provides a highly config urable environment which uses the same pronoun resolution code regardless of the parser frontend and language type under analysis", "It can be used inter alia to compare anaphora resolution tech niques for a given application to compare new techniques to published baselines or to compare a particular techniques performance across lan guage types"]}, "H01-1052": {"title": ["Mitigating the PaucityofData Problem Exploring the"], "abstract": ["In this paper we discuss experiments applying machine learning techniques to the task of confusion set disambiguation using three orders of magnitude more training data than has previously been used for any disambiguationinstringcontext problem", "In an attempt to determine when current learning methods will cease to benefit from additional training data we analyze residual errors made by learners when issues of sparse data have been significantly mitigated", "Finally in the context of our results we discuss possible directions for the empirical natural language research community", "Keywords Learning curves data scaling very large corpora natural language disambiguation"], "introduction": ["A significant amount of work in empirical natural language processing involves developing and refining machine learning techniques to automatically extract linguistic knowledge from online text corpora", "While the number of learning variants for various problems has been increasing the size of training sets such learning algorithms use has remained essentially unchanged", "For instance for the muchstudied problems of part of speech tagging base noun phrase labeling and parsing the Penn Treebank first released in 1992 remains the de facto training corpus", "The average training corpus size reported in papers published in the ACLsponsored Workshop on Very Large Corpora was essentially unchanged from the 1995 proceedings to the 2000 proceedings", "While the amount of available online text has been growing at an amazing rate over the last five years by some estimations there are currently over 500 billion readily accessible words on the web the size of training corpora used by our field has remained static", "Confusable word set disambiguation the problem of choosing the correct use of a word given a set of words with which it is commonly confused eg to too two your youre is a prototypical problem in NLP", "At some level this task is identical to many other natural language problems including word sense disambiguation determining lexical features such as pronoun case and determiner number for machine translation part of speech tagging named entity labeling spelling correction and some formulations of skeletal parsing", "All of these problems involve disambiguating from a relatively small set of tokens based upon a string context", "Of these disambiguation problems lexical confusables possess the fortunate property that supervised training data is free since the differences between members of a confusion set are surfaceapparent within a set of wellwritten text", "To date all of the papers published on the topic of confusion set disambiguation have used training sets for supervised learning of less than one million words", "The same is true for most if not all of the other disambiguationinstringcontext problems", "In this paper we explore what happens when significantly larger training corpora are used", "Our results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues rather than continuing to explore different learning methods applied to the relatively small extant training corpora"]}, "H05-1001": {"title": ["Improving LSAbased Summarization with Anaphora Resolution"], "abstract": ["We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver and using Singular Value Decomposition S V D to identify the main terms", "We demonstrate that adding anaphoric information results in significant performance improvements over a previously developed system in which only lexical terms are used as the input to S V D However we also show that how anaphoric information is used is crucial whereas using this information to add new terms does result in improved performance simple substitution makes the performance worse"], "introduction": ["Many approaches to summarization can be very broadly characterized as T E R M BA S E D they attempt to identify the main topics which generally are T E R M S and then to extract from the document the most important information about these terms Hovy and Lin 1997", "These approaches can be divided again very broadly in lex Kennedy 1999 Azzam et al 1999 Bergler et al 2003 Stuckardt 2003 identify these terms by running a coreference or anaphoric resolver over the text1 We are not aware however of any attempt to use both lexical and anaphoric information to identify the main terms", "In addition to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure a reference corpus and baseline and widely accepted evaluation measures", "In this paper we compare two sentence extraction based summarizers", "Both use Latent Semantic Analysis L S A Landauer 1997 to identify the main terms of a text for summarization however the first system Steinberger and Jezek 2004 discussed in Section 2 only uses lexical information to identify the main topics whereas the second system exploits both lexical and anaphoric information", "This second system uses an existing anaphora resolution system to resolve anaphoric expressions G U I  TA R Poesio and Kabadjov 2004 but crucially two different ways of using this information for summarization were tested", "Section 3", "Both sum marizers were tested over the C A S T corpus Orasan et al 2003 as discussed in Section 4 and sig ical approaches among which we would include L S Abased approaches and coreferencebased approaches  Lexical approaches to termbased sum marization use lexical relations to identify central terms Barzilay and Elhadad 1997 Gong and Liu 2002 coreference or anaphora based approaches Baldwin and Morton 1998 Boguraev and 1 The terms anaphora resolution and coreference resolu", "tion have been variously defined Stuckardt 2003 but the latter term is generally used to refer to the coreference task as defined in M U C and AC E We use the term anaphora resolution to refer to the task of identifying successive mentions of the same discourse entity realized via any type of noun phrase proper noun definite description or pronoun and whether such discourse entities refer to objects in the world or not", "1 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing HLTEMNLP pages 18 Vancouver October 2005", "Qc 2005 Association for Computational Linguistics nificant improvements were observed over both the baseline C A S T system and our previous L S Abased summarizer"]}, "H05-1083": {"title": ["MultiLingual Coreference Resolution With Syntactic  Features"], "abstract": ["In this paper we study the impact of a group of features extracted automatically from machinegenerated parse trees on coreference resolution", "One focus is on designing syntactic features using the binding theory as the guideline to improve pronoun resolution although linguistic phenomenon such as apposition is also modeled", "These features are applied to the Arabic Chinese and English coreference resolution systems and their effectiveness is evaluated on data from the Automatic Content Extraction ACE task", "The syntactic features improve the Arabic and English systems significantly but play a limited role in the Chinese one", "Detailed analyses are done to understand the syntactic features impact on the three coreference systems"], "introduction": ["A coreference resolution system aims to group together mentions referring to the same entity where a mention is an instance of reference to an object and the collection of mentions referring to the same object in a document form an entity", "In the following example I John believes himself to be the best student mentions are underlined", "The three mentions John himself  the best student are of type name pronoun 1  and nominal respectively", "They form an entity since they all refer to the same person", "Syntactic information plays an important role in coreference resolution", "For example the binding theory Haege man 1994 Beatrice and Kroch 2000 provides a good account of the constraints on the antecedent of English pronouns", "The theory relies on syntactic parse trees to determine the governing category which defines the scope 1 Pronoun in this paper refers to both anaphor and normal pronoun", "of binding constraints", "We will use the theory as a guideline to help us design features in a machine learning framework", "Previous pronoun resolution work Hobbs 1976 Lappin and Leass 1994 Ge et al 1998 Stuckardt 2001 explicitly utilized syntactic information before", "But there are unique challenges in this study 1 Syntactic information is extracted from parse trees automatically generated", "This is possible because of the availability of statistical parsers which can be trained on humanannotated tree banks Marcus et al 1993 Xia et al 2000 Maamouri and Bies 2004 for multiple languages 2 The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system 3 The syntactic features are evaluated on three languages Arabic Chinese and English one goal is to see if features motivated by the English language can help coreference resolution in other languages", "All con trastive experiments are done on publiclyavailable data 4 Our coreference system resolves coreferential relationships among all the annotated mentions not just for pronouns", "Using machinegenerated parse trees eliminates the need of handlabeled trees in a coreference system", "However it is a major challenge to extract useful information from these noisy parse trees", "Our approach is encoding the structures contained in a parse tree into a set of computable features each of which is associated with a weight automatically determined by a machine learning algorithm", "This contrasts with the approach of extracting rules and assigning weights to these rules by hand Lap pin and Leass 1994 Stuckardt 2001", "The advantage of our approach is robustness if a particular structure is helpful it will be assigned a high weight if a feature is extracted from a highly noisy parse tree and is not informative in coreference resolution it will be assigned a small weight", "By avoiding writing rules we automatically incorporate useful information into our model and at the same time limit the potentially negative impact from noisy parsing output", "660 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing HLTEMNLP pages 660667 Vancouver October 2005", "Qc 2005 Association for Computational Linguistics"]}, "H89-2014": {"title": ["Augmenting a Hidden Markov Model"], "abstract": [], "introduction": ["The paper describes refinements that are currently being investigated in a model for partofspeech assignment to words in unrestricted text", "The model has the advantage that a pretagged training corpus is not required", "Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabularyindependent model", "State chains are used to model selective higherorder conditioning in the model which obviates the proliferation of parameters attendant in uniformly higherorder models", "The structure of the state chains is based on both an analysis of errors and linguistic knowledge", "Examples show how word dependency across phrases can be modeled"]}, "I05-1065": {"title": ["Anaphora Resolution for Biomedical Literature by"], "abstract": ["In this paper a resolution system is presented to tackle nominal and pronominal anaphora in biomedical literature by using rich set of syntactic and semantic features", "Unlike previous researches the verification of semantic association between anaphors and their antecedents is facilitated by exploiting more outer resources including UMLS WordNet GENIA Corpus 302p and PubMed", "Moreover the resolution is implemented with a genetic algorithm on its feature selection", "Experimental results on different biomedical corpora showed that such approach could achieve promising results on resolving the two common types of anaphora"], "introduction": ["Correct identification of antecedents for an anaphor is essential in message understanding systems as well as knowledge acquisition systems", "For example efficient anaphora resolution is needed to enhance protein interaction extraction from biomedical literature by mining more protein entity instances which are represented with pronouns or general concepts", "In biomedical literature pronominal and nominal anaphora are the two common types of anaphora", "In past literature different strategies to identify antecedents of an anaphor have been presented by using syntactic semantic and pragmatic clues", "For example grammatical roles of noun phrases were used in 9 10", "In addition to the syntactic information statistical information like cooccurring patterns obtained from a corpus is employed during antecedent finding in 3", "However a large corpus is needed for acquiring sufficient cooccurring patterns and for dealing with data sparseness", "On the other hand outer resources like WordNet1 are applied in 41215 and proved to be helpful to improve the system like the one described in 12 where ani macy information is exploited by analyzing the hierarchical relation of nouns and verbs in the surrounding context learned from WordNet", "Nevertheless using Word Net alone for acquiring semantic information is not sufficient for solving unknown words", "To tackle this problem a richer resource the Web was exploited in 16 1 httpwordnetprincetonedu where anaphoric information is mined from Google search results at the expense of less precision", "The domainspecific ontologies like UMLS2 Unified Medical Language System has been employed in 2 in such a way that frequent semantic types associated to agent subject and patient object role of subjectaction or actionobject patterns can be extracted", "The result showed such kind of patterns could gain increase in both precision 76 to 80 and recall 67 to 71", "On the other hand Kim and Park 11 built their BioAR to relate protein names to SWISSProt entries by using the centering theory presented by 7 and salience measures by 2", "In this paper a resolution system is presented for tackling both nominal anaphora and pronominal anaphora in biomedical literature by using various kinds of syntactic and semantic features", "Unlike previous approaches our verification of the semantic association between anaphors and their antecedents is facilitated with the help of both general domain and domainspecific resources", "For example the semantic type checking for resolving nominal anaphora can be done by the domain ontology UMLS and PubMed3 the search engine for MEDLINE databases", "Here UMLS is used not only for tagging the semantic type for the noun phrase chunks if they are in UMLS but also for generating the key lexicons for each type so that we can use them to tag those chunks if they are not in UMLS", "If no type information can be obtained from an chunk then its type finding will be implemented through the web mining of PubMed", "On the other hand the domain corpus GENIA 302p corpus 20 is exploited while we solve the semantic type checking for pronominal anaphora", "With simple weight calculation the key SAAO subjectaction or actionobject patterns for each type can be mined from the corpus and they turn out to be helpful in resolution", "Beside the semantic type agreement the implicit resemblance between an anaphor and its antecedents is another evidence useful for verifying the semantic association", "Hence the general domain thesaurus WordNet which supporting more relationship between concepts and subconcepts is also employed to enhance the resemblance extraction", "The presented resolution system is constructed on a basis of a salience grading", "In order to boost the system we implemented a simple genetic algorithm on its selection of the rich feature set", "The system was developed on the small evaluation corpus MedStract 4  Nevertheless we constructed a larger test corpus denoted as 100MEDLINE so that more instances of anaphors can be resolved", "Experimental results show that our resolution on MedStract can yield 92 and 78 FScores on resolving pronominal and nominal anaphora respectively", "Promising results were also obtained on the larger corpus in terms of 8743 and 8061 Fscores on resloving pronominal and nominal anaphora respectively"]}, "I05-2040": {"title": ["Transformation Based Chinese Entity Detection and Tracking"], "abstract": ["This paper proposes a unified Transformation Based Learning TBL Brill 1995 framework for Chinese Entity Detection and Tracking EDT", "It consists of two sub models a mention detection model and an entity trackingcoreference model", "The first submodel is used to adapt existing Chinese word segmentation and Named Entity NE recognition results to a specific EDT standard to find all the mentions", "The second submodel is used to find the coreference relation between the mentions", "In addition a feedback technique is proposed to further improve the performance of the system", "We evaluated our methods on the Automatic Content Extraction ACE NIST 2003 Chinese EDT corpus", "Results show that it outperforms the baseline and achieves comparable performance with the state oftheart methods"], "introduction": ["The task of Entity Detection and Tracking EDT is suggested by the Automatic Content Extraction ACE project NIST 2003", "The goal is to detect all entities in a given text and track all mentions that refer to the same entity", "The task is a fundamental to many Natural LanguageProcessing NLP applications such as informa tion retrieval and extraction text classification summarization question answering and machine translation", "EDT is an extension of the task of coreference resolution in that in EDT we not only resolve the coreference between mentions but also detect the entities", "Each of those entities may have one or more mentions", "In the ACE project there are five types of entities defined in EDT person PER geography political Entity GPE organization ORG location LOC and facility FAC", "Many traditional coreference techniques can be extended to EDT for entity tracking", "Early work on pronoun anaphora resolution usually uses rulebased methods eg Hobbs 1976 Ge et al 1998 Mitkov 1998 which try to mine the cues of the relation between the pronouns and its antecedents", "Recent research Soon et al 2001 Yang et al 2003 Ng and Cardie 2002 Ittycherah et al 2003 Luo et al 2004 focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases including namenominal and pronoun phrase", "One common ap proach applied by them is to first train a binary statistical model to measure how likely a pair of  This work is done while the first author is visiting Microsoft Research Asia", "mentions corefer and then followed by a greedy procedure to group the mentions into entities", "Mention detection is to find all the named entity noun or noun phrase pronoun or pronoun phrase", "Therefore it needs Named Entity Rec ognition but not only", "Though the detection of entity mentions is an essential problem for EDTcoreference there has been relatively less previous research", "Ng and Cardie 2002 shows that improving the recall of noun phrase identification can improve the performance of a coreference system", "Florian et al", "2004 formulate the mention detection problem as a characterbased classification problem", "They assign for each character in the text a label indicating whether it is the start of a specific mention inside a specific mention or outside of any mention", "In this paper we propose a unified EDT model based on the Transformation Based Learning TBL Brill 1995 framework for Chinese", "The model consists of two sub models a mention detection model and a coreference model", "The first submodel is used to adapt existing Chinese word segmentation and Named Entity NE recognition system to a specific EDT standard", "TBL is a widely used machine learning method but it is the first time it is applied to coreference resolution", "In addition a feedback technique is proposed to further improve the performance of the system", "The rest of the paper is organized as follows", "Raw Document MSRSegPOS Tagging SegPOSNE Document Coreference Model Mentions Entities Mention Detection Model In section 2 we propose the unified TBL Chinese EDT model framework", "We describe the four key techniques of our Chinese EDT the word segmentation adaptation model the mention detection model the coreference model and the feedback technique in section 3 4 5 and 6 accordingly", "The experimental results on the ACE Chinese EDT corpus are shown in section 7"]}, "I05-3013": {"title": [""], "abstract": ["Informal language is actively used in networkmediated communication eg chat room BBS email and text message", "We refer the anomalous terms used in such context as network informal language NIL expressions", "For example ou3 is used to replace  wo3 in Chinese ICQ", "Without unconventional resource knowledge and techniques the existing natural language processing approaches exhibit less effectiveness in dealing with NIL text", "We propose to study NIL expressions with a NIL corpus and investigate techniques in processing NIL expressions", "Two methods for Chinese NIL expression recognition are designed in NILER system", "The experimental results show that pattern matching method produces higher precision and support vector machines method higher F1 measure", "These results are encouraging and justify our future research effort in NIL processing"], "introduction": ["The rapid global proliferation of Internet applications has been showing no deceleration since the new millennium", "For example in commerce more and more physical customer servicescall centers are replaced by Internet solutions eg via MSN ICQ etc Network informal language NIL is actively used in these applications", "Following this trend we forecast that NIL would become a key language for human communication via network", "Today NIL expressions are ubiquitous", "They appear for example in chat rooms BBS email text message etc There is growing importance in understanding NIL expressions from both technology and humanity research points of view", "For instance comprehension of customeroperator dialogues in the aforesaid commercial application would facilitate effective Customer Relationship Management CRM", "Recently sociologists showed many interests in studying impact of networkmediated communication on language evolution from psychological and cognitive perspectives Danet 2002 McElhearn 2000 Nishimura 2003", "Researchers claim that languages have never been changing as fast as today since inception of the Internet and the language for Internet communication ie NIL gets more concise and effective than formal language", "Processing NIL text requires unconventional linguistic knowledge and techniques", "Unfortunately developed to handle formal language text the existing natural language processing NLP approaches exhibit less effectiveness in dealing with NIL text", "For example we use ICTCLAS Zhang et al 2003 tool to process sentence  Is he going to attend a meeting", "The word segmentation result is ", "In this sentence     xi4 ba1 xi4 is a NIL expression which means is he  in this case", "It can be concluded that without identifying the expression further Chinese text processing techniques are not able to produce reasonable result", "This problem leads to our recent research in NIL is Not Nothing project which aims to produce techniques for NIL processing thus avails understanding of change patterns and behaviors in language particularly in Internet language evolution", "The latter could make us more adaptive to the dynamic language environment in the cyber worldRecently some linguistic works have been car ried out on NIL for English", "A shared dictionary has been compiled and made available online", "It contains 308 English NIL expressions including English abbreviations acronyms and emoticons", "Similar efforts for Chinese are rare", "This is because Chinese language has not been widely used on the Internet until ten years ago", "Moreover Chinese NIL expression involves processing of Chinese Pinyin and dialects which results in higher complexity in Chinese NIL processing", "In NIL is Not Nothing project we develop a comprehensive Chinese NIL dictionary", "This is a difficult task because resource of NIL text is rather restricted", "We download a collection of BBS text from an Internet BBS system and construct a NIL corpus by annotating NIL expressions in this collection by hand", "An empirical study is conducted on the NIL expressions with the NIL corpus and a knowledge mining tool is designed to construct the NIL dictionary and generate statistical NIL features automatically", "With these knowledge and resources the NIL processing system ie NILER is developed to extract NIL expressions from NIL text by employing stateoftheart information extraction techniques", "The remaining sections of this paper are organized as follow", "In Section 2 we observe formation of NIL expressions", "In Section 3 we present the related works", "In Section 4 we describe NIL corpus and the knowledge engineering component in NIL dictionary construction and NIL features generation", "In Section 5 we present the methods for NIL expression recognition", "We outline the experiments discussions and error analysis in Section 6 and finally Section 7 concludes the paper"]}, "I08-1004": {"title": ["ContextSensitive Convolution Tree Kernel"], "abstract": ["This paper proposes a contextsensitive convolution tree kernel for pronoun resolution", "It resolves two critical problems in previous researches in two ways", "First given a parse tree and a pair of an anaphor and an antecedent candidate it implements a dynamicexpansion scheme to automatically determine a proper tree span for pronoun resolution by taking predicate and antecedent competitorrelated information into consideration", "Second it applies a contextsensitive convolution tree kernel which enumerates both contextfree and contextsensitive subtrees by considering their ancestor node paths as their contexts", "Evaluation on the ACE 2003 corpus shows that our dynamicexpansion tree span scheme can well cover necessary structured information in the parse tree for pronoun resolution and the contextsensitive tree kernel much outperforms previous tree kernels"], "introduction": ["It is well known that syntactic structured information plays a critical role in many critical NLP applications such as parsing semantic role labeling semantic relation extraction and coreference resolution", "However it is still an open question on what kinds of syntactic structured information are effective and how to well incorporate such structured information in these applications", "Much research work has been done in this direction", "Prior researches apply featurebased methods to select and define a set of flat features which can be mined from the parse trees to represent particular structured information in the parse tree such as the grammatical role eg subject or object according to the particular application", "Indeed such featurebased methods have been widely applied in parsing Collins 1999 Charniak 2001 semantic role labeling Pradhan et al 2005 semantic relation extraction Zhou et al 2005 and coreference resolution Lapin and Leass 1994 Aone and Bennett 1995 Mitkov 1998 Yang et al 2004 Luo and Zitouni 2005 Bergsma and Lin 2006", "The major problem with featurebased methods on exploring structured information is that they may fail to well capture complex structured information which is critical for further performance improvement", "The current trend is to explore kernelbased methods Haussler 1999 which can implicitly explore features in a high dimensional space by employing a kernel to calculate the similarity between two objects directly", "In particular the kernelbased methods could be very effective at reducing the burden of feature engineering for structured objects in NLP eg the parse tree structure in coreference resolution", "During recent years various tree kernels such as the convolution tree kernel Collins and Duffy 2001 the shallow parse tree kernel Zelenko et al 2003 and the dependency tree kernel Culota and Sorensen 2004 have been proposed in the literature", "Among previous tree kernels the convolution tree kernel represents the stateoftheart and have been successfully applied by Collins and Duffy 2002 on parsing Moschitti 2004 on semantic role labeling Zhang et al 2006 on semantic relation extraction and Yang et al 2006 on pronoun resolution", "However there exist two problems in Collins and Duffys kernel", "The first is that the subtrees enumerated in the tree kernel are contextfree", "That is each subtree enumerated in the tree kernel does not consider the context information outside the subtree", "The second is how to decide a proper tree span in the tree kernel computation according to the particular application", "To resolve above two problems this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic structured information in pronoun resolution whose task is to find the corresponding antecedent for a given pronominal anaphor in text", "The rest of this paper is organized as follows", "In Section 2 we review related work on exploring syntactic structured information in pronoun resolution and their comparison with our method", "Section 3 first presents a dynamicexpansion tree span scheme by automatically expanding the shortest path to include necessary structured information such as predicate and antecedent competitor related information", "Then it presents a context sensitive convolution tree kernel which not only enumerates contextfree subtrees but also context sensitive subtrees by considering their ancestor node paths as their contexts", "Section 4 shows the experimental results", "Finally we conclude our work in Section 5"]}, "I08-2080": {"title": ["Japanese Named Entity Recognition"], "abstract": ["This paper presents an approach that uses structural information for Japanese named entity recognition NER", "Our NER system is based on Support Vector Machine SVM and utilizes four types of structural information cache features coreference relations syntactic features and caseframe features which are obtained from structural analyses", "We evaluated our approach on CRL NE data and obtained a higher Fmeasure than existing approaches that do not use structural information", "We also conducted experiments on IREX NE data and an NEannotated web corpus and conrmed that structural information improves the performance of NER"], "introduction": ["Named entity recognition NER is the task of identifying and classifying phrases into certain classes of named entities NEs such as names of persons organizations and locations", "Japanese texts which we focus on are written without using blank spaces", "Therefore Japanese NER has tight relation with morphological analysis and thus it is often performed immediately after morphological analysis Masayuki and Matsumoto 2003 Yamada 2007", "However such approaches rely only on local context", "The Japanese NER system proposed in Nakano and Hirai 2004 which achieved the highest Fmeasure among conventional systems introduced the bunsetsu1 feature in order to consider wider context but considers only adjacent bunsetsus", "Research Fellow of the Japan Society for the Promotion of Science JSPS 1 Bunsetsu is a commonly used linguistic unit in Japanese consisting of one or more adjacent content words and zero or more following functional words", "On the other hand as for English or Chinese various NER systems have explored global information and reported their effectiveness", "In Malouf 2002 Chieu and Ng 2002 information about features assigned to other instances of the same token is utilized", "Ji and Grishman 2005 uses the information obtained from coreference analysis for NER", "Mohit and Hwa 2005 uses syntactic features in building a semisupervised NE tagger", "In this paper we present a Japanese NER system that uses global information obtained from several structural analyses", "To be more specic our system is based on SVM recognizes NEs after syntactic case and coreference analyses and uses information obtained from these analyses and the NER results for the previous context integrally", "At this point it is true that NER results are useful for syntactic case and coreference analyses and thus these analyses and NER should be performed in a complementary way", "However since we focus on NER we recognize NE after these structural analyses"]}, "I08-4015": {"title": ["The Characterbased CRF Segmenter of MSRANEU"], "abstract": ["This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff", "Base on Conditional Random Field CRF model a basic segmenter is designed as a problem of characterbased tagging", "To further improve the performance of our segmenter we employ a wordbased approach to increase the invocabulary IV word recall and a postprocessing to increase the outofvocabulary OOV word recall", "We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora"], "introduction": ["Since Chinese Word Segmentation was firstly treated as a characterbased tagging task in Xue and Converse 2002 this method has been widely accepted and further developed by researchers Peng et al 2004 Tseng et al 2005 Low et al 2005 Zhao et al 2006", "Thus as a powerful sequence tagging model CRF became the dominant method in the Bakeoff 2006 Levow 2006In this paper we improve basic segmenter un der the CRF work frame in two aspects namely IV and OOV identification respectively", "We use the result from wordbased segmentation to revise the CRF output so that we gain a higher IV word recall", "For the OOV part a postprocessing rule is proposed to find those OOV words which are wrongly segmented into several fractions", "Our system performs well in the Fourth Bakeoff achieving four second best and on the fifth in all the five corpora", "In the following of this paper we describe our method in more detail", "The rest of this paper is organized as follows", "In Section 2 we first give a brief review to the basic CRF tagging approach and then we propose our methods to improve IV and OOV performance respectively", "In Section 3 we give the experiment results on the fourth Bakeoff corpora to show that our method is effective to improve the performance of the segmenter", "In Section 4 we conclude our work"]}, "J00-4003": {"title": ["An Empirically Based System for Processing Definite Descriptions"], "abstract": ["We present an implemented system for processing definite descriptions in arbitrary domains", "The design of the system is based on the results of a corpus analysis previousl y reported which highlighted the prevalence of discoursenew descriptions in newspa per corpora", "The annotated corpus was used to extensivel y evaluate the pro posed techniques for matching definite descriptions with their antecedents discourse segmentation recognizing discoursenew descriptions and suggesting anchors for bridging descriptions"], "introduction": ["Most models of definite description processing proposed in the literature tend to emphasise the anaphoric role of these elements1 Heim 1982 is perhaps the best for malization of this type of theory", "This approach is challenged by the results of exper iments we reported previously Poesio and Vieira 1998 in which subjects were asked to classify the uses of definite descriptions in Wall Street Journal articles according to schemes derived from proposals by Hawkins 1978 and Prince 1981", "The results of these experiments indicated that definite descriptions are not primarily anaphoric about half of the time they are used to introduce a new entity in the discourse", "In this paper we present an implemented system for processing definite descriptions based on the results of that earlier study", "In our system techniques for recognizing discoursenew descriptions play a role as important as techniques for identifying the antecedent of anaphoric ones", "A central characteristic of the work described here is that we intended from the start to develop a system whose performance could be evaluated using the texts an notated in the experiments mentioned above", "Assessing the performance of an NLP system on a large number of examples is increasingly seen as a much more thorough evaluation of its performance than trying to come up with counterexamples it is con sidered essential for language engineering applications", "These advantages are thought by many to offset some of the obvious disadvantages of this way of developing NLP theoriesin particular the fact that given the current state of language processing technology many hypotheses of interest cannot be tested yet see below", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic  Univcrsidade do Vale do Rio dos SinosUNISINOS Av", "Unisinos 950Cx", "Postal 275 93022000 Sao Leopoldo RS Brazil", "Email renataexatasunisinosbr t University of Edinburgh ICCS and Informatics 2 Buccleuch Place EHS 9LW Edinburgh UK", "Email MassimoPoesioedacuk 1 We use the term definite description Russell 1905 to indicate definite noun phrases with the definite", "article the such as the car", "We are not concerned with other types of definite noun phrases such as pronouns demonstratives or possessive deiptions", "Anaphoric expressions are those linguistic expressions used to signat evoke or refer to previously mentioned entities", " 2001 Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd ing Conferences MUC6 and MUC7 Sundheim 1995 Chinchor 1997 which also included evaluations of systems on the socalled coreference task a subtask of which is the resolution of definite descriptions", "The system we present was developed to be evaluated in a quantitative fashion as well but because of the problems concerning agreement between annotators observed in our previous study we evaluated the sys tem both by measuring precisionrecall against a gold standard as done in MUC and by measuring agreement between the annotations produced by the system and those proposed by the annotators", "The decision to develop a system that could be quantitatively evaluated on a large number of examples resulted in an important constraint we could not make use of inference mechanisms such as those assumed by traditional computational theories of definite description resolution eg Sidner 1979 Carter 1987 Alshawi 1990 Poesio 1993", "Too many facts and axioms would have to be encoded by hand for theories of this type to be tested even on a mediumsized corpus", "Our system therefore is based on a shallowprocessing approach more radical even than that attempted by the first advocate of this approach Carter 1987 or by the systems that participated in the MUC evaluations Appelt et al 1995 Gaizaukas et al 1995 Humphreys et al 1988 since we made no attempt to finetune the system to maximize performance on a particular domain", "The system relies only on structural information on the in formation provided by preexisting lexical sources such as WordNet Fellbaum 1998 on minimal amounts of general handcoded information or on information that could be acquired automatically from a corpus", "As a result the system does not really have the resources to correctly resolve those definite descriptions whose interpretation does require complex reasoning we grouped these in what we call the bridging class", "We nevertheless developed heuristic techniques for processing these types of definites as well the idea being that these heuristics may provide a baseline against which the gains in performance due to the use of commonsense knowledge can be assessed more clearly2 The paper is organized as follows We first summarize the results of our previous corpus study Poesio and Vieira 1998 Section 2 and then discuss the model of defi nite description processing that we adopted as a result of that work and the general architecture of the system Section 3", "In Section 4 we discuss the heuristics that we developed for resolving anaphoric definite descriptions recognizing discoursenew descriptions and processing bridging descriptions and in Section 5 how the per formance of these heuristics was evaluated using the annotated corpus", "Finally we present the final configuration of the two versions of the system that we developed Section 6 review other systems that perform similar tasks Section 7 and present our conclusions and indicate future work Section 8"]}, "J01-2004": {"title": ["Probabilistic TopDown Parsing and Language Modeling"], "abstract": ["This pa per describes thefunctioning of a broadcoverage probabilistic topdown parser and its application to the problem of language modelingfor speech recognition", "The pa per first introduces key notions in language modeling and probabilistic parsing and briefly reviews some previous approaches to using syntactic structure for language modeling", "A lexicalized probabilistic top down parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they arefound relative to the best broadcoverage statistical parsers", "A new language model that utilizes probabilistic topdown parsing is then outlined and empirical results show that it improves upon previous work in test corpus per plexit y Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model", "A small recognition experiment also demonstrates the utility of the model"], "introduction": ["With certain exceptions computational linguists have in the past generally formed a separate research community from speech recognition researchers despite some obvious overlap of interest", "Perhaps one reason for this is that until relatively re cently few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems", "In the past few years however some improve ments have been made over these language models through the use of statistical meth ods of natural language processing and the development of innovative linguistically wellmotivated techniques for improving language models for speech recognition is generating more interest among computational linguists", "While language models built around shallow local dependencies are still the standard in stateoftheart speech recognition systems there is reason to hope that better language models can and will be developed by computational linguists for this task", "This paper will examine language modeling for speech recognition from a nat ural language processing point of view", "Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture longdistance depen dencies for language modeling will be reviewed", "A new language model based on probabilistic topdown parsing will be outlined and compared with the previous liter ature and extensive empirical results will be presented which demonstrate its utility", "Two features of our topdown parsing approach will emerge as key to its success", "First the topdown parsing algorithm builds a set of rooted candidate parse trees from left to right over the string which allows it to calculate a generative probability for  Department of Cognitive and Linguistic Sciences Box 1978 Brown University Providence RI 02912  2001 Association for Computational Linguistics each prefix string from the probabilistic grammar and hence a conditional probability for each word given the previous words and the probabilistic grammar", "A leftto right parser whose derivations are not rooted ie with derivations that can consist of disconnected tree fragments such as an LR or shiftreduce parser cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar because their derivations include probability mass from unrooted structures", "Only at the point when their derivations become rooted at the end of the string can generative string probabilities be calculated from the grammar", "These parsers can calculate word probabilities based upon the parser stateas in Chelba and Jelinek 1998abut such a distribution is not generative from the probabilistic grammar", "A parser that is not left to right but which has rooted derivations eg a head first parser will be able to calculate generative joint probabilities for entire strings however it will not be able to calculate probabilities for each word conditioned on previously generated words unless each derivation generates the words in the string in exactly the same order", "For example suppose that there are two possible verbs that could be the head of a sentence", "For a headfirst parser some derivations will have the first verb as the head of the sentence and the second verb will be generated after the first hence the second verbs probability will be conditioned on the first verb", "Other derivations will have the second verb as the head of the sentence and the first verbs probability will be conditioned on the second verb", "In such a scenario there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule", "Of course the joint probability can be used as a language model but it cannot be interpolated on a wordbyword basis with say a trigram model which we will demonstrate is a useful thing to do", "Thus our topdown parser allows for the incremental calculation of generative conditional word probabilities a property it shares with other lefttoright parsers with rooted derivations such as Earley parsers Earley 1970 or leftcorner parsers Rosenkrantz and Lewis II 1970", "A second key feature of our approach is that topdown guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model", "Because the rooted partial derivation is fully connected all of the conditioning information that might be extracted from the topdown left context has already been specified and a conditional probability model built on this information will not impose any additional burden on the search", "In contrast an Earley or leftcorner parser will underspecify certain connections between constituents in the left context and if some of the underspecified information is used in the conditional probability model it will have to become specified", "Of course this can be done but at the expense of search efficiency the more that this is done the less benefit there is from the underspecification", "A topdown parser will in contrast derive an efficiency benefit from precisely the information that is underspecified in these other approaches", "Thus our topdown parser makes it very easy to condition the probabilistic gram mar on an arbitrary number of values extracted from the rooted fully specified deriva tion", "This has lead us to a formulation of the conditional probability model in terms of values returned from treewalking functions that themselves are contextually sen sitive", "The topdown guidance that is provided makes this approach quite efficient in practice", "The following section will provide some background in probabilistic contextfree grammars and language modeling for speech recognition", "There will also be a brief review of previous work using syntactic information for language modeling before we introduce our model in Section 4", "a st I s b st S STOP  c  s t S STOP NP VP I NP V P I s NP VP I Spot VBD NP I chased DT NN I I the ball Spot VBD JP I chased DT NN I I the ball Spot Figure 1 Three parse trees a a complete parse tree b a complete parse tree with an explicit stop symbol and c a partial parse tree"]}, "J98-1006": {"title": ["Using  Corpus Statistics  and WordNet"], "abstract": ["Corpusbased approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneckWe show how knowledgebased techniques can be used to open the bottleneck by automatically locating training corpora", "We describe a statistical classifier that combines topical context with local cues to identify a word sense", "The classifier is used to disambiguate a nouna verband an adjective", "A knowledge base in the form ofWordNets lexical relations is used to automatically locate training examples in a general text corpus", "Test results are compared with those from manually tagged training examples"], "introduction": ["An impressive array of statistical methods have been developed for word sense identi fication", "They range from dictionarybased approaches that rely on definitions Veronis and Ide 1990 Wilks et al 1993 to corpusbased approaches that use only word co occurrence frequencies extracted from large textual corpora Schiitze 1995 Dagan and Itai 1994", "We have drawn on these two traditions using corpusbased cooccurrence and the lexical knowledge base that is embodied in the WordNet lexicon", "The two traditions complement each other", "Corpusbased approaches have the advantage of being generally applicable to new texts domains and corpora without needing costly and perhaps errorprone parsing or semantic analysis", "They require only training corpora in which the sense distinctions have been marked but therein lies their weakness", "Obtaining training materials for statistical methods is costly and time consumingit is a knowledge acquisition bottleneck Gale Church and Yarowsky 1992a", "To open this bottleneck we use WordNets lexical relations to locate unsuper vised training examples", "Section 2 describes a statistical classifier TLC TopicalLocal Classifier that uses topical context the openclass words that cooccur with a particular sense local con text the open and closedclass items that occur within a small window around a word or a combination of the two", "The results of combining the two types of context to disambiguate a noun line a verb serve and an adjective hard are presented", "The following questions are discussed When is topical context superior to local context and vice versa", "Is their combination superior to either type alone", "Do the answers to these questions depend on the size of the training", "Do they depend on the syntactic category of the target", " Division of Cognitive and Instructional Science Princeton NJ 08541 email cleacocketsorg", "The work reported here was done while the author was at Princeton University", "t Department of Psychology 695 Park Avenue New York NY 10021 email mschccunyvmcunyedu j Cognitive Science Laboratory 221 Nassau Street Princeton NJ 08542 email geoclarityprincetonedu  1998 Association for Computational Linguistics Manually tagged training materials were used in the development of TLC and the experiments in Section 2", "The Cognitive Science Laboratory at Princeton Univer sity with support from NSFARPA is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation", "Examples of the different meanings of one thousand common polysemous openclass English words are being manually tagged", "The results of this effort will be a useful resource for train ing statistical classifiers but what about the next thousand polysemous words and the next", "In order to identify senses of these words it will be necessary to learn how to harvest training examples automatically", "Section 3 describes WordNets lexical relations and the role that monosemous relatives of polysemous words can play in creating unsupervised training materials", "TLC is trained with automatically extracted examples its performance is compared with that obtained from manually tagged training materials"]}, "N01-1012": {"title": ["An Algorithm for Aspects of Semantic Interpretation Using an"], "abstract": ["An algorithm for semantic interpretation is explained", "The algorithm is based on predicatesdefined for WordNet verb classes", "The algorithm is driven by the definition of these predicates whose thematic roles are linked to theWordNet ontology for nouns and to the syntactic relations that realize them", "The algorithm has been tested in the identification of the meaning of the verb thematic roles and temporal and spatial adjuncts"], "introduction": ["The semantic interpretation algorithm ex plained in this paper offers a solution to thefollowing interpretation problems determina tion of the meaning of the verb identification of thematic roles and adjuncts and attachments of prepositional phrases PPs", "An interesting aspect of the algorithm is that the solution ofall these problems is interdependent", "The inter pretation algorithm uses WordNet Miller et al 1993 as its lexical knowledgebase", "Predicatesor verbal concepts have been defined for Word Net verb classes which have been reorganized considerably following the criteria imposed bythe interpretation algorithm", "WordNet ontology for nouns has also undergone some reorga nization and redefinition to conform with the entries in the thematic roles of the predicates", "One of the views that guides this research is that the syntax of many verbs is determined by theirmeaning", "Some verbs that are highly ambigu ous say more than 10 senses and light verbswhich do not lexicalize anything will need spe cial definitions", "Briefly the algorithm is as follows", "For every verb in a sentence WordNet provides a list ofverb synsets for which we have defined predicates", "These predicates can be viewed as contenders for the meaning of the verb", "As syntac tic relations are parsed the interpreter checks each predicate in order to see if the predicatehas a thematic role which is realized by the syn tactic relation", "If so the interpreter records this fact and gets the next syntactic relation", "Thepredicate that realizes the most syntactic rela tions in the sentence is selected as the meaning of the verb", "This paper is organized as follows", "The first part of the paper  sections 2 to 5  explains themethodology for building predicates for Word Net verb classes and the second part  sections 6 to 9  describes the semantic interpretation algorithm testing and conclusions"]}, "N03-2035": {"title": ["A ContextSensitive  Homograph Disambiguation"], "abstract": ["Homograph ambiguity is an original issue in TexttoSpeech TTS", "To disambiguate homograph several efficient approaches have been proposed such as partofspeech POS ngram Bayesian classifier decision tree and Bayesianhybrid approaches", "These methods need words orand POS tags surrounding the question homographs in disambiguation", "Some languages such as Thai Chinese and Japanese have no wordboundary delimiter", "Therefore before solving homograph ambiguity we need to identify word boundaries", "In this paper we propose a unique framework that solves both word segmentation and homograph ambiguity problems altogether", "Our model employs both local and long distance contexts which are automatically extracted by a machine learning technique called Winnow"], "introduction": ["In traditional Thai TTS it consists of four main modules word segmentation graphemetophoneme prosody generation and speech signal processing", "The accuracy of pronunciation in Thai TTS mainly depends on accuracies of two modules word segmentation and graphemetophoneme", "In word segmentation process if word boundaries cannot be identified correctly it leads Thai TTS to the incorrect pronunciation such as a string  which can be separated into two different ways with different meanings and pronunciations", "The first one is eye round pronounced ta0 klom0and the other one is expose wind pronounced tak1 lom0", "In graphemetophoneme mod ule it may produce error pronunciations for a homograph which can be pronounced more than one way such as a word  which can be pronounced phlaw0 or phe0 la0", "Therefore to improve an accuracy of Thai TTS we have to focus on solving the problems of word boundary ambiguity and homograph ambiguity which can be viewed as a disambiguation task", "A number of featurebased methods have been tried for several disambiguation tasks in NLP including decision lists Bayesian hybrids and Winnow", "These methods are superior to the previously proposed methods in that they can combine evidence from various sources in disambiguation", "To apply the methods in our task we treat problems of word boundary and homograph ambiguity as a task of word pronunciation disambiguation", "This task is to decide using the context which was actually intended", "Instead of using only one type of syntactic evidence as in Ngram approaches we employ the synergy of several types of features", "Following previous works 4 6 we adopted two types of features context words and collections", "Contextword feature is used to test for the presence of a particular word within  K words of the target word and collocation test for a pattern of up to L contiguous words andor partofspeech tags surrounding the target word", "To automatically extract the discriminative features from feature space and to combine them in disambiguation we have to investigate an efficient technique in our task", "The problem becomes how to select and combine various kinds of features", "Yarowsky 11 proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is Golding 3 proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one", "The method was applied to the task of contextsentitive spelling correction and was reported to be superior to decision lists", "Later Golding and Roth 4 applied Winnow algorithm in the same task and found that the algorithm performs comparably to the Bayesian hybrid method when using pruned feature sets and is better when using unpruned sets or unfamiliar test set", "In this paper we propose a unified framework in solving the problems of word boundary ambiguity and homograph ambiguity altogether", "Our approach employs both local and longdistance contexts which can be automatically extracted by a machine learning technique", "In this task we employ the machine learning technique called Winnow", "We then construct our system based on the algorithm and evaluate them by comparing with other existing approaches to Thai homograph problems"]}, "N04-1016": {"title": ["The Web as a Baseline Evaluating the Performance of"], "abstract": ["Previous work demonstrated that web counts can be used to approximate bigram frequencies and thus should be useful for a wide variety of NLP tasks", "So far only two generation tasks candidate selection for machine translation and confusionset disambiguation have been tested using webscale data sets", "The present paper investigates if these results generalize to tasks covering both syntax and semantics both generation and analysis and a larger range of ngrams", "For the majority of tasks we fi nd that simple unsupervised models perform better when ngram frequencies are obtained from the web rather than from a large corpus", "However in most cases webbased models fail to outperform more sophisticated stateofthe art models trained on small corpora", "We argue that webbased models should therefore be used as a baseline for rather than an alternative to standard models"], "introduction": ["Keller and Lapata 2003 investigated the validity of web counts for a range of predicateargument bigrams verb object adjectivenoun and nounnoun bigrams", "They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts a correlate with frequencies obtained from a carefully edited balanced corpus such as the 100M words British National Corpus BNC b correlate with frequencies recreated using smoothing methods in the case of unseen bigrams c reliably predict human plausibility judgments and d yield stateoftheart performance on pseudodisambiguation tasks", "Keller and Lapatas 2003 results suggest that web based frequencies can be a viable alternative to bigram frequencies obtained from smaller corpora or recreated using smoothing", "However they do not demonstrate that realistic NLP tasks can benefi t from web counts", "In order to show this web counts would have to be applied to a diverse range of NLP tasks both syntactic and seman Task n POS Ling Type MT candidate select", "12 V N Sem Generation Spelling correction 123 Any SynSem Generation Adjective ordering 12 Adj Sem Generation Compound bracketing 12 N Syn Analysis Compound interpret", "123 N P Sem Analysis Countability detection 12 N Det Sem Analysis Table 1 Overview of the tasks investigated in this paper n size of ngram POS parts of speech Ling linguistic knowledge Type type of task tic involving analysis eg disambiguation and generation eg selection among competing outputs", "Also it remains to be shown that the webbased approach scales up to larger ngrams eg trigrams and to combinations of different parts of speech Keller and Lapata 2003 only tested bigrams involving nouns verbs and adjectives", "Another important question is whether webbased methods which are by defi nition unsupervised can be competitive alternatives to supervised approaches used for most tasks in the literature", "This paper aims to address these questions", "We start by using web counts for two generation tasks for which the use of large data sets has shown promising results a target language candidate selection for machine translation Grefenstette 1998 and b context sensitive spelling correction Banko and Brill 2001ab", "Then we investigate the generality of the webbased approach by applying it to a range of analysis and generations tasks involving both syntactic and semantic knowledge c ordering of prenominal adjectives d compound noun bracketing e compound noun interpretation and f noun count ability detection", "Table 1 gives an overview of these tasks and their properties", "In all cases we propose a simple unsupervised ngram based model whose parameters are estimated using web counts", "We compare this model both against a baseline same model but parameters estimated on the BNC and against stateoftheart models from the literature which are either supervised ie use annotated training data or unsupervised but rely on taxonomies to recreate missing counts"]}, "N06-1017": {"title": ["Unknown word sense detection as outlier detection"], "abstract": ["We address the problem of unknown word sense detection the identification of corpus occurrences that are not covered by a given sense inventory", "We model this as an instance of outlier detection using a simple nearest neighborbased approach to measuring the resemblance of a new item to a training set", "In combination with a method that alleviates data sparseness by sharing training data across lemmas the approach achieves a precision of 077 and recall of 082"], "introduction": ["If a system has seen only positive examples how does it recognize a negative example", "This is the problem addressed by outlier detection also called novelty detection1 Markou and Singh 2003a Markou and Singh 2003b Marsland 2003 to detect novel or unknown items that differ from all the seen training data", "Outlier detection approaches typically derive some model of normal objects from the training set and use a distance measure and a threshold to detect abnormal items", "In this paper we apply outlier detection techniques to the task of unknown sense detection the identification of corpus occurrences that are not covered by a given sense inventory", "The training set 1 The term novelty detection is also used for the distinction of novel and repeated information in information retrieval a different if related topic", "Figure 1 Wrong assignment due to missing sense from the Hound of the Baskervilles Ch", "14 against which new occurrences are compared will consist of senseannotated text", "Unknown sense detection is related to word sense disambiguation WSD and to word sense discrimination Schu tze 1998 but differs from both", "In WSD all senses are assumed known and the task is to select one of them while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them and all training instances regardless of the sense to which they belong are modeled as one group of known data", "Unknown sense detection also differs from word sense discrimination where no sense inventory is given and the task is to group occurrences into senses", "In unknown sense detection the model respects the given word senses", "The main motivation for this study comes from shallow semantic parsing by which we mean a combination of WSD and the automatic assignment of semantic roles to free text", "In cases where a sense is missing from the inventory WSD will wrongly assign one of the existing senses", "Figure 1 shows an example a sentence from the Hound of the Baskervilles analyzed by the SH A L M A N E S E R Erk and Pado 2006 shallow semantic parser", "The analysis is based on FrameNet Baker et al 1998 a resource that lists senses and semantic roles for English expressions", "FrameNet is lacking a sense of expectation or being mentally prepared for the verb prepare so prepared has been assigned the sense CO O K I N G C R E AT I O N a possible but improbable analysis2", "Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing eg for drawing inferences", "Unknown sense detection can prevent such mistakes", "All sense inventories face the problem of missing senses either because of their small overall size as is the case for some nonEnglish WordNets or when they encounter domainspecific senses", "Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing but the method we propose is applicable to any sense inventory that has annotated data in particular it is also applicable to WordNet", "In this paper we model unknown sense detection as outlier detection using a simple Nearest Neighborbased method Tax and Duin 2000 that compares the local probability density at each test item with that of its nearest training item", "To our knowledge there exists no other approach to date to the problem of detecting unknown senses", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows 2003 Curran 2005 Burchardt et al 2005 which can be viewed as the logical next step after unknown sense detection", "Plan of the paper", "After a brief sketch of FrameNet in Section 2 we describe the experimental setup used throughout this paper in Section 3", "Section 4 tests whether a very simple model suffices for detecting unknown senses a threshold on confidence scores returned by the SH A L M A N E S E R WSD 2 Unfortunately the semantic roles have been misassigned by the system", "The word I should fill the FO O D role while for a hound could be assigned the optional RE C E I V E R role", "system", "The result is that recall is much too low", "Section 5 introduces the NNbased outlier detection approach that we use in section 6 for unknown sense detection with better results than in the first experiment but still low recall", "Section 7 repeats the experiment of section 6 with added training data making use of the fact that one semantic class in FrameNet typically pertains to several lemmas and achieving a marked improvement in results"]}, "N06-1037": {"title": ["Exploring Syntactic Features for Relation Extraction using"], "abstract": ["This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction", "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel", "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous bestreported featurebased methods on the 24 ACE relation subtypes", "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types"], "introduction": ["Relation extraction is a subtask of information extraction that finds various predefined semantic relations such as location affiliation rival etc between pairs of entities in text", "For example the sentence George Bush is the president of the United States conveys the semantic relation President between the entities George Bush PER and the United States GPE a GeoPolitical Entity  an entity with land and a government ACE 2004", "Prior featurebased methods for this task Kambhatla 2004 Zhou et al 2005 employed a large amount of diverse linguistic features varyingfrom lexical knowledge entity mention informa tion to syntactic parse trees dependency trees and semantic features", "Since a parse tree contains rich syntactic structure information in principle the features extracted from a parse tree should contribute much more to performance improvement for relation extraction", "However it is reported Zhou et al 2005 Kambhatla 2004 that hierarchical structured syntactic features contributes less to performance improvement", "This may be mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features", "As an alternative kernel methods Collins and Duffy 2001 provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees", "But to our surprise the sole tworeported dependency tree kernels for relation extraction on the ACE corpus Bunescu and Mooney 2005 Culotta and Sorensen 2004 showed much lower performance than the featurebased methods", "One may ask are the syntactic tree features very useful for relation extraction", "Can tree kernel methods effectively capture the syntactic tree features and other various features that have been proven useful in the featurebased methods", "In this paper we demonstrate the effectiveness of the syntactic tree features for relation extraction and study how to capture such features via a convolution tree kernel", "We also study how to select the optimal feature space eg the set of subtrees to represent relation instances to optimize the system performance", "The experimental results show that the convolution tree kernel plus entity features achieves slightly better performance than the previous bestreported featurebased methods", "It also shows that our method significantly outperforms the two dependency tree kernels Bunescu and Mooney 2005 Culotta and Sorensen 2004 on the 5 ACE relation types", "The rest of the paper is organized as follows", "In Section 2 we review the previous work", "Section 3 discusses our tree kernel based learning algorithm", "288 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL pages 288295 New York June 2006", "Qc 2006 Association for Computational Linguistics Section 4 shows the experimental results and compares our work with the related work", "We conclude our work in Section 5"]}, "N07-1015": {"title": ["A Systematic Exploration of the Feature Space for Relation Extraction"], "abstract": ["Relation extraction is the task of finding semantic relations between entities from text", "The stateoftheart methods for relation extraction are mostly based on statistical learning and thus all have to deal with feature selection which can significantly affect the classification performance", "In this paper we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces", "We present a general definition of feature spaces based on a graphic representation of relation instances and explore three different representations of relation instances and features of different complexities within this framework", "Our experiments show that using only basic unit features is generally sufficient to achieve stateoftheart performance while over inclusion of complex features may hurt the performance", "A combination of features of different levels of complexity and from different sentence representations coupled with taskoriented feature pruning gives the best performance"], "introduction": ["An important information extraction task is relation extraction whose goal is to detect and characterize semantic relations between entities in text", "For example the text fragment hundreds of Palestinians converged on the square contains the located relation between the Person entity hundreds of Palestinians and the BoundedArea entity the square", "Relation extraction has applications in many domains including finding affiliation relations from web pages and finding proteinprotein interactions from biomedical literature", "Recent studies on relation extraction have shown the advantages of discriminative modelbased statistical machine learning approach to this problem", "There are generally two lines of work following this approach", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from partofspeech POS tagging to full parsing and dependency parsing Kambhatla 2004 Zhao and Grishman 2005 Zhou et al 20051", "The second line of work designs kernel functions on some structured representation sequences or trees of the relation instances to capture the similarity between two relation instances Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005a Bunescu and Mooney 2005b Zhang et al 2006a Zhang et al 2006b", "Of particular interest among the various kernels proposed are the convolution kernels Bunescu and Mooney 2005b Zhang et al 2006a because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature", "Apart from their computational efficiency convolution kernels also implicitly correspond to some feature space", "Therefore both lines of work rely on an appropriately de 1 Although Zhao and Grishman 2005 defined a number of kernels for relation extraction the method is essentially similar to featurebased methods", "113 Proceedings of NAACL HLT 2007 pages 113120 Rochester NY April 2007", "Qc 2007 Association for Computational Linguistics fined set of features", "As in any learning problem the choice of features can affect the performance significantly", "Despite the importance of feature selection there has not been any systematic exploration of the feature space for relation extraction and the choices of features in existing work are somewhat arbitrary", "In this paper we conduct a systematic study of the feature space for relation extraction and evaluate the effectiveness of different feature subspaces", "Our motivations are twofold", "First based on previous studies we want to identify and characterize the types of features that are potentially useful for relation extraction and define a relatively complete and structured feature space that can be systematically explored", "Second we want to compare the effectiveness of different features", "Such a study can guide us to choose the most effective feature set for relation extraction or to design convolution kernels in the most effective way", "We propose and define a unified graphic representation of the feature space and experiment with three feature subspaces corresponding to sequences syntactic parse trees and dependency parse trees", "Experiment results show that each subspace is effective by itself with the syntactic parse tree subspace being the most effective", "Combining the three subspaces does not generate much improvement", "Within each feature subspace using only the basic unit features can already give reasonably good performance", "Adding more complex features may not improve the performance much or may even hurt the performance", "Taskoriented heuristics can be used to prune the feature space and when appropriately done can improve the performance", "A combination of features of different levels of complexity and from different sentence representations coupled with taskoriented feature pruning gives the best performance"]}, "N07-1024": {"title": ["Hybrid Models for Semantic Classification of"], "abstract": ["This paper addresses the problem of classifying Chinese unknown words into finegrained semantic categories defined in a Chinese thesaurus", "We describe three novel knowledgebased models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways", "We then combine two of the knowledgebased models with a corpusbased model which classifies unknown words using contextual information", "Experiments show that the knowledgebased models outperform previous methods on the same task but the use of contextual information does not further improve performance"], "introduction": ["Research on semantic annotation has focused primarily on word sense disambiguation WSD ie the task of determining the appropriate sense for each instance of a polysemous word out of a set of senses defined for the word in some lexicon", "Much less work has been done on semantic classification of unknown words ie words that are not listed in the lexicon", "However real texts typically contain a large number of unknown words", "Successful classification of unknown words is not only useful for lexical acquisition but also necessary for natural language processing NLP tasks that require semantic annotation", "This paper addresses the problem of classifying Chinese unknown words into finegrained semantic categories defined in a Chinese thesaurus Cilin Mei et al 1984", "This thesaurus classifies over 70000 words into 12 major categories including human A concrete object B time and space C abstract object D attributes E actions F mental activities G activities H physical states I relations J auxiliaries K and honorifics L", "The 12 major categories are further divided into 94 medium categories which in turn are subdivided into 1428 small categories", "Each small category contains synonyms that are close in meaning", "For example under the major category D the medium category Dm groups all words that refer to institutions and the small category Dm05 groups all words that refer to educational institutions eg  xuxio school", "Unknown word classification involves a much larger search space than WSD", "In classifying words into small categories in Cilin the search space for a polysemous known word consists of all the categories the word belongs to but that for an unknown word consists of all the 1428 small categories", "Research on WSD has concentrated on using contextual information which may be limited for infrequent unknown words", "On the other hand Chinese characters carry semantic information that is useful for predicting the semantic properties of the words containing them", "We present three novel knowledgebased models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways and combine two of them with a corpusbased model that uses contextual information to classify unknown words", "Experiments show that the combined knowledgebased model achieves an accuracy of 616 for classifying unknown words into small categories in Cilin but the use of contextual information does not further improve performance", "The rest of the paper is organized as follows", "Section 2 details the three novel knowledge based models proposed for this task", "Section 3 describes a corpusbased model", "Section 4 reports the experiment results of the proposed 188 Proceedings of NAACL HLT 2007 pages 188195 Rochester NY April 2007", "Qc 2007 Association for Computational Linguistics models", "Section 5 compares these results with previous results", "Section 6 concludes the paper and points to avenues for further research"]}, "N09-1057": {"title": ["More than Words Syntactic Packaging and Implicit Sentiment"], "abstract": ["Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text", "In this paper we introduce a new approach to the problem that focuses not on lexical indicators but on the syntactic packaging of ideas which is well suited to investigating the identification of implicit sentiment or perspective", "We establish a strong predictive connection between linguistically well motivated features and implicit sentiment and then show how computational approximations of these features can be used to improve on existing stateoftheart sentiment classification results"], "introduction": ["As Pang and Lee 2008 observe the last several years have seen a land rush in research on sentiment analysis and opinion mining with a frequent emphasis on the identification of opinions in evaluative text such as movie or product reviews", "However sentiment also may be carried implicitly by statements that are not only nonevaluative but not even visibly subjective", "Consider for example the following two descriptions of the same invented event 1a On November 25 a soldier veered his jeep into a crowded market and killed three civilians", "b On November 25 a soldiers jeep veered into a crowded market causing three civilian deaths", "Both descriptions appear on the surface to be objective statements and they use nearly the same words", "Lexically the sentences first clauses differ only in the difference between s and his to express the relationship between the soldier and the jeep and in the second clauses both kill and death are terms with negative connotations at least according to the General Inquirer lexicon Stone 1966", "Yet the descriptions clearly differ in the feelings they evoke if the soldier were being tried for his role in what happened on November 25 surely the prosecutor would be more likely to say 1a to the jury and the defense attorney 1b rather than the reverse1 Why then should a description like 1a be perceived as less sympathetic to the soldier than 1b", "If the difference is not in the words it must be in the way they are put together that is the structure of the sentence", "In Section 2 we offer a specific hypothesis about the connection between structure and implicit sentiment we suggest that the relationship is mediated by a set of grammatically relevant semantic properties well known to be important cross linguistically in characterizing the interface between syntax and lexical semantics", "In Section 3 we validate this hypothesis by means of a human ratings study showing that these properties are highly predictive of human sentiment ratings", "In Section 4 we introduce observable proxies for underlying semantics OPUS a practical way to approximate the relevant semantic properties automatically as features in a supervised learning setting", "In Section 5 we show that these features improve on the existing state ofthe art in automatic sentiment classification", "Sec This work was done while the first author was a student in the Department of Linguistics University of Maryland", "1 We refer readers not sharing this intuition to Section 3", "503 Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 503511 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics tions 6 and 7 discuss related work and summarize"]}, "N09-2003": {"title": ["Efficient Extraction of Oraclebest Translations from Hypergraphs"], "abstract": ["Hypergraphs are used in several syntax inspired methods of machine translation to compactly encode exponentially many translation hypotheses", "The hypotheses closest to given reference translations therefore cannot be found via brute force particularly for popular measures of closeness such as BLEU", "We develop a dynamic program for extracting the so called oraclebest hypothesis from a hyper graph by viewing it as the problem of finding the most likely hypothesis under an ngram language model trained from only the reference translations", "We further identify and remove massive redundancies in the dynamic program state due to the sparsity of ngrams present in the reference translations resulting in a very efficient program", "We present run time statistics for this program and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components"], "introduction": ["A hypergraph as demonstrated by Huang and Chi ang 2007 is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation MT system eg Koehn et al", "2003 or a syntax based MT system eg Chiang 2007", "While the hypergraph represents a very large set of translations it is quite possible that some desired translations eg the reference translations are not contained in the hypergraph due to pruning or inherent deficiency of the translation model", "In this case one is often required to find the translations in the hypergraph that are most similar to the desired translations with similarity computed via some automatic metric such as BLEU Papineni et al 2002", "Such maximally similar translations will be called oracle best translations and the process of extracting them oracle extraction", "Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph and the exponentially large number of hypotheses makes a bruteforce linear search intractable", "Therefore efficient algorithms that can exploit the structure of the hypergraph are required", "We present an efficient oracle extraction algorithm which involves two key ideas", "Firstly we view the oracle extraction as a bottomup model scoring process on a hypergraph where the model is trained on the reference translations", "This is similar to the algorithm proposed for a lattice by Dreyer et al", "2007", "Their algorithm however requires maintaining a separate dynamic programming state for each distinguished sequence of state words and the number of such sequences can be huge making the search very slow", "Secondly therefore we present a novel lookahead technique called equivalent oraclestate maintenance to merge multiple states that are equivalent for similarity computation", "Our experiments show that the equivalent oracle state maintenance technique significantly speeds up more than 40 times the oracle extraction", "Efficient oracle extraction has at least three important applications in machine translation", "Discriminative Training In discriminative training the objective is to tune the model parameters eg weights of a perceptron model or conditional random field such that the reference translations are preferred over competitors", "However the reference translations may not be reachable by the translation system in which case the oraclebest hypotheses should be substituted in training", "9 Proceedings of NAACL HLT 2009 Short Papers pages 912 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics System Combination In a typical system combination task eg Rosti et al", "2007 each component system produces a set of translations which are then grafted to form a confusion network", "The confusion network is then rescored often employing additional language models to select the final translation", "When measuring the goodness of a hypothesis in the confusion network one requires its score under each component system", "However some translations in the confusion network may not be reachable by some component systems in which case a systems score for the most similar reachable translation serves as a good approximation", "Multisource Translation In a multisource translation task Och and Ney 2001 the input is given in multiple source languages", "This leads to a situation analogous to system combination except that each component translation system now corresponds to a specific source language"]}, "N09_csl2013": {"title": ["Senselevel Subjectivity in a Multilingual Setting"], "abstract": ["Recent research on English word sense subjectivity has shown that the subjective aspect of an entity is a characteristic that is better delineated at the sense level instead of the traditional word level", "In this paper we seek to explore whether senses aligned across languages exhibit this trait consistently and if this is the case we investigate how this property can be leveraged in an automatic fashion", "We first conduct a manual annotation study to gauge whether the subjectivity trait of a sense can be robustly transferred across language boundaries", "An automatic framework is then introduced that is able to predict subjectivity labeling for unseen senses using either cross lingual or multilingual training enhanced with bootstrapping", "We show that the multilingual model consistently outperforms the crosslingual one with an accuracy of over 73 across all iterations", "Keywords Sentiment and text classification multilingual subjectivity analysis sense level subjectivity Email addresses carmenbaneagmailcom Carmen Banea radacsuntedu Rada Mihalcea wiebecspittedu Janyce Wiebe Preprint submitted to Computer Speech and Language December 27 2012"], "introduction": ["Sentiment and subjectivity analysis seeks to automatically identify opinions beliefs speculations emotions sentiments and other private states in natural text Wiebe et al 2005", "Quirk et al", "1985 define a private state as a state that does not lend itself to an objective external validation or in other words a person may be observed to assert that God exists but not to believe that God exists", "Belief is in this sense private p 1181", "In the field of natural language processing researchers have used the term subjectivity analysis to denote identifying private states in text namely separating objective from subjective instances while sentiment or polarity analysis further refines the subjective text into positive negative or neutral", "Sentiment and subjectivity analysis has stemmed into a prolific area of research mainly due to the fact that numerous text processing applications stand to gain from incorporating sentiment dimensions into their models including automatic expressive texttospeech synthesis Alm et al 1990 tracking sentiment timelines in online forums and news Balog et al 2006 Lloyd et al 2005 and mining opinions from product reviews Hu and Liu 2004", "In many natural language processing tasks subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data", "Research that benefited from this additional layering ranges from question answering Yu and Hatzivassiloglou 2003 to conversation summarization Carenini et al 2008 text semantic analysis Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 and lexical substitution Su and Markert 2010", "In experiments carried out on English Wiebe and Mihalcea 2006 have shown that the most robust subjectivity delineation occurs at sense and not at word level", "Following this more finegrained perspective Esuli and Sebastiani 2006 and Andreevskaia and Bergler 2006 have proposed methods to embed senselevel automatic sentiment annotations objectiveneutral negative and positive over the English WordNet structure Miller 1995 using its relationships synonymy antonymy meronymy etc", "On the other hand noticing the scarcity of hand crafted senselevel subjectivitypolarity lexica Markert and Su 2008 have explored ways to infer them from data annotated at either the word or sentence level", "Senselevel subjectivity and crosslingual subjectivity and sentiment analysis have received considerable attentions in recent years yet our paper explores the area that lies at the intersection of these two topics", "To our knowledge this area has not been formally investigated and while the techniques may be similar to those applied in sentiment and subjectivity analysis at the sentence or the review level our work explores the more difficult task of senselevel subjectivity which also involves deep semantic aspects of the language", "The manual annotation study we performed for this task cross lingual senselevel subjectivity annotations as well as the methods we proposed crosslingual and multilingual learning using dictionaries in multiple languages are novel to our knowledge", "This work seeks to answer the following questions", "First for word senses aligned across languages is their subjectivity content consistent or in other words does a subjective sense in language A map to a subjective sense in language B and similarly for an objective sense", "Second can we employ a multilingual framework that can automatically discover new subjectiveobjective senses starting with a limited amount of annotated data", "We seek to answer the first question by conducting a manual annotation study in Section 2", "For the second question we propose two models see Section 3 one cross lingual and one multilingual which are able to simultaneously use information extracted from several languages when making subjectivity senselevel predictions"]}, "N09prod": {"title": ["Sentiment Analysis with Multisource Product Reviews"], "abstract": ["More and more product reviews emerge on Ecommerce sites and microblog systems nowadays", "This information is useful for consumers to know the others opinion on the products before purchasing or companies who want to learn the public sentiment of their products", "In order to effectively utilize this information this paper has done some sentiment analysis on these multisource reviews", "For one thing a binary classification framework based on the aspects of product is proposed", "Both explicit and implicit aspect is considered and multiple kinds of feature weighing and classifiers are compared in our framework", "For another we use several machine learning algorithms to classify the product reviews in microblog systems into positive negative and neutral classes and find OVASVMs perform best", "Part of our work in this paper has been applied in a Chinese Product Review Mining System", "Keywords product review sentiment analysis microblog SVM"], "introduction": ["With the development of Internet more and more customers get used to purchasing products on Ecommerce sites such as 360buy1 and Newegg2", "They also write reviews on the products after using them which produce a large number of reviews on the Internet", "In addition microblog a system that allows users to post messages of no more than 140 words and share information instantaneously based on the relationship between users is under rapid development such as Twitter3 Sina microblog4 and Tencent microblog5", "A lot of microblogs contain latest product reviews", "Reviews from the above two large data sources contain much useful information for users and companies", "Users can make better purchasing decisions based on these reviews while companies can also analyze customers satisfaction according to these reviews and further improve the quality of their products", "Since there is a mass of 1 httpwww360buycom 2 httpwwwneweggcomcn 3 httpwwwtwittercom 4 httpweibocom 5 httptqqcn DS", "Huang et al", "Eds ICIC 2012 LNCS 7389 pp", "301308 2012", " SpringerVerlag Berlin Heidelberg 2012 product reviews and a single user cannot read all of them automatically mining the reviews from multiple sources is particularly important", "Most reviews in Chinese Ecommerce sites are labeled with advantage or disadvantage which is naturally suitable for binary classification", "The stateofart research in Chinese sentiment analysis mainly focuses on the whole review classification while customers often desire a more detailed understanding of products", "For example they want to know others opinion on the battery of a cell phone", "Therefore we propose a framework of sentiment classification at aspect level to solve this problem", "In our framework not only explicit but also implicit aspects are taken into account", "To our knowledge no implicit aspect discovery work of product review in Chinese language has been reported before", "For the reviews of each aspect the unigram features of words are used as text features", "We also compare the performance of three feature weighing strategies three reduction dimension and three classification approaches", "The sentiment analysis for reviews of products on microblogs is in its infancy", "Besides the microblogs that express opinion on the products some microblogs only give some statements relative to the products which contain no sentiment polarity or are neutral", "Therefore in this paper we exploit linear regression multiclass classification twostage classification and Mincut model optimization to classify the product related microblogs into three classes and compare the performance of these methods"]}, "N10-1019": {"title": ["Using Mostly Native Data to Correct Errors in Learners Writing"], "abstract": ["We present results from a range of experiments on article and preposition error correction for nonnative speakers of English", "We first compare a language model and error specific classifiers all trained on large English corpora with respect to their performance in error detection and correction", "We then combine the language model and the classifiers in a metaclassification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier", "The metaclassifier in turn is trained on errorannotated learner data optimizing the error detection and correction performance on this domain", "The metaclassification approach results in substantial gains over the classifier only and languagemodelonly scenario", "Since the metaclassifier requires errorannotated data for training we investigate how much training data is needed to improve results over the baseline of not using a metaclassifier", "All evaluations are conducted on a large error annotated corpus of learner English"], "introduction": ["Research on the automatic correction of grammatical errors has undergone a renaissance in the past decade", "This is at least in part based on the recognition that nonnative speakers of English now outnumber native speakers by 21 in some estimates so any tool in this domain could be of tremendous value", "While earlier work in both native and nonnative error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors see Heift and Schulze 2005 for a detailed overview more recent approaches have been based on datadriven methods", "The majority of the datadriven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by Golding 1995 and Golding and Roth 1996", "The words investigated are typically articles and prepositions", "They have two distinct advantages as the subject matter for investigation They are a closed class and they comprise a substantial proportion of learners errors", "The investigation of preposition corrections can even be narrowed further amongst the more than 150 English prepositions the usage of the ten most frequent prepositions accounts for 82 of preposition errors in the 20 million word Cambridge University Press Learners Corpus", "Learning correct article use is most difficult for native speakers of an L1 that does not overtly mark definiteness and indefiniteness as English does", "Prepositions on the other hand pose difficulties for language learners from all L1 backgrounds Dalgish 1995 Bitchener et al 2005", "Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the prepositionarticle", "Different systems typically vary along three dimensions choice of features choice of classifier and choice of training data", "Features range from words and morphological information Knight and Chander 1994 to the inclusion of partofspeech tags Minnen et al 2000 Han et al 2004 2006 Chodorow et al 2007 Gamon et al 2008 2009 Izumi et al 2003 2004 Tetrault and Chodorow 2008 to features based on linguistic analysis and on WordNet Lee 2004 DeFelice and Pulman 2007 2008", "Knight and Chander 1994 and Gamon et al", "2008 used decision tree classifiers but in general maximum entropy classifiers have become the classification 163 Human Language Technologies The 2010 Annual Conference of the North American Chapter of the ACL pages 163171 Los Angeles California June 2010", "Qc 2010 Association for Computational Linguistics algorithm of choice", "Training data are normally drawn from sizeable corpora of native English text British National Corpus for DeFelice and Pulman 2007 2008 Wall Street Journal in Knight and Chander 1994 a mix of Reuters and Encarta in Gamon et al", "2008 2009", "In order to partially address the problem of domain mismatch between learners writing and the newsheavy data sets often used in datadriven NLP applications Han et al", "2004 2006 use 315 million words from the MetaMetrics corpus a diverse corpus of fiction nonfiction and textbooks categorized by reading level", "In addition to the classification approach to error detection there is a line of research  going back to at least Atwell 1987  that uses language models", "The idea here is to detect errors in areas where the language model score is suspiciously low", "Atwell 1987 uses a partofspeech tag language model to detect errors Chodorow and Leacock 2000 use mutual information and chi square statistics to identify unlikely function word and partofspeech tag sequences Turner and Charniak 2007 employ a language model based on a generative statistical parser and Stehouwer and van Zaanen 2009 investigate a diverse set of language models with different backoff strategies to determine which choice from a set of confusable words is most likely in a given context", "Gamon et al", "2008 2009 use a combination of errorspecific classifiers and a large generic language model with hand tuned heuristics for combining their scores to maximize precision", "Finally Yi et al", "2008 and Her met et al", "2008 use ngram counts from the web as a language model approximation to identify likely errors and correction candidates"]}, "N10-1040": {"title": ["Improving  PhraseBased Translation with Prototypes of Short Phrases"], "abstract": ["We investigate methods of generating additional bilingual phrase pairs for a phrase based decoder by translating short sequences of source text", "Because our translation task is more constrained we can use a model that employs more linguistically rich features than a traditional decoder", "We have implemented an example of this approach", "Experimental results suggest that the phrase pairs produced by our method are useful to the decoder and lead to improved sentence translations"], "introduction": ["Recently there have been a number of successful attempts at improving phrasebased statistical machine translation by exploiting linguistic knowledge such as morphology partofspeech tags and syntax", "Many translation models use such knowledge before decoding Xia and McCord 2004 and during decoding Birch et al 2007 Gimpel and Smith 2009 Koehn and Hoang 2007 Chiang et al 2009 but they are limited to simpler features for practical reasons often restricted to conditioning leftto right on the target sentence", "Traditionally nbest rerankers Shen et al 2004 have applied expensive analysis after the translation process on both the source and target side though they suffer from being limited to whatever is on the nbest list Hasan et al 2007", "We argue that it can be desirable to pretranslate parts of the source text before sentencelevel decoding begins using a richer model that would typically be out of reach during sentencelevel decoding", "In this paper we describe a particular method of generating additional bilingual phrase pairs for a new source text using what we call phrase prototypes which are are learned from bilingual training data", "Our goal is to generate improved translations of relatively short phrase pairs to provide the SMT decoder with better phrasal choices", "We validate the idea through experiments on ArabicEnglish translation", "Our method produces a 13 BLEU score increase 33 relative on a test set"]}, "N12-1006": {"title": ["Machine Translation of Arabic Dialects"], "abstract": ["Arabic Dialects present many challenges for machine translation not least of which is the lack of data resources", "We use crowdsourcing to cheaply and quickly build LevantineEnglish and EgyptianEnglish parallel corpora consisting of 11M words and 380k words respectively", "The dialectal sentences are selected from a large corpus of Arabic web text and translated using Amazons Mechanical Turk", "We use this data to build Dialectal Arabic MT systems and find that small amounts of dialectal data have a dramatic impact on translation quality", "When translating Egyptian and Levantine test sets our Dialectal Arabic MT system performs 63 and 70 BLEU points higher than a Modern Standard Arabic MT system trained on a 150Mword ArabicEnglish parallel corpus"], "introduction": ["The Arabic language is a wellknown example of diglossia Ferguson 1959 where the formal variety of the language which is taught in schools and used in written communication and formal speech religion politics etc differs significantly in its grammatical properties from the informal varieties that are acquired natively which are used mostly for verbal communication", "The spoken varieties of the Arabic language which we refer to collectively as Dialectal Arabic differ widely among themselves depending on the geographic distribution and the socioeconomic conditions of the speakers and they diverge from the formal variety known as Modern Standard Arabic MSA Embarki and Ennaji 2011", "Significant differences in the phonology morphology lexicon and even syntax render some of these varieties mutually incomprehensible", "The use of Dialectal Arabic has traditionally been confined to informal personal speech while writ ing has been done almost exclusively using MSA or its ancestor Classical Arabic", "This situation is quickly changing however with the rapid proliferation of social media in the Arabicspeaking part of the world where much of the communication is composed in dialect", "The focus of the Arabic NLP research community which has been mostly on MSA is turning towards dealing with informal communication with the introduction of the DARPA BOLT program", "This new focus presents new challenges the most obvious of which is the lack of dialectal linguistic resources", "Dialectal text which is usually user generated is also noisy and the lack of standardized orthography means that users often improvise spelling", "Dialectal data also includes a wider range of topics than formal data genres such as newswire due to its informal nature", "These challenges require innovative solutions if NLP applications are to deal with Dialectal Arabic effectively", "In this paper  We describe a process for cheaply and quickly developing parallel corpora for LevantineEnglish and EgyptianEnglish using Amazons Mechanical Turk crowdsourcing service 3", " We use the data to perform a variety of machine translation experiments showing the impact of morphological analysis the limited value of adding MSA parallel data the usefulness of crossdialect training and the effects of translating from dialect to MSA to English 4", "We find that collecting dialect translations has a low cost 003word and that relatively small amounts of data has a dramatic impact on translation quality", "When trained on 15M words of dialectal data our system performs 63 to 70 BLEU points higher than when it is trained on 100 times more MSA data from a mismatching domain", "49 2012 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies pages 4959", "Montreal Canada June 38 2012", "Qc 2012 Association for Computational Linguistics"]}, "N13-1093": {"title": ["Exploiting the Scope of Negations and Heterogeneous Features"], "abstract": ["This paper presents an approach that exploits the scope of negation cues for relation extraction RE without the need of using any specifically annotated dataset for building a separate negation scope detection classifier", "New features are proposed which are used in two different stages", "These also include nontarget entity specific features", "The proposed RE approach outperforms the previous state of the art for drugdrug interaction DDI extraction"], "introduction": ["Negation is a linguistic phenomenon where a negation cue eg not can alter the meaning of a particular text segment or of a fact", "This text segment or fact is said to be inside the scope of that negation cue", "In the context of RE there is not much work that aims to exploit the scope of negations1 The only work on RE that we are aware of is SanchezGraillet and Poesio 2007 where they used various heuristics to extract negative protein interaction", "Despite the recent interest on automatically detecting the scope of negation2 till now there seems to be no empirical evidence supporting its exploitation for the purpose of RE", "Even if we could manage to obtain highly accurate automatically detected 1 In the context of event extraction a closely related task of", "RE there have been efforts in BioNLP shared tasks of 2009 and 2011 for nonmandatory subtask of event negation detection 3 participants in 2009 2 in 2011 Kim et al 2009 Kim et al 2011", "The participants approached the subtask using either predefined patterns or some heuristics"]}, "N13-1095": {"title": ["Distant Supervision for Relation Extraction with an Incomplete Knowledge Base"], "abstract": ["Distant supervision heuristically labeling a corpus using a knowledge base has emerged as a popular choice for training relation extractors", "In this paper we show that a significant number of negative examples generated by the labeling process are false negatives because the knowledge base is incomplete", "Therefore the heuristic for generating negative examples has a serious flaw", "Building on a stateoftheart distantlysupervised extraction algorithm we proposed an algorithm that learns from only positive and unlabeled labels at the pairofentity level", "Experimental results demonstrate its advantage over existing algorithms"], "introduction": ["Relation Extraction is a wellstudied problem Miller et al 2000 Zhou et al 2005 Kambhatla 2004 Min et al 2012a", "Recently Distant Supervision DS Craven and Kumlien 1999 Mintz et al 2009 has emerged to be a popular choice for training relation extractors without using manually labeled data", "It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base KB", "This method significantly reduces human efforts for relation extraction", "The labeling heuristic has a serious flaw", "Knowledge bases are usually highly incomplete", "For exam 1 An occurrence of a pair of entities with the source sentence", "ple 938 of persons from Freebase2 have no place of birth and 785 have no nationality section 3", "Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation all the corresponding relation mentions are considered negative examples3 This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation", "The number of such false negative matches even exceeds the number of positive pairs by 3 to 10 times leading to a significant problem for training", "Previous approaches Riedel et al 2010 Hoffmann et al 2011 Surdeanu et al 2012 bypassed this problem by heavily undersampling the negative class", "We instead deal with a learning scenario where we only have entitypair level labels that are either positive or unlabeled", "We proposed an extension to Surdeanu et al", "2012 that can train on this dataset", "Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS", "Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions"]}, "N13-1104": {"title": ["Proceedings of NAACLHLT 2013 pages 837846"], "abstract": ["In naturallanguage discourse related events tend to appear near each other to describe a larger scenario", "Such structures can be formalized by the notion of a frame aka template which comprises a set of related events and prototypical participants and event transitions", "Identifying frames is a prerequisite for information extraction and natural language generation and is usually done manually", "Methods for inducing frames have been proposed recently but they typically use ad hoc procedures and are difficult to diagnose or extend", "In this paper we propose the first probabilistic approach to frame induction which incorporates frames events and participants as latent topics and learns those frame and event transitions that best explain the text", "The number of frame components is inferred by a novel application of a splitmerge method from syntactic parsing", "In endtoend evaluations from text to induced frames and extracted facts our method produces stateoftheart results while substantially reducing engineering effort"], "introduction": ["Events with causal or temporal relations tend to occur near each other in text", "For example a BOMBING scenario in an article on terrorism might begin with a DETONATION event in which terrorists set off a bomb", "Then a DAMAGE event might ensue to describe the resulting destruction and any casualties followed by an INVESTIGATION event This research was undertaken during the authors internship at Microsoft Research", "covering subsequent police investigations", "Afterwards the BOMBING scenario may transition into a CRIMINALPROCESSING scenario which begins with police catching the terrorists and proceeds to a trial sentencing etc A common set of participants serves as the event arguments eg the agent or subject of DETONATION is often the same as the theme or object of INVESTIGATION and corresponds to a PERPETRATOR", "Such structures can be formally captured by the notion of a frame aka template scenario which consists of a set of events with prototypical transitions as well as a set of slots representing the common participants", "Identifying frames is an explicit or implicit prerequisite for many NLP tasks", "Information extraction for example stipulates the types of events and slots that are extracted for a frame or template", "Online applications such as dialogue systems and personalassistant applications also model users goals and subgoals using framelike representations", "In naturallanguage generation frames are often used to represent contents to be expressed as well as to support surface realization", "Until recently frames and related representations have been manually constructed which has limited their applicability to a relatively small number of domains and a few slots within a domain", "Furthermore additional manual effort is needed after the frames are defined in order to extract frame components from text eg in annotating examples and designing features to train a supervised learning model", "This paradigm makes generalizing across tasks difficult and might suffer from annotator biasRecently there has been increasing interest in au 837 Proceedings of NAACLHLT 2013 pages 837846 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics tomatically inducing frames from text", "A notable example is Chambers and Jurafsky 2011 which first clusters related verbs to form frames and then clusters the verbs syntactic arguments to identify slots", "While Chambers and Jurafsky 2011 represents a major step forward in frame induction it is also limited in several aspects", "The clustering used ad hoc steps and customized similarity metrics as well as an additional retrieval step from a large external text corpus for slot generation", "This makes it hard to replicate their approach or adapt it to new domains", "Lacking a coherent model it is also difficult to incorporate additional linguistic insights and prior knowledge", "In this paper we present PROFINDER PRObabilistic Frame INDucER the first probabilistic approach to frame induction", "PROFINDER defines a joint distribution over the words in a document and their frame assignments by modeling frame and event transitions correlations among events and slots and their surface realizations", "Given a set of documents PROFINDER outputs a set of induced frames with learned parameters as well as the most probable frame assignments that can be used for event and entity extraction", "The numbers of events and slots are dynamically determined by a novel application of the splitmerge approach from syntactic parsing Petrov et al 2006", "In endtoend evaluations from text to entity extraction using standard MUC and TAC datasets PROFINDER achieved stateoftheart results while significantly reducing engineering effort and requiring no external data"]}, "N13-1110": {"title": ["Proceedings of NAACLHLT 2013 pages 897906"], "abstract": ["Coreference resolution systems rely heavily on string overlap eg Google Inc and Google performing badly on mentions with very different words opaque mentions like Google and the search giant", "Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall", "We present a new unsupervised method for mining opaque pairs", "Our intuition is to restrict distributional semantics to articles about the same event thus promoting referential match", "Using an English comparable corpus of tech news we built a dictionary of opaque coreferent mentions only 3 are in WordNet", "Our dictionary can be integrated into any coreference system it increases the performance of a stateoftheart system by 1 F1 on all measures and is easily extendable by using news aggregators"], "introduction": ["Repetition is one of the most common coreferential devices in written text making stringmatch features important to all coreference resolution systems", "In fact the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of stateoftheart systems Recasens and Hovy 2010", "This suggests that opaque mentions ie lexically different such as iPad and the Cupertino slate are a serious problem for modern systems they comprise 65 of the nonpronominal 1 Closest NP with the same gender and number", "errors made by the Stanford system on the CoNLL 2011 data", "Solving this problem is critical for overcoming the recall gap of stateoftheart systems Haghighi and Klein 2010 Stoyanov et al 2009", "Previous systems have turned either to ontologies Ponzetto and Strube 2006 Uryupina et al 2011 Rahman and Ng 2011 or distributional semantics Yang and Su 2007 Kobdani et al 2011 Bansal and Klein 2012 to help solve these errors", "But neither semantic similarity nor hypernymy are the same as coreference Microsoft and Google are distributionally similar but not coreferent people is a hypernym of both voters and scientists but the people can corefer with the voters but is less likely to corefer with the scientists", "Thus ontologies lead to precision problems and to recall problems like missing NE descriptions eg Apple and the iPhone maker and metonymies eg agreement and wording while distributional systems lead to precision problems like coreferring Microsoft and the Mountain View giant because of their similar vector representation release software update", "We increase precision by drawing on the intuition that referents that are both similar and participate in the same event are likely to corefer", "We restrict dis tributional similarity to collections of articles that discuss the same event", "In the following two documents on the Nexus One from different sources we take the subjects of the identical verb release Google and the Mountain View giantas coreferent", "Document 1 Google has released a software update", "Document 2 The Mountain View giant released an update", "Based on this idea we introduce a new unsupervised method that uses verbs in comparable corpora 897 Proceedings of NAACLHLT 2013 pages 897906 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics as pivots for extracting the hard cases of coreference resolution and build a dictionary of opaque coreferent mentions ie the dictionary entries are pairs of mentions", "This dictionary is then integrated into the Stanford coreference system Lee et al 2011 resulting in an average 1 improvement in the F1 score of all the evaluation measures", "Our work points out the importance of context to decide whether a specific mention pair is coreferent", "On the one hand we need to know what semantic relations are potentially coreferent eg content and video", "On the other we need to distinguish contexts that are compatible for coreference1 and 2a from those that are not1 and 2b", "1 Elemental helps those big media entities process content across a full slate of mobile devices", "2 a Elemental provides the picks and shovels to b make video work across multiple devices", "Elemental is powering the video for HBO Go", "Our dictionary of opaque coreferent pairs is our solution to the first problem and we report on some preliminary work on context compatibility to address the second problem"]}, "N13-1118": {"title": ["Unsupervised Metaphor  Identification Using Hierarchical  Graph"], "abstract": ["We present a novel approach to automatic metaphor identification that discovers both metaphorical associations and metaphorical expressions in unrestricted text", "Our system first performs hierarchical graph factorization clustering HGFC of nouns and then searches the resulting graph for metaphorical connections between concepts", "It then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions", "In contrast to previous work our method is fully unsupervised", "Despite this fact it operates with an encouraging precision 069 and recall 061", "Our approach is also the first one in NLP to exploit the cognitive findings on the differences in or ganisation of abstract and concrete concepts in the human brain"], "introduction": ["Metaphor has traditionally been viewed as a form of linguistic creativity that gives our expression more vividness distinction and artistism", "While this is true on the surface the mechanisms of metaphor have a much deeper origin in our reasoning", "Today metaphor is widely understood as a cognitive phenomenon operating at the level of mental processes whereby one concept or domain is systematically viewed in terms of the properties of another Lakoff and Johnson 1980", "Consider the examples 1 He shot down all of my arguments and 2 He attacked every weak point in my argument", "They demonstrate a metaphorical mapping of the concept of argument to that of war", "The argument which is the target concept is viewed in terms of a battle or a war the source concept", "The existence of such a link allows us to systematically describe arguments using the war terminology thus leading to a number of metaphorical expressions", "Lakoff and Johnson call such generalisations a sourcetarget domain mapping or conceptual metaphor", "The ubiquity of metaphor in language has been established in a number of corpus studies Cameron 2003 Martin 2006 Steen et al 2010 Shutova and Teufel 2010 and the role it plays in human reasoning has been confirmed in psychological experiments Thibodeau and Boroditsky 2011", "This makes metaphor an important research area for computational and cognitive linguistics and its automatic processing indispensable for any semantics oriented NLP application", "The problem of metaphor modeling is gaining interest within NLP with a growing number of approaches exploiting statistical techniques Mason 2004 Gedigian et al 2006 Shutova 2010 Shutova et al 2010 Turney et al 2011 Shutova et al 2012", "Compared to more traditional approaches based on handcoded knowledge Fass 1991 Martin 1990 Narayanan 1997 Narayanan 1999 Feldman and Narayanan 2004 Barnden and Lee 2002 Agerri et al 2007 these more recent methods tend to have a wider coverage as well as be more efficient accurate and robust", "However even the statistical metaphor processing approaches so far often focused on a limited domain or a subset of phenomena Gedigian et al 2006 Krishnakumaran and Zhu 2007 and only addressed one of the metaphor processing sub tasks identification of metaphorical mappings Mason 2004 or identification of metaphorical expressions Shutova et al 2010 Turney et al 2011", "In this paper we present the first computational method 978 Proceedings of NAACLHLT 2013 pages 978988 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics that identifies the generalisations that govern the production of metaphorical expressions ie conceptual metaphors and then uses these generalisations to identify metaphorical expressions in unrestricted text", "As opposed to previous works on statistical metaphor processing that were supervised or semisupervised and thus required training data our method is fully unsupervised", "It relies on building a hierarchical graph of concepts connected by their association strength using hierarchical clustering and then searching for metaphorical links in this graph", "Shutova et al", "2010 introduced the hypothesis of clustering by association and claimed that in the course of distributional noun clustering abstract concepts tend to cluster together if they are associated with the same source domain while concrete concepts cluster by meaning similarity", "We share this intuition but take this idea a significant step further", "Our approach is theoretically grounded in the cognitive science findings suggesting that abstract and concrete concepts are organised differently in the human brain Crutch and Warrington 2005 Binder et al 2005 WiemerHastings and Xu 2005 Huang et al 2010 Crutch and Warring ton 2010 Adorni and Proverbio 2012", "According to Crutch and Warrington 2005 these differences emerge from their general patterns of relation with other concepts", "However most NLP systems to date treat abstract and concrete concepts as identical", "In contrast we incorporate this distinction into our model by creating a network or a graph of concepts and automatically learning the different patterns of association of abstract and concrete concepts with other concepts", "We expect that while concrete concepts would tend to naturally organise into a treelike structure with more specific terms descending from the more general terms abstract concepts would exhibit a more complex pattern of associations", "Consider the example in Figure 1", "The figure schematically shows a small portion of the graph describing the concepts of mechanism concrete political system and relationship abstract at two levels of generality", "One can see from this graph that if concrete concepts such as bike or engine tend to be connected to only one concept at the higher level in the hierarchy mechanism abstract concepts may have multiple higherlevel associates the literal ones and the metaphorical ones", "For ex ample the abstract concept of democracy is literally associated with a more general concept of political system as well as metaphorically associated with the concept of mechanism", "Such multiple associations are due to the fact that political systems are metaphorically viewed as mechanisms they can function break they can be oiled etc We often discuss them using mechanism terminology and thus a corpusbased distributional learning approach would learn that they share features with political systems from their literal uses as well as with mechanisms from their metaphorical uses as shown next to the respective graph edges in the figure", "Our system discovers such association patterns within the graph and uses them to identify metaphorical connections between the concepts", "To the best of our knowledge our method is the first one to use a hierarchical clustering model for the metaphor processing task", "The original graph of concepts is built using hierarchical graph factorization clustering HGFC Yu et al 2006 of nouns yielding a network of clusters with different levels of generality", "The weights on the edges of the graph indicate association between the clusters concepts", "HGFC has not been previously employed for noun clustering in NLP but showed successful results in the verb clustering task Sun and Korhonen 2011", "In summary our system 1 builds a graph of concepts using HGFC 2 traverses it to find metaphorical associations between clusters using weights on the edges of the graph 3 generates lists of salient features for the metaphorically connected clusters and 4 searches the British National Corpus BNC Burnard 2007 for metaphorical expressions describing the target domain concepts using the verbs from the set of salient features", "We evaluated the performance of the system with the aid of human judges in precision and recalloriented settings", "In addition we compared its performance to that of two baselines an unsupervised baseline using agglomerative clustering AGG and a supervised baseline built upon WordNet Fellbaum 1998 WN"]}, "N15-1071": {"title": ["Subsentential Sentiment on a Shoestring"], "abstract": ["Sentiment analysis has undergone a shift from documentlevel analysis where labels expresses the sentiment of a whole document or whole sentence to subsentential approaches which assess the contribution of individual phrases in particular including the composition of sentiment terms and phrases such as negators and intensifiers", "Starting from a small sentiment treebank modeled after the Stanford Sentiment Treebank of Socher et al", "2013 we investigate suitable methods to perform compositional sentiment classification for German in a datascarce setting harnessing crosslingual methods as well as existing generaldomain lexical resources"], "introduction": ["In sentiment classification we find a general tendency from documentlevel classification towards more finegrained approaches that yield a more detailed appraisal of the judgement performed in the text  in particular using composition over syntactic structure to get a more detailed approach over phrases", "For English movie reviews work using the Stanford Sentiment Treebank SSTb has shown that such subsentential sentiment information can yield approaches with both very high accuracy Socher et al 2013 Dong et al 2014 Hall et al 2014 and precise information about the role of each phrase  information which can subsequently used for extracting or summarizing the sentiment expressed in the text", "The effort for creating a sentiment treebank such as the SSTb however seems prohibitive if we wanted to create such a resource for each pair of relevant domain and language Compared to documentlevel annotations for sentiment which are easy to come by eg star ratings annotating individual syntactic phrases requires considerable effort", "The main focus of this paper is the question if and how it is possible to reach sensible performance for compositional sentiment classification when we only have limited resources to spend on an inlanguage indomain sentiment treebank", "For this goal we use a new resource the Heidelberg Sentiment Treebank HeiST which is a Germanlanguage counterpart to the Stanford Sentiment Treebank in the sense that it makes explicit the composition of sentiment expression over syntactic phrases", "Our experiments on HeiST provide a direct comparison of different techniques for harnessing crosslingual crossdomain or crosstask information and are the first of this kind to specifically target compositional sentiment analysis", "Figure 1 next page shows a schematic overview of the experiments beyond supervised baseline experiments using SVM classification and a supervised RNTN model section 3 we evaluated cross lingual projection section 4 lexiconbased approaches section 5 as well as semisupervised approaches based on word clusters section 6"]}, "P01-1005": {"title": ["Scaling to Very Very Large Corpora for"], "abstract": ["The amount of readily available online text has reached hundreds of billions of words and continues to grow", "Yet for most core natural language tasks algorithms continue to be optimized tested and compared after training on corpora consisting of only one million words or less", "In this paper we evaluate the performance of different learning methods on a prototypical natural language disambiguation task confusion set disambiguation when trained on orders of magnitude more labeled data than has previously been used", "We are fortunate that for this particular application correctly labeled training data is free", "Since this will often not be the case we examine methods for effectively exploiting very large corpora when labeled data comes at a cost"], "introduction": ["Machine learning techniques which automatic ally learn linguistic information from online text corpora have been applied to a number of natural language problems throughout the last decade", "A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora", "While the amount of available online text has been increasing at a dramatic rate the size of training corpora typically used for learning has not", "In part this is due to the standardization of data sets used within the field as well as the potentially large cost of annotating data for those learning methods that rely on labeled text", "The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed and relatively small data sets", "Yet since we now have access to significantly more data one has to wonder what conclusions that have been drawn on small data sets may carry over when these learning methods are trained using much larger corpora", "In this paper we present a study of the effects of data size on machine learning for natural language disambiguation", "In particular we study the problem of selection among confusable words using orders of magnitude more training data than has ever been applied to this problem", "First we show learning curves for four different machine learning algorithms", "Next we consider the efficacy of voting sample selection and partially unsupervised learning with large training corpora in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost"]}, "P03-1028": {"title": ["Closing the Gap LearningBased Information Extraction Rivaling"], "abstract": ["In this paper we present a learning approach to the scenario template task of information extraction where information filling one template could come from multiple sentences", "When tested on the MUC4 task our learning approach achieves accuracy competitive to the best of the MUC4 systems which were all built with manually engineered rules", "Our analysis reveals that our use of full parsing and stateoftheart learning algorithms have contributed to the good performance", "To our knowledge this is the first research to have demonstrated that a learning approach to the fullscale information extraction task could achieve performance rivaling that of the knowledge engineering approach"], "introduction": ["The explosive growth of online texts written in natural language has prompted much research into information extraction IE the task of automatically extracting specific information items of interest from natural language texts", "The extracted information is used to fill database records also known as templates in the IE literature", "Research efforts on IE tackle a variety of tasks", "They include extracting information from semi structured texts such as seminar announcements rental and job advertisements etc as well as from free texts such as newspaper articles Soderland 1999", "IE from semistructured texts is easier than from free texts since the layout and format of a semistructured text provide additional useful clues AYACUCHO 19 JAN 89  TODAY TWO PEOPLE WERE WOUNDED WHEN A BOMB EXPLODED IN SAN JUAN BAUTISTA MUNICIPALITY", "OFFICIALS SAID THAT SHINING PATH MEMBERS WERE RESPONSIBLE FOR THE ATTACK ", "POLICE SOURCES STATED THAT THE BOMB ATTACK INVOLVING THE SHINING PATH CAUSED SERIOUS DAMAGES ", "Figure 1 Snippet of a MUC4 document to aid in extraction", "Several benchmark data sets have been used to evaluate IE approaches on semi structured texts Soderland 1999 Ciravegna 2001 Chieu and Ng 2002a", "For the task of extracting information from free texts a series of Message Understanding Conferences MUC provided benchmark data sets for evaluation", "Several subtasks for IE from free texts have been identified", "The named entity NE task extracts person names organization names location names etc The template element TE task extracts information centered around an entity like the acronym category and location of a company", "The template relation TR task extracts relations between entities", "Finally the fullscale IE task the scenario template ST task deals with extracting generic information items from free texts", "To tackle the full ST task an IE system needs to merge information from multiple sentences in general since the information needed to fill one template can come from multiple sentences and thus discourse processing is needed", "The fullscale ST task is considerably harder than all the other IE tasks or subtasks outlined above", "As is the case with many other natural language processing NLP tasks there are two main approaches to IE namely the knowledgeengineering approach and the learning approach", "Most early IE systems adopted the knowledgeengineering ap 0 MESSAGE ID TST3MUC40014", "1 MESSAGE TEMPLATE 1"]}, "P04-1046": {"title": ["Building Verb Predicates A Computational View"], "abstract": ["A method for the definition of verb predicates is proposed", "The definition of the predicates isessentially tied to a semantic interpretation algorithm that determines the predicate for theverb its semantic roles and adjuncts", "As predicate definitions are complete they can be tested by running the algorithm on some sentences andverifying the resolution of the predicate semantic roles and adjuncts in those sentences", "The predicates are defined semiautomatically with the help of a software environment that uses several sections of a corpus to provide feedback for the definition of the predicates and then forthe subsequent testing and refining of the definitions", "The method is very flexible in adding anew predicate to a list of already defined predicates for a given verb", "The method builds on an existing approach that defines predicates for WordNet verb classes and that plans to definepredicates for every English verb", "The definitions of the predicates and the semantic interpretation algorithm are being used to automatically create a corpus of annotated verb predicates semantic roles and adjuncts"], "introduction": ["This paper deals with the definition of verbpredicates which will make possible the determination of verb meaning semantic roles adjuncts and attachment and meaning of postverbal PPs", "Hence the adequacy of the defini tions is measured by comparing the output of asemantic interpretation algorithm with the so lution of those semantic interpretation tasks on sentences randomly taken from any corpus or typed by a user at the console", "The algorithmthus must provide immediate feedback by test ing the definitions on these randomly selected sentences", "In Gomez 2001 generic predicates have been defined for WordNet 16 henceforthWN verb classes Fellbaum 1998", "The se mantic roles of the predicates are linked to the selectional restrictions categories in WordNetontology for nouns and the grammatical relations that realize them", "The selectional restric tions of the predicates are solidly grounded on the WordNet ontology for nouns Miller 1998 whose upper level ontology has been modified and rearranged Gomez 2004 based on thefeedback obtained by testing the predicate def initionsHowever we have found out that the ini tial idea in that work of defining predicates for a WN verb class which will be also valid for most of the verbs under that class has proven to be too optimistic because many of the verb forms included under that class realize theirsemantic roles by different selectional restric tions and grammatical relations", "This is due to the fact that many of the verbs under a given class have been grouped in many instances bysome kind of implication or troponymy Fellbaum 1998 rather that by sharing seman tic roles in a hierarchy of predicates", "Even in many cases some of the verbs in the same synset list differ semantically and syntactically between them", "For instance the third sense of ampquotgainampquot in WN is ampquotprofit gain benefit derive benefit fromampquot The theme the thing obtained is syntactically realized by the PP from NPfor two of the verbs listed in that synset ampquotbene fitampquot and ampquotprofitampquot while the theme for ampquotgainampquot is realized by a direct object", "The differences in grammatical relations are even more prevailing within the verbs under one given class", "But in those cases in which the verb polysemy is not high one predicate definition for the verbclass may apply to many of the verb forms un der that class", "Notwithstanding these problems WN verb classes have provided an important basis for the construction of a general ontology of predicates that will cover every English verb", "Moreover if one considers that there are 5752 verbs in WordNet 16 having only one sense and2199 verbs having exactly two senses the pred icates that have been constructed for the verb classes of 7951 verbs are very close to be done", "However the method explained in this paper deviates considerably from the WN approach to constructing verb meaning", "In particular iteschews the synset list in favor of predicate def initions for individual verbs as opposed to a listof synonymous verbs and allows for an easy in tegration of a new predicate into a list of already defined predicates for a given verb", "Briefly the algorithm Gomez 2001 that tests the predicates is as follows", "For every verb in a sentence we provide a list of predicates for that verb", "These predicates can be viewed as contenders for the meaning of the verb", "The goals of the algorithm are to select one predicate from that list thus determining the sense of the verb identify its semantic roles and adjuncts and attach postverbal PPs", "All these tasks aresimultaneously achieved", "For each grammati cal relation CR in the clause starting with the NP complements and for every predicate in the list of predicates the algorithm checks if the predicate explains the CR", "A predicate explainsa CR if there is a semantic role in the predi cate realized by the grammatical relation and the selectional restrictions of the semantic role subsume the ontological category of the head noun of the grammatical relation", "This process is repeated for each CR in the clause and each predicate in the list of predicates for the verb of the clause", "Then the predicate that explains the most CRs is selected as the meaning of the verb", "The semantic roles of the predicate have been identified as a result of this process", "In case of ties the predicate that has the greatest number of semantic roles realized is selected", "Every grammatical relation that has not been mapped to a semantic role must be an adjunct or an NP modifier", "The entries for adjuncts are stored in the root node action or description for stative verbs and are inherited by all predicates in those categories", "Adjuncts are identified after the predicate of the verb has been determined because adjuncts are not part of the argument structure of the predicateIn the next section we will show how to de fine predicates for individual verbs as different from WN verb classes how these predicates for individual verbs can reuse entries in the generic predicates for WordNet verb classes and howthey are integrated into the ontology of pred icates that have been defined with the help ofWN verb classes", "Section 3 provides a discus sion of the semiautomatic construction of thepredicates", "Section 4 gives a view of the upper level ontology of predicates section 5 discusses the testing and sections 6 and 7 related research and conclusions respectively"]}, "P05-1004": {"title": ["Supersense Tagging of Unknown Nouns using Semantic Similarity"], "abstract": [], "introduction": ["The limited coverage of lexicalsemantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words", "Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET", "Ciaramita and Johnson 2003 present a tagger which uses synonym set glosses as annotated training examples", "We describe an unsupervised approach based on vectorspace similarity which does not require annotated examples but significantly outperforms their tagger", "We also demonstrate the use of an extremely large shallowparsed corpus for calculating vectorspace semantic similarity"]}, "P05-1020": {"title": ["Proceedings of the 43rd Annual Meeting of the ACL pages 157164"], "abstract": ["In this paper we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems", "We propose a set of partitionbased features to learn a ranking model for distinguishing good and bad partitions", "Our approach compares favorably to two stateoftheart coreference systems when evaluated on three standard coreference data sets"], "introduction": ["Recent research in coreference resolution  the problem of determining which noun phrases NPs in a text or dialogue refer to which realworld entity  has exhibited a shift from knowledge based approaches to datadriven approaches yielding learningbased coreference systems that rival their handcrafted counterparts in performance eg Soon et al", "2001 Ng and Cardie 2002b Strube et al", "2002 Yang et al", "2003 Luo et al", "2004", "The central idea behind the majority of these learning based approaches is to recast coreference resolution as a binary classification task", "Specifically a classifier is first trained to determine whether two NPs in a document are coreferring or not", "A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs with one cluster for each set of coreferent NPs", "Though reasonably successful this standard approach is not as robust as one may think", "First de sign decisions such as the choice of the learning algorithm and the clustering procedure are apparently critical to system performance but are often made in an adhoc and unprincipled manner that may be suboptimal from an empirical point of view", "Second this approach makes no attempt to search through the space of possible partitions when given a set of NPs to be clustered employing instead a greedy clustering procedure to construct a partition that may be far from optimal", "Another potential weakness of this approach concerns its inability to directly optimize for clustering level accuracy the coreference classifier is trained and optimized independently of the clustering procedure to be used and hence improvements in classification accuracy do not guarantee corresponding improvements in clusteringlevel accuracy", "Our goal in this paper is to improve the robustness of the standard approach by addressing the above weaknesses", "Specifically we propose the following procedure for coreference resolution given a set of NPs to be clustered 1 use preselected learning based coreference systems to generate candidate partitions of the NPs and then 2 apply an automatically acquired ranking model to rank these candidate hypotheses selecting the best one to be the final partition", "The key features of this approach are Minimal human decision making", "In contrast to the standard approach our method obviates to a large extent the need to make tough or potentially suboptimal design decisions1 For instance if we 1 We still need to determine the coreference systems to be employed in our framework however", "Fortunately the choice of is flexible and can be as large as we want subject to the 157 Proceedings of the 43rd Annual Meeting of the ACL pages 157164 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics cannot decide whether learner is better to use than learner in a coreference system we can simply create two copies of the system with one employing and the other  and then add both into our pre selected set of coreference systems", "Generation of multiple candidate partitions", "Although an exhaustive search for the best partition is not computationally feasible even for a document with a moderate number of NPs our approach explores a larger portion of the search space than the standard approach via generating multiple hypotheses making it possible to find a potentially better partition of the NPs under consideration", "Optimization for clusteringlevel accuracy via ranking", "As mentioned above the standard approach trains and optimizes a coreference classifier without necessarily optimizing for clusteringlevel accuracy", "In contrast we attempt to optimize our ranking model with respect to the target coreference scoring function essentially by training it in such a way that a higher scored candidate partition according to the scoring function would be assigned a higher rank see Section 32 for details", "Perhaps even more importantly our approach provides a general framework for coreference resolution", "Instead of committing ourselves to a particular resolution method as in previous approaches our framework makes it possible to leverage the strengths of different methods by allowing them to participate in the generation of candidate partitions", "We evaluate our approach on three standard coreference data sets using two different scoring metrics", "In our experiments our approach compares favorably to two stateoftheart coreference systems adopting the standard machine learning approach outperforming them by as much as 47 on the three data sets for one of the performance metrics"]}, "P05-1021": {"title": ["Proceedings of the 43rd Annual Meeting of the ACL pages 165172"], "abstract": ["In this paper we focus on how to improve pronoun resolution using the statistics based semantic compatibility information", "We investigate two unexplored issues that influence the effectiveness of such information statistics source and learning framework", "Specifically we for the first time propose to utilize the web and the twincandidate model in addition to the previous combination of the corpus and the singlecandidate model to compute and apply the semantic information", "Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twincandidate learning model and significantly improve the resolution of neutral pronouns"], "introduction": ["Semantic compatibility is an important factor for pronoun resolution", "Since pronouns especially neutral pronouns carry little semantics of their own the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphors context based on the statistics that the corresponding predicateargument tuples occur in a particular large corpus", "Consider the example given in the work of Dagan and Itai 1990 1 They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2", "For anaphor it1 the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus", "A similar pattern could also be observed for it2", "So far the corpusbased semantic knowledge has been successfully employed in several anaphora resolution systems", "Dagan and Itai 1990 proposed a heuristicsbased approach to pronoun resolution", "It determined the preference of candidates based on predicateargument frequencies", "Recently Bean and Riloff 2004 presented an unsupervised approach to coreference resolution which mined the coreferring NP pairs with similar predicate arguments from a large corpus using a bootstrapping method", "However the utility of the corpusbased semantics for pronoun resolution is often argued", "Kehler et al", "2004 for example explored the usage of the corpusbased statistics in supervised learning based systems and found that such information did not produce apparent improvement for the overall pronoun resolution", "Indeed existing learningbased approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg Mitkov 1998 Soon et al", "2001 Strube and Muller 2003", "Could the relatively noisy semantic knowledge give us further system improvement", "In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information", "We propose to enhance the utility of the statisticsbased knowledge from two aspects Statistics source", "Corpusbased knowledge usually suffers from data sparseness problem", "That is many predicateargument tuples would be unseen even in a large corpus", "A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL pages 165172 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics web", "It is believed that the size of the web is thousands of times larger than normal large corpora and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicateargument bigrams Keller and Lapata 2003", "So far the web has been utilized in nominal anaphora resolution Modjeska et al 2003 Poesio et al 2004 to determine the semantic relation between an anaphor and candidate pair", "However to our knowledge using the web to help pronoun resolution still remains unexplored", "Learning framework", "Commonly the predicate argument statistics is incorporated into anaphora resolution systems as a feature", "What kind of learning framework is suitable for this feature", "Previous approaches to anaphora resolution adopt the single candidate model in which the resolution is done on an anaphor and one candidate at a time Soon et al 2001 Ng and Cardie 2002", "However as the purpose of the predicateargument statistics is to evaluate the preference of the candidates in semantics it is possible that the statisticsbased semantic feature could be more effectively applied in the twin candidate Yang et al 2003 that focusses on the preference relationships among candidates", "In our work we explore the acquisition of the semantic compatibility information from the corpus and the web and the incorporation of such semantic information in the singlecandidate model and the twincandidate model", "We systematically evaluate the combinations of different statistics sources and learning frameworks in terms of their effectiveness in helping the resolution", "Results on the MUC data set show that for neutral pronoun resolution in which an anaphor has no specific semantic category the webbased semantic information would be the most effective when applied in the twincandidate model Not only could such a system significantly improve the baseline without the semantic feature it also outperforms the system with the combination of the corpus and the singlecandidate model by 115 success", "The rest of this paper is organized as follows", "Section 2 describes the acquisition of the semantic com and finally Section 5 gives the conclusion"]}, "P05-1045": {"title": ["Incorporating Nonlocal Information into Information"], "abstract": ["Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference but this makes them unable to fully account for the long distance structure that is prevalent in language use", "We show how to solve this dilemma with Gibbs sampling a simple Monte Carlo method used to perform approximate inference in factored probabilistic models", "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs CMMs and CRFs it is possible to incorporate nonlocal structure while preserving tractable inference", "We use this technique to augment an existing CRFbased information extraction system with longdistance dependency models enforcing label consistency and extraction template consistency constraints", "This technique results in an error reduction of up to 9 over stateoftheart systems on two established information extraction tasks"], "introduction": ["Most statistical models currently used in natural language processing represent only local structure", "Although this constraint is critical in enabling tractable model inference it is a key limitation in many tasks since natural language contains a great deal of non local structure", "A general method for solving this problem is to relax the requirement of exact inference substituting approximate inference algorithms instead thereby permitting tractable inference in models with nonlocal structure", "One such algorithm is Gibbs sampling a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model including sequence models and probabilistic context free grammars Geman and Ge man 1984", "Although Gibbs sampling is widely used elsewhere there has been extremely little use of it in natural language processing1 Here we use it to add nonlocal dependencies to sequence models for information extraction", "Statistical hidden state sequence models such as Hidden Markov Models HMMs Leek 1997 Freitag and McCallum 1999 Conditional Markov Models CMMs Borthwick 1999 and Conditional Random Fields CRFs Lafferty et al 2001 are a prominent recent approach to information extraction tasks", "These models all encode the Markov property decisions about the state at a particular position in the sequence can depend only on a small local window", "It is this property which allows tractable computation the Viterbi Forward Backward and Clique Calibration algorithms all become intractable without it", "However information extraction tasks can benefit from modeling nonlocal structure", "As an example several authors see Section 8 mention the value of enforcing label consistency in named entity recognition NER tasks", "In the example given in Figure 1 the second occurrence of the token Tanjug is mis labeled by our CRFbased statistical NER system because by looking only at local evidence it is unclear whether it is a person or organization", "The first occurrence of Tanjug provides ample evidence that it is an organization however and by enforcing label consistency the system should be able to get it right", "We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure and demonstrate that this technique yields significant improvements on two established IE tasks", "1 Prior uses in NLP of which we are aware include Kim et al", "1995 Della Pietra et al", "1997 and Abney 1997", "363 Proceedings of the 43rd Annual Meeting of the ACL pages 363370 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics the news agency Tanjug reported   ", "airport  Tanjug said  Figure 1 An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset"]}, "P05-1051": {"title": ["Improving Name Tagging by"], "abstract": ["Information extraction systems incorporate multiple stages of linguistic analysis", "Although errors are typically compounded from stage to stage it is possible to reduce the errors in one stage by harnessing the results of the other stages", "We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger", "We use an Nbest approach to generate multiple hypotheses and have them reranked by subsequent stages of processing", "We obtained thereby a reduction of 24 in spurious and incorrect name tags and a reduction of 14 in missed tags"], "introduction": ["Systems which extract relations or events from a document typically perform a number of types of linguistic analysis in preparation for information extraction", "These include name identification and classification parsing or partial parsing semantic classification of noun phrases and coreference analysis", "These tasks are reflected in the evaluation tasks introduced for MUC6 named entity coreference template element and MUC7 template relation", "In most extraction systems these stages of analysis are arranged sequentially with each stage using the results of prior stages and generating a single analysis that gets enriched by each stage", "This provides a simple modular organization for the extraction systemUnfortunately each stage also introduces a cer tain level of error into the analysis", "Furthermore these errors are compounded  for example errors in name recognition may lead to errors in parsing", "The net result is that the final output relations or events may be quite inaccurate", "This paper considers how interactions between the stages can be exploited to reduce the error rate", "For example the results of coreference analysis or relation identification may be helpful in name classification and the results of relation or event extraction may be helpful in coreference", "Such interactions are not easily exploited in a simple sequential model  if name classification is performed at the beginning of the pipeline it cannot make use of the results of subsequent stages", "It may even be difficult to use this information implicitly by using features which are also used in later stages because the representation used in the initial stages is too limitedTo address these limitations some recent sys tems have used more parallel designs in which a single classifier incorporating a wide range of features encompasses what were previously several separate stages Kambhatla 2004 Zelenko et al 2004", "This can reduce the compounding of errors of the sequential design", "However it leads to a very large feature space and makes it difficult to select linguistically appropriate features for particular analysis tasks", "Furthermore because these decisions are being made in parallel it becomes much harder to express interactions between the levels of analysis based on linguistic intuitions", "411 Proceedings of the 43rd Annual Meeting of the ACL pages 411418 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics In order to capture these interactions more explicitly we have employed a sequential design in which multiple hypotheses are forwarded from each stage to the next with hypotheses being reranked and pruned using the information from later stages", "We shall apply this design to show how named entity classification can be improved by feedback from coreference analysis and relation extraction", "We shall show that this approach can capture these interactions in a natural and efficient manner yielding a substantial improvement in name identification and classification"]}, "P05-1053": {"title": ["Exploring Various Knowledge in Relation Extraction"], "abstract": [], "introduction": ["Extracting semantic relationships between entities is challenging", "This paper investigates the incorporation of diverse lexical syntactic and semantic knowledge in featurebased relation extraction using SVM", "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement", "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking", "We also demonstrate how semantic information such as WordNet and Name List can be used in featurebased relation extraction to further improve the performance", "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously bestreported systems on the 24 ACE relation subtypes and significantly outperforms tree kernelbased systems by over 20 in Fmeasure on the 5 ACE relation types"]}, "P06-1016": {"title": ["Modeling Commonality among Related Classes in Relation Extraction"], "abstract": ["This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes", "For each class in the hierarchy either manually predefined or automatically clustered a linear dis criminative function is determined in a top down way using a perceptron algorithm with the lowerlevel weight vector derived from the upperlevel weight vector", "As the upperlevel class normally has much more positive training examples than the lowerlevel class the corresponding linear discriminative function can be determined more reliably", "The upper level discriminative function then can effectively guide the discriminative function learning in the lowerlevel which otherwise might suffer from limited training data", "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 56 and 51 in Fmeasure on least and medium frequent relations respectively", "It also shows that our system outperforms the previous bestreported system by 27 in Fmeasure on the 24 subtypes using the same feature set"], "introduction": ["With the dramatic increase in the amount of textual information available in digital archives and the WWW there has been growing interest in techniques for automatically extracting information from text", "Information Extraction IE is such a technology that IE systems are expected to identify relevant information usually of predefined types from text documents in a certain domain and put them in a structured format", "According to the scope of the ACE program ACE 20002005 current research in IE has three main objectives Entity Detection and Tracking EDT Relation Detection and Characterization RDC and Event Detection and Characterization EDC", "This paper will focus on the ACE RDC task which detects and classifies various semantic relations between two entities", "For example we want to determine whether a person is at a location based on the evidence in the context", "Extraction of semantic relationships between entities can be very useful for applications such as question answering eg to answer the query Who is the president of the United States", "One major challenge in relation extraction is due to the data sparseness problem Zhou et al 2005", "As the largest annotated corpus in relation extraction the ACE RDC 2003 corpus shows that different subtypestypes of relations are much unevenly distributed and a few relation subtypes such as the subtype Founder under the type ROLE suffers from a small amount of annotated data", "Further experimentation in this paper please see Figure 2 shows that most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size", "Given the relative large size of this corpus it will be timeconsuming and very expensive to further expand the corpus with a reasonable gain in performance", "Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes it will be still far beyond practice for those minor sub types given the much unevenly distribution among different relation subtypes", "While various machine learning approaches such as generative modeling Miller et al 2000 maximum entropy Kambhatla 2004 and support vector machines Zhao and Grisman 2005 Zhou et al 2005 have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", "This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes", "Through organizing various classes hierarchically a linear discriminative function is determined for each class in a top down way using a perceptron algorithm with the lowerlevel weight vector derived from the upperlevel weight vector", "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical 121 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 121128 Sydney July 2006", "Qc 2006 Association for Computational Linguistics strategy achieves much better performance than the flat strategy on least and mediumfrequent relations", "It also shows that our system based on the hierarchical strategy outperforms the previous bestreported system", "The rest of this paper is organized as follows", "Section 2 presents related work", "Section 3 describes the hierarchical learning strategy using the perceptron algorithm", "Finally we present experimentation in Section 4 and conclude this paper in Section 5"]}, "P06-1017": {"title": ["Relation Extraction Using Label Propagation Based Semisupervised"], "abstract": ["Shortage of manually labeled data is an obstacle to supervised relation extraction methods", "In this paper we investigate a graph based semisupervised learning algorithm a label propagation LP algorithm for relation extraction", "It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph and tries to obtain a labeling function to satisfy two constraints 1 it should be fixed on the labeled nodes 2 it should be smooth on the whole graph", "Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available and it also performs better than bootstrap ping for the relation extraction task"], "introduction": ["Relation extraction is the task of detecting and classifying relationships between two entities from text", "Many machine learning methods have been proposed to address this problem eg supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithms Hasegawa et al 2004", "Supervised methods for relation extraction perform well on the ACE Data but they require a large amount of manually labeled relation instances", "Unsupervised methods do not need the definition of relation types and manually labeled data but they cannot detect relations between entity pairs and its result cannot be directly used in many NLP tasks since there is no relation type label attached to each instance in clustering result", "Considering both the availability of a large amount of untagged corpora and direct usage of extracted relations semi supervised learning methods has received great attention", "DIPRE Dual Iterative Pattern Relation Expansion Brin 1998 is a bootstrappingbased system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations", "Snowball Agichtein and Gravano 2000 is another system that used bootstrap ping techniques for extracting relations from unstructured text", "Snowball shares much in common with DIPRE including the employment of the boot strapping framework as well as the use of pattern matching to extract new candidate relations", "The third system approaches relation classification problem with bootstrapping on top of SVM proposed by Zhang 2004", "This system focuses on the ACE sub problem RDC and extracts various lexical and syntactic features for the classification task", "However Zhang 2004s method doesnt actually detect re laitons but only performs relation classification between two entities given that they are known to be related", "Bootstrapping works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled data using a model learned from augmented labeled data in previous iteration", "It 129 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 129136 Sydney July 2006", "Qc 2006 Association for Computational Linguistics can be found that the affinity information among unlabeled examples is not fully explored in this boot strapping process", "Recently a promising family of semisupervised learning algorithm is introduced which can effectively combine unlabeled data with labeled data in learning process by exploiting manifold structure cluster structure in data Belkin and Niyogi 2002 Blum and Chawla 2001 Blum et al 2004 Zhu and Ghahramani 2002 Zhu et al 2003", "These graphbased semisupervised methods usually define a graph where the nodes represent labeled and unlabeled examples in a dataset and edges may be weighted reflect the similarity of examples", "Then one wants a labeling function to satisfy two constraints at the same time 1 it should be close to the given labels on the labeled nodes and 2 it should be smooth on the whole graph", "This can be expressed in a regularization framework where the first term is a loss function and the second term is a regularizer", "These methods differ from traditional semi supervised learning methods in that they use graph structure to smooth the labeling function", "To the best of our knowledge no work has been done on using graph based semisupervised learning algorithms for relation extraction", "Here we investigate a label propagation algorithm LP Zhu and Ghahramani 2002 for relation extraction task", "This algorithm works by representing labeled and unlabeled examples as vertices in a connected graph then propagating the label information from any vertex to nearby vertices through weighted edges iteratively finally inferring the labels of unlabeled examples after the propagation process converges", "In this paper we focus on the ACE RDC task1"]}, "P06-1104": {"title": ["A Composite Kernel to Extract Relations between Entities with"], "abstract": ["This paper proposes a novel composite kernel for relation extraction", "The composite kernel consists of two individual kernels an entity kernel that allows for entityrelated features and a convolution parse tree kernel that models syntactic information of relation examples", "The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction", "Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering and can also easily scale to include more features", "Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms previous two dependency tree kernels for relation extraction"], "introduction": ["The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text", "The research on relation extraction has been promoted by the Message Understanding Conferences MUCs MUC 1987 1998 and Automatic Content Extraction ACE program ACE 20022005", "According to the ACE Program an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities", "For example the sentence Bill Gates is chairman and chief software architect of Microsoft Corporation conveys the ACEstyle relation EMPLOYMENTexec between the entities Bill Gates PERSONName and Microsoft Corporation ORGANIZATION", "Commercial", "In this paper we address the problem of relation extraction using kernel methods Schlkopf and Smola 2001", "Many featurebased learning algorithms involve only the dotproduct between feature vectors", "Kernel methods can be regarded by replacing the dotproduct with a kernel function between two vectors or even between two objects", "A kernel function is a similarity function satisfying the properties of being symmetric and positivedefinite", "Recently kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects", "For example the kernels for structured natural language data such as parse tree kernel Collins and Duffy 2001 string kernel Lodhi et al 2002 and graph kernel Suzuki et al 2003 are example instances of the well known convolution kernels1 in NLP", "In relation extraction typical work on kernel methods includes Zelenko et al", "2003 Culotta and Sorensen 2004 and Bunescu and Mooney 2005", "This paper presents a novel composite kernel to explore diverse knowledge for relation extraction", "The composite kernel consists of an entity kernel and a convolution parse tree kernel", "Our study demonstrates that the composite kernel is very effective for relation extraction", "It alsoshows without the need for extensive feature en gineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively", "An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels", "Evaluation on the ACE corpus shows that our method outperforms the previous best reported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features", "The rest of the paper is organized as follows", "In Section 2 we review the previous work", "Section 3 discusses our composite kernel", "Section 4 reports the experimental results and our observations", "Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure", "by Haussler 1999 in the machine learning field", "This framework defines a kernel between input objects by applying convolution subkernels that are the kernels for the decompositions parts of the objects", "825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 825832 Sydney July 2006", "Qc 2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration", "We conclude our work and indicate the future work in Section 6"]}, "P06-1117": {"title": ["Semantic Role Labeling via FrameNet VerbNet and PropBank"], "abstract": ["This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources FrameNet VerbNet and PropBank", "The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs", "We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes", "The PropBank corpus which is tightly connected to the VerbNet lexicon is used to increase the verb coverage and also to test the effectiveness of our approach", "The results indicate that our model is an interesting step towards the design of more robust semantic parsers"], "introduction": ["During the last years a noticeable effort has been devoted to the design of lexical resources that can provide the training ground for automatic semantic role labelers", "Unfortunately most of the systems developed until now are confined to the scope of the resource used for training", "A very recent example in this sense was provided by the CONLL 2005 shared task Carreras and Marquez 2005 on PropBank PB Kingsbury and Palmer 2002 role labeling", "The systems that participated in the task were trained on the Wall Street Journal corpus WSJ and tested on portions of WSJ and Brown corpora", "While the best Fmeasure recorded on WSJ was 80 on the Brown corpus the Fmeasure dropped below 70", "The most significant causes for this performance decay were highly ambiguous and unseen predicates ie predicates that do not have training examples", "The same problem was again highlighted by the results obtained with and without the frame information in the Senseval3 competition Litkowski 2004 of FrameNet Johnson et al 2003 role labeling task", "When such information is not used by the systems the performance decreases by 10 percent points", "This is quite intuitive as the semantics of many roles strongly depends on the focused frame", "Thus we cannot expect a good performance on new domains in which this information is not available", "A solution to this problem is the automatic frame detection", "Unfortunately our preliminary experiments showed that given a FrameNet FN predicateargument structure the task of identifying the associated frame can be performed with very good results when the verb predicates have enough training examples but becomes very challenging otherwise", "The predicates belonging to new application domains ie not yet included in FN are especially problematic since there is no training data available", "Therefore we should rely on a semantic context alternative to the frame Giuglea and Moschitti 2004", "Such context should have a wide coverage and should be easily derivable from FN data", "A very good candidate seems to be the Intersective Levin class ILC Dang et al 1998 that can be found as well in other predicate resources like PB and VerbNet VN Kipper et al 2000", "In this paper we have investigated the above claim by designing a semiautomatic algorithm that assigns ILCs to FN verb predicates and by carrying out several semantic role labeling SRL experiments in which we replace the frame with the ILC information", "We used support vector ma 929 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 929936 Sydney July 2006", "Qc 2006 Association for Computational Linguistics chines Vapnik 1995 with a polynomial kernels to learn the semantic role classification and b Tree Kernels Moschitti 2004 for learning both frame and ILC classification", "Tree kernels were applied to the syntactic trees that encode the subcategorization structures of verbs", "This means that although FN contains three types of predicates nouns adjectives and verbs we only concentrated on the verb predicates and their roles", "The results show that 1 ILC can be derived with high accuracy for both FN and Probank and 2 ILC can replace the frame feature with almost no loss in the accuracy of the SRL systems", "At the same time ILC provides better predicate coverage as it can also be learned from other corpora eg PB", "In the remainder of this paper Section 2 summarizes previous work done on FN automatic role detection", "It also explains in more detail why models based exclusively on this corpus are not suitable for freetext parsing", "Section 3 focuses on VN and PB and how they can enhance the robustness of our semantic parser", "Section 4 describes the mapping between frames and ILCs whereas Section 5 presents the experiments that support our thesis", "Finally Section 6 summarizes the conclusions"]}, "P06-1141": {"title": ["An Effective TwoStage Model for Exploiting NonLocal Dependencies in"], "abstract": ["This paper shows that a simple twostage approach to handle nonlocal dependencies in Named Entity Recognition NER can outperform existing approaches that handle nonlocal dependencies while being much more computationally efficient", "NER systems typically use sequence models for tractable inference but this makes them unable to capture the long distance structure present in text", "We use a Conditional Random Field CRF based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF", "Using features capturing nonlocal dependencies from the same document our approach yields a 126 relative error reduction on the F1 score over stateofthe art NER systems using localinformation alone when compared to the 93 relative error reduction offered by the best systems that exploit nonlocal information", "Our approach also makes it easy to incorporate nonlocal information from other documents in the test corpus and this gives us a 133 error reduction over NER systems using localinformation alone", "Additionally our running time for inference is just the inference time of two sequential CRFs which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference"], "introduction": ["Named entity recognition NER seeks to locate and classify atomic elements in unstructured text into predefined entities such as the names of persons organizations locations expressions of times quantities monetary values percentages etc A particular problem for Named Entity RecognitionNER systems is to exploit the presence of useful information regarding labels assigned at a long distance from a given entity", "An example is the labelconsistency constraint that if our text has two occurrences of New York separated by other tokens we would want our learner to encourage both these entities to get the same label", "Most statistical models currently used for Named Entity Recognition use sequence models and thereby capture local structure", "Hidden Markov Models HMMs Leek 1997 Freitag and McCallum 1999 Conditional Markov Models CMMs Borthwick 1999 McCallum et al 2000 and Conditional Random Fields CRFs Lafferty et al 2001 have been successfully employed in NER and other information extraction tasks", "All these models encode the Markov property ie labels directly depend only on the labels assigned to a small window around them", "These models exploit this property for tractable computation as this allows the ForwardBackward Viterbi and Clique Calibration algorithms to become tractable", "Although this constraint is essential to make exact inference tractable it makes us unable to exploit the nonlocal structure present in natural language", "Label consistency is an example of a nonlocal dependency important in NER", "Apart from label consistency between the same token sequences we would also like to exploit richer sources of dependencies between similar token sequences", "For example as shown in Figure 1 we would want it to encourage Einstein to be labeled Person if there is strong evidence that Albert Einstein should be labeled Person", "Sequence models unfortu 1121 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 11211128 Sydney July 2006", "Qc 2006 Association for Computational Linguistics told that Albert Einstein proved   ", "on seeing Einstein at the Figure 1 An example of the label consistency problem", "Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label so as to improve the chance that both are labeled PERSON", "nately cannot model this due to their Markovian assumption", "Recent approaches attempting to capture non local dependencies model the nonlocal dependencies directly and use approximate inference algorithms since exact inference is in general not tractable for graphs with nonlocal structure", "Bunescu and Mooney 2004 define a Relational Markov Network RMN which explicitly models longdistance dependencies and use it to represent relations between entities", "Sutton and McCallum 2004 augment a sequential CRF with skipedges ie edges between different occurrences of a token in a document", "Both these approaches use loopy belief propagation Pearl 1988 Yedidia et al 2000 for approximate inference", "Finkel et al", "2005 handset penalties for inconsistency in entity labeling at different occurrences in the text based on some statistics from training data", "They then employ Gibbs sampling Geman and Geman 1984 for dealing with their local feature weights and their nonlocal penalties to do approximate inference", "We present a simple twostage approach where our second CRF uses features derived from the output of the first CRF", "This gives us the advantage of defining a rich set of features to model nonlocal dependencies and also eliminates the need to do approximate inference since we do not explicitly capture the nonlocal dependencies in a single model like the more complex existing approaches", "This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRFs in contrast Finkel et al", "2005 reported an increase in running time by a factor of 30 over the sequential CRF with their Gibbs sampling approximate inference", "In all our approach is simpler yields higher F1 scores and is also much more computationally efficient than existing approaches modeling non local dependencies"]}, "P06-2012": {"title": ["Unsupervised Relation Disambiguation Using Spectral Clustering"], "abstract": ["This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts", "It works by calculating eigen vectors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors", "Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods"], "introduction": ["In this paper we address the task of relation extraction which is to find relationships between name entities in a given context", "Many methods have been proposed to deal with this task including supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithm Hasegawa et al 2004", "Among these methods supervised learning is usually more preferred when a large amount of labeled training data is available", "However it is timeconsuming and laborintensive to manually tag a large amount of training data", "Semisupervised learning methods have been put forward to minimize the corpus annotation requirement", "Most of semisupervised methods employ the bootstrapping framework which only need to predefine some initial seeds for any particular relation and then bootstrap from the seeds to acquire the relation", "However it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal number of them", "Compared with supervised and semisupervised methods Hasegawa et al", "2004s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels", "Hasegawa et al", "2004s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities", "However the drawback of hierarchical clustering is that it required providing cluster number by users", "Furthermore clustering is performed in original high dimensional space which may induce nonconvex clusters hard to identified", "This paper presents a novel application of spectral clustering technique to unsupervised relation extraction problem", "It works by calculating eigenvec tors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensional space and then performing cluster number estimation on a transformed space defined by the first few eigen vectors", "This method may help us find nonconvex clusters", "It also does not need to predefine the number of the context clusters or prespecify the similarity threshold for the clusters as Hasegawa et al", "2004s method", "The rest of this paper is organized as follows", "Section 2 formulates unsupervised relation extraction and presents how to apply the spectral clustering 89 Proceedings of the COLINGACL 2006 Main Conference Poster Sessions pages 8996 Sydney July 2006", "Qc 2006 Association for Computational Linguistics technique to resolve the task", "Then section 3 reports experiments and results", "Finally we will give a conclusion about our work in section 4"]}, "P06-3008": {"title": ["Discursive Usage of Six Chinese Punctuation Marks"], "abstract": ["Both rhetorical structure and punctuation have been helpful in discourse processing", "Based on a corpus annotation project this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts Colon Dash Ellipsis Exclamation Mark Question Mark and Semicolon", "The rhetorical patterns of these marks are compared against patterns around cue phrases in general", "Results show that these Chinese punctuation marks though fewer in number than cue phrases are easy to identify have strong correlation with certain relations and can be used as distinctive indicators of nuclearity in Chinese texts"], "introduction": ["Rhetorical structure has been proven useful in NLP projects such as text generation summarization machine translation and essay scoring", "Automatic discourse parsing remains an elusive task however despite much rulebased research on lexical cues such as anaphora and conjunctions", "Parsing through machine learning has encountered a bottleneck due to limited resourcesthere is only one English RST treebank publicly available and one RSTannotated German corpus on its way", "Punctuation marks PMs have been proven useful in RST annotation as well as in many other NLP tasks such as PartofSpeech tagging Word Sense Disambiguation Nearduplicate detection bilingual alignment eg Chuang and Yeh 2005 etc Dale 1991 noticed the role of PMs in determining rhetorical relations", "Say 1998 did a study on their roles in English discourse structure", "Marcu 1997 and CorstonOliver 1998 based their automatic discourse parser partially on PMs and other orthographical cues", "Tsou et al", "1999 and Chan et al", "2000 use PMs to disambiguate candidate Discourse Markers for a Chinese summarization system", "Reitter 2003 also used PMs to distinguish ATTRIBUTION and ELABORATION relations in his Featurerich SVM rhetorical analysis system", "All these inspired us to survey on the rhetorical patterns around Chinese PMs so as to provide more direct a priori scores for the coarse rhetorical analyzer by Zhang et al", "2000 in their hybrid summarization system", "This paper is organized into 5 parts Section 2 gives an overview of a Chinese RST treebank under construction and a survey on the syntax of six main PMs in the corpus Colon Dash Ellipses Exclamation Mark Question Mark and Semicolon", "Section 3 reports rhetorical patterns around these PMs", "Section 4 is a discussion on the effectiveness of these PMs in comparison with Chinese cue phrases", "Section 5 is a summary and Section 6 directions for future work"]}, "P07-1061": {"title": ["Finding document topics for improving topic segmentation"], "abstract": ["Topic segmentation and identification are often tackled as separate problems whereas they are both part of topic analysis", "In this article we study how topic identification can help to improve a topic segmenter based on word reiteration", "We first present an unsupervised method for discovering the topics of a text", "Then we detail how these topics are used by segmentation for finding topical similarities between text segments", "Finally we show through the results of an evaluation done both for French and English the interest of the method we propose"], "introduction": ["In this article we address the problem of linear topic segmentation which consists in segmenting documents into topically homogeneous segments that does not overlap each other", "This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as Hearst 1994", "One criterion for classifying topic segmentation systems is the kind of knowledge they depend on", "Most of them only rely on surface features of documents word reiteration in Hearst 1994 Choi 2000 Utiyama and Isahara 2001 Galley et al 2003 or discourse cues in Passonneau and Lit man 1997 Galley et al 2003", "As such systems do not require external knowledge they are not sensitive to domains but they are limited by the type of documents they can be applied to lexical reiteration is reliable only if concepts are not too frequently ex 480 pressed by several means synonyms etc and discourse cues are often rare and corpusspecific", "To overcome these difficulties some systems make use of domainindependent knowledge about lexical cohesion a lexical network built from a dictionary in Kozima 1993 a thesaurus in Morris and Hirst 1991 a large set of lexical co occurrences collected from a corpus in Choi et al 2001", "To a certain extent these lexical networks enable topic segmenters to exploit a sort of concept reiteration", "However their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high", "The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents", "Such topic models are generally built from a large set of example documents as in Yam ron et al 1998 Blei and Moreno 2001 or in one component of Beeferman et al 1999", "These statistical topic models enable segmenters to improve their precision but they also restrict their scope", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination Job bins and Evett 1998 combined word recurrence cooccurrences and a thesaurus Beeferman et al 1999 relied on both lexical modeling and discourse cues Galley et al 2003 made use of word reiteration through lexical chains and discourse cues", "The work we report in this article takes place in the first category we have presented", "It does not rely on any a priori knowledge and exploits word usage rather than discourse cues", "More precisely we present a new method for enhancing the results Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 480487 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics of segmentation systems based on word reiteration without relying on any external knowledge"]}, "P07-1066": {"title": ["BilingualLSA Based LM Adaptation for Spoken Language Translation"], "abstract": ["We propose a novel approach to crosslingual language model LM adaptation based on bilingual Latent Semantic Analysis bLSA", "A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a onetoone topic correspondence during training", "Using the proposed bLSA framework crosslingual LM adaptation can be performed by first inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language Ngram LM via marginal adaptation", "The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language", "On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27 for a unigram LM and up to 136 for a 4gram LM", "Furthermore the proposed approach consistently improved machine translation quality on both speech and text based adaptation"], "introduction": ["Language model adaptation is crucial to numerous speech and translation tasks as it enables higher level contextual information to be effectively incorporated into a background LM improving recognition or translation performance", "One approach is 520 to employ Latent Semantic Analysis LSA to capture indomain word unigram distributions which are then integrated into the background Ngram LM", "This approach has been successfully applied in automatic speech recognition ASR Tam and Schultz 2006 using the Latent Dirichlet Allocation LDA Blei et al 2003", "The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution", "For LM adaptation the topic mixture weights are estimated based on indomain adaptation text eg ASR hypotheses", "The adapted mixture weights are then used to interpolate a topic dependent unigram LM which is finally integrated into the background Ngram LM using marginal adaptation Kneser et al 1997 In this paper we propose a framework to perform LM adaptation across languages enabling the adaptation of a LM from one language based on the adaptation text of another language", "In statistical machine translation SMT one approach is to apply LM adaptation on the target language based on an initial translation of input references Kim and Khudanpur 2003 Paulik et al 2005", "This scheme is limited by the coverage of the translation model and overall by the quality of translation", "Since this approach only allows to apply LM adaptation after translation available knowledge cannot be applied to extend the coverage", "We propose a bilingual LSA model bLSA for crosslingual LM adaptation that can be applied before translation", "The bLSA model consists of two LSA models one for each side of the language trained on parallel document corpora", "The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 520527 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics the latent topic of the source and target LSA models can be assumed to be a onetoone correspondence and thus share a common latent topic space since the training corpora consist of bilingual parallel data", "For instance say topic 10 of the Chinese LSA model is about politics", "Then topic 10 of the English LSA model is set to also correspond to politics and so forth", "During LM adaptation we first infer the topic mixture weights from the source text using the source LSA model", "Then we transfer the inferred mixture weights to the target LSA model and thus obtain the target LSA marginals", "The challenge is to enforce the onetoone topic correspon dence", "Our proposal is to share common variational ASR hypo Chinese ASR ChineseEnglish SMT Chinese Ngram LM English Ngram LM Adapt Adapt Topic distribution Chinese LSA English LSA Chinese text English text ChineseEnglish Parallel document corpus MT hypo Dirichlet posteriors over the topic mixture weights of a document pair in the LDAstyle model", "The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion rather than to require manual interaction", "Since the topic space is language independent our approach supports topic transfer in multiple language pairs in ON where N is the number of languages", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by Zhao and Xing 2006", "Basically the BiTAM model consists of topicdependent transla tion lexicons modeling P rce k where c e and k denotes the source Chinese word target English word and the topic index respectively", "On the other hand the bLSA framework models P rck and P rek which is different from the BiTAM model", "By their different modeling nature the bLSA model usually supports more topics than the BiTAM model", "Another work by Kim and Khudanpur 2004 employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection", "We organize the paper as follows In Section 2 we introduce the bLSA framework including Latent DirichletTree Allocation LDTA Tam and Schultz 2007 as a correlated LSA model bLSA training and crosslingual LM adaptation", "In Section 3 we present the effect of LM adaptation on word perplexity followed by SMT experiments reported in BLEU on both speech and text input in Section 33", "Section 4 describes conclusions and fu Figure 1 Topic transfer in bilingual LSA model", "ture works"]}, "P07-1067": {"title": ["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 528535"], "abstract": ["Semantic relatedness is a very important factor for the coreference resolution task", "To obtain this semantic information corpus based approaches commonly leverage patterns that can express a specific semantic relation", "The patterns however are designed manually and thus are not necessarily the most effective ones in terms of accuracy and breadth", "To deal with this problem in this paper we propose an approach that can automatically find the effective patterns for coreference resolution", "We explore how to automatically discover and evaluate patterns and how to exploit the patterns to obtain the semantic relatedness information", "The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution"], "introduction": ["Semantic relatedness is a very important factor for coreference resolution as noun phrases used to refer to the same entity should have a certain semantic relation", "To obtain this semantic information previous work on reference resolution usually leverages a semantic lexicon like WordNet Vieira and Poesio 2000 Harabagiu et al 2001 Soon et al 2001 Ng and Cardie 2002", "However the drawback of WordNet is that many expressions especially for proper names word senses and semantic relations are not available from the database Vieira and Poesio 2000", "In recent years increasing interest has 528 been seen in mining semantic relations from large text corpora", "One common solution is to utilize a pattern that can represent a specific semantic relation eg X such as Y  for isa relation and X and other Y  for otherrelation", "Instantiated with two given noun phrases the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness Markert et al 2003 Modjeska et al 2003 Poesio et al 2004", "However in the previous pattern based approaches the selection of the patterns to represent a specific semantic relation is done in an ad hoc way usually by linguistic intuition", "The manually selected patterns nevertheless are not necessarily the most effective ones for coreference resolution from the following two concerns  Accuracy", "Can the patterns eg X such as Y  find as many NP pairs of the specific semantic relation eg isa as possible with a high precision", " Breadth", "Can the patterns cover a wide variety of semantic relations not just isa by which coreference relationship is realized", "For example in some annotation schemes like ACE BeijingChina are coreferential as the capital and the country could be used to represent the government", "The pattern for the common is a relation will fail to identify the NP pairs of such a capitalcountry relation", "To deal with this problem in this paper we propose an approach which can automatically discover effective patterns to represent the semantic relations Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 528535 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics for coreference resolution", "We explore two issues in our study 1 How to automatically acquire and evaluate the patterns", "We utilize a set of coreferential NP pairs as seeds", "For each seed pair we search a large corpus for the texts where the two noun phrases co occur and collect the surrounding words as the surface patterns", "We evaluate a pattern based on its commonality or association with the positive seed pairs", "2 How to mine the patterns to obtain the semantic relatedness information for coreference resolution", "We present two strategies to exploit the patterns choosing the top best patterns as a set of pattern features or computing the reliability of semantic relatedness as a single feature", "In either strategy the obtained features are applied to do coreference resolution in a supervisedlearning way", "To our knowledge our work is the first effort that systematically explores these issues in the coreference resolution task", "We evaluate our approach on ACE data set", "The experimental results show that the pattern based semantic relatedness information is helpful for the coreference resolution", "The remainder of the paper is organized as follows", "Section 2 gives some related work", "Section 3 introduces the framework for coreference resolution", "Section 4 presents the model to obtain the pattern based semantic relatedness information", "Section 5 discusses the experimental results", "Finally Section 6 summarizes the conclusions"]}, "P07-1068": {"title": ["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 536543"], "abstract": ["This paper examines whether a learning based coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the Penn Treebank in which the noun phrases are labeled with their semantic classes", "Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2 in Fmeasure over one that exploits heuristically computed semantic class knowledge", "In addition the induced knowledge improves the accuracy of common noun resolution by 26"], "introduction": ["In the past decade knowledgelean approaches have significantly influenced research in noun phrase NP coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document", "In knowledgelean approaches coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process eg Mitkov 1998 Tetreault 2001", "While these approaches have been reasonably successful see Mitkov 2002 Kehler et al", "2004 speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance", "In fact semantics plays a crucially important role in the resolution of common NPs allowing us to identify the coreference relation between two lexically dissimilar common nouns eg talks 536 and negotiations and to eliminate George W Bush from the list of candidate antecedents of the city for instance", "As a result researchers have readopted the oncepopular knowledgerich approach investigating a variety of semantic knowledge sources for common noun resolution such as the semantic relations between two NPs eg Ji et al", "2005 their semantic similarity as computed using WordNet eg Poesio et al", "2004 or Wikipedia Ponzetto and Strube 2006 and the contextual role played by an NP see Bean and Riloff 2004", "Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class SC of an NP which can be used to disallow coreference between semantically incompatible NPs", "However learningbased resolvers have not been able to benefit from having an SC agreement feature presumably because the method used to compute the SC of an NP is too simplistic while the SC of a proper name is computed fairly accurately using a named entity NE recognizer many resolvers simply assign to a common noun the first ie most frequent WordNet sense as its SC eg Soon et al", "2001 Markert and Nissim 2005", "It is not easy to measure the accuracy of this heuristic but the fact that the SC agreement feature is not used by Soon et als decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this firstsense heuristic", "Motivated in part by this observation we examine whether automatically induced semantic class knowledge can improve the performance of a learningbased coreference resolver reporting evaluation results on the commonlyused ACE corefer Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 536543 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics ence corpus", "Our investigation proceeds as follows", "Train a classifier for labeling the SC of an NP", "In ACE we are primarily concerned with classifying an NP as belonging to one of the ACE semantic classes", "For instance part of the ACE Phase 2 evaluation involves classifying an NP as PER SO N O R G A N IZATIO N G PE a geographicalpolitical region FAC ILITY LO C ATIO N or OTH ER S We adopt a corpusbased approach to SC determination recasting the problem as a sixclass classification task", "Derive two knowledge sources for coreference resolution from the induced SCs", "The first knowledge source KS is semantic class agreement SCA", "Following Soon et al", "2001 we represent SCA as a binary value that indicates whether the induced SCs of the two NPs involved are the same or not", "The second KS is mention which is represented as a binary value that indicates whether an NP belongs to one of the five ACE SCs mentioned above", "Hence the mention value of an NP can be readily derived from its induced SC the value is N O if its SC is OTH ER S and Y ES otherwise", "This KS could be useful for ACE coreference since ACE is concerned with resolving only NPs that are mentions", "Incorporate the two knowledge sources in a coreference resolver", "Next we investigate whether these two KSs can improve a learningbased baseline resolver that employs a fairly standard feature set", "Since 1 the two KSs can each be represented in the resolver as a constraint for filtering nonmentions or disallowing coreference between semantically incompatible NPs or as a feature and 2 they can be applied to the resolver in isolation or in combination we have eight ways of incorporating these KSs into the baseline resolver", "In our experiments on the ACE Phase 2 coreference corpus we found that 1 our SC induction method yields a significant improvement of 2 in accuracy over Soon et als firstsense heuristic method as described above 2 the coreference resolver that incorporates our induced SC knowledge by means of the two KSs mentioned above yields a significant improvement of 2 in Fmeasure over the resolver that exploits the SC knowledge computed by Soon et als method 3 the mention KS when used in the baseline resolver as a constraint improves the resolver by approximately 57 in F measure and 4 SCA when employed as a feature by the baseline resolver improves the accuracy of common noun resolution by about 58"]}, "P07-3016": {"title": ["Clustering Hungarian Verbs on the Basis of Complementation Patterns"], "abstract": ["Our paper reports an attempt to apply an unsupervised clustering algorithm to a Hungarian treebank in order to obtain semantic verb classes", "Starting from the hypothesis that semantic metapredicates underlie verbs syntactic realization we investigate how one can obtain semantically motivated verb classes by automatic means", "The 150 most frequent Hungarian verbs were clustered on the basis of their complementation patterns yielding a set of basic classes and hints about the features that determine verbal subcategorization", "The resulting classes serve as a basis for the subsequent analysis of their alternation behavior"], "introduction": ["For over a decade automatic construction of wide coverage structured lexicons has been in the center of interest in the natural language processing community", "On the one hand structured lexical databases are easier to handle and to expand because they allow making generalizations over classes of words", "On the other hand interest in the automatic acquisition of lexical information from corpora is due to the fact that manual construction of such resources is timeconsuming and the resulting database is difficult to update", "Most of the work in the field of acquisition of verbal lexical properties aims at learning subcategorization frames from corpora eg", "Pereira et al 1993 Briscoe and Carroll 1997 Sass 2006", "However semantic group ing of verbs on the basis of their syntactic distribution or other quantifiable features has also gained attention Schulte im Walde 2000 Schulte im Walde and Brew 2002 Merlo and Stevenson 2001 Dorr and Jones 1996", "The goal of these investigations is either the validation of verb classes based on Levin 1993 or finding algorithms for the categorization of new verbs", "Unlike these projects we report an attempt to cluster verbs on the basis of their syntactic properties with the further goal of identifying the semantic classes relevant for the description of Hungarian verbs alternation behavior", "The theoretical grounding of our clustering attempts is provided by the socalled Semantic Base Hypothesis Levin 1993 Koenig et al 2003", "It is founded on the observation that semantically similar verbs tend to occur in similar syntactic contexts leading to the assumption that verbal semantics determines argument structure and the surface realization of arguments", "While in English semantic argument roles are mapped to configurational positions in the tree structure Hungarian codes complement structure in its highly rich nominal inflection system", "Therefore we start from the examination of casemarked NPs in the context of verbs", "The experiment discussed in this paper is the first stage of an ongoing project for finding the semantic verb classes which are syntactically relevant in Hungarian", "As we do not have presuppositions about which classes have to be used we chose an unsupervised clustering method described in Schulte im Walde 2000", "The 150 most frequent Hungarian verbs were categorized according to their comp 91 Proceedings of the ACL 2007 Student Research Workshop pages 9196 Prague June 2007", "Qc 2007 Association for Computational Linguistics lementation structures in a syntactically annotated corpus the Szeged Treebank Csendes et al 2005", "We are seeking the answer to two questions 1", "Are the resulting clusters semantically coherent", "thus reinforcing the Semantic Base Hypothesis"]}, "P08-1090": {"title": ["Proceedings of ACL08 HLT pages 789797"], "abstract": ["Handcoded scripts were used in the 197080s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge", "We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text", "A narrative event chain is a partially ordered set of events related by a common protagonist", "We describe a three step process to learning narrative event chains", "The first uses unsupervised distributional methods to learn narrative relations between events sharing corefer ring arguments", "The second applies a temporal classifier to partially order the connected events", "Finally the third prunes and clusters selfcontained chains from the space of events", "We introduce two evaluations the narrative cloze to evaluate event relatedness and an order coherence task to evaluate narrative order", "We show a 36 improvement over baseline for narrative prediction and 25 for temporal coherence"], "introduction": ["This paper induces a new representation of structured knowledge called narrative event chains or narrative chains", "Narrative chains are partially ordered sets of events centered around a common protagonist", "They are related to structured sequences of participants and events that have been called scripts Schank and Abelson 1977 or Fillmorean frames", "These participants and events can be filled in and instantiated in a particular text situation to draw inferences", "Chains focus on a single actor to facili tate learning and thus this paper addresses the three tasks of chain induction narrative event induction temporal ordering of events and structured selection pruning the event space into discrete sets", "Learning these prototypical schematic sequences of events is important for rich understanding of text", "Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization coreference resolution and question answering", "For example Schank and Abelson 1977 proposed that understanding text about restaurants required knowledge about the Restaurant Script including the participants Customer Waiter Cook Tables etc the events constituting the script entering sitting down asking for menus etc and the various preconditions ordering and results of each of the constituent actions", "Consider these two distinct narrative chains", "accused X W joined X claimed W served X argued W oversaw dismissed X W resigned It would be useful for question answering or textual entailment to know that X denied  is also a likely event in the left chain while  replaces W temporally follows the right", "Narrative chains such as Firing of Employee or Executive Resigns offer the structure and power to directly infer these new subevents by providing critical background knowledge", "In part due to its complexity automatic induction has not been addressed since the early non statistical work of Mooney and DeJong 1985", "The first step to narrative induction uses an entity based model for learning narrative relations by fol 789 Proceedings of ACL08 HLT pages 789797 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics lowing a protagonist", "As a narrative progresses through a series of events each event is characterized by the grammatical role played by the protagonist and by the protagonists shared connection to surrounding events", "Our algorithm is an unsupervised distributional learning approach that uses core ferring arguments as evidence of a narrative relation", "We show using a new evaluation task called narrative cloze that our protagonistbased method leads to better induction than a verbonly approach", "The next step is to order events in the same narrative chain", "We apply work in the area of temporal classification to create partial orders of our learned events", "We show using a coherencebased evaluation of temporal ordering that our partial orders lead to better coherence judgements of real narrative instances extracted from documents", "Finally the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains"]}, "P08-2021": {"title": [""], "abstract": ["Given several systems automatic translations of the same sentence we show how to combine them into a confusion network whose various paths represent composite translations that could be considered in a subsequent rescoring step", "We build our confusion networks using the method of Rosti et al", "2007 but instead of forming alignments using the tercom script Snover et al 2006 we create alignments that minimize invWER Leusch et al 2003 a form of edit distance that permits properly nested block movements of substrings", "Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better in terms of BLEU and TER than those in tercombased confusion networks"], "introduction": ["Large improvements in machine translation MT may result from combining different approaches to MT with mutually complementary strengths", "Systemlevel combination of translation outputs is a promising path towards such improvements", "Yet there are some significant hurdles in this path", "One must somehow align the multiple outputsto identify where different hypotheses reinforce each other and where they offer alternatives", "One must then This work was partially supported by the DARPA GALE program Contract No HR00110620001", "Also we would like to thank the IBM Rosetta team for the availability of several MT system outputs", "use this alignment to hypothesize a set of new composite translations and select the best composite hypothesis from this set", "The alignment step is difficult because different MT approaches usually reorder the translated words differently", "Training the selection step is difficult because identifying the best hypothesis relative to a known reference translation means scoring all the composite hypotheses of which there may be exponentially many", "Most MT combination methods do create an exponentially large hypothesis set representing it as a confusion network of strings in the target language eg English", "A confusion network is a lattice where every node is on every path ie each time step presents an independent choice among several phrases", "Note that our contributions in this paper could be applied to arbitrary lattice topologies", "For example Bangalore et al", "2001 show how to build a confusion network following a multistring alignment procedure of several MT outputs", "The procedure used primarily in biology Thompson et al 1994 yields monotone alignments that minimize the number of insertions deletions and substitutions", "Unfortunately monotone alignments are often poor since machine translations particularly from different models can vary significantly in their word order", "Thus when Matusov et al", "2006 use this procedure they deterministically reorder each translation prior to the monotone alignment", "The procedure described by Rosti et al", "2007 has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment", "TER is defined as the minimum number of inser 81 Proceedings of ACL08 HLT Short Papers Companion Volume pages 8184 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics tions deletions substitutions and block shifts between two strings", "A remarkable feature of that procedure is that it performs the alignment of the output translations i without any knowledge of the translation model used to generate the translations and ii without any knowledge of how the target words in each translation align back to the source words", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate reorderings", "For this Rosti et al", "2007 use the tercom script Snover et al 2006 which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another", "In this paper we show that one can build better confusion networks in terms of the best translation possible from the confusion network when the pairwise alignments are computed not by tercom which approximately minimizes TER but instead by an exact minimization of invWER Leusch et al 2003 which is a restricted version of TER that permits only properly nested sets of block shifts and can be computed in polynomial time", "The paper is organized as follows a summary of TER tercom and invWER is presented in Section"]}, "P09-1045": {"title": ["Reducing semantic drift with bagging and distributional similarity"], "abstract": ["Iterative bootstrapping algorithms are typically compared using a single set of handpicked seeds", "However we demonstrate that performance varies greatly depending on these seeds and favourable seeds for one algorithm can perform very poorly with others making comparisons unreliable", "We exploit this wide variation with bagging sampling from automatically extracted seeds to reduce semantic drift", "However semantic drift still occurs in later iterations", "We propose an integrated distributional similarity filter to identify and censor potential semantic drifts ensuring over 10 higher precision when extracting large semantic lexicons"], "introduction": ["Iterative bootstrapping algorithms have been proposed to extract semantic lexicons for N L P tasks with limited linguistic resources", "Bootstrapping was initially proposed by Riloff and Jones 1999 and has since been successfully applied to extracting general semantic lexicons Riloff and Jones 1999 Thelen and Riloff 2002 biomedical entities Yu and Agichtein 2003 facts Pasca et al 2006 and coreference data Yang and Su 2007", "Bootstrapping approaches are attractive because they are domain and language independent require minimal linguistic preprocessing and can be applied to raw text and are efficient enough for terascale extraction Pasca et al 2006", "Bootstrapping is minimally supervised as it is initialised with a small number of seed instances of the information to extract", "For semantic lexicons these seeds are terms from the category of interest", "The seeds identify contextual patterns that express a particular semantic category which in turn recognise new terms Riloff and Jones 1999", "Unfortunately semantic drift often occurs when ambiguous or erroneous terms andor patterns are introduced into and then dominate the iterative process Curran et al 2007", "Bootstrapping algorithms are typically compared using only a single set of handpicked seeds", "We first show that different seeds cause these algorithms to generate diverse lexicons which vary greatly in precision", "This makes evaluation unreliable  seeds which perform well on one algorithm can perform surprisingly poorly on another", "In fact random goldstandard seeds often outperform seeds carefully chosen by domain experts", "Our second contribution exploits this diversity we have identified", "We present an unsupervised bagging algorithm which samples from the extracted lexicon rather than relying on existing gazetteers or handselected seeds", "Each sample is then fed back as seeds to the bootstrapper and the results combined using voting", "This both improves the precision of the lexicon and the robustness of the algorithms to the choice of initial seeds", "Unfortunately semantic drift still dominates in later iterations since erroneous extracted terms andor patterns eventually shift the categorys direction", "Our third contribution focuses on detecting and censoring the terms introduced by semantic drift", "We integrate a distributional similarity filter directly into W M E B McIntosh and Curran 2008", "This filter judges whether a new term is more similar to the earlier or most recently extracted terms a sign of potential semantic drift", "We demonstrate these methods for extracting biomedical semantic lexicons using two bootstrap ping algorithms", "Our unsupervised bagging approach outperforms carefully handpicked seeds by  10 in later iterations", "Our distributionalsimilarity filter gives a similar performance im provement", "This allows us to produce large lexicons accurately and efficiently for domainspecific language processing", "396 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 396404 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP"]}, "P09-1068": {"title": ["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 602610"], "abstract": ["We describe an unsupervised system for learning narrative schemas coherent sequences or sets of events arrestedPOLICESUSPECT convicted JUDGE SUSPECT whose arguments are filled with participant semantic roles defined over words JUDGE  judge jury court POLICE  police agent authorities", "Unlike most previous work inevent structure or semantic role learning our system does not use supervised techniques handbuilt knowledge or predefined classes of events or roles", "Our unsupervised learning algorithm uses corefer ring arguments in chains of verbs to learn both rich narrative event structure and argument roles", "By jointly addressing both tasks we improve on previous results in narrativeframe learning and induce rich framespecific semantic roles"], "introduction": ["be learned", "Even unsupervised attempts to learn semantic roles have required a pre defined set of roles Grenager and Manning 2006 and often a handlabeled seed corpus Swier and Stevenson 2004 He and Gildea 2006", "In this paper we describe our attempts to learn scriptlike information about the world including both event structures and the roles of their participants but without pre defined frames roles or tagged corpora", "Consider the following Narrative Schema to be defined more formally later", "The events on the left follow a set of participants through a series of connected events that constitute a narrative Events Roles This paper describes a new approach to event semantics that jointly learns event relations and their A search B A arrest B B plead C A  Police B  Suspect C  Plea participants from unlabeled corpora", "The early years of natural language processing NLP took a topdown approach to language D acquit B D convict B D sentence B D  Jury understanding using representations like scripts Schank and Abelson 1977 structured representations of events their causal relationships and their participants and frames to drive interpretation of syntax and word use", "Knowledge structures such as these provided the interpreter rich information about many aspects of meaning", "The problem with these rich knowledge structures is that the need for hand construction specificity and domain dependence prevents robust and flexible language understanding", "Instead modern work on understanding has focused on shallower representations like semantic roles which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank Palmer et al 2005 and Framenet Baker et al 1998", "Unfortunately creating these supervised corpora is an expensive and difficult multiyear effort requiring complex decisions about the exact set of roles to Being able to robustly learn sets of related events left and framespecific role information about the argument types that fill them right could assist a variety of NLP applications from question answering to machine translation", "Our previous work Chambers and Jurafsky 2008 relied on the intuition that in a coherent text any two events that are about the same participants are likely to be part of the same story or narrative", "The model learned simple aspects of narrative structure narrative chains by extracting events that share a single participant the protagonist", "In this paper we extend this work to represent sets of situationspecific events not unlike scripts caseframes Bean and Riloff 2004 and FrameNet frames Baker et al 1998", "This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema while the shared arguments across verbs can provide rich information for inducing semantic roles", "602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 602610 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP"]}, "P09-1074": {"title": ["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 656664"], "abstract": ["We aim to shed light on the stateoftheart in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions the assumptions made in evaluation methodologies and inherent differences in text corpora", "First we examine three subproblems that play a role in coreference resolution named entity recognition anaphoricity determination and coreference element detection", "We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task", "Second we measure the performance of a stateoftheart coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets"], "introduction": ["As is common for many natural language processing problems the stateoftheart in noun phrase NP coreference resolution is typically quantified based on system performance on manually annotated text corpora", "In spite of the availability of several benchmark data sets eg MUC6 1995 ACE NIST 2004 and their use in many formal evaluations as a field we can make surprisingly few conclusive statements about the stateofthe art in NP coreference resolution", "In particular it remains difficult to assess the effectiveness of different coreference resolution approaches even in relative terms", "For example the 915 Fmeasure reported by McCallum and Well ner 2004 was produced by a system using perfect information for several linguistic subproblems", "In contrast the 713 Fmeasure reported by Yang et al", "2003 represents a fully automatic endtoend resolver", "It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation", "Results vary widely across data sets", "Coreference resolution scores range from 8590 on the ACE 2004 and 2005 data sets to a much lower 60 70 on the MUC 6 and 7 data sets eg Soon et al", "2001 and Yang et al", "2003", "What accounts for these differences", "Are they due to properties of the documents or domains", "Or do differences in the coreference task definitions account for the differences in performance", "Given a new text collection and domain what level of performance should we expect", "We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by stateoftheart systems", "Except for some fairly general statements for example that proper names are easier to resolve than pronouns which are easier than common nouns there has been little analysis of which aspects of the problem have achieved success and which remain elusive", "The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resolution", "For our investigations we employ a stateoftheart classificationbased NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets", "We hypothesize that performance variation within and across coreference resolvers is at least in part a function of 1 the sometimes unstated assumptions in evaluation methodologies and 2 the relative difficulty of the benchmark text corpora", "With these in mind Section 3 first examines three subproblems that play an important role in coreference resolution named entity recognition anaphoricity determination and coreference element detection", "We quantitatively measure the impact of each of these subproblems on coreference resolution performance as a whole", "Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of stateof theart resolvers while improvements to named entity recognition likely offer little gains", "Our results also confirm that the assumptions adopted in 656 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 656664 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP MUC ACE Relative Pronouns no yes Gerunds no yes Nested nonNP nouns yes no Nested NEs no GPE  LOC premod Semantic Types all 7 classes only Singletons no yes Table 1 Coreference Definition Differences for MUC and ACE", "GPE refers to geopolitical entities", "some evaluations dramatically simplify the resolution task rendering it an unrealistic surrogate for the original problem", "In Section 4 we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes", "Our goals are twofold to measure the level of performance of stateoftheart coreference resolvers on different types of anaphora and to develop a quantitative measure for estimating coreference resolution performance on new data sets", "We introduce a coreference performance prediction CPP measure and show that it accurately predicts the performance of our coreference resolver", "As a side effect of our research we provide a new set of muchneeded benchmark results for coreference resolution under common sets of fullyspecified evaluation assumptions"]}, "P09-1113": {"title": ["Distant supervision for relation extraction without labeled data"], "abstract": ["Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small handlabeled corpora", "We investigate an alternative paradigm that does not require labeled corpora avoiding the domain dependence of ACE style algorithms and allowing the use of corpora of any size", "Our experiments use Freebase a large semantic database of several thousand relations to provide distant supervision", "For each pair of entities that appears in some Freebase relation we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier", "Our algorithm combines the advantages of supervised IE combining 400000 noisy pattern features in a probabilistic classifier and unsupervised IE extracting large numbers of relations from large corpora of any domain", "Our model is able to extract 10000 instances of 102 relations at a precision of 676", "We also analyze feature performance showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression"], "introduction": ["At least three learning paradigms have been applied to the task of extracting relational facts from text for example learning that a person is employed by a particular organization or that a geographic entity is located in a particular region", "In supervised approaches sentences in a corpus are first handlabeled for the presence of entities and the relations between them", "The NIST Automatic Content Extraction ACE RDC 2003 and 2004 corpora for example include over 1000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations totaling 16771 relation instances", "ACE systems then extract a wide variety of lexical syntactic and semantic features and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence optionally combining relation men tions Zhou et al 2005 Zhou et al 2007 Surdeanu and Ciaramita 2007", "Supervised relation extraction suffers from a number of problems however", "Labeled training data is expensive to produce and thus limited in quantity", "Also because the relations are labeled on a particular corpus the resulting classifiers tend to be biased toward that text domain", "An alternative approach purely unsupervised information extraction extracts strings of words between entities in large amounts of text and clusters and simplifies these word strings to produce relationstrings Shinyama and Sekine 2006 Banko et al 2007", "Unsupervised approaches can use very large amounts of data and extract very large numbers of relations but the resulting relations may not be easy to map to relations needed for a particular knowledge base", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin 1998 Riloff and Jones 1999 Agichtein and Gravano 2000 Ravichandran and Hovy 2002 Etzioni et al 2005 Pennacchiotti and Pantel 2006 Bunescu and Mooney 2007 Rozenfeld and Feldman 2008", "These seeds are used with a large corpus to extract a new set of patterns which are used to extract more instances which are used to extract more patterns in an iterative fashion", "The resulting patterns often suffer from low precision and semantic drift", "We propose an alternative paradigm distant supervision that combines some of the advantages of each of these approaches", "Distant supervision is an extension of the paradigm used by Snow et al", "2005 for exploiting WordNet to extract hyper nym isa relations between entities and is similar to the use of weakly labeled data in bioinformatics Craven and Kumlien 1999 Morgan et al 1003 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 10031011 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP Relation name New instance locationlocationcontains locationlocationcontains musicartistorigin peopledeceased personplace of death peoplepersonnationality peoplepersonplace of birth bookauthorworks written businesscompanyfounders peoplepersonprofession Paris Montmartre Ontario Fort Erie Mighty Wagon Cincinnati Fyodor Kamensky Clearwater Marianne Yvonne Heemskerk Netherlands Wavell Wayne Hinds Kingston Upton Sinclair Lanny Budd WWE Vince McMahon Thomas Mellon judge Table 1 Ten relation instances extracted by our system that did not appear in Freebase", "2004", "Our algorithm uses Freebase Bollacker et al 2008 a large semantic database to provide distant supervision for relation extraction", "Freebase contains 116 million instances of 7300 relations between 9 million entities", "The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way", "Since there may be many sentences containing a given entity pair we can extract very large numbers of potentially noisy features that are combined in a logistic regression classifier", "Thus whereas the supervised training paradigm uses a small labeled corpus of only 17000 relation instances as training data our algorithm can use much larger amounts of data more text more relations and more instances", "We use 12 million Wikipedia articles and 18 million instances of 102 relations connecting 940000 entities", "In addition combining vast numbers of features in a large classifier helps obviate problems with bad features", "Because our algorithm is supervised by a database rather than by labeled text it does not suffer from the problems of overfitting and domaindependence that plague supervised systems", "Supervision by a database also means that unlike in unsupervised approaches the output of our classifier uses canonical names for relations", "Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities", "Because our algorithm can use large amounts of unlabeled data a pair of entities may occur multiple times in the test set", "For each pair of entities we aggregate the features from the many different sentences in which that pair appeared into a single feature vector allowing us to provide our classifier with more information resulting in more accurate labels", "Table 1 shows examples of relation instances extracted by our system", "We also use this system to investigate the value of syntactic versus lexi cal word sequence features in relation extraction", "While syntactic features are known to improve the performance of supervised IE at least using clean handlabeled ACE data Zhou et al 2007 Zhou et al 2005 we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", "Most previous research in bootstrapping or unsupervised IE has used only simple lexical features thereby avoiding the computational expense of parsing Brin 1998 Agichtein and Gravano 2000 Etzioni et al 2005 and the few systems that have used unsupervised IE have not compared the performance of these two types of feature"]}, "P09-1114": {"title": ["MultiTask Transfer Learning for WeaklySupervised Relation Extraction"], "abstract": ["Creating labeled training data for relation extraction is expensive", "In this paper we study relation extraction in a special weaklysupervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types", "Observing that different relation types can share certain common structures we propose to use a multitask learning method coupled with human guidance to address this weaklysupervised relation extraction problem", "The proposed framework models the commonality among different relation types through a shared weight vector enables knowledge learned from the auxiliary relation types to be transferred to the target relation type and allows easy control of the tradeoff between precision and recall", "Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods"], "introduction": ["Relation extraction is the task of detecting and characterizing semantic relations between entities from free text", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides stateoftheart solutions to the problem Culotta and Sorensen 2004 Zhou et al 2005 Bunescu and Mooney 2005 Qian et al 2008", "However supervised learning heavily relies on a sufficient amount of labeled data for training which is not always available in practice due to the laborintensive nature of human annotation", "This problem is especially serious for relation ex traction because the types of relations to be extracted are highly dependent on the application domain", "For example when working in the financial domain we may be interested in the employment relation but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation and thus have to create training data for the new relation type", "However is the old training data really useless", "Inspired by recent work on transfer learning and domain adaptation in this paper we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weaklysupervised setting where only a few seed instances of the new relation type are available", "While transfer learning was proposed more than a decade ago Thrun 1996 Caruana 1997 its application in natural language processing is still a relatively new territory Blitzer et al 2006 Daume III 2007 Jiang and Zhai 2007a Arnold et al 2008 Dredze and Crammer 2008 and its application in relation extraction is still unexplored", "Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures which can possibly be transferred from the old types to the new type", "We therefore propose to use a general multitask learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together", "By treating classification of different relation types as related tasks the learning framework can naturally model the common syntactic structures among different relation types in a principled manner", "It also allows us to introduce human guidance in separating the common model component from the typespecific components", "The framework naturally transfers the knowledge learned from the old relation types to the new relation type and helps improve the recall of the relation extractor", "We also exploit ad 1012 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 10121020 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP ditional human knowledge about the entity type constraints on the relation arguments which can usually be derived from the definition of a relation type", "Imposing these constraints further improves the precision of the final relation extractor", "Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods improving the average F1 measure from 01532 to 04132 when only 10 seed instances of the new relation type are used"]}, "P10-1115": {"title": ["CrossLingual Latent Topic Extraction"], "abstract": ["Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way", "One common deficiency of existing topic models though is that they would not work well for extracting crosslingual latent topics simply because words in different languages generally do not cooccur with each other", "In this paper we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages", "Specifically we propose a new topic model called Probabilistic CrossLingual Latent Semantic Analysis PCLSA which extends the Probabilistic Latent Semantic Analysis PLSA model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary", "Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract crosslingual latent topics from multilingual text data"], "introduction": ["As a robust unsupervised way to perform shallow latent semantic analysis of topics in text probabilistic topic models Hofmann 1999a Blei et al 2003b have recently attracted much attention", "The common idea behind these models is the following", "A topic is represented by a multinomial word distribution so that words characterizing a topic generally have higher probabilities than other words", "We can then hypothesize the existence of multiple topics in text and define a generative model based on the hypothesized topics", "By fitting the model to text data we can obtain an estimate of all the word distributions corresponding to the latent topics as well as the topic distributions in text", "Intuitively the learned word distributions capture clusters of words that cooccur with each other probabilistically", "Although many topic models have been proposed and shown to be useful see Section 2 for more detailed discussion of related work most of them share a common deficiency they are designed to work only for monolingual text data and would not work well for extracting crosslingual latent topics ie topics shared in text data in two different natural languages", "The deficiency comes from the fact that all these models rely on cooccurrences of words forming a topical cluster but words in different language generally do not cooccur with each other", "Thus with the existing models we can only extract topics from text in each language but cannot extract common topics shared in multiple languages", "In this paper we propose a novel topic model called Probabilistic CrossLingual Latent Semantic Analysis PCLSA model which can be used to mine shared latent topics from unaligned text data in different languages", "PCLSA extends the Probabilistic Latent Semantic Analysis PLSA model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary", "The dictionarybased constraints are key to bridge the gap of different languages and would force the captured cooccurrences of words in each language by PCLSA to be synchronized so that related words in the two languages would have similar probabilities", "PCLSA can be estimated efficiently using the General Expectation Maximization GEM algorithm", "As a topic extraction algorithm PCLSA would take a pair of unaligned document sets in different languages and a bilingual dictionary as input and output a set of aligned word distributions in both languages that can characterize the shared topics in the two languages", "In addition it also outputs a topic cov 1128 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 11281137 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics erage distribution for each language to indicate the relative coverage of different shared topics in each language", "To the best of our knowledge no previous work has attempted to solve this topic extraction problem and generate the same output", "The closest existing work to ours is the MuTo model proposed in BoydGraber and Blei 2009 and the JointLDA model published recently in Jagaralamudi and Daume III 2010", "Both used a bilingual dictionary to bridge the language gap in a topic model", "However the goals of their work are different from ours in that their models mainly focus on mining crosslingual topics of matching word pairs and discovering the correspondence at the vocabulary level", "Therefore the topics extracted using their model cannot indicate how a common topic is covered differently in the two languages because the words in each word pair share the same probability in a common topic", "Our work focuses on discovering correspondence at the topic level", "In our model since we only add a soft constraint on word pairs in the dictionary their probabilities in common topics are generally different naturally capturing which shows the different variations of a common topic in different languages", "We use a crosslingual news data set and a review data set to evaluate PCLSA", "We also propose a crosscollection likelihood measure to quantitatively evaluate the quality of mined topics", "Experimental results show that the PCLSA model can effectively extract crosslingual latent topics from multilingual text data and it outperforms a baseline approach using the standard PLSA on text data in each language"]}, "P10-2025": {"title": ["Word Alignment with Synonym Regularization"], "abstract": ["We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model", "Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language", "We design a generative model for word alignment that uses synonym information as a regularization term", "The experimental results show that our proposed method significantly improves word alignment quality"], "introduction": ["Word alignment is an essential step in most phrase and syntax based statistical machine translation SMT", "It is an inference problem of word correspondences between different languages given parallel sentence pairs", "Accurate word alignment can induce high quality phrase detection and translation probability which leads to a significant improvement in SMT performance", "Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner Vo gel et al 1996 Och and Ney 2003 Fraser and Marcu 2007", "One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus", "This monolingual knowledge makes it easier to determine corresponding words correctly", "For instance functional words in one language tend to correspond to functional words in another language Deng and Gao 2007 and the syntactic dependency of words in each language can help the alignment process Ma et al 2008", "It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality", "A large number of monolingual lexical semantic resources such as WordNet Miller 1995 have been constructed in more than fifty languages Sagot and Fiser 2008", "They include word level relations such as synonyms hypernyms and hyponyms", "Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language", "In this paper we explore a method for using synonym information effectively to improve word alignment quality", "In general synonym relations are defined in terms of word sense not in terms of word form", "In other words synonym relations are usually context or domain dependent", "For instance head and chief are synonyms in contexts referring to working environment while head and forefront are synonyms in contexts referring to physical positions", "It is difficult however to imagine a context where chief and forefront are synonyms", "Therefore it is easy to imagine that simply replacing all occurrences of chief and forefront with head do sometimes harm with word alignment accuracy and we have to model either the context or senses of words", "We propose a novel method that incorporates synonyms from monolingual resources in a bilingual word alignment model", "We formulate a synonym pair generative model with a topic variable and use this model as a regularization term with a bilingual word alignment model", "The topic variable in our synonym model is helpful for disambiguating the meanings of synonyms", "We extend HMBiTAM which is a HMMbased word alignment model with a latent topic with a novel synonym pair generative model", "We applied the proposed method to an EnglishFrench word alignment task and successfully improved the word 137 Proceedings of the ACL 2010 Conference Short Papers pages 137141 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics translation probability from e to f under the kth topic p f e z  k ", "T  Tii  is a state tran sition probability of a first order Markov process", "Fig", "1 shows a graphical model of HMBiTAM", "The total likelihood of bilingual sentence pairsE F  can be obtained by marginalizing out la tent variables z a and  p F E     f p F E z a   d 1 z a Figure 1 Graphical model of HMBiTAM alignment quality"]}, "P10-2066": {"title": ["Distributional Similarity vs PU Learning for Entity Set Expansion"], "abstract": ["Distributional similarity is a classic technique for entity set expansion where the system is given a set of seed entities of a particular class and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds", "This paper shows that a machine learning model called positive and unlabeled learning PU learning can model the set expansion problem better", "Based on the test results of 10 corpora we show that a PU learning technique outperformed distributional similarity significantly"], "introduction": ["The entity set expansion problem is defined as follows Given a set S of seed entities of a particular class and a set D of candidate entities eg extracted from a text corpus we wish to determine which of the entities in D belong to S In other words we expand the set S based on the given seeds", "This is clearly a classification problem which requires arriving at a binary decision for each entity in D belonging to S or not", "However in practice the problem is often solved as a ranking problem ie ranking the entities in D based on their likelihoods of belonging to S The classic method for solving this problem is based on distributional similarity Pantel et al 2009 Lee 1998", "The approach works by comparing the similarity of the surrounding word distributions of each candidate entity with the seed entities and then ranking the candidate entities using their similarity scores", "In machine learning there is a class of semi supervised learning algorithms that learns from positive and unlabeled examples PU learning for short", "The key characteristic of PU learning is that there is no negative training example available for learning", "This class of algorithms is less known to the natural language processing NLP community compared to some other semi supervised learning models and algorithms", "PU learning is a twoclass classification model", "It is stated as follows Liu et al 2002 Given a set P of positive examples of a particular class and a set U of unlabeled examples containing hidden positive and negative cases a classifier is built using P and U for classifying the data in U or future test cases", "The results can be either binary decisions whether each test case belongs to the positive class or not or a ranking based on how likely each test case belongs to the positive class represented by P Clearly the set expansion problem can be mapped into PU learning exactly with S and D as P and U respectively", "This paper shows that a PU learning method called SEM Liu et al 2002 outperforms distributional similarity considerably based on the results from 10 corpora", "The experiments involved extracting named entities eg product and organization names of the same type or class as the given seeds", "Additionally we also compared SEM with a recent method called Bayesian Sets Ghahramani and Heller 2005 which was designed specifically for set expansion", "It also does not perform as well as PU learning", "We will explain why PU learning performs better than both methods in Section 5", "We believe that this finding is of interest to the NLP community", "359 Proceedings of the ACL 2010 Conference Short Papers pages 359364 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics There is another approach used in the Web environment for entity set expansion", "It exploits Web page structures to identify lists of items using wrapper induction or other techniques", "The idea is that items in the same list are often of the same type", "This approach is used by Google Sets Google 2008 and BooWa", "Wang and Cohen 2008", "However as it relies on Web page structures it is not applicable to general free texts"]}, "P101121_p07": {"title": ["BLEUSP INVWER CDER Three improved MT evaluation measures"], "abstract": ["We present three modifications of well established automatic machine translation evaluation measures to improve correlation between those measures and human evaluation", "Following Lin  Och we present an improved version of the BLEU score which uses a smoothed geometric mean for combining different ngram precisions", "We use segment boundary markers to increase the weight of words near the segment boundaries in the BLEU score", "Our second MT evaluation measure is a variant of the WER which allows for block movements but does not demand complete and disjoint coverage of the source sentence", "As this might be problematic if MT systems are tuned on this score we later investigate a linear combination of this measure with PER", "Finally we describe an edit distance similar to TER which also allows for block reordering", "Our measure uses a full search but with the constraint that block operations must be bracketed", "We describe this measure using a Bracketing Transduction Grammar and sketch a polynomialtime algorithm for its calculation", "We also modify the WERlike measures such that they use worddependent substitution costs instead of fixed ones to model the similarity between words", "Experimental comparison of these measures show that our new measures correlate significantly better with human judgment than the original measures"], "introduction": ["For a couple of reasons automatic evaluation of Machine Translation MT systems is a difficult task mostly because it is difficult to define when a translation is good and when it is bad", "Or which of two given translations is better and which one is worse", "The main reason for this are ambiguities in natural languages Usually there is more than one correct translation for a source sentences there are ambiguities in the choice of synonyms as well as in the order of the words", "Because of the difficulty of this task a multitude of automatic MT evaluation measures have been defined over the last couple of years", "Some of these measures have become well established for example BLEU or TER others are only of medium or small significance", "We expect that in the context of NISTs Metrics MATR evaluation more measures will be added to the pool of evaluation measures", "In a few previous papers the proposed measures seemed to be more of theoretical interest than of practical use While they certainly emphasis important linguistic effects it is not investigated systematically in how far these effects play a role in the difference in quality in different MT systems", "Some other proposed evaluation measures seemed to focus on specific properties and features of the previously generated translations they are trained or optimized on which can but does not need to lead to evaluation measures which are basically classifiers dividing previously good from previously poor systems or easy from difficult source sentences", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to play against this and might even learn to produce translations which show the good features without actually being good translations", "For example Rosti et al", "2007 report such an effect", "This is not to say that all new measures share these problems nor that there is no need for MT evaluation measures which go beyond lexical comparison  quite the opposite", "But these issues were the motivation for us to start from established evaluation measures with known properties especially with regard to tuning and alter them at a few selected points to improve their correlation with human judgment", "This paper is organized as follows In Section 2 we describe some modifications to the BLEU score following Lin and Och 2004 and Leusch et al", "2005", "We present a simple variant of WER in Section 3 called CDER which allows for block transposition similar to TER following Leusch et al", "2006", "This measure can be efficiently calculated exactly without having to resort to shift heuristics or greedy search as in TER", "The tradeoff is that this measure by itself measures basically recall not precision", "To overcome this bias we will later propose a linear combination of this measure and PER in Section 6", "Before this in Section 4 we describe another variant of TER which can be exactly calculated in polynomial time this time by restricting possible shifts to ITG constrains", "This method follows Leusch et al", "2003", "We call this measure IN VWER", "In Section 5 we introduce two simple methods following Leusch et al", "2006 to improve edit operationbased measures like PER WER TER and CDERINVWER by taking into account the lexical difference of words in a substitution operation", "After an experimental evaluation of our three proposed evaluation measures in Section 7 we conclude this paper in Section 8"]}, "P101194": {"title": ["A New  Czech Morphological Analyser  ajka"], "abstract": ["new Czech morphological analyser ajka which is based on the algorithmic description of the Czech formal morphology", "First we present two most important wordforming processes in Czech  inflection and derivation", "A brief description of the data structures used for storing morphological information as well as a discussion of the efficient storage of lexical items stem bases of Czech words is included too", "Finally we bring some interesting features of the designed and implemented system ajka together with current statistic data"], "introduction": ["Typically morphological analysis returns the base form lemma and associates it with all the possible POS partofspeech labels together with all grammatical information for each known word form", "In analytical languages a simple approach can be taken it is enough to list all word forms to catch the most of morphological processes", "In English for example a regular verb has usually only 4 distinct forms and irregular ones have at most 8 forms", "On the other hand the highly inflectional languages like Czech or Finnish present a difficulty for such simple approaches as the expansion of the dictionary is at least an order of magnitude greater1 4", "Specialised finitestate compilers have been implemented 1 which allow the use of specific operations for combining base forms and affixes and applying rules for morphophonological variations 3", "Descriptions of morphological analysers for other languages can be found in 8 11", "Basically there are three major types of wordforming processes  inflection derivation and compounding", "Inflection refers to the systematic modification of a stem by means of prefixes and suffixes", "Inflected forms express morphological distinctions like case or number but do not change meaning or POS", "In contrast the process of derivation usually causes change in meaning and often change of POS", "Compounding deals with the process of merging several word bases to form a new word", "1 As our effective implementation of spellchecker for Czech based on finite state au", "tomata suggests it does not necessarily mean that no application can take advantage of a simple listing of word forms in highly inflecting languages", "Czech belongs to the family of inflectional languages which are characterised by the fact that one morpheme typically an ending carries the values of several grammatical categories together for example an ending of nouns typically expresses a value of grammatical category of case number and gender", "This feature requires a special treatment of Czech words in text processing systems", "To this end we developed a universal morphological analyser which performs the morphological analysis based on dividing all words in Czech texts to their smallest relevant components that we call segments  The notion of segment roughly corresponds to the linguistic concept morpheme  which denotes the smallest meaningful unit of a language", "Presented morphological analyser consists of three major parts a formal description of morphological processes via morphological patterns an assignment of Czech stems to their relevant patterns and a morphological analysis algorithm", "The description of Czech formal morphology is represented by a system of inflectional patterns and sets of endings and it includes lists of segments and their correct combinations", "The assignment of Czech stems to their patterns is contained in the Czech Machine Dictionary 10", "Finally algorithm of morphological analysis using this information splits each word into appropriate segments", "The morphological analyser is being used for lemmatisation and morphological tagging of Czech texts in large corpora as well as for generating correct word forms and also as a spelling checker", "It can also be applied to other problems that arise in the area of processing Czech texts eg creating stop lists for building indexes used in information retrieval systems"]}, "P11-1053": {"title": ["Semisupervised Relation Extraction with Largescale Word Clustering"], "abstract": ["We present a simple semisupervised relation extraction system with largescale word clustering", "We focus on systematically exploring the effectiveness of different clusterbased features", "We also propose several statistical methods for selecting clusters at an appropriate level of granularity", "When training on different sizes of data our semisupervised approach consistently outperformed a stateoftheart supervised baseline system"], "introduction": ["Relation extraction is an important information extraction task in natural language processing NLP with many practical applications", "The goal of relation extraction is to detect and characterize semantic relations between pairs of entities in text", "For example a relation extraction system needs to be able to extract an Employment relation between the entities US soldier and US in the phrase US soldier", "Current supervised approaches for tackling this problem in general fall into two categories feature based and kernel based", "Given an entity pair and a sentence containing the pair both approaches usually start with multiple level analyses of the sentence such as tokenization partial or full syntactic parsing and dependency parsing", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al 2000 Kambhatla 2004 Boschee et al 2005 Grishman et al 2005 Zhou et al 2005 Jiang and Zhai 2007", "In contrast the kernel based method does not explicitly extract features it designs kernel functions over the structured sentence representations sequence dependency or parse tree to capture the similarities between different relation instances Zelenko et al 2003 Bunescu and Mooney 2005a Bunescu and Mooney 2005b Zhao and Grishman 2005 Zhang et al 2006 Zhou et al 2007 Qian et al 2008", "Both lines of work depend on effective features either explicitly or implicitly", "The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features", "For example unless the example US soldier has previously been seen in the training data it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not", "Because the syntactic feature of the phrase US soldier is simply a nounnoun compound which is quite general the words in it are crucial for extracting the relation", "This motivates our work to use word clusters as additional features for relation extraction", "The assumption is that even if the word soldier may never have been seen in the annotated Employment relation instances other words which share the same cluster membership with soldier such as president and ambassador may have been observed in the Employment instances", "The absence of lexical features can be compensated by 521 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 521529 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics the cluster features", "Moreover word clusters may implicitly correspond to different relation classes", "For example the cluster of president may be related to the Employment relation as in US president while the cluster of businessman may be related to the Affiliation relation as in US businessman", "The main contributions of this paper are we explore the clusterbased features in a systematic way and propose several statistical methods for selecting effective clusters", "We study the impact of the size of training data on cluster features and analyze the performance improvements through an extensive experimental study", "The rest of this paper is organized as follows Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm", "Section 4 describes in detail a stateoftheart supervised baseline system", "Section 5 describes the cluster based features and the cluster selection methods", "Though Boschee et al", "2005 and Chan and Roth 2010 used word clusters in relation extraction they shared the same limitation as the above approaches in choosing clusters", "For example Boschee et al", "2005 chose clusters of different granularities and Chan and Roth 2010 simply used a single threshold for cutting the word hierarchy", "Moreover Boschee et al", "2005 only augmented the predicate typically a verb or a noun of the most importance in a relation in their definition with word clusters while Chan and Roth 2010 performed this for any lexical feature consisting of a single word", "In this paper we systematically explore the effectiveness of adding word clusters to different lexical features", "3 Background", "31 Relation Extraction", "One of the well defined relation extraction tasks is 1 We present experimental results in Section 6 and the Automatic Content Extraction ACE program conclude in Section 7"]}, "P11-1056": {"title": ["Exploiting SyntacticoSemantic Structures for Relation Extraction"], "abstract": ["In this paper we observe that there exists a second dimension to the relation extraction RE problem that is orthogonal to the relation type dimension", "We show that most of these second dimensional structures are relatively constrained and not difficult to identify", "We propose a novel algorithmic approach to RE that starts by first identifying these structures and then within these identifying the semantic type of the relation", "In the real RE problem where relation arguments need to be identified exploiting these structures also allows reducing pipelined propagated errors", "We show that this RE framework provides significant improvement in RE performance"], "introduction": ["Relation extraction RE has been defined as the task of identifying a given set of semantic binary relations in text", "For instance given the span of text ", "the Seattle zoo   ", " one would like to extract the relation that the Seattle zoo is locatedat Seattle", "RE has been frequently studied over the last few years as a supervised learning task learning from spans of text that are annotated with a set of semantic relations of interest", "However most approaches to RE have assumed that the relations arguments are given as input Chan and Roth 2010 Jiang and Zhai 2007 Jiang 2009 Zhou et al 2005 and therefore offer only a partial solution to the problem", "Conceptually this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest", "However these approaches to RE require a large amount of manually annotated training data to achieve good performance making it difficult to expand the set of target relations", "Moreover as we show these approaches become brittle when the relations arguments are not given but rather need to be identified in the data too", "In this paper we build on the observation that there exists a second dimension to the relation extraction problem that is orthogonal to the relation type dimension all relation types are expressed in one of several constrained syntacticosemantic structures", "As we show identifying where the text span is on the syntacticosemantic structure dimension first can be leveraged in the RE process to yield improved performance", "Moreover working in the second dimension provides robustness to the real RE problem that of identifying arguments along with the relations between them", "For example in the Seattle zoo the entity mention Seattle modifies the noun zoo", "Thus the two mentions Seattle and the Seattle zoo are involved in what we later call a premodifier relation one of several syntacticosemantic structures we identify in Section 3", "We highlight that all relation types can be expressed in one of several syntacticosemantic structures  Premodifiers Possessive Preposition Formulaic and Verbal", "As it turns out most of these structures are relatively constrained and are not difficult to identify", "This suggests a novel algorithmic approach to RE that starts by first identifying these structures and then within these identifying the semantic type of the relation", "Not only does this approach provide significantly improved RE perfor 551 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 551560 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics mance it carries with it two additional advantages", "First leveraging the syntacticosemantic structure is especially beneficial in the presence of small amounts of data", "Second and more important is the fact that exploiting the syntacticosemantic dimension provides several new options for dealing with the full RE problem  incorporating the argument identification into the problem", "We explore one of these possibilities making use of the constrained structures as a way to aid in the identification of the relations arguments", "We show that this already provides significant gain and discuss other possibilities that can be explored", "The contributions of this paper are summarized below  We highlight that all relation types are expressed as one of several syntacticosemantic structures and show that most of these are relatively constrained and not difficult to identify", "Consequently working first in this structural dimension can be leveraged in the RE process to improve performance", " We show that when one does not have a large number of training examples exploiting the syntacticosemantic structures is crucial for RE performance", " We show how to leverage these constrained structures to improve RE when the relations arguments are not given", "The constrained structures allow us to jointly entertain argument can didates and relations built with them as arguments", "Specifically we show that considering argument candidates which otherwise would have been discarded provided they exist in syntacticosemantic structures we reduce error propagation along a standard pipeline RE architecture and that this joint inference process leads to improved RE performance", "In the next section we describe our relation extraction framework that leverages the syntactico semantic structures", "We then present these structures in Section 3", "We describe our mention entity typing system in Section 4 and features for the RE system in Section 5", "We present our RE experiments in Section 6 and perform analysis in Section 7 before concluding in Section 8", "S  premodifier possessive preposition formulaic gold mentions in training data Mtrain Dg  mi  mj   Mtrain  Mtrain  mi in same sentence as mj  i  j  i  j REbase  RE classifier trained on Dg Ds   for each mi  mj   Dg do p  structure inference on mi  mj  using patterns if p  S  mi  mj  was annotated with a S structure Ds  Ds  mi  mj  done REs  RE classifier trained on Ds Output REbase and REs Figure 1 Training a regular baseline RE classifier REbase and a RE classifier leveraging syntactico semantic structures REs "]}, "P11-1082": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 814824"], "abstract": [], "introduction": ["Noun phrase NP coreference resolution is the task of determining which NPs in a text or dialogue refer to the same realworld entity", "The difficulty of the task stems in part from its reliance on world knowledge Charniak 1972", "To exemplify consider the following text fragment", "Martha Stewart is hoping people dont run out on her", "The celebrity indicted on charges stemming from   ", "Having the world knowledge that Martha Stewart is a celebrity would be helpful for establishing the coreference relation between the two NPs", "One may argue that employing heuristics such as subject preference or syntactic parallelism which prefers resolving an NP to a candidate antecedent that has the same grammatical role in this example would also allow us to correctly resolve the celebrity Mitkov 814 and Marcu 2005 Ng 2007 and coreference annotated data eg Bengtson and Roth 2008", "While each of these three sources of world knowledge has been shown to improve coreference resolution the improvements were typically obtained by incorporating world knowledge as features into a baseline resolver composed of a rather weak coreference model ie the mentionpair model and a small set of features ie the 12 features adopted by Soon et als 2001 knowledgelean approach", "As a result some questions naturally arise", "First can world knowledge still offer benefits when used in combination with a richer set of features", "Second since automatically extracted world knowledge is typically noisy Ponzetto and Poesio 2009 are recentlydeveloped coreference models more noise tolerant than the mentionpair model and if so can they profit more from the noisily extracted world knowledge", "Finally while different world knowl Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 814824 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics edge sources have been shown to be useful when applied in isolation to a coreference system do they offer complementary benefits and therefore can further improve a resolver when applied in combination", "We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learningbased coreference resolution", "Specifically we 1 derive world knowledge from encyclopedic sources that are under investigated for coreference resolution including FrameNet Baker et al 1998 and YAGO Suchanek et al 2007 in addition to coreferenceannotated data and unannotated data 2 incorporate such knowledge as features into a richer baseline feature set that we previously employed Rahman and Ng 2009 and 3 evaluate their utility using two coreference models the traditional mentionpair model Soon et al 2001 and the recently developed clusterranking model Rahman and Ng 2009", "Our evaluation corpus contains 410 documents which are coreferenceannotated using the ACE annotation scheme as well as the OntoNotes annotation scheme Hovy et al 2006", "By evaluating on two sets of coreference annotations for the same set of documents we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to annotate the documents"]}, "P11-1125": {"title": ["Machine Translation System Combination by Confusion Forest"], "abstract": ["The stateoftheart system combination method for machine translation MT is based on confusion networks constructed by aligning hypotheses with regard to word similarities", "We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest a packed forest representing alternative trees", "The forest is generated using syntactic consensus among parsed hypotheses First MT outputs are parsed", "Second a context free grammar is learned by extracting a set of rules that constitute the parse trees", "Third a packed forest is generated starting from the root symbol of the extracted grammar through nonterminal rewriting", "The new hypothesis is produced by searching the best derivation in the forest", "Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space"], "introduction": ["System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields such as speech recognition Fiscus 1997 Mangu et al 2000 or parsing Henderson and Brill 1999", "One of the stateoftheart system combination methods for MT is based on confusion networks which are compact graphbased structures representing multiple hypotheses Bangalore et al 2001", "Confusion networks are constructed based on string similarity information", "First one skeleton or backbone sentence is selected", "Then other hypotheses are aligned against the skeleton forming a lattice with each arc representing alternative word candidates", "The alignment method is either modelbased Matusov et al 2006 He et al 2008 in which a statistical word aligner is used to compute hypothesis alignment or editbased Jayaraman and Lavie 2005 Sim et al 2007 in which alignment is measured by an evaluation metric such as translation error rate TER Snover et al 2006", "The new translation hypothesis is generated by selecting the best path through the network", "We present a novel method for system combination which exploits the syntactic similarity of system outputs", "Instead of constructing a stringbased confusion network we generate a packed forest Billot and Lang 1989 Mi et al 2008 which encodes exponentially many parse trees in a polynomial space", "The packed forest or confusion forest is constructed by merging the MT outputs with regard to their syntactic consensus", "We employ a grammarbased method to generate the confusion forest First system outputs are parsed", "Second a set of rules are extracted from the parse trees", "Third a packed forest is generated using a variant of Earleys algorithm Earley 1970 starting from the unique root symbol", "New hypotheses are selected by searching the best derivation in the forest", "The grammar a set of rules is limited to those found in the parse trees", "Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each nonterminal symbol such as parent and sibling labels using the state representation in Earleys algorithm", "1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 12491257 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation WMT10 in four directions Czech French German Spanishto English CallisonBurch et al 2010 and we found comparable performance to the conventional confusion network based system combination in two language pairs and statistically significant improve  I saw the forest I walked the blue forest I saw the green trees the forest was found a Pairwise alignment using the first starred hypothesis as a skeleton", "ments in the others", "First we will review the stateoftheart method which is a system combination framework based on confusion networks 2", "Then we will introduce I saw   walked the blue  forest green trees  was found a novel system combination method based on con b Confusion network from a fusion forest 3 and present related work in consensus translations 4", "Experiments are presentedin Section 5 followed by discussion and our conclu sion", "I saw   walked the blue green forest trees was  found "]}, "P11-2032": {"title": ["Bayesian Word Alignment for Statistical Machine Translation"], "abstract": ["In this work we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectationmaximization EM", "We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 integrating over all possible parameter values in finding the alignment distribution", "We show that Bayesian inference outperforms EM in all of the tested language pairs domains and data set sizes by up to 299 BLEU points", "We also show that the proposed method effectively addresses the wellknown rare word problem in EMestimated models and at the same time induces a much smaller dictionary of bilingual wordpairs"], "introduction": ["Word alignment is a crucial early step in the training of most statistical machine translation SMT systems in which the estimated alignments are used for constraining the set of candidates in phrasegrammar extraction Koehn et al 2003 Chiang 2007 Galley et al 2006", "Stateoftheart word alignment models such as IBM Models Brown et al 1993 HMM Vogel et al 1996 and the jointlytrained symmetric HMM Liang et al 2006 contain a large number of parameters eg word translation probabilities that need to be estimated in addition to the desired hidden alignment variables", "The most common method of inference in such models is expectationmaximization EM Dempster et al 1977 or an approximation to EM when exact EM is intractable", "However being a maxi mization eg maximum likelihood ML or maximum a posteriori MAP technique EM is generally prone to local optima and overfitting", "In essence the alignment distribution obtained via EM takes into account only the most likely point in the parameter space but does not consider contributions from other points", "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore 2004 and a number of heuristic changes to the estimation procedure such as smoothing the parameter estimates were shown to reduce the alignment error rate but the effects on translation performance was not reported", "Zhao and Xing 2006 note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", "Bayesian inference the approach in this paper have recently been applied to several unsupervised learning problems in NLP Goldwater and Griffiths 2007 Johnson et al 2007 as well as to other tasks in SMT such as synchronous grammar induction Blunsom et al 2009 and learning phrase alignments directly DeNero et al 2008", "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al", "2008 Nguyen et al", "2010 and Chung and Gildea 2009", "The former two works place nonparametric priors also known as cache models on the parameters and utilize Gibbs sampling", "However alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA Xu et al 2008 or by local maximization Nguyen et al 2010", "On the other hand 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 182187 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics Chung and Gildea 2009 apply a sparse Dirichlet prior on the multinomial parameters to prevent over fitting", "They use variational Bayes for inference but they do not investigate the effect of Bayesian inference to word alignment in isolation", "Recently Zhao and Gildea 2010 proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters and their inference method is actually stochastic EM also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence", "The set of alignments for a sentence corpus is denoted by a AThe model parameters consist of a VE  VF ta ble T of word translation probabilities such that tef  P f e", "The joint distribution of the Model1 variables is given by the following generative model3 n approximate the expected counts in the Estep", "Even though they report substantial reductions in align P E F A T  P eP aeP f a e T 1 s J ment error rate the translation BLEU scores do not improve", "Our approach in this paper is fully Bayesian in  n P e I  1J s n t j1 eaj fj 2 which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive sparse prior", "We develop a Gibbs sampler for alignments under IBM Model 1 In the proposed Bayesian setting we treat T as a random variable with a prior P T", "To find a suitable prior for T we rewrite 2 as e VE VFwhich is relevant for the stateoftheart SMT sys tems since 1 Model 1 is used in bootstrapping the parameter settings for EM training of higher P E F AT  n s P I  1J n n t e1 f 1 ef nef 3 VE VF P eorder alignment models and 2 many stateofthe  n n tef Nef n J 4art SMT systems use Model 1 translation probabilities as features in their loglinear model", "We eval e1 f 1 I  1 s uate the inferred alignments in terms of the endto end translation performance where we show the results with a variety of input data to illustrate the general applicability of the proposed technique", "To our knowledge this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance"]}, "P11-3012": {"title": ["An Error Analysis of Relation Extraction in Social Media Documents"], "abstract": ["Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another eg part of", "This paper presents an analysis of a relation extraction system based on prior work but applied to the JD Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media", "The results are examined on three different subsets of the JDPA Corpus showing that the system performs much worse on documents from certain sources", "The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs"], "introduction": ["To summarize accurately determine the sentiment or answer questions about a document it is often necessary to be able to determine the relationships between entities being discussed in the document such as partof or memberof", "In the simple sentiment example Example 11 I bought a new car yesterday", "I love the powerful engine", "determining the sentiment the author is expressing about the car requires knowing that the engine is a part of the car so that the positive sentiment being expressed about the engine can also be attributed to the car", "In this paper we examine our preliminary results from applying a relation extraction system to the JD Power and Associates JDPA Sentiment Corpus Kessler et al 2010", "Our system uses lexical features from prior work to classify relations and we examine how the system works on different subsets from the JDPA Sentiment Corpus breaking the source documents down into professionally written reviews blog reviews and social networking reviews", "These three document types represent quite different writing styles and we see significant difference in how the relation extraction system performs on the documents from different sources"]}, "P12-1001": {"title": ["Learning to Translate with Multiple Objectives"], "abstract": ["We introduce an approach to optimize a machine translation MT system on multiple metrics simultaneously", "Different metrics eg BLEU TER focus on different aspects of translation quality our multiobjective approach leverages these diverse aspects to improve overall quality", "Our approach is based on the theory of Pareto Optimality", "It is simple to implement on top of existing singleobjective optimization methods eg MERT PRO and outperforms ad hoc alternatives based on linearcombination of metrics", "We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization"], "introduction": ["Weight optimization is an important step in building machine translation MT systems", "Discrimi native optimization methods such as MERT Och 2003 MIRA Crammer et al 2006 PRO Hopkins and May 2011 and DownhillSimplex Nelder and Mead 1965 have been influential in improving MT systems in recent years", "These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU which serve as surrogate objective for translation quality", "However we know that a single metric such as BLEU is not enough", "Ideally we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality", "Now at Nara Institute of Science  Technology NAIST While many alternatives have been proposed such a perfect evaluation metric remains elusive", "As a result many MT evaluation campaigns now report multiple evaluation metrics CallisonBurch et al 2011 Paul 2010", "Different evaluation metrics focus on different aspects of translation quality", "For example while BLEU Papineni et al 2002 focuses on wordbased ngram precision METEOR Lavie and Agarwal 2007 allows for stemsynonym matching and incorporates recall", "TER Snover et al 2006 allows arbitrary chunk movements while permutation metrics like RIBES Isozaki et al 2010 Birch et al 2010 measure deviation in word order", "Syntax Owczarzak et al 2007 and semantics Pado et al 2009 also help", "Arguably all these metrics correspond to our intuitions on what is a good translation", "The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics", "Can we really claim that a system is good if it has high BLEU but very low METEOR", "Similarly is a highMETEOR lowBLEU system desirable", "Our goal is to propose a multiobjective optimization method that avoids overfitting to a single metric", "We want to build a MT system that does well with respect to many aspects of translation quality", "In general we cannot expect to improve multiple metrics jointly if there are some inherent trade offs", "We therefore need to define the notion of Pareto Optimality Pareto 1906 which characterizes this tradeoff in a rigorous way and distinguishes the set of equally good solutions", "We will describe Pareto Optimality in detail later but roughly speaking a 1 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 110 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics hypothesis is paretooptimal if there exist no other hypothesis better in all metrics", "The contribution of this paper is twofold  We introduce PMO Paretobased Multi objective Optimization a general approach for learning with multiple metrics", "Existing single objective methods can be easily extended to multiobjective using PMO", " We show that PMO outperforms the alternative singleobjective optimization of linearly combined metrics in multiobjective space 1 09 08 07 06 05 04 03 02 01 0 0 01 02 03 04 05 06 07 08 09 1 metric1 and especially obtains stronger results for metrics that may be difficult to tune individually", "In the following we first explain the theory of Pareto Optimality Section 2 and then use it to build up our proposed PMO approach Section 3", "Experiments on NIST ChineseEnglish and PubMed EnglishJapanese translation using BLEU TER and RIBES are presented in Section 4", "We conclude by discussing related work Section 5 and opportunitieslimitations Section 6"]}, "P12-1048": {"title": ["Translation Model Adaptation for Statistical Machine Translation with"], "abstract": ["To adapt a translation model trained from the data in one domain to another previous works paid more attention to the studies of parallel corpus while ignoring the indomain monolingual corpora which can be obtained more easily", "In this paper we propose a novel approach for translation model adaptation by utilizing indomain monolingual topic information instead of the indomain bilingual corpora which incorporates the topic information into translation probability estimation", "Our method establishes the relationship between the outofdomain bilingual corpus and the indomain monolingual corpora via topic mapping and phrasetopic distribution probability estimation from indomain monolingual corpora", "Experimental result on the NIST ChineseEnglish translation task shows that our approach significantly outperforms the baseline system"], "introduction": ["In recent years statistical machine translationSMT has been rapidly developing with more and more novel translation models being proposed and put into practice Koehn et al 2003 Och and Ney 2004 Galley et al 2006 Liu et al 2006 Chiang 2007 Chiang 2010", "However similar to other natural language processingNLP tasks SMT systems often suffer from domain adaptation problem during practical applications", "The simple reason is that the underlying statistical models always tend to closely Part of this work was done during the first authors internship at Baidu", "approximate the empirical distributions of the training data which typically consist of bilingual sentences and monolingual target language sentences", "When the translated texts and the training data come from the same domain SMT systems can achieve good performance otherwise the translation quality degrades dramatically", "Therefore it is of significant importance to develop translation systems which can be effectively transferred from one domain to another for example from newswire to weblog", "According to adaptation emphases domain adaptation in SMT can be classified into translation model adaptation and language model adaptation", "Here we focus on how to adapt a translation model which is trained from the largescale outofdomain bilingual corpus for domainspecific translation task leaving others for future work", "In this aspect previous methods can be divided into two categories one paid attention to collecting more sentence pairs by information retrieval technology Hildebrand et al 2005 or synthesized parallel sentences Ueffing et al 2008 Wu et al 2008 Bertoldi and Federico 2009 Schwenk and Senellart 2009 and the other exploited the full potential of existing parallel corpus in a mixturemodeling Foster and Kuhn 2007 Civera and Juan 2007 Lv et al 2007 framework", "However these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora therefore limiting the potential of further translation quality improvement", "In this paper we propose a novel adaptation method to adapt the translation model for domain specific translation task by utilizing indomain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 459468 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics monolingual corpora", "Our approach is inspired by the recent studies Zhao and Xing 2006 Zhao and Xing 2007 Tam et al 2007 Gong and Zhou 2010 Ruiz and Federico 2011 which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", "For example bank often occurs in the sentences related to the economy topic when translated into yinhang and occurs in the sentences related to the geography topic when translated to hean", "Therefore the cooccurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases", "In a monolingual corpus if bank occurs more often in the sentences related to the economy topic than the ones related to the geography topic it is more likely that bank is translated to yinhang than to hean", "With the outofdomain bilingual corpus we first incorporate the topic information into translation probability estimation aiming to quantify the effect of the topical context information on translation selection", "Then we rescore all phrase pairs according to the phrase topic and the wordtopic posterior distributions of the additional indomain monolingual corpora", "As compared to the previous works our method takes advantage of both the indomain monolingual corpora and the outofdomain bilingual corpus to incorporate the topic information into our translation model thus breaking down the corpus barrier for translation quality improvement", "The experimental effect on the performance of SMT system", "Phrase probability measures the cooccurrence frequency of a phrase pair and lexical probability is used to validate the quality of the phrase pair by checking how well its words are translated to each other", "According to the definition proposed by Koehn et al 2003 given a source sentence f  f J  f1   ", " fj    ", " fJ  a target sentence e  eI  e1   ", " ei   ", " eI  and its word alignment a which is a subset of the Cartesian product of word position s a  j i  j  1   ", " J  i  1   ", " I  the phrase pair f e is said to be consistent Och and Ney 2004 with the alignment if and only if 1 there must be at least one word inside one phrase aligned to a word inside the other phrase and 2 no words inside one phrase can be aligned to a word outside the other phrase", "After all consistent phrase pairs are extracted from training corpus the phrase probabilities are estimated as relative frequencies Och and Ney 2004 countf e e f  1  countf  e  e  l Here countf e indicates how often the phrase pair f e occurs in the training corpus", "To obtain the corresponding lexical weight we first estimate a lexical translation probability distri bution wef  by relative frequency from the train ing corpus f e results on the NIST data set demonstrate the effectiveness of our method", "The reminder of this paper is organized as follows Section 2 provides a brief description of translation probability estimation", "Section 3 introduces the adaptation method which incorporates the topic information into the translation model Section cou nt wef   countf e  2 e l Retaining the alignment a between the phrase pair f e the corresponding lexical weight is calculated as 4 describes and discusses the experimental results Section 5 briefly summarizes the recent related work about translation model adaptation", "Finally we end e 1 pw ef a  n i1 jj i  a  weifj  3 jia with a conclusion and the future work in Section 6"]}, "P12-1079": {"title": ["A Topic Similarity Model"], "abstract": ["Previous work using topic model for statistical machine translation SMT explore topic information at the word level", "However SMT has been advanced from wordbased paradigm to phraserulebased paradigm", "We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrasebased translation", "We associate each synchronous rule with a topic distribution and select desirable rules according to the similarity of their topic distributions with given documents", "We show that our model significantly improves the translation performance over the baseline on NIST ChinesetoEnglish translation experiments", "Our model also achieves a better performance and a faster speed than previous approaches that work at the word level"], "introduction": ["Topic model Hofmann 1999 Blei et al 2003 is a popular technique for discovering the underlying topic structure of documents", "To exploit topic information for statistical machine translation SMT researchers have proposed various topicspecific lexicon translation models Zhao and Xing 2006 Zhao and Xing 2007 Tam et al 2007 to improve translation quality", "Topicspecific lexicon translation models focus on wordlevel translations", "Such models first estimate word translation probabilities conditioned on topics and then adapt lexical weights of phrases  Corresponding author by these probabilities", "However the stateofthe art SMT systems translate sentences by using sequences of synchronous rules or phrases instead of translating word by word", "Since a synchronous rule is rarely factorized into individual words we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level", "Consequently we propose a topic similarity model for hierarchical phrasebased translation Chiang 2007 where each synchronous rule is associated with a topic distribution", "In particular  Given a document to be translated we calculate the topic similarity between a rule and the document based on their topic distributions", "We augment the hierarchical phrasebased system by integrating the proposed topic similarity model as a new feature Section 31", " As we will discuss in Section 32 the similarity between a generic rule and a given source document computed by our topic similarity model is often very low", "We dont want to penalize these generic rules", "Therefore we further propose a topic sensitivity model which rewards generic rules so as to complement the topic similarity model", " We estimate the topic distribution for a rule based on both the source and target side topic models Section 41", "In order to calculate sim ilarities between targetside topic distributions of rules and sourceside topic distributions of given documents during decoding we project 750 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 750758 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics 06 06 06 06 04 04 04 04 02 02 02 02 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30a      opera tional capability b  X1  grands X1 c  X1  give X1 d X1   X2  held talks X1 X2 Figure 1 Four synchronous rules with topic distributions", "Each subgraph shows a rule with its topic distribution where the Xaxis means topic index and the Yaxis means the topic probability", "Notably the rule b and rule c shares the same source Chinese string but they have different topic distributions due to the different English translations", "the targetside topic distributions of rules into the space of sourceside topic model by oneto many projection Section 42", "Experiments on ChineseEnglish translation tasks Section 6 show that our method outperforms the baseline hierarchial phrasebased system by 09 BLE U points", "This result is also 05 points higher and 3 times faster than the previous topicspecific lexicon translation method", "We further show that both the sourceside and targetside topic distributions improve translation quality and their improvements are complementary to each other"]}, "P12-2050": {"title": ["Coarse Lexical Semantic Annotation with Supersenses"], "abstract": ["Lightweight semantic annotation of textcalls for a simple representation ideally without requiring a semantic lexicon to achieve good coverage in the language and domainIn this paper we repurpose WordNets super sense tags for annotation developing specificguidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains", "The resulting corpus has high coverage and was completed quickly with reasonable interannotator agreement"], "introduction": ["The goal of lightweight semantic annotation of text particularly in scenarios with limited resources and expertise presents several requirements for arepresentation simplicity adaptability to new lan guages topics and genres and coverage", "This paper describes coarse lexical semantic annotationof Arabic Wikipedia articles subject to these con straints", "Traditional lexical semantic representations are either narrow in scope like named entities1 or make reference to a fullfledged lexiconontology which may insufficiently cover the languagedomainof interest or require prohibitive expertise and ef fort to apply2 We therefore turn to supersense tags SSTs 40 coarse lexical semantic classes 25 fornouns 15 for verbs originating in WordNet", "Previ ously these served as groupings of English lexicon 1Some ontologies like those in Sekine et al", "2002 and BBN Identifinder Bikel et al 1999 include a large selection of classes which tend to be especially relevant to proper names", "2Eg a WordNet Fellbaum 1998 sense annotation effortreported by Passonneau et al", "2010 found considerable inter annotator variability for some lexemes FrameNet Baker etal 1998 is limited in coverage even for English and Prop Bank Kingsbury and Palmer 2002 does not capture semanticrelationships across lexemes", "We note that the Omega ontology Philpot et al 2003 has been used for finegrained cross lingual annotation Hovy et al 2006 Dorr et al 2010", "COMMUNICATION GROUP 859  XCJ   AD ACT TIME The Guinness Book of World Records considers the University of AlKaraouine in Fez Morocco established in the year 859 AD the oldest university in the world Figure 1 A sentence from the article Islamic GoldenAge with the supersense tagging from one of two anno tators", "The Arabic is shown lefttoright", "entries but here we have repurposed them as target labels for direct human annotation", "Part of the earliest versions of WordNet the supersense categories originally lexicographer classes were intended to partition all English noun and verb senses into broad groupings or semanticfields Miller 1990 Fellbaum 1990", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson 2003 Curran 2005 Ciaramita and Altun 2006 Paa and Reichartz 2009 as well as for Italian Picca et al 2008 Picca et al 2009 Attardi et al 2010 and Chinese Qiu et al 2011 languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", "SSTs both refine and relate lexical items they capture lexical polysemy on the one handeg3Note that work in supersense tagging used text with fine grained sense annotations that were then coarsened to SSTs", "4The nounverb distinction might prove problematic in some languages", "QJK  considers  JJ k Guinness H AJ book  C JAJ  AP   that forrecords thestandard Ag", " university Q AlKaraouine Ai in Fez H Q   Morocco Ag", " oldest university  Y     in ARTIFACT LOCATION A theworld  ADJAK established J in year IJ    k was where   LOCATION 253 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 253258 Jeju Republic of Korea 814 July 2012", "c2012 Association for Computational Linguistics Crusades  Damascus  Ibn Tolun Mosque  Imam Hussein Shrine  Islamic Golden Age  Islamic History  Ummayad Mosque 434s 16185t 5859m Atom  Enrico Fermi  Light  Nuclear power  Periodic Table  Physics  Muhammad alRazi 777s 18559t 6477m 2004 Summer Olympics  Christiano Ronaldo  Football  FIFA World Cup  Portugal football team  Raul Gonzales  Real Madrid 390s 13716t 5149m", "Computer  Computer Software  Internet  Linux  Richard Stallman  Solaris  X Window System 618s 16992t 5754m Table 1 Snapshot of the supersenseannotated data", "The 7 article titles translated in each domain with total counts of sentences tokens and supersense mentions", "Overall there are 2219 sentences with 65452 tokens and 23239 mentions 13 tokensmention on average", "Counts exclude sentences marked as problematic and mentions marked ", "disambiguating PERSON vs POSSESSION for the noun principaland generalize across lexemes on the othereg principal teacher and student can all be PERSONs", "This lumping property might be expected to give too much latitude to annotators yetwe find that in practice it is possible to elicit reason able interannotator agreement even for a languageother than English", "We encapsulate our interpreta tion of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language our annotators had no prior expertise in linguistics or linguistic annotation", "Finally we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering Li and Roth 2002 to animacy hierarchy representation for corpus linguistics Zaenen et al 2004", "We believe the interpretation of the SSTs adopted here can serveas a single starting point for diverse resource en gineering efforts and applications especially when finegrained sense annotation is not feasible"]}, "P13-1110": {"title": ["Online Relative Margin Maximization for Statistical Machine Translation"], "abstract": ["Recent advances in largemargin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization such as the spread of the data", "However these solutions are impractical in complex structured prediction problems such as statistical machine translation", "We present an online gradientbased algorithm for relative margin maximization which bounds the spread of the projected data while maximizing the margin", "We evaluate our optimizer on ChineseEnglish and ArabicEnglish translation tasks each with small and large feature sets and show that our learner is able to achieve significant improvements of 122 BLEU and 1743 TER on average over stateoftheart optimizers with the large feature set"], "introduction": ["The desire to incorporate highdimensional sparse feature representations into statistical machine translation SMT models has driven recent research away from Minimum Error Rate Training MERT Och 2003 and toward other discriminative methods that can optimize more features", "Examples include minimum risk Smith and Eisner 2006 pairwise ranking PRO Hopkins and May 2011 RAMPION Gimpel and Smith 2012 and variations of the margininfused relaxation algorithm MIRA Watanabe et al 2007 Chiang et al 2008 Cherry and Foster 2012", "While the objective function and optimization method vary for each optimizer they can all be broadly described as learning a linear model or parameter vector w which is used to score alternative translation hypotheses", "In every SMT system and in machine learning in general the goal of learning is to find a model that generalizes well ie one that will yield good translations for previously unseen sentences", "However as the dimension of the feature space increases generalization becomes increasingly difficult", "Since only a small portion of all sparse features may be observed in a relatively small fixed set of instances during tuning we are prone to overfit the training data", "An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bi text Tillmann and Zhang 2006 Blunsom et al 2008 Simianer et al 2012 which is usually substantially larger than the tuning set but this is complementary to our goal here of better generalization given a fixed size tuning set", "In order to achieve that goal we need to carefully choose what objective to optimize and how to perform parameter estimation of w for this objective", "We focus on largemargin methods such as SVM Joachims 1998 and passiveaggressive algorithms such as MIRA", "Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one", "This criterion performs well in practice at finding a linear separator in highdimensional feature spaces Tsochantaridis et al 2004 Crammer et al 2006", "Now recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information as in the Second Order Perceptron CesaBianchi et al 2005 Gaussian Margin Machines Crammer et al 2009b confidence weighted learning Dredze and Crammer 2008 AROW Crammer et al 2009a Chiang 2012 and Relative Margin Machines RMM Shivaswamy and Jebara 2009b", "The latter RMM was introduced as an effective and less computationally expensive way to incorporate the spread of the data  second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 11161126 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w Unfortunately not all advances in machine learning are easy to apply to structured prediction problems such as SMT the latter often involve latent variables and surrogate references resulting in loss functions that have not been well explored in machine learning Mcallester and Keshet 2011 Gimpel and Smith 2012", "Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction Shivaswamy and Jebara even where previously MERT was shown to be advantageous 5", "Finally we discuss the spread and other key issues of RM 6 and conclude with discussion of future work 7"]}, "P13-1147": {"title": ["Embedding Semantic Similarity in Tree Kernels for Domain Adaptation"], "abstract": ["Relation Extraction RE is the task of extracting semantic relationships between entities in text", "Recent studies on relation extraction are mostly supervised", "The clear drawback of supervised methods is the need of training data labeled data is expensive to obtain and there is often a mismatch between the training data and the data the system will be applied to", "This is the problem of domain adaptation", "In this paper we propose to combine i term generalization approaches such as word clustering and latent semantic analysis LSA and ii structured kernels to improve the adaptability of relation extractors to new text genresdomains", "The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation"], "introduction": ["Relation extraction is the task of extracting semantic relationships between entities in text eg to detect an employment relationship between the person Larry Page and the company Google in the following text snippet Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21 2012", "Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve stateoftheart accuracy Zelenko et al 2002 Culotta and Sorensen 2004  The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento Povo Italy during the design of the models experiments and writing of the paper", "Zhang et al 2005 Zhou et al 2005 Zhang et al 2006 Bunescu 2007 Nguyen et al 2009 Chan and Roth 2010 Sun et al 2011", "However the clear drawback of supervised methods is the need of training data which can slow down the delivery of commercial applications in new domains labeled data is expensive to obtain and there is often a mismatch between the training data and the data the system will be applied to", "Approaches that can cope with domain changes are essential", "This is the problem of domain adaptation DA or transfer learning TL", "Technically domain adaptation addresses the problem of learning when the assumption of independent and identically distributed iid samples is violated", "Domain adaptation has been studied extensively during the last couple of years for various NLP tasks eg two shared tasks have been organized on domain adaptation for dependency parsing Nivre et al 2007 Petrov and McDonald 2012", "Results were mixed thus it is still a very active research area", "However to the best of our knowledge there is almost no work on adapting relation extraction RE systems to new domains1 There are some prior studies on the related tasks of multitask transfer learning Xu et al 2008 Jiang 2009 and distant supervision Mintz et al 2009 which are clearly related but different the former is the problem of how to transfer knowledge from old to new relation types while distant supervision tries to learn new relations from unlabeled text by exploiting weaksupervision in the form of a knowledge resource eg Freebase", "We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project but it is not clear what data was used", "httptinyurlcom bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 14981507 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics data distribution", "Weak supervision is a promising approach to improve a relation extraction system especially to increase its coverage in terms of types of relations covered", "In this paper we examine the related issue of changes in the underlying data distribution while keeping the relations fixed", "Even a weakly supervised system is expected to perform well when applied to any kind of text other domaingenre thus ideally we believe that combining domain adaptation with weak supervision is the way to go in the future", "This study is a first step towards this", "We focus on unsupervised domain adaptation ie no labeled target data", "Moreover we consider a particular domain adaptation setting single system DA ie learning a single system able to cope with different but related domains", "Most studies on DA so far have focused on building a specialized system for every specific target domain eg Blitzer et al", "2006", "In contrast the goal here is to build a single system that can robustly handle several domains which is in line with the setup of the recent shared task on parsing the web Petrov and McDonald 2012", "Participants were asked to build a single system that can robustly parse all domains reviews weblogs answers emails newsgroups rather than to build several domainspecific systems", "We consider this as a shift in what was considered domain adaptation in the past adapt from source to a specific target and what can be considered a somewhat different recent view of DA that became widespread since 20112012", "The latter assumes that the target domains isare not really known in advance", "In this setup the domain adaptation problem boils US are terms indicating an employment relation between a person and a location", "Rather than only matching the surface string of words lexical similarity enables soft matches between similar words in convolution tree kernels", "In the empirical evaluation on Automatic Content Extraction ACE data we evaluate the impact of convolution tree kernels embedding lexical semantic similarities", "The latter is derived in two ways with a Brown word clustering Brown et al 1992 and b Latent Semantic Analysis LSA", "We first show that our system aligns well with the state of the art on the ACE 2004 benchmark", "Then we test our RE system on the ACE 2005 data which exploits kernels structures and similarities for domain adaptation", "The results show that combining the huge space of tree fragments generalized at the lexical level provides an effective model for adapting RE systems to new domains"]}, "P13-2002": {"title": ["Exact Maximum Inference for the Fertility Hidden Markov Model"], "abstract": ["The notion of fertility in word alignment the number of words emitted by a single state is useful but difficult to model", "Initial attempts at modeling fertility used heuristic search methods", "Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation", "Yet in practice we also need the single best alignment which is difficult to find using Gibbs", "Building on recent advances in dual decomposition this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM", "Finding the best alignment appears important as this model leads to a substantial improvement in alignment quality"], "introduction": ["Wordbased translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs", "These word alignments are a crucial training component in most machine translation systems", "Furthermore they are useful in other NLP applications such as entailment identification", "The simplest models may use lexical information alone", "The seminal Model 1 Brown et al 1993 has proved very powerful performing nearly as well as more complicated models in some phrasal systems Koehn et al 2003", "With minor improvements to initialization Moore 2004 which may be important Toutanova and Galley 2011 it can be quite competitive", "Subsequent IBM models include more detailed information about context", "Models 2 and 3 incorporate a positional model based on the absolute position of the word Models 4 and 5 use a relative position model instead an English word tends to align to a French word that is nearby the French word aligned to the previous English word", "Models 3 4 and 5 all incorporate a notion of fertility the number of French words that align to any English word", "Although these latter models covered a broad range of phenomena estimation techniques and MAP inference were challenging", "The authors originally recommended heuristic procedures based on local search for both", "Such methods work reasonably well but can be computationally inefficient and have few guarantees", "Thus many researchers have switched to the HMM model Vogel et al 1996 and variants with more parameters He 2007", "This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference though the objective function is not concave local maxima are a concern", "Modeling fertility is challenging in the HMM framework as it violates the Markov assumption", "Where the HMM jump model considers only the prior state fertility requires looking across the whole state space", "Therefore the standard forwardbackward and Viterbi algorithms do not apply", "Recent work Zhao and Gildea 2010 described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", "However they do not have a efficient means of MAP inference which is necessary in many applications such as machine translation", "This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition", "The resulting model leads to substantial improvements in alignment quality", "7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 711 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics"]}, "P13-2015": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 8186"], "abstract": ["Most coreference resolvers rely heavily on string matching syntactic properties and semantic attributes of words but they lack the ability to make decisions based on individual words", "In this paper we explore the benefits of lexicalized features in the setting of domainspecific coreference resolution", "We show that adding lexicalized features to offtheshelf coreference resolvers yields significant performance gains on four domainspecific data sets and with two types of coreference resolution architectures"], "introduction": ["Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics such as the ACE ACE03 2003 ACE04 2004 ACE05 2005 and OntoNotes Pradhan et al 2007 data sets", "Many NLP applications however involve text analysis for specialized domains such as clinical medicine Gooch and Roudsari 2012 Glinos 2011 legal text analysis BouayadAgha et al 2009 and biological literature BatistaNavarro and Ananiadou 2011 Castan o et al 2002", "Learningbased coreference resolvers can be easily retrained for a specialized domain given annotated training texts for that domain", "However we found that retraining an offtheshelf coreference resolver with domain specific texts showed little benefit", "This surprising result led us to question the nature of the feature sets used by noun phrase NP coreference resolvers", "Nearly all of the features employed by recent systems fall into three categories string match and word overlap syntactic properties eg appositives predicate nominals parse features etc and semantic matching eg gender agreement WordNet similarity named entity classes etc", "Conspicuously absent from most systems are lexical features that allow the classifier to consider the specific words when making a coreference decision", "A few researchers have experimented with lexical features but they achieved mixed results in evaluations on broadcoverage corpora Bengston and Roth 2008 Bjo rkelund and Nugues 2011 Rahman and Ng 2011a", "We hypothesized that lexicalized features can have a more substantial impact in domainspecific settings", "Lexical features can capture domain specific knowledge and subtle semantic distinctions that may be important within a domain", "For example based on the resolutions found in domainspecific training sets our lexicalized features captured the knowledge that tomcat can be coreferent with plane UAW can be coreferent with union and anthrax can be coreferent with diagnosis", "Capturing these types of domainspecific information is often impossible using only generalpurpose resources", "For example WordNet defines tomcat only as an animal does not contain an entry for UAW and categorizes anthrax and diagnosis very differently1 In this paper we evaluate the impact of lexicalized features on 4 domains management succession MUC6 data vehicle launches MUC7 data disease outbreaks ProMed texts and terrorism MUC4 data", "We incorporate lexical ized feature sets into two different coreference architectures Reconcile Stoyanov et al 2010 a pairwise coreference classifier and Sieve Raghunathan et al 2010 a rulebased system", "Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures"]}, "P13-2122": {"title": ["Incremental TopicBased Translation Model Adaptation for"], "abstract": ["We describe a translation model adaptation approach for conversational spoken language translation CSLT which encourages the use of contextually appropriate translation options from relevant training conversations", "Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations which is used to bias translation choices towards the current context", "A significant novelty of our adaptation technique is its incremental nature we continuously update the topic distribution on the evolving test conversation as new utterances become available", "Thus our approach is wellsuited to the causal constraint of spoken conversations", "On an EnglishtoIraqi CSLT task the proposed approach gives significant improvements over a baseline system as measured by BLEU TER and NIST", "Interestingly the incremental approach outperforms a nonincremental oracle that has upfront knowledge of the whole conversation"], "introduction": ["Conversational spoken language translation CSLT systems facilitate communication between subjects who do not speak the same language", "Current systems are typically used to achieve a specific task eg vehicle checkpoint search medical diagnosis etc", "These taskdriven Disclaimer This paper is based upon work supported by the DARPA BOLT program", "The views expressed here are those of the authors and do not reflect the official policy or position of the Department of Defense or the US Government", "Distribution Statement A Approved for Public Release Distribution Unlimited conversations typically revolve around a set of central topics which may not be evident at the beginning of the interaction", "As the conversation progresses however the gradual accumulation of contextual information can be used to infer the topics of discussion and to deploy contextually appropriate translation phrase pairs", "For example the word drugs will predominantly translate into Spanish as medicamentos medicines in a medical scenario whereas the translation drogas illegal drugs will predominate in a law enforcement scenario", "Most CSLT systems do not take highlevel global context into account and instead translate each utterance in isolation", "This often results in contextually inappropriate translations and is particularly problematic in conversational speech which usually exhibits short spontaneous and often ambiguous utterances", "In this paper we describe a novel topicbased adaptation technique for phrasebased statistical machine translation SMT of spoken conversations", "We begin by building a monolingual latent Dirichlet allocation LDA topic model on the training conversations each conversation corresponds to a document in the LDA paradigm", "At runtime this model is used to infer a topic distribution over the evolving test conversation up to and including the current utterance", "Translation phrase pairs that originate in training conversations whose topic distribution is similar to that of the current conversation are given preference through a single similarity feature which augments the standard phrasebased SMT loglinear model", "The topic distribution for the test conversation is updated incrementally for each new utterance as the available history grows", "With this approach we demonstrate significant improvements over a baseline phrasebased SMT system as measured by BLEU TER and NIST scores on an EnglishtoIraqi CSLT task", "697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 697701 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics"]}, "P160500_w06": {"title": ["A Hybrid Method for Entity Hyponymy Acquisition"], "abstract": ["Extracting entity hyponymy in Chinese complex sentences can be a highly difficult process", "This paper proposes a novel hybrid approach that combines parsing with supervised learning and semisupervised learning", "First conditional random fields CRF model is employed to obtain the candidate domain named entity", "Pattern matching is then used to acquire candidate hyponymy", "Next predicate and symbol features syntactic analysis and semantic roles are introduced into the CRF features template to identify the hyponymy entity pairs", "Finally analysis of both the parallel relationship of entities among sentences and entity pairs in simple sentences is conducted to obtain the hyponymy entity pairs in Chinese complex sentences", "The experimental results show that the proposed method reduces the manual work required for CRF markers and has an improved overall performance in comparison with the baseline methods", "Keywords Chinese complex sentences hyponymy entity identification CRF pattern matching syntactic analysis DOI 103103S0146411616050035"], "introduction": ["The automatic extraction of hyponymy between entities in a specific field is often a difficult step to accomplish when developing ontology knowledge bases or during construction of knowledge graphs which is regarded as a crucial part for developing highlevel natural language applications", "Many algorithms have been developed to achieve hyponymy automatically including dictionarybased pattern based and statistical machine learning", "The dictionarybased method used to acquire hyponymy usually depends on field words synonyms or approximate synonyms in the manmade dictionary", "For instance Nakaya 1 obtained English domain concepts hyponymy with WordNet 2", "Moreover Li 3 et al used the geographical names dictionary for acquisition of toponym ontology concept relations in Chinese", "However the coverage of WordNet or HowNet 4 is still limited in the general field and they are weak in the coverage of domain specific terms and named entities", "Patternbased methods mainly use linguistics and natural language processing technologies which consider patterns as rules summarized according to some inherent regular language", "For example in this Chinese sentence    The scenic spots of Lijiang contains Lijiang old town Yulong jokul and Lugu lake etc here Lijiang Lijiang old town Yulong jokul Lugu lake are entities Ei is used to represent the entity i i  1 2  and makes E1  Lijiang E2  Lijiang old town E3  Yulong jokul E4  Lugu lake the pattern E1 contains E2  E3 and E4 can be obtained where E1 is the hypernym of E2 E3 and E4 in this context", "Several researchers have studied the subject in regards to the English language", "Hearst 5 raised 6 lexicalsyntactic patterns and avoided the use of the precoding method of knowledge and automatic recognition hyponymy from large text corpus Tuan 6 et al put forward to build a term classification hierarchy model based on the combination of pattern matching and statistical methods Bansal 7 et al used an automatic acquisition pattern and a belief propagation algorithm to obtain the high precision hypernym of arbitrary noun phrases", "In addition 1 The article is published in the original", "369 to the English language various studies have been conducted on Chinese", "For example Wu 8 et al established a universal grammar pattern library for the hyponymy relationship according to the partwhole relationship of the six different kinds of forms In view of the ISA pattern Tang 9 et al demonstrated hyponym concepts using syntactic parsing results Tian 10 et al proposed a combination of a bootstrap ping method and a Chinese doubly anchored pattern of the hypernym", "The patternbased method is highly accurate for specific sentences but compared to English Chinese language has the more complicated sentence structure and semantic so it is difficult to use this technique in generalization", "The statistical machine learning method is mainly based on corpus linguistic knowledge and the statistical language model and it uses the machinelearning algorithm to obtain the concept hyponymy relationship", "In the field of English Fan 11 et al considered the task as a matrix factorization problem to extract the relation", "For Chinese Xia 12 et al proposed a method of graph clustering to extract the hypo nymy Fu 13 et al joined a variety of features into SVM support vector machine based on the RBF radial basis function kernel", "Statistical machine learning is considered the main method for hyponymy extraction due to its advantages of reducing manual annotation and extracting information automatically", "The three methods described above generally extract entity hyponymy relations in a simple sentence", "For complex sentences it is necessary to cooperate syntactic patterns with entity recognition as well as other approaches to improve the overall performances especially for the Chinese", "In addition patterns should be extracted using the bootstrapping method", "The current paper proceeds as follows Section 2 describes the methods and the framework Section 3 discusses the process of hyponymy acquisition Section 4 reports the results Section 5 presents the conclusions"]}, "P39_p07": {"title": ["Efficient Extraction of Oraclebest Translations from Hypergraphs"], "abstract": ["Hypergraphs are used in several syntax inspired methods of machine translation to compactly encode exponentially many trans lation hypotheses", "The hypotheses closest to given reference translations therefore cannot be found via brute force particularly for pop ular measures of closeness such as BLEU", "We develop a dynamic program for extracting the so called oraclebest hypothesis from a hyper graph by viewing it as the problem of finding the most likely hypothesis under an ngram language model trained from only the refer ence translations", "We further identify and re move massive redundancies in the dynamic program state due to the sparsity of ngrams present in the reference translations resulting in a very efficient program", "We present run time statistics for this program and demon strate successful application of the hypothe ses thus found as the targets for discriminative training of translation system components"], "introduction": ["A hypergraph as demonstrated by Huang and Chi ang 2007 is a compact datastructure that can en code an exponential number of hypotheses gener ated by a regular phrasebased machine translation MT system eg Koehn et al", "2003 or a syntax based MT system eg Chiang 2007", "While the hypergraph represents a very large set of transla tions it is quite possible that some desired transla tions eg the reference translations are not con tained in the hypergraph due to pruning or inherent deficiency of the translation model", "In this case one is often required to find the translations in the hy pergraph that are most similar to the desired transla tions with similarity computed via some automatic metric such as BLEU Papineni et al 2002", "Such maximally similar translations will be called oracle best translations and the process of extracting them oracle extraction", "Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph and the exponentially large number of hypotheses makes a bruteforce linear search intractable", "Therefore efficient algorithms that can exploit the structure of the hypergraph are required", "We present an efficient oracle extraction algo rithm which involves two key ideas", "Firstly we view the oracle extraction as a bottomup model scoring process on a hypergraph where the model is trained on the reference translations", "This is sim ilar to the algorithm proposed for a lattice by Dreyer et al", "2007", "Their algorithm however requires maintaining a separate dynamic programming state for each distinguished sequence of state words and the number of such sequences can be huge mak ing the search very slow", "Secondly therefore we present a novel lookahead technique called equiv alent oraclestate maintenance to merge multiple states that are equivalent for similarity computation", "Our experiments show that the equivalent oracle state maintenance technique significantly speeds up more than 40 times the oracle extraction", "Efficient oracle extraction has at least three im portant applications in machine translation", "Discriminative Training In discriminative train ing the objective is to tune the model parameters eg weights of a perceptron model or conditional random field such that the reference translations are preferred over competitors", "However the reference translations may not be reachable by the translation system in which case the oraclebest hypotheses should be substituted in training", "9 Proceedings of NAACL HLT 2009 Short Papers pages 912 Boulder Colorado June 2009", "2009 Association for Computational Linguistics System Combination In a typical system combi nation task eg Rosti et al", "2007 each compo nent system produces a set of translations which are then grafted to form a confusion network", "The confusion network is then rescored often employ ing additional language models to select the fi nal translation", "When measuring the goodness of a hypothesis in the confusion network one requires its score under each component system", "However some translations in the confusion network may not be reachable by some component systems in which case a systems score for the most similar reachable translation serves as a good approximation", "Multisource Translation In a multisource translation task Och and Ney 2001 the input is given in multiple source languages", "This leads to a situation analogous to system combination except that each component translation system now corresponds to a specific source language"]}, "P41718_w06": {"title": ["Construction of a Russian Paraphrase Corpus"], "abstract": ["This paper presents a crowdsourcing project on the creation of a publicly available corpus of sentential paraphrases for Russian", "Collected from the news headlines such corpus could be applied for information extraction and text summarization", "We collect news headlines from dierent agencies in real time paraphrase candidates are extracted from the headlines using an unsuper vised matrix similarity metric", "We provide userfriendly online interface for crowdsourced annotation which is available at paraphraserru", "There are 5181 annotated sentence pairs at the moment with 4758 of them included in the corpus", "The annotation process is going on and the current version of the corpus is freely available at httpparaphraserru", "Keywords Russian paraphrase corpus  Lexical similarity metric  Unsupervised paraphrase extraction  Crowdsourcing"], "introduction": ["Our aim is to create a publicly available Russian paraphrase corpus which could be applied for information extraction IE text summarization TS and compression", "We believe that such corpus can be helpful for paraphrase identication and generation for Russian and that is why we focus on the sentential paraphrases", "Indeed a sentential corpus does not impose any specic methods of further paraphrase identication or generation on the researcher", "If such corpus is representative enough it can serve as a dataset for the experiments on the extraction of word phrase and syntactic level para phrases", "Paraphrase is restatement of a text it conveys the same meaning in another form", "Such natural language processing NLP tasks as paraphrase identication and genera tion have been shown to be helpful for IE 25 question answering 14 machine trans lation 7 TS 19 text simplication 29 etc Paraphrase identication is used to detect plagiarism 6 and to remove redundancies in TS 19 and IE 25 while paraphrase generation  to expand queries in information retrieval and question answering 14 and patterns  in IE", "Paraphrase generation is also useful for text normalization 28 and textual entailment recognition tasks 9", "As far as the denition of paraphrase is concerned it generally implies that the same message is expressed in dierent words but it does not prescribe which portion of text is replaced by paraphrasing", "Neither does it state whether common knowledge can be  Springer International Publishing Switzerland 2016 P Braslavski et al", "Eds RuSSIR 2015 CCIS 573 pp", "146157 2016", "DOI 10100797833194171898 used when judging on the similarity of the two messages", "As a consequence of this ambiguity some researchers believe that paraphrases should have absolute semantic equivalence while others allow for bidirectional textual entailment when the two messages convey roughly the same meaning", "Let us consider an example from our corpus 1 BT oe poa o  Tele2  ae ee", "VTB might sell its shares in TELE2 in the nearest weeks 2 BT aocpoa poay Tele2", "VTB announced the sale of TELE2 Although it is clear that the two sentences describe the same event the rst one has additional details indication of the time and the fact that the shares are going to be sold", "A human judger with hisher knowledge about the world might consider these sentences paraphrases", "But if we intend to teach a machine to identify semantically equivalent paraphrases a threshold for paraphrases should be higher", "On the other hand the second sentence can be considered a summarization of the rst one and therefore such types of paraphrases can be used in automatic TS", "In our research we intend to construct paraphrase corpus for IE and TS", "We believe that the former task requires semantically equivalent or precise paraphrases while the latter one demands roughly similar ones socalled loose paraphrases like those in our example", "Thus it is important for us to distinguish precise paraphrases PP and loose paraphrases LP while constructing our paraphrase corpus", "Today there are already a number of available paraphrase resources Microsoft Para phrase Corpus being the most wellknown of them 13", "A wide number of metrics for paraphrase identication for English are evaluated against this corpus", "For Russian there are no publicly available paraphrase resources known to us with the only exception of the dataset published by Ganitkevich et al as part of The Para phrase Database project 17", "The latter includes paraphrases on the word phrase and syntactic levels and each paraphrase pair is annotated with the set of count and prob abilitybased features", "Such corpus can be used for both IE and TS but it lacks infor mation on the context of paraphrases", "We believe that if such context the original sentences was provided it could improve both these NLP tasks", "That is why we aim at constructing a sentential corpus", "Thus our task is to construct a corpus with both PPs and LPs and to make it helpful for paraphrase identication and generation in IE TS and text compression tasks", "Our research is a part of an ongoing crowdsourcing project available at para phraserru with our current results available at paraphraserruscorerstat"]}, "P846406_w06": {"title": ["Learning by Reading An Experiment in Text Analysis"], "abstract": ["It has long been a dream to build computer systems that learn automatically by reading text", "This dream is generally considered infeasible but some surprising developments in the US over the past three years have led to the funding of several shortterm investigations into whether and how much the best current practices in Natural Language Processing and Knowledge Representation and Reasoning when combined actually enable this dream", "This paper very briefly describes one of these efforts the Learning by Reading project at ISI which has converted a high school textbook of Chemistry into very shallow logical form and is investigating which semantic features can plausibly be added to support the kinds of inference required for answering standard high school text questions"], "introduction": ["From almost the beginnings of Artificial Intelligence it was clear that automated systems require knowledge to reason intelligently and that for multipurpose widedomain robust reasoning the amount required is nontrivial", "Experience especially with expert systems during the 1970s illustrated just how hard it is to acquire enough of the right knowledge and how difficult it is to formalize that knowledge in ways suitable for supporting reasoning", "Naturally then the dream arose to enable systems to read text and learn by themselves", "But this dream has never been realized", "In fact as research in Knowledge Representation and Reasoning KRR and Natural Language Processing NLP progressed the two areas diverged to the point where today they are more or less entirely separate with unrelated conferences journals and research paradigms", "A few years ago three research groups funded by Vulcan Inc participated in an audacious experiment called Project Halo to manually convert the information contained in one chapter of a high school textbook on Chemistry into knowledge representation statements and then to have the knowledge representation system take a standard high school Advanced Placement AP exam", "Surprisingly two of the three systems passed albeit at a relatively low level of performance", "The project engendered wide interest see Friedland et al 2003", "Over the past year DARPA has funded five groups in the US to conduct pilot studies that investigate the feasibility of building fully Learning by Reading LbR systems", "The largest Project Mbius is a consortium of some 20 researchers from numerous institutions", "Its goal Petr Sojka Ivan Kopec ek and Karel Pala Eds TSD 2006 LNAI 4188 pp", "312 2006", "Oc SpringerVerlag Berlin Heidelberg 2006 is to design a general framework for LbR systems in the future and to advise DARPA on the wisdom of funding a new program in this area", "Typical questions include How feasible is fully automated LbR", "What are the different phasescomponentssteps of LbR", "What are the current levels of capability of the component technologies and where are the major bottlenecks and failure points", "What kind of research would best further the dream of LbR", "What sorts of results could one expect after five years", "The remaining four projects proceed independently but report back to Mbius", "All are smaller 9month efforts and each focuses on one or more specific aspects of the general LbR problem  A project jointly at Boeing and SRI led by Peter Clark of Boeing focuses on the mismatch between English sentences and their equivalent knowledge representations propositions with the methodology of building manually the representations for a carefully selected extract of 5 pages from the Chemistry textbook", " A project at Northwestern University led by Ken Forbus concentrates on the processes of selfguided inference that occurs after new information is read", "Called introspection or rumination these processes work in parallel with the reading and serve as a source of expectations questions and background checking", "This project focuses on a few selected sentences from the Chemistry textbook and the thoughts that may arise from them", " A project at CYC Corp led by Michael Witbrock addresses the problem of learning the meaning of new unknown words in context", "Starting with the knowledge already inside their very large ontology and reasoning system Cyc researchers develop methods to apply inferences in order to build up likely interpretations of a sentence and from these hypothesize the meaning of the unknown word", " The fourth project is the subject of this paper", "Located at ISI we are investigating how much can be done by combining traditional and statistical NLP methods and what kinds of KRR are absolutely required at which points in the process", "We have parsed the whole Chemistry textbook have developed methods to convert the parses into shallow logical form and are investigating the what types of semantics should be added to support the reasoning required for question answering", "In this paper we briefly outline the architecture and general aspects of ISIs LbR project which will finish in August namely about three months after the time of writing this paper"]}, "P87-94": {"title": ["International Journal of Advanced Intelligence"], "abstract": ["We present an approximative IBM Model 4 for word alignment", "Dierent with the most widelyused word aligner GIZA which implements all the 5 IBM models and HMM model in the framework of Expectation Maximum EM we adopt a full Bayesian inference which integrates over all possible parameter values rather than estimating a single parameter value", "Empirical results show promising improvements in alignment quality as well as in BLEU score for the translation performance over baselines", "Keywords  Bayesian inference Word alignment Statistical machine translation"], "introduction": ["Word alignment can be dened as a procedure for detecting the corresponding words in a bilingual sentence pair", "One of the notorious criticisms of word alignment is the inconsistence between the word alignment model to the phrase based translation model", "In this paper we have no intention to avoid mentioning this inherent weakness of word alignment but we would say as far as we know word alignment is a fundamental component for most of the SMT systems", "Phrase or the other higher level translation knowledge is extracted based on the word alignment which is called twostage approach", "And even for approaches of socalled direct phrase alignment they can rarely abandon word alignment thoroughly", "Because of the computation complexity of phrase alignment word alignment is usually used to constrain the inference1 DeNero proposes a relative pure joint phrase model but still uses the word alignment as initialization and smoothing which shows the least dependency on word alignment2 Neubig uses Bayesian methods and Inversion Transduction Grammar for joint phrase alignment3 and the base distribution for the Dirichlet Process 5 prior is constructed by the word alignment model", "Therefore word alignment is well worth concern", "Our hope is to induce a better word alignment by 111 NojiHigashi Kusatsu Shiga Japan", "87 88 Z Li H Ikeda utilizing the stateoftheart learning technology and establish a better baseline for the word level alignment models", "Bayesian inference the approach we adopt in this paper has been broadly applied to various learning of latent structure", "Goldwater points out that two theoretical factors contribute to the superiority of Bayesian inference7 First integrating over parameter values leads to greater robustness in decision", "One of the problems that trouble EM algorithm is overtting", "Moore discusses details of how a Garbage collector is generated11 He also suggests a number of heuristic solutions but Bayesian inference can oer a more principled solution", "The second factor is that the integration permits the use of priors favoring sparse distributions which proved to be more consistent with nature of natural language", "Another practical advantage is that the implementation can be much easier than EM12 In the following sections we will have a review for IBM Model 4 in Section 2 and reformulate it into a simpler and Bayesian form in Section 3", "Section 4 gives the Bayesian inference and Section 5 reports results of experiment", "Section 6 compares related research and Section 7 concludes"]}, "P96-1010": {"title": [""], "abstract": ["This paper addresses the problem of cor recting spelling errors that result in valid though unintended words such as peace and piece or quiet and quite and also the problem of correcting particular word usage errors such as amount and num ber or among and between", "Such cor rections require contextual information and are not handled by conventional spelling programs such as Unix spell", "First we introduce a method called Trigrams that uses partofspeech trigrams to encode the context", "This method uses a small num ber of parameters compared to previous methods based on word trigrams", "How ever it is effectively unable to distinguish among words that have the same part of speech", "For this case an alternative featurebased method called Bayes per forms better but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints", "A hybrid method called Tribayes is then in troduced that combines the best of the pre vious two methods", "The improvement in performance of Tribayes over its compo nents is verified experimentally", "Tribayes is also compared with the grammar checker in Microsoft Word and is found to have sub stantially higher performance"], "introduction": ["Spelling correction has become a very common tech nology and is often not perceived as a problem where progress can be made", "However conventional spelling checkers such as Unix spell are concerned only with spelling errors that result in words that cannot be found in a word list of a given language", "One analysis has shown that up to 15 of spelling errors that result from elementary typographical er rors character insertion deletion or transposition yield another valid word in the language Peterson 1986", "These errors remain undetected by tradi tional spelling checkers", "In addition to typographical errors words that can be easily confused with each other for instance the homophones peace and piece also remain undetected", "Recent studies of actual ob served spelling errors have estimated that overall errors resulting in valid words account for anywhere from 25 to over 50 of the errors depending on the application Kukich 1992", "We will use the term contextsensitive spelling cor rection to refer to the task of fixing spelling errors that result in valid words such as I  Can I have a peace of cake", "where peace was typed when piece was intended", "The task will be cast as one of lexical disambigua tion we are given a predefined collection of confu sion sets such as peace piece than then etc which circumscribe the space of spelling errors to look for", "A confusion set means that each word in the set could mistakenly be typed when another word in the set was intended", "The task is to predict given an occurrence of a word in one of the confusion sets which word in the set was actually intended", "Previous work on contextsensitive spelling cor rection and related lexical disambiguation tasks has its limitations", "Wordtrigram methods Mays Dam erau and Mercer 1991 require an extremely large body of text to train the wordtrigram model even with extensive training sets the problem of sparse data is often acute", "In addition huge wordtrigram tables need to be available at run time", "More over word trigrams are ineffective at capturing long distance properties such as discourse topic and tense", "Featurebased approaches such as Bayesian clas sifiers Gale Church and Yarowsky 1993 deci sion lists Yarowsky 1994 and Bayesian hybrids Golding 1995 have had varying degrees of suc cess for the problem of contextsensitive spelling correction", "However we report experiments that show that these methods are of limited effective ness for cases such as their there theyre and than then where the predominant distinction to be made among the words is syntactic", "Confusion set Train Test Most freq", "Base their there theyre 3265 850 than then 2096 514 its its 1364 366 your youre 750 187 begin being 559 146 passed past 307 74 quiet quite 264 66 weather whether 239 61 accept except 173 50 lead led 173 49 cite sight site 115 34 principal principle 147 34 raise rise 98 39 affect effect 178 49 peace piece 203 50 country county 268 62 amount number 460 123 among between 764 186 their 568 than 634 its 913 your 893 being 932 past 689 quite 833 whether 869 except 700 led 469 sight 647 principle 588 nse 641 effect 918 peace 440 country 919 number 715 between 715 Table 1 Performance of the baseline method for 18 confusion sets", "Train and Test give the number of occurrences of any word in the confusion set in the training and test corpora", "Most freq is the word in the confusion set that occurred most often in the training corpus", "Base is the percentage of correct predictions of the baseline system on the test corpus", "In this paper we first introduce a method called Trigrams that uses partofspeech trigrams to en code the context", "This method greatly reduces the number of parameters compared to known methods which are based on word trigrams", "This method also has the advantage that training can be done once and for all and quite manageably for all con fusion sets new confusion sets can be added later without any additional training", "This feature makes Trigrams a very easily expandable system", "Empirical evaluation of the trigram method demonstrates that it performs well when the words to be discriminated have different parts of speech but poorly when they have the same part of speech", "In the latter case it is reduced to simply guessing whichever word in the confusion set is the most com mon representative of its partofspeech class", "We consider an alternative method Bayes a Bayesian hybrid method Golding 1995 for the case where the words have the same part of speech", "We confirm experimentally that Bayes and Trigrams have complementary performance Trigrams being better when the words in the confusion set have dif ferent parts of speech and Bayes being better when they have the same part of speech", "We introduce a hybrid method Tribayes that exploits this com plementarity by invoking each method when it is strongest", "Tribayes achieves the best accuracy of the methods under consideration in all situations", "To evaluate the performance of Tribayes with re spect to an external standard we compare it to the grammar checker in Microsoft Word", "Tribayes is found to have substantially higher performance", "This paper is organized as follows first we present the methodology used in the experiments", "We then discuss the methods mentioned above interleaved with experimental results", "The comparison with Mi crosoft Word is then presented", "The final section concludes"]}, "P98-1046": {"title": [""], "abstract": [], "introduction": ["In this paper we specifically address questions of polysemy with respect to verbs and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases", "We see verb classes as the key to making gen eralizations about regular extensions of mean ing", "Current approaches to English classifica tion Levin classes and WordNet have limita tions in their applicability that impede their utility as general classification schemes", "We present a refinement of Levin classes intersec tive sets which are a more finegrained clas sification and have more coherent sets of syn tactic frames and associated semantic compo nents", "We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig inal Levin classes", "We also have begun to ex amine related classes in Portuguese and find that these verbs demonstrate similarly coherent syntactic and semantic properties"]}, "P98-1081": {"title": ["Improving Data Driven Wordclass Tagging"], "abstract": [], "introduction": ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system", "We do this by means of an experiment involving the task of morphosyntactic wordclass tagging", "Four wellknown tagger generators Hidden Markov Model MemoryBased Transformation Rules and Maximum Entropy are trained on the same corpus data", "After comparison their outputs are combined using several voting strategies and second stage classifiers", "All combination taggers outperform their best component with the best combination showing a 191 lower error rate than the best individual tagger"]}, "P98-2138": {"title": [""], "abstract": ["For languages that have no explicit word bound ary such as Thai Chinese and Japanese cor recting words in text is harder than in English because of additional ambiguities in locating er ror words", "The traditional method handles this by hypothesizing that every substrings in the input sentence could be error words and trying to correct all of them", "In this paper we pro pose the idea of reducing the scope of spelling correction by focusing only on dubious areas in the input sentence", "Boundaries of these dubious areas could be obtained approximately by ap plying word segmentation algorithm and finding word sequences with low probability", "To gener ate the candidate correction words we used a modified edit distance which reflects the charac teristic of Thai OCR errors", "Finally a partof speech trigram model and Winnow algorithm are combined to determine the most probable correction"], "introduction": ["Optical character recognition OCR is useful in a wide range of applications such as office automation and information retrieval system", "However OCR in Thailand is still not widely used partly because existing Thai OCRs are not quite satisfactory in terms of accuracy", "Re cently several research projects have focused on spelling correction for many types of errors in cluding those from OCR Kukich 1992", "Nev ertheless the strategy is slightly different from language to language since the characteristic of each language is different", "Two characteristics of Thai which make the task of error correction different from those of other languages are 1 there is no explicit word boundary and 2 characters are written in three levels ie the middle the upper and the lower levels", "In order to solve the prob lem of OCR error correction the first task is usually to detect error strings in the input sen tence", "For languages that have explicit word boundary such as English in which each word is separated from the others by white spaces this task is comparatively simple", "If the tok enized string is not found in the dictionary it could be an error string or an unknown word", "However for the languages that have no ex plicit word boundary such as Chinese Japanese and Thai this task is much more complicated", "Even without errors from OCR it is difficult to determine word boundary in these languages", "The situation gets worse when noises are intro duced in the text", "The existing approach for correcting the spelling error in the languages that have no word boundary assumes that all substrings in input sentence are error strings and then tries to correct them Nagata 1996", "This is computationally expensive since a large portion of the input sentence is correct", "The other characteristic of Thai writing system is that we have many levels for placing Thai char acters and several characters can occupy more than one level", "These characters are easily con nected to other characters in the upper or lower level", "These connected characters cause diffi culties in the process of character segmentation which then cause errors in Thai OCR", "Other than the above problems specific to Thai realword error is another source of er rors that is difficult to correct", "Several previous works on spelling correction demonstrated that Figure 1 No explicit word delimiter in Thai upper level topHne middle level baseline lower level featurebased approaches are very effective for solving this problem", "In this paper a hybrid method for Thai OCR error correction is proposed", "The method com bines the partofspeech POS trigram model with a featurebased model", "First the POS tri gram model is employed to correct nonword as well as realword errors", "In this step the num ber of nonword errors are mostly reduced but some realword errors still remain because the POS trigram model cannot capture some use ful features in discriminating candidate words", "A featurebased approach using Winnow algo rithm is then applied to correct the remaining errors", "In order to overcome the expensive com putation cost of the existing approach we pro pose the idea of reducing the scope of correc tion by using word segmentation algorithm to find the approximate error strings from the in put sentence", "Though the word segmentation algorithm cannot give the accurate boundary of an error string many of them can give clues of unknown strings which may be error strings", "We can use this information to reduce the scope of correction from entire sentence to a more mir row scope", "Next to capture the characteristic of Thai OCR errors we have defined the modi fied edit distance and use it to enumerate plau sible candidates which deviate from the word in question within kedit distance"]}, "P9852_p00": {"title": ["A TwoLevel Morphological Analyser  for the Indonesian Language"], "abstract": ["This paper presents our efforts at developing an Indonesian morphological analyser that pro vides a detailed analysis of the rich affixation process 1 We model Indonesian morphology us ing a twolevel morphology approach decom posing the process into a set of morphotactic and morphophonemic rules", "These rules are modelled as a network of finite state transduc ers and implemented using xfst and lexc", "Our approach is able to handle reduplication a nonconcatenative morphological process"], "introduction": ["Morphology is the study of the way that words are built up from smaller units called morphemes the minimal meaningbearing units in a language Ju rafsky 2000", "For example the English word kind consists of a single morpheme the root word kind whilst the word players consists of three mor phemes play er and s", "The morphemes kind and play can stand alone as words while affixes er and s must appear bound to another morpheme", "By applying a set of morphological rules we can produce not just morphemes but also other in formation relating to the words for example the grammatical category of the whole word as well as the subcategorisation frame of the word if it is a verb", "This process is called morphological analysis", "In this respect the value of a morphological ana lyser would be twofold from a theoretical linguis tic viewpoint it is a very useful tool for linguistic modelling and for testing certain analyses", "On the 1 This research is part of a collaborative research project", "funded by ARC Discovery Grant DP0877595", "other hand from a practical viewpoint it supports many applications eg information retrieval search engines and machine translation among others", "There is currently some interest in developing morphological tools for the Indonesian language", "In previous work Siregar 1995 and Adriani et al", "2007 discuss the development of Indonesian stemmers that recover a root from an affixed word implemented procedurally", "However stemmers are of limited use as they do not provide any more lin guistic information beyond the stem", "Hartono 2002 presents an initial version of a morphologi cal analyser developed with PCKIMMO but un fortunately it does not handle reduplication a key aspect of Indonesian morphology", "The morpho logical analyser that we are developing and that we describe here is designed to be able to handle the rich semantic lexical and grammatical information associated with words and word formation in NLP applications", "In Section 2 we first discuss Indonesian mor phology followed by a brief explanation of two level morphology in Section 3", "Sections 4 and 5 present our work in applying twolevel morphol ogy for the Indonesian language", "Finally Section 6 presents the results of some evaluations we carried out on our developed analyser"]}, "P99-1061": {"title": ["A Bag of Useful Techniques for Efficient and Robust Parsing"], "abstract": ["This paper describes new and improved techniques which help a unificationbased parser to process input efficiently and robustly", "In combination these methods result in a speedup in parsing time of more than an order of magnitude", "The methods are correct in the sense that none of them rule out legal rule applications"], "introduction": ["This paper describes several generallyapplicable techniques which help a unification based parser to process input efficiently and robustly", "As well as presenting a number of new methods we also report significant improvements we have made to existing techniques", "The methods preserve correctness in the sense they do not rule out legal rule applications", "In particular none of the techniques involve statistical or approximate processing", "We also claim that these methods are independent of the concrete parser and neutral with respect to a given unificationbased grammar theoryformalism", "How can we gain reasonable efficiency in parsing when using large integrated grammars with several thousands of huge lexicon entries", "Our belief is that there is no single method which achieves this goal alone", "Instead we have to develop and use a set of cheap filters which are correct in the above sense", "As we indicate in section 10 combining these methods leads to a speedup in parsing time and reduction of space consumption of more than an order of magnitude when applied to a mature well engineered unificationbased parsing system", "We have implemented our methods as extensions to a HPSG grammar development environment Uszkoreit et al 1994 which employs a sophisticated typed feature formalism Krieger and Schifer 1994 Krieger and Schifer 1995 and an advanced agendabased bottomup chart parser Kiefer and Scherf 1996", "A specialized runtime version of this system is currently used in VERBMOBIL as the primary deep analysis component", "I In the next three sections we report on transformations we have applied to the knowledge base grammarlexicon and on modifications in the core formalism unifier type system", "In Section 58 we describe how a given parser can be extended to filter out possible rule applications efficiently before performing expensive unification", "Section 9 shows how to compute best partial analyses in order to gain a certain level of robustness", "Finally we present empirical results to demonstrate the efficiency gains and speculate on extensions we intend to work on in the near future", "Within the different sections we refer to three corpora we have used to measure the effects of our methods", "The reference corpora for English German and Japanese consist of 12005000 samples"]}, "PEAAI_n09": {"title": ["Engineering Applications of  Articial Intelligence 51 2016 5057"], "abstract": ["The present work describes a classication schema for irony detection in Greek political tweets", "Our hypothesis states that humorous political tweets could predict actual election results", "The irony detection concept is based on subjective perceptions so only relying on human annotator driven labor might not be the best route", "The proposed approach relies on limited labeled training data thus a semisupervised approach is followed where collectivelearning algorithms take both labeled and unlabeled data into consideration", "We compare the semi supervised results with the supervised ones from a previous research of ours", "The hypothesis is evaluated via a correlation study between the irony that a party receives on Twitter its respective actual election results during the Greek parliamentary elections of May 2012 and the difference between these results and the ones of the preceding elections of 2009"], "introduction": ["Irony as a paralinguistic element is used to guratively express a concept with a semantic meaning that is very different from its actual initial purpose", "It is a challenging eld for computational linguistics and natural language processing due to the high ambiguity and the difculty to detect it objectively", "Language use is vigorous and creative there is no predened consensual agreement on how to recognize an ironic expression due to the high subjectivity involved", "In the last decade irony expression has been thriving on social networks and particularly Twitter because of the 140 characters restraint on the status updates being a perfect t for good old one liners", "As a public social medium users realize that their writings may be read and reproduced by potentially everyone gaining popularity and followers", "But this publicity contrary to Facebooks real name policy often has no direct consequences to their everyday lives since the majority participate anonymously using an avatar and a nickname", "This nocensorship state contributes to the freedom of expressing personal thoughts on tough taboo unpopular or controversial issues part of which contains the political satire", "n Corresponding author", "Tel  30 6946354612", "D Spathis", "Email addresses p11charioniogr B Charalampakis p11spationiogr sdimitriscsdauthgr D Spathis p11kousioniogr E Kouslis kermanioniogr K Kermanidis", "Political satire is a signicant part of comedy which specializes in drawing entertainment from politics", "Most of the times it aims just to please", "By nature it does not offer a constructive view by itself when it is used as part of criticism it tends to simply pinpoint the unexpected or different", "The high topicality of Twitter combined with the ephemerality of political news forms a state which is described asecho chamber a groupthinking effect on virtually enclosed spaces ampli ed by repetition Colleoni et al 2014", "As a result the occasional user might write something political just to jump on the bandwagon without an initial conscious aim to criticize", "Adding to that politics is a topic that almost everybody is familiar with and makes more sense from the engagement and attention side to write about Obama instead of an obscure book you just read", "Studies focus on the simultaneous usage of Twitter and the TV on circumstances like a political debate where metatalk tweets reveal critical scrutiny of the agenda or the debate about the debate Kalsnes et al 2014", "In the rapidly changing web there is a plethora of available text especially from social networks which is unlabeled raw or unprocessed", "Adding to the traditional supervised methods there are quite a few techniques that enable us to take these huge unstructured data into account", "An insight from our previous work was the subjectivity involved during the tagging of a text as ironic", "Three of our authors who took up the tedious task of annotation could not agree on what should be considered as ironic or not", "As a result there cannot be a gold standard corpus of ironic tweets", "This was our main motivation to explore semisupervised techniques since they take httpdxdoiorg101016jengappai201601007 09521976 2016 Elsevier Ltd All rights reserved", "into account both train and test data", "To be specic the technique we chose is collective classication a type of semisupervised learning that presents an interesting method for optimizing the classication of partiallylabeled data", "Considering the above our empirical study tries to detect irony on a corpus of Greek political tweets by training a classier using appropriate linguistic features some of which are proposed for the rst time herein for irony detection", "Our goal is to nd a relation between the ironic tweets that refer to the political parties and leaders in Greece in the preelection period of May 2012 and their actual election results", "We compare the semisupervised results with the supervised ones from a previous research of ours", "Regarding the novelty of our study this is a rst exploration on the eld of irony detection with semisupervised learning and an application in politics", "The remainder of this paper is organized as follows In Section 2 we present the related literature on the topics of irony detection Twitter sentiment analysis and political expression", "The next Sections 3 and 4 are dedicated to data preprocessing and its representation schema through the set of linguistic features that affect irony detection", "The Section 5 describes the training procedure the evaluation of the algorithms performance and their test procedure on a large unlabeled dataset", "An overview of the study limitations future research prospects and a summary of the empirical study are described in Section 6"]}, "PMTS_n09": {"title": ["Domain Adaptation in Statistical Machine Translation of UserForum Data"], "abstract": ["This paper reports experiments on adapting components of a Statistical Machine Translation SMT system for the task of translating online usergenerated forum data from Symantec", "Such data is monolingual and differs from available bitext MT training resources in a number of important respects", "For this reason adaptation techniques are important to achieve optimal results", "We investigate the use of mixture modelling to adapt our models for this specic task", "Individual models created from different indomain and outofdomain data sources are combined using linear and loglinear weighting methods for the different components of an SMT system", "The results show a more profound effect of language model adaptation over translation model adaptation with respect to translation quality", "Surprisingly linear combination outperforms loglinear combination of the models", "The best adapted systems provide a statistically signicant improvement of 178 absolute BLEU points 685 relative and 273 absolute BLEU points 805 relative over the baseline system for EnglishGerman and EnglishFrench respectively"], "introduction": ["In recent years Statistical Machine Translation SMT technology has been used in many online applications concentrating on professionally edited enterprise quality online content", "At the same time very little research has gone into adapting Work done while at CNGL School of Computing DCUSMT technology to the translation of user generated content on the web", "While translation of online chats Flournoy and CallisonBurch 2000 has received some attention there is surprisingly little work on translation of online user forum data despite growing interest in the area Flournoy and Rueppel 2010", "In this paper we describe our efforts in building a system to address this particular application area", "Our experiments are conducted on data collected from online forums on Symantec Security tools and services1 For a multinational company like Symantec the primary motivation behind translation of user forum data is to enable access across language barriers to information in the forums", "Forum posts are rich in information about issues and problems with tools and services provided by the company and often provide solutions to problems even before traditional customercare help lines are even aware of them", "The major challenge in developing MT systems for user forum data concerns the lack of proper parallel training material", "Forum data is monolingual and hence cannot be used directly to train SMT systems", "We use parallel training data in the form of Symantec Enterprise Translation Memories TMs from different product and service domains to train the SMT models", "As an auxiliary source we also used portions of the Europarl dataset2 Koehn 2005 selected according to their similarity with the forum data Section 32 to supplement the TM based training data", "Symantec TM data being a part of enterprise documentation is professionally 1 httpcommunitynortoncom 2 httpwwwstatmtorgeuroparl edited and by and large conforms to the Symantec controlled language guidelines and is signicantly different in nature from the user forum data which is loosely moderated and does not use controlled language at all", "In contrast Europarl data is out ofdomain with respect to the forum data", "The differences between available training and test datasets necessitate the use of adaptation techniques for optimal translation", "We use mixture model adaptation Foster and Kuhn 2007 creating individual models from different sources of data and combining them using different weights", "Monolingual forum posts were used for language modelling along with the target side of the TM training data", "A system trained only on the Symantec TM and forum data serves as the baseline system", "All our experiments are conducted on the EnglishGerman En De and EnglishFrench EnFr language pairs with a special emphasis on translation from English", "For the sake of completeness however we report translation scores for both directions here", "Apart from using models created from concatenation of indomain Symantec TM and outof domain Europarl datasets we used linear and log linear combination frameworks to combine individual models", "Both translation models and language models were separately combined using the two methods and the effect of the adaptation was measured on the translation output using established automatic evaluation metrics", "Our experiments reveal that for the current task in terms of translation quality language model adaptation is more effective than translation model adaptation and linear combination performs slightly better than the loglinear setting", "The remainder of this paper is organized as follows Section 2 briey describes related work relevant to the context", "Section 3 reports the tools and algorithms used along with a description of the datasets used", "Section 4 focuses on the mixture modelling experiments and how weights are learnt in different settings", "Section 5 presents the experiments and analysis of results followed by conclusions and future work in Section 6"]}, "PS15684_w09": {"title": ["Paraphrase Extraction using fuzzy hierarchical clustering"], "abstract": ["Paraphrase Extraction involves the discovery of equivalent text segments from large corpora and nds application in tasks such as multidocument summarization and document clustering", "Semantic similarity identication is a challenging problem which is further compounded by the large size of the corpus", "In this paper a two stage approach which involves clustering followed by Paraphrase Recognition has been proposed for extraction of sentencelevel paraphrases from text collections", "In order to handle the ambiguity and inherent variability of natural language a fuzzy hierarchical clustering approach which combines agglomeration based on verbs and division on nouns has been used", "Sentences within each resultant cluster are then processed by a machinelearning based Paraphrase Recognizer to discover the paraphrases", "The twostage approach has been applied on the Microsoft Research Paraphrase Corpus and a subset of the Microsoft Research Video Description Corpus", "The performance has been evaluated against an existing kmeans clustering approach as well as cosinesimilarity technique and Fuzzy CMeans clustering and the two stage system has consistently demonstrated better performance", "2015 Elsevier BV All rights reserved"], "introduction": ["Vast amounts of natural language text are available on the web as well as in largescale repositories much of which is redundant", "The task of Paraphrase Extraction focuses on identication of text units which convey the same meaning", "The detection of similar text is complicated due to the rich variability of natural languages", "The large scale of the corpora is another factor which poses hurdles in Paraphrase Extraction", "Performing oneonone matching of sentences is practically ruled out even if it is assumed that a suitable Paraphrase Recognition system is available", "Therefore efcient techniques are required to identify possibly similar candidates from large scale corpora and then subject them to further processing to detect exact matches", "An effective Paraphrase Extraction system will benet various Natural Language Processing applications such as multidocument summarization plagiarism detection question answering and document clustering", "The signicant aspect of this work is that a novel twolevel fuzzy clustering technique has been proposed for sentencelevel Paraphrase Extraction", "As similar sentences tend to describe the same or similar actions Fuzzy Agglomerative Clustering based on verbs  Corresponding author", "Tel 91 9443760000", "Email addresses ctrpsggmailco A Chitra anupriya rajkumaryahoocoin A Rajkumar", "1 Tel 91 9843222273", "is performed initially", "Divisive clustering is then applied to identify subgroups of sentences which center on the same nouns", "A Support Vector Machine SVM based Paraphrase Recognizer is then used to identify the paraphrases within each cluster", "The performance of the Paraphrase Extraction system has been evaluated using the Microsoft Research Paraphrase Corpus MSRPC and a subset of the Microsoft Research Video Description Corpus MSRVDC", "The outline of the paper is as follows Section 2 contains an overview of previous work related to Paraphrase Extraction and Hierarchical Clustering", "Section 3 describes the methodology adopted for extracting paraphrases using a fuzzy hierarchical clustering approach and machine learning based Paraphrase Recognizer", "Section 4 presents the results of experiments conducted using two different corpora", "Possible directions for future work and applications are discussed in Section 5 which concludes the paper"]}, "PSMPT_n09": {"title": ["Supertags as Source Language Context in Hierarchical PhraseBased SMT"], "abstract": ["Statistical machine translation SMT models have recently begun to include source context modeling under the assumption that the proper lexical choice of the translation for an ambiguous word can be determined from the context in which it appears", "Various types of lexical and syntactic features have been explored as effective source context to improve phrase selection in SMT", "In the present work we introduce lexicosyntactic descriptions in the form of supertags as sourceside context features in the stateoftheart hierarchical phrasebased SMT HPB model", "These features enable us to exploit source similarity in addition to target similarity as modelled by the language model", "In our experiments two kinds of supertags are employed those from lexicalized treeadjoining grammar LTAG and combinatory categorial grammar CCG", "We use a memorybased classification framework that enables the efficient estimation of these features", "Despite the differences between the two supertagging approaches they give similar improvements", "We evaluate the performance of our approach on an EnglishtoDutch translation task and report statistically significant improvements of 448 and 63 BLEU scores in translation quality when adding CCG and LTAG supertags respectively as contextinformed features"], "introduction": ["The stateoftheart hierarchical phrasebased SMT model Chiang 2007 uses the bilingual phrase pairs of phrasebased SMT PBSMT Koehn et al 2003 as a starting point to learn hierarchial rules using probabilistic synchronous contextfree grammar PSCFG", "The decoding process in the hierarchical phrasebased SMT HPB model is based on bottomup chart parsing Chiang 2007", "This chart parsing decoder also known as Hiero does not require explicit syntactic representation on either side of the phrases in rules", "Stateoftheart SMT models Koehn et al 2003 Chiang 2007 can be viewed as loglinear combinations of features Och and Ney 2002 that usually comprise translational features and the language model", "The translational features typically involved in these models express dependencies between the source and target phrases but not dependencies between the phrases in the source language themselves ie they do not take into account the contexts of those phrases", "Word sense disambiguation WSD a task intricately related to MT typically employs rich context sensitive features to determine contextually the most likely sense of a polysemous word", "Inspired by these contextrich WSD techniques researchers have tried to integrate various contextual knowledge sources into stateoftheart SMT models", "In recent years source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account", "These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information positionspecific neighbouring words Gimenez and Marquez 2007 Stroppa et al 2007 shallow and deep syntactic features Gimpel and Smith 2008 full sentential context Carpuat and Wu 2007 lexical syntactic descriptions in the form of supertags Haque et al 2009a and grammatical dependency relations Haque et al 2009b", "A limitation that Hiero Chiang 2007 shares with the PBSMT model Koehn et al 2003 is that it does not take into account the contexts in which the sourcesides of the rules appear", "In other words it can be argued that rule selection in Hiero is suboptimally modelled", "So far a small number of studies have made use of sourcelanguage context for improving rule selection in Hiero", "Positionspecific neighbouring words and their partofspeech POS prove to be effective source contexts in the HPB model He et al 2008", "In a study involving PBSMT Haque et al", "2009b showed that the translations of ambiguous words are also influenced by more distant words in the sentence", "Syntactic contexts that capture longdistance dependencies between words in a sentence can be a useful means to disambiguate among translations", "Accordingly integration of such syntactic contexts could lead to improved translation quality in PBSMT", "For instance Haque et al", "2009a showed that supertags are more powerful source contexts than neighbouring words and partofspeech tags to disambiguate a source phrase in PBSMT", "Inspired by Haque et al 2009a in the present work we extend the stateoftheart Hiero system by adopting its lexical entries with the robust and efficient supertagging approaches", "Grammars in these approaches consist of a syntactically rich lexicon and a small set of combinatory operators", "These combinatory rules combine syntactically rich lexical entries together to form parse trees", "Supertaggers assign a syntactic structure an elementary tree or a lexical category to each word in a sentence", "These syntactic structures supertag provide rich and complex linguistic information that describe the POS tag of a word its subcategorisation information and the hierarchy of phrase categories in which the word appears", "The remainder of the paper is organized as follows", "In Section 2 we discuss related work", "Section 3 provides a brief overview of HPB", "In Section 4 we describe the contextinformed features contained in our baseline HPB model", "In Section 5 we de scribe our memorybased classification approach", "Section 6 describes experimental setups", "Section 7 presents the results obtained and offers a brief qualitative analysis", "In Section 8 we formulate our conclusions and offer some avenues for further work"]}, "PbaneaCSL": {"title": ["Computer Speech and Language 28 2014 719"], "abstract": ["Recent research on English word sense subjectivity has shown that the subjective aspect of an entity is a characteristic that is better delineated at the sense level instead of the traditional word level", "In this paper we seek to explore whether senses aligned across languages exhibit this trait consistently and if this is the case we investigate how this property can be leveraged in an automatic fashion", "We first conduct a manual annotation study to gauge whether the subjectivity trait of a sense can be robustly transferred across language boundaries", "An automatic framework is then introduced that is able to predict subjectivity labeling for unseen senses using either crosslingual or multilingual training enhanced with bootstrapping", "We show that the multilingual model consistently outperforms the crosslingual one with an accuracy of over 73 across all iterations", " 2013 Elsevier Ltd All rights reserved", "Keywords Sentiment and text classification Multilingual subjectivity analysis Sense level subjectivity"], "introduction": ["Sentiment and subjectivity analysis seeks to automatically identify opinions beliefs speculations emotions sentiments and other private states in natural text Wiebe et al 2005", "Quirk et al", "1985 define a private state as a state that does not lend itself to an objective external validation or in other words a person may be observed to assert that God exists but not to believe that God exists", "Belief is in this sense private p 1181", "In the field of natural language processing researchers have used the term subjectivity analysis to denote identifying private states in text namely separating objective from subjective instances while sentiment or polarity analysis further refines the subjective text into positive negative or neutral", "Sentiment and subjectivity analysis has stemmed into a prolific area of research mainly due to the fact that numerous text processing applications stand to gain from incorporating sentiment dimensions into their models including automatic expressive texttospeech synthesis Alm et al 1990 tracking sentiment timelines in online forums and news Balog et al 2006 Lloyd et al 2005 and mining opinions from product reviews Hu and Liu 2004", "In many natural language processing tasks subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data", "Research that benefited from this additional layering ranges from question  This paper has been recommended for acceptance by Prof RK Moore", " Corresponding author", "Tel 1 940 369 7630", "Email addresses carmenbaneagmailcom C Banea radacsuntedu R Mihalcea wiebecspittedu J Wiebe", "08852308  see front matter  2013 Elsevier Ltd All rights reserved", "httpdxdoiorg101016jcsl201303002 answering Yu and Hatzivassiloglou 2003 to conversation summarization Carenini et al 2008 text semantic analysis Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 and lexical substitution Su and Markert 2010", "In experiments carried out on English Wiebe and Mihalcea 2006 have shown that the most robust subjectivity delineation occurs at sense and not at word level", "Following this more finegrained perspective Esuli and Sebastiani 2006 and Andreevskaia and Bergler 2006 have proposed methods to embed senselevel automatic sentiment annotations objectiveneutral negative and positive over the English WordNet structure Miller 1995 using its relationships synonymy antonymy meronymy etc", "On the other hand noticing the scarcity of hand crafted senselevel subjectivitypolarity lexica Markert and Su 2008 have explored ways to infer them from data annotated at either the word or sentence level", "Senselevel subjectivity and crosslingual subjectivity and sentiment analysis have received considerable attentions in recent years yet our paper explores the area that lies at the intersection of these two topics", "To our knowledge this area has not been formally investigated and while the techniques may be similar to those applied in sentiment and subjectivity analysis at the sentence or the review level our work explores the more difficult task of senselevel subjectivity which also involves deep semantic aspects of the language", "The manual annotation study we performed for this task crosslingual senselevel subjectivity annotations as well as the methods we proposed crosslingual and multilingual learning using dictionaries in multiple languages are novel to our knowledge", "This work seeks to answer the following questions", "First for word senses aligned across languages is their subjectivity content consistent or in other words does a subjective sense in language A map to a subjective sense in language B and similarly for an objective sense", "Second can we employ a multilingual framework that can automatically discover new subjectiveobjective senses starting with a limited amount of annotated data", "We seek to answer the first question by conducting a manual annotation study in Section 2", "For the second question we propose two models see Section 3 one crosslingual and one multilingual which are able to simultaneously use information extracted from several languages when making subjectivity senselevel predictions"]}, "Pjournal": {"title": ["Mach Translat 2011 25239285"], "abstract": ["The translation features typically used in PhraseBased Statistical Machine Translation PBSMT model dependencies between the source and target phrases but not among the phrases in the source language themselves", "A swathe of research has demonstrated that integrating source context modelling directly into loglinear PBSMT can positively influence the weighting and selection of target phrases and thus improve translation quality", "In this contribution we present a revised extended account of our previous work on using a range of contextual features including lexical features of neighbouring words supertags and dependency information", "We add a number of novel aspects including the use of semantic roles as new contextual features in PBSMT adding new language pairs and examining the scalability of our research to larger amounts of training data", "While our results are mixed across feature selections classifier hyperparameters language pairs and learning curves we observe that including contextual features of the source sentence in general produces improvements", "The most significant improvements involve the integration of longdistance contextual features such as dependency relations in combination with partofspeech tags in DutchtoEnglish subtitle translation the combination of dependency parse and semantic role information in EnglishtoDutch parliamentary debate translation or supertag features in EnglishtoChinese translation", "R Haque  S K Naskar  A Way CNGL School of Computing Dublin City University Dublin 9 Ireland A van den Bosch B ILK Research Group Tilburg center for Cognition and Communication Tilburg University Tilburg The Netherlands email AntalvdnBoschuvtnl 123 Keywords Statistical machine translation  Phrasebased statistical machine translation  Syntax in machine translation  Translation modelling  Word alignment  Memorybased classification"], "introduction": ["In loglinear phrasebased statistical machine translation PBSMT Koehn et al 2003 the probability Pek  fk  of a target phrase ek given a source phrase fk is modelled as a loglinear combination of features which typically consist of a finite set of translation features and a language model Och and Ney 2002", "The translation features normally used in such models express dependencies between the source and target phrases but not among phrases or tokens in the source language themselves", "Stroppa et al", "2007 observed that incorporating sourcelanguage context using neighbouring words and partofspeech tags had the potential to improve translation quality", "This has led to a whole tranche of research of which we provide an overview in Sect", "3 which has shown that integrating source context modelling into PBSMT can positively influence the weighting and selection of target phrases and thus improve translation quality", "Approaches to include sourcelanguage context to help select more appropriate target phrases have partly been inspired by methods used in wordsense disambiguation WSD where rich contextual features are employed to determine the most likely sense of a polysemous word given that context", "These contextual features may include lexical features of words appearing in the immediate context Gimnez and Mrquez 2007 Stroppa et al 2007 shallow and deep syntactic features of the sentential context Gimpel and Smith 2008 and full sentential context Carpuat and Wu 2007", "Studies in which syntactic features are employed have made use of partofspeech taggers Stroppa et al 2007 supertaggers Haque et al 2009a and shallow and deep syntactic parsers Gimpel and Smith 2008 Haque et al 2009b", "In prior work we have shown that exploring local sentential context information in the form of both supertags Haque et al 2009a and syntactic dependencies Haque et al 2009b can be successfully integrated into a PBSMT model", "Here we provide a revised extended account of this previous research", "We add a number of novel aspects including using semantic roles as new contextual features in PBSMT adding new language pairs and examining the scalability of our research to larger amounts of training data", "Our results allow us to conclude that incorporating sourcelanguage contextual features benefits a range of different language pairs both with English as source language translating to Dutch Chinese Japanese Hindi Spanish and Czech and target language from Dutch on different types of data such as news articles and commentary parliamentary debates patents and subtitles according to a range of automatic evaluation measures", "The remainder of this contribution is organized as follows", "Section 2 provides some motivation for work in this direction and related work is discussed in Sect", "3", "Section 4 provides a brief overview of PBSMT which acts as the baseline throughout the study", "In Sect", "5 we describe the range of contextinformed features we add to the baseline PBSMT model", "Section 6 describes the memorybased classification approach and how we integrated the output of the memorybased classifier into a stateoftheart PBSMT system", "In Sect", "7 we present the results obtained", "We formulate our conclusions in Sect", "8 and offer some avenues for further work"]}, "Pmert_n09": {"title": ["Regularized Minimum Error Rate Training"], "abstract": ["Minimum Error Rate Training MERT remains one of the preferred methods for tuning linear parameters in machine translation systems yet it faces significant issues", "First MERT is an unregularized learner and is therefore prone to overfitting", "Second it is commonly used on a noisy nonconvex loss function that becomes more difficult to optimize as the number of parameters increases", "To address these issues we study the addition of a regularization term to the MERT objective function", "Since standard regularizers such as 2 are inapplicable to MERT due to the scale invariance of its objective function we turn to two regularizers0 and a modification of 2  and present methods for efficiently integrating them during search", "To improve search in large parameter spaces we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERTs exact line searches", "Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO a learner often used with large feature sets"], "introduction": ["Minimum Error Rate Training emerged a decade ago Och 2003 as a superior training method for small numbers of linear model parameters of machine translation systems improving over prior work using maximum likelihood criteria Och and Ney 2002", "This technique quickly rose to prominence becoming standard in many research and commercial MT systems", "Variants operating over lattices Macherey et al 2008 or hypergraphs Kumar et al 2009 were subsequently developed with the benefit of reducing the approximation error from nbest lists", "The primary advantages of MERT are twofold", "It directly optimizes the evaluation metric under consideration eg BLEU instead of some surrogate loss", "Secondly it offers a globally optimal line search", "Unfortunately there are several potential difficulties in scaling MERT to larger numbers of features due to its nonconvex loss function and its lack of regularization", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric Chiang et al 2009 Hopkins and May 2011 Cherry and Foster 2012 which correspond to easier optimization problems and which naturally incorporate regularization", "In particular recent work Chiang et al 2009 has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a marginbased approximation", "On simulated datasets Hopkins and May 2011 found that conventional MERT struggles to find reasonable parameter vectors where a smooth loss function based on Pairwise Ranking Optimization PRO performs much better on real data this PRO method appears at least as good as MERT on small feature sets and also scales better as the number of features increases", "In this paper we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search", "The idea of adding a regularization term to the MERT objective function can be perplexing at first because the most common regularizers such as 1 and 2 are not directly applicable to MERT", "Indeed these regularizers are scale sensitive while the MERT objective function is not scaling the weight vector neither changes the predictions of the linear model nor affects the error count", "Hence MERT can hedge any regularization penalty by maximally scaling down linear model weights", "The first contribution of this paper is to analyze various forms of regularization that are not susceptible to this scaling problem", "We analyze and experiment with 0 a form of regularization that is scale insensitive", "We also present new parameterizations of 2 regularization where we apply 2 regularization to scalesenstive linear transforms of the original linear model", "In addition we introduce efficient methods set of reference translations rS  r1   ", "rS ", "This yields the following optimization problem of incorporating regularization in Och 2003s exact line searches", "For all of these regularizers our methods let us find the true optimum of the regularized w  arg min w S  Ers efs w  s1 S M objective function along the line", "arg min   Ers e sm e sm  efs w Finally we address the issue of searching in a highdimensional space by using the gradient of expected BLEU Smith and Eisner 2006 to find better w where s1 m1 1 search directions for our line searches", "This direction finder addresses one of the serious concerns raised by Hopkins and May 2011 MERT widely failed to reach the optimum of a synthetic linear objective function", "In replicating Hopkins and Mays experiments we confirm that existing search algorithms for MERTincluding coordinate ascent Powells algorithm Powell 1964 and random direction sets Cer et al 2008perform poorly in this experimental condition", "However when using our gradientbased direction finder MERT has no problem finding the true optimum even in a 1000dimensional space", "Our results suggest that the combination of a regularized objective function and a gradientinformed line search algorithm enables MERT to scale well with a large number of features", "Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO Hopkins and May 2011 a parameter tuning method known to be effective with large feature sets"]}, "Pmorph_p00": {"title": ["Computing with Realizational Morphology"], "abstract": ["The theory of realizational morphology presented by Stump in his inuential book Inflectional Morphology 2001 describes the derivation of inected surface forms from underlying lexical forms by means of ordered blocks of realization rules", "The theory presents a rich formalism for expressing generalizations about phenomena commonly found in the morphological systems of natural languages", "This paper demonstrates that in spite of the apparent complexity of Stumps formalism the system as a whole is no more powerful than a collection of regular relations", "Consequently a Stumpstyle description of the morphology of a particular language such as Lingala or Bulgarian can be compiled into a nitestate transducer that maps the underlying lexical representations directly into the corresponding surface forms or forms and vice versa yielding a single lexical transducer", "For illustration we will present an explicit nitestate implementation of an analysis of Lingala based on Stumps description and other sources"], "introduction": ["Morphology is a domain of linguistics that studies the formation of words", "It is traditional to distinguish between surface forms and their analyses called lemmas", "The lemma for a surface form such as the English word bigger typically consists of the traditional dictionary citation form of the word together with terms that convey the morphological properties of the particular form", "For example the lemma for bigger might be represented as bigAdjComp to indicate that bigger is the comparative form of the adjective big", "Alternatively the morphological properties might be encoded in terms of attributevalue pairs CatAdj DegrComp", "There are two challenges in modeling naturallanguage morphology 1", "Morphotactics", "Words are typically composed of smaller units stems and axes that mustbe combined in a certain order", "Most languages build words by concatena tion but some languages also exhibit nonconcatenative processes such as interdigitation and reduplication 3"]}, "Ponto_w06": {"title": ["Acquiring Ontological Relationships from Wikipedia Using RMRS"], "abstract": ["We investigate the extraction of ontologies from biological text using a semantic representation derived from a robust parser", "The use of a semantic representation avoids the problems that traditional patternbased approaches have with complex syntactic constructions and longdistance dependencies", "The discovery of taxonomic relationships is explored in a corpus consisting of 12200 animalrelated articles from the online encyclopaedia Wikipedia", "The semantic representation used is Robust Minimal Recursion Semantics RMRS", "Initial experiments show good results in systematising extraction across a variety of hyponymic constructions", "Key words ontologies ontology extraction Wikipedia semantics"], "introduction": ["Ontology extraction has traditionally relied on pattern matching algorithms", "Hearst 5 introduced hyponymic extraction using lexicosyntactic patterns", "In the Hearst algorithm the system looks for instances of certain expressions in the text for example X is a Y or X such as Y and Z and infers the relations X isa Y and X isa Z", "Such systems are usually based on regular expressions over text or POStagged text and sentences containing apposition bracketing longdistance dependencies or uncommon structures have to be coded explicitly", "An example of such a sentence extracted from the Wikipedia encyclopaedia is The Firemouth Cichlid is one of the typical and most commonly seen in pet stores of the Cichlasomatype South American cichlids", "Here obtaining the relationship Firemouth cichlid isa cichlid involves the identification of the hyponym and hypernym as the first and last noun phrases of a lengthy sentence", "This suggests that traditional methods might be improved by using deeper syntactic and semantic analysis", "The work presented here investigates the use of a semantic model to address some of these issues", "Robust Minimal Recursion Semantics RMRS 4 provides argumentbased representation of sentences", "The theoretical idea is that in the problematic sentence above an RMRS output would contain the predicate associated with the identity copula be with a first argument corresponding to the term Firemouth cichlid and a second argument corresponding to cichlids regardless of the word order modification and so on", "Thus having obtained the RMRS representation of a given corpus it would be possible to extract ontological relationships from a semantic structure that abstracts over those morphological and syntactic details that do not affect the ontological relationship", "As pointed out by Pennacchiotti and Pantel 6 most ontology extraction systems so far have focused on generalised isa or partof relationships", "Our work involves extracting general hyponymic relations with RMRS and applying a filter to the results to obtain biological taxonomic relationships", "The corpus was gathered by extracting 12200 animal articles from the Wikipedia online encyclopaedia httpwwwwikipediaorg providing a semiedited setting where the added robustness of semantics might prove its usefulness", "The next section of this paper gives an overview of relevant prior work", "It is followed by the description of an extraction system based on RMRS using hard wired rules", "Results are discussed in the light of four different evaluation methods covering both manual and automatic recall and precision", "A brief overview is then given of a further system still under development the aim of which is to automate the pattern extraction process", "The conclusion presents different avenues for future work"]}, "Pproc2014_n09": {"title": ["EffectWordNet Senselevel Lexicon Acquisition  for Opinion Inference"], "abstract": ["Recently work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities effect events", "This paper addresses methods for creating a lexicon of such events to support such work on opinion inference", "Due to significant sense ambiguity our goal is to develop a senselevel rather than wordlevel lexicon", "To maximize the effectiveness of different types of information we combine a graphbased method using WordNet1 relations and a standard classifier using gloss information", "A hybrid between the two gives the best results", "Further we provide evidence that the model is an effective way to guide manual annotation to find effect senses that are not in the seed set"], "introduction": ["Opinion mining or sentiment analysis identifies positive or negative opinions in many kinds of texts such as reviews blogs and news articles", "It has been exploited in many application areas such as review mining election analysis and information extraction", "While most previous research focusses on explicit opinion expressions recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities Deng et al 2013 Deng and Wiebe 2014", "We call such events effect events2 Deng and Wiebe 2014 show how sentiments toward one 1 WordNet 30 httpwordnetprincetonedu", "2 While the term goodForbadFor is used in previous papers Deng et al 2013 Deng and Wiebe 2014 Deng et al 2014 we have since decided that effect is a better term", "entity may be propagated to other entities via opinion inference rules", "They give the following example 1 The bill would curb skyrocketing health care costs", "The writer expresses an explicit negative sentiment by skyrocketing toward the object health care costs", "The event curb has a negative effect on costs since they are reduced", "We can reason that the writer is positive toward the event because it has a negative effect on costs toward which the writer is negative", "From there we can reason that the writer is positive toward the bill since it is the agent of the positive event", "Deng and Wiebe 2014 show that such inferences may be exploited to significantly improve explicit sentiment analysis systems", "However to achieve its results the system developed by Deng and Wiebe 2014 requires that all instances of effect events in the corpus be manually provided as input", "For the system to be fully automatic it needs to be able to recognize effect events automatically", "This paper addresses methods for creating lexicons of such events to support such work on opinion inference", "We have discovered that there is significant sense ambiguity meaning that words often have mixtures of senses among the classes effect effect and Null", "Thus we develop a senselevel rather than wordlevel lexicon", "One of our goals is to investigate whether the effect property tends to be shared among semanticallyrelated senses and another is to use a method that applies to all word senses not just to the senses of words in a given wordlevel lexicon", "Thus we build a graphbased model in which each node is a WordNet sense and edges represent semantic WordNet relations between senses", "In addition we hypothesized that glosses also contain useful information", "Thus we develop a supervised gloss classifier and define a hybrid model which gives the best overall performance", "Finally because all WordNet verb senses are incorporated into the model we investigate the ability of the method to identify unlabeled senses that are likely to be effect senses", "We find that by iteratively labeling the topweighted unlabeled senses and rerunning the model it may be used as an effective method for guiding annotation efforts"]}, "Pproc9_w11": {"title": ["Determining Compositionality of Word Expressions"], "abstract": ["This research focuses on determining seman tic compositionality of word expressions us ing word space models WSMs", "We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs corpora preprocess ing techniques methods for determining com positionality and evaluation testbeds", "We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components", "The vec tors were obtained by Latent Semantic Analy sis LSA applied to the ukWaC corpus", "Our results outperform those of all the participants in the Distributional Semantics and Composi tionality DISCO 2011 shared task"], "introduction": ["A word expression is semantically compositional if its meaning can be understood from the literal meaning of its components", "Therefore semanti cally compositional expressions involve eg small island or hot water on the other hand seman tically noncompositional expressions are eg red tape or kick the bucket", "The notion of compositionality is closely related to idiomacy the higher the compositionality the lower the idiomacy and vice versa Sag et al 2002 Baldwin and Kim 2010", "Noncompositional expressions are often referred to as Multiword Expressions MWEs", "Baldwin and Kim 2010 differentiate the following subtypes of Pavel Pecina Charles University in Prague Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Prague Czech Republic pecinaufalmffcunicz compositionality lexical syntactic semantic prag matic and statistical", "This paper is concerned with semantic compositionality", "Compositionality as a feature of word expressions is not discrete", "Instead expressions populate a con tinuum between two extremes idioms and free word combinations McCarthy et al 2003 Bannard et al 2003 Katz 2006 Fazly 2007 Baldwin and Kim 2010 Biemann and Giesbrecht 2011", "Typical ex amples of expressions between the two extremes are zebra crossing or blind alley", "Our research in compositionality is motivated by the hypothesis that a special treatment of se mantically noncompositional expressions can im prove results in various Natural Language Process ing NPL tasks as shown for example by Acosta et al", "2011 who utilized MWEs in Information Re trieval IR", "Besides that there are other NLP ap plications that can benefit from knowing the degree of compositionality of expressions such as machine translation Carpuat and Diab 2010 lexicography Church and Hanks 1990 word sense disambigua tion Finlayson and Kulkarni 2011 partofspeech POS tagging and parsing Seretan 2008 as listed in Rarnisch 2012", "The main goal of this paper is to present an anal ysis of previous approaches using WSMs for de termining the semantic compositionality of expres sions", "The analysis can be found in Section 2", "A special attention is paid to the evaluation of the pro posed models that is described in Section 3", "Section 4 presents our first intuitive experimental setup and results ofLSA applied to the DISCO 2011 task", "Sec tion 5 concludes the paper", "42 Proceedings of the 9th Workshop on Multiword Expressions MWE 2013 pages 4250 Atlanta Georgia 1314 June 2013", "2013 Association for Computational Linguistics"]}, "Pproc_d09": {"title": ["Reordering by Parsing"], "abstract": ["We present a new discriminative reordeting model for statistical machine translation", "The model employs a standard datadriven depen dency parser to predict reordetings based on syntactic information", "This is made possi ble through the introduction of a reorderiog structure which is a word alignment structure where the target word order is transposed onto the source sentence as a path", "The approach is iotegrated io a phrasebased system", "Exper iments show a large iocrease io long distance reorderiogs", "Both automatic and human evalu ations show substantial increases io translation quality on an English to German task"], "introduction": ["Handling word order differences between languages is one of the main challenges of statistical machine translation SMT today", "These differences are of ten most natorally handled at a syntactic level since they pertaio to entire syntactic constituents", "We present a syntactically motivated discrimioa tive reordering model", "The model exploits a reorder ing structure which is a word alignment where the target sentence is unknown", "This structure allows us to treat the reordering problem as a dependency parsing problem", "We use a standard datadriven de pendency parser to predict reorderings instead of de pendencies", "This is integrated into a phrasebased SMT PSMT framework Koehn et al 2003"]}, "Pproc_d10": {"title": ["Exact Maximum Inference for the Fertility Hidden Markov Model"], "abstract": ["The notion of fertility in word alignment the number of words emitted by a sin gle state is useful but difficult to model", "Initial attempts at modeling fertility used heuristic search methods", "Recent ap proaches instead use more principled ap proximate inference techniques such as Gibbs sampling for parameter estimation", "Yet in practice we also need the single best alignment which is difficult to find us ing Gibbs", "Building on recent advances in dual decomposition this paper introduces an exact algorithm for finding the sin gle best alignment with a fertility HMM", "Finding the best alignment appears impor tant as this model leads to a substantial improvement in alignment quality"], "introduction": ["Wordbased translation models intended to model the translation process have found new uses iden tifying word correspondences in sentence pairs", "These word alignments are a crucial training com ponent in most machine translation systems", "Fur thermore they are useful in other NLP applica tions such as entailment identification", "The simplest models may use lexical infor mation alone", "The seminal Model 1 Brown et al 1993 has proved very powerful per forming nearly as well as more complicated models in some phrasal systems Koehn et al 2003", "With minor improvements to initializa tion Moore 2004 which may be important Toutanova and Galley 2011 it can be quite competitive", "Subsequent IBM models include more detailed information about context", "Models 2 and 3 incorporate a positional model based on the absolute position of the word Models 4 and 5 use a relative position model instead an English word tends to align to a French word that is nearby the French word aligned to the previous English word", "Models 3 4 and 5 all incorporate a no tion offertility the number of French words that align to any English word", "Although these latter models covered a broad range of phenomena estimation techniques and MAP inference were challenging", "The au thors originally recommended heuristic proce dures based on local search for both", "Such meth ods work reasonably well but can be computation ally inefficient and have few guarantees", "Thus many researchers have switched to the HMM model Vogel et al 1996 and variants with more parameters He 2007", "This captures the posi tional information in the IBM models in a frame work that admits exact parameter estimation infer ence though the objective function is not concave local maxima are a concern", "Modeling fertility is challenging in the HMM framework as it violates the Markov assump tion", "Where the HMM jump model considers only the prior state fertility requires looking across the whole state space", "Therefore the standard forwardbackward and Viterbi algorithms do not apply", "Recent work Zhao and Gildea 2010 de scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es timation", "However they do not have a efficient means of MAP inference which is necessary in many applications such as machine translation", "This paper introduces a method for exact MAP inference with the fertility HMM using dual de composition", "The resulting model leads to sub stantial improvements in alignment quality", "7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 711 Sofia Bulgaria August 49 2013", "2013 Association for Computational Linguistics"]}, "Pproc_w09": {"title": ["Monolingual Distributional Similarity for TexttoText Generation"], "abstract": ["Previous work on paraphrase extraction and application has relied on either parallel datasets or on distributional similarity met tics over large text corpora", "Our approach combines these two orthogonal sources of in formation and directly integrates them into our paraphrasing systems loglinear model", "We compare different distributional similar ity featuresets and show significant improve ments in grarnmaticality and meaning reten tion on the example texttotext generation task of sentence compression achieving state oftheart quality"], "introduction": ["A wide variety of applications in natural language processing can be cast in terms of texttotext gen eration", "Given input in the form of natural lan guage a texttotext generation system produces natural language output that is subject to a set of constraints", "Compression systems for instance pro duce shorter sentences", "Paraphrases ie differ ing textual realizations of the same meaning are a crucial components of texttotext generation sys tems and have been successfully applied to tasks such as multidocument summarization Barzilay et a 1999 Barzilay 2003 query expansion Arr ick and Tipirneni 1999 Riezler eta 2007 ques tion answering McKeown 1979 Ravichandran and Hovy 2002 sentence compression Cohn and La pata 2008 Zhao et a 2009 and simplification Wubben et a 2012", "Paraphrase collections for texttotext generation have been extracted from a variety of different corpora", "Several approaches rely on bilingual para", "lei data Bannard and CallisonBurch 2005 Zhao et a 2008 CallisonBurch 2008 Ganitkevitch et a 2011 while others leverage distributional meth ods on monolingual text corpora Lin and Pantel 2001 Bhagat and Ravichandran 2008", "So far how ever ouly preliminary studies have been undertaken to combine the information from these two sources Chan eta 2011", "In this paper we describe an extension of Gan itkevitch et a", "2011s bilingual databased ap proach", "We augment the bilinguallysourced para phrases using features based on monolingual distri butional similarity", "More specifically  We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features esti mated from bilingual data", " We define distributional similarity for para phrase patterns that contain constituentlevel gaps eg simone JJ instance of NP a JJ case of NP", "This generalizes over distributional similarity for contiguous phrases", " We compare different types of monolingual distributional information and show that they can be used to achieve siguificant improve ments in grammaticality", " Finally we compare our method to several strong baselines on the texttotext generation task of sentence compression", "Our method shows stateoftheart results beating a purely bilingually sourced paraphrasing system", "256 First Joint Conference on Lexical and Computational Semantics SEM pages 256264 Montreal Canada June 78 2012", "2012 Association for Computational Linguistics their IJIInsithe term would", " t  ihre langfrisligen Ple wOrden", "ne seine lang en e aufzuQeben", "   ", " without giving up his longlenn plans Figure 1Pivotbased paraphrase extraction for con tiguous phrasesTwo phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another"]}, "Psem_p07": {"title": ["Mach Translat"], "abstract": ["This paper describes a new evaluation metric TERPlus TERp for automatic evaluation of machine translation MT", "TERp is an extension of Translation Edit Rate TER", "It builds on the success of TER as an evaluation metric and alignment tool and addresses several of its weaknesses through the use of paraphrases stemming synonyms as well as edit costs that can be automatically optimized to correlate better with various types of human judgments", "We present a correlation study comparing TERp to BLEU METEOR and TER and illustrate that TERp can better evaluate translation adequacy", "Keywords Machine translation evaluation  Paraphrasing  Alignment M G Snover B  N Madnani  B Dorr Laboratory for Computational Linguistics and Information Processing Institute for Advanced Computer Studies University of Maryland College Park MD USA email snoverumiacsumdedu N Madnani email nmadnaniumiacsumdedu B Dorr email bonnieumiacsumdedu R Schwartz BBN Technologies Cambridge MA USA email schwartzbbncom"], "introduction": ["TERPlus or TERp1 Snover et al 2009 is an automatic evaluation metric for machine translation MT that scores a translation the hypothesis of a foreign language text the source against a translation of the source text that was created by a human translator which we refer to as a reference translation", "The set of possible correct translations is very large possibly infinite and any one reference translation represents a single point in that space", "Frequently multiple reference translations typically 4are provided to give broader sampling of the space of correct translations", "Automatic MT evaluation metrics compare the hypothesis against this set of reference translations and assign a score to the similarity such that a better score is given when the hypothesis is more similar to the references", "TERp follows this methodology and builds upon an already existing evaluation metric Translation Error Rate TER Snover et al 2006", "In addition to assigning a score to a hypothesis TER provides an alignment between the hypothesis and the reference enabling it to be useful beyond general translation evaluation", "While TER has been shown to correlate well with translation quality it has several flaws it only considers exact matches when measuring the similarity of the hypothesis and the reference and it can only compute this measure of similarity against a single reference", "The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems Rosti et al 2007", "TERp does not utilize this methodology and instead addresses the exact matching flaw of TER", "In addition to aligning words in the hypothesis and reference if they are exact matches TERp uses stemming and synonymy to allow matches between words", "It also uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference", "These phrase substitutions are generated by considering possible paraphrases of the reference words", "Matching using stems and synonyms Banerjee and Lavie 2005 as well as using paraphrases Zhou et al 2006 Kauchak and Barzilay 2006 have been shown to be beneficial for automatic MT evaluation", "Paraphrases have been shown to be additionally useful in expanding the number of references used for evaluation Madnani et al 2008 although they are not used in this fashion within TERp", "The use of synonymy stemming and paraphrases allows TERp to better cope with the limited number of reference translations provided", "TERp was one of the top metrics submitted to the NIST MetricsMATR 2008 challenge Przybocki et al 2008 having the highest average rank over all the test conditions Snover et al 2009", "We first discuss the original TER metric in Sect", "2", "In Sect", "3 we present the details of our various enhancements to TER", "We then briefly review the alignment capability of TERp along with some examples in Sect", "4", "Finally in Sect", "5 we show the results of optimizing TERp for human judgments of adequacy and compare with other established evaluation metrics followed by an analysis of the relative benefits of each of the new features of TERp in Sect", "6", "1 TERp is named after the nicknameterpof the University of Maryland College Park Mascot the", "diamondback terrapin"]}, "Pstat_p00": {"title": ["FiniteState Morphological Analysis and Generation of Arabic at Xerox"], "abstract": ["This paper describes a finitestate morphological analyzer of Modern Standard Arabic words that can be tested on the Internet", "An overview of the system is provided including the history the finitestate technology and the dictionary coverage", "This research system is scheduled for testing and commercial development in 2001"], "introduction": ["In 1996 the Xerox Research Centre Europe produced a morphological analyzer for Modern Standard Arabic henceforth Arabic Beesley 1996", "In 1997 a Javaapplet interface was added to allow testing on the Internet using standard Arabic orthography", "The analyzergenerator is based on dictionaries from an earlier project at ALPNET Beesley 1990 Buckwalter 1990 but the system was extensively redesigned and rebuilt using Xerox finitestate technology Beesley and Karttunen 2001", "The system analyzes orthographical words that may include full partial or no diacritics and if diacritics are present they automatically constrain the ambiguity of the output", "A fully voweled spelling and a terse English gloss are also returned with each analysis", "The system is intended to serve as a pedagogical aid a comprehensionassistance tool and as a component in larger naturallanguageprocessing systems", "morphological analyses Morphological Analyzer word Figure 1 A Generic Morphological Analyzer as a Black Box"]}, "Q13-1015": {"title": ["Combined Distributional and Logical Semantics"], "abstract": ["We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics", "Distributional models have been successful in modelling the meanings of content words but logical semantics is necessary to adequately represent many function words", "We follow formal semantics in mapping language to logical representations but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate argument structure", "Our clustering algorithm is highly scalable allowing us to run on corpora the size of Gigaword", "Different senses of a word are disambiguated based on their induced types", "We outperform a variety of existing approaches on a widecoverage question answering task and demonstrate the ability to make complex multisentence inferences involving quantifiers on the FraCaS suite"], "introduction": ["Mapping natural language to meaning representations is a central challenge of NLP", "There has been much recent progress in unsupervised distributional semantics in which the meaning of a word is induced based on its usage in large corpora", "This approach is useful for a range of key applications including question answering and relation extraction Lin and Pantel 2001 Poon and Domingos 2009 Yao et al 2011", "Because such a semantics can be automically induced it escapes the limitation of depending on relations from handbuilt training data knowledge bases or ontologies which have proved of limited use in capturing the huge variety of meanings that can be expressed in language", "However distributional semantics has largely developed in isolation from the formal semantics literature", "Whilst distributional semantics has been effective in modelling the meanings of content words such as nouns and verbs it is less clear that it can be applied to the meanings of function words", "Semantic operators such as determiners negation conjunctions modals tense mood aspect and plurals are ubiquitous in natural language and are crucial for high performance on many practical applications but current distributional models struggle to capture even simple examples", "Conversely computational models of formal semantics have shown low recall on practical applications stemming from their reliance on ontologies such as WordNet Miller 1995 to model the meanings of content words Bobrow et al 2007 Bos and Markert 2005", "For example consider what is needed to answer a question like Did Google buy YouTube", "from the following sentences 1", "Google purchased YouTube"]}, "Q13-1024-parscit130908": {"title": [""], "abstract": ["Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language", "It has been verified to be a useful constraint for word alignment", "However previous work either treats this as a hard constraint or uses it as a feature in discriminative models which is ineffective for largescale tasks", "In this paper we take dependency cohesion as a soft constraint and integrate it into a generative model for largescale word alignment experiments", "We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner", "Experiments on largescale ChineseEnglish translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality"], "introduction": ["Word alignment is the task of identifying word correspondences between parallel sentence pairs", "Word alignment has become a vital component of statistical machine translation SMT systems since it is required by almost all stateoftheart SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules Koehn et al 2007 Galley et al 2004", "During the past two decades generative word alignment models such as the IBM Models Brown et al 1993 and the HMM model Vogel et al 1996 have been widely used primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA toolkit Och and Ney 2003", "However the word alignment quality of generative models is still far from satisfactory for SMT systems", "In recent years discriminative alignment models incorporating linguistically motivated features have become increasingly popular Moore 2005 Taskar et al 2005 Riesa and Marcu 2010 Saers et al 2010 Riesa et al 2011", "These models are usually trained with manually annotated parallel data", "However when moving to a new language pair large amount of handaligned data are usually unavailable and expensive to create", "A more practical way to improve largescale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner Wu 1997 Yamada and Knight 2001 Lopez and Resnik 2005 DeNero and Klein 2007 Pauls et al 2010", "In this paper we take dependency cohesion Fox 2002 into account which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation", "Instead of treating dependency cohesion as a hard constraint Lin and Cherry 2003 or using it as a feature in discriminative models Cherry and Lin 2006b we treat dependency cohesion as a distortion constraint and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates", "We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner", "Experiments on a largescale ChineseEnglish translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality", "The remainder of this paper is organized as follows Section 2 introduces dependency cohesion 291 Transactions of the Association for Computational Linguistics 1 2013 291300", "Action Editor Chris CallisonBurch", "Submitted 52013 Published 72013", "c2013 Association for Computational Linguistics", "constraint for word alignment", "Section 3 presents our generative model for word alignment using dependency cohesion constraint", "Section 4 describes algorithms for parameter estimation", "We discuss and analyze the experiments in Section 5", "Section 6 gives the related work", "Finally we conclude this paper and mention future work in Section 7"]}, "S07-1032": {"title": ["GPLSI Word Coarsegrained Disambiguation aided by Basic Level"], "abstract": ["We present a corpusbased supervised learning system for coarsegrained sense disambiguation", "In addition to usual features for training in word sense disambiguation our system also uses Base Level Concepts automatically obtained from WordNet", "Base Level Concepts are some synsets that generalize a hyponymy subhierarchy and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated", "Our experiments proved that using this type of features results on a significant improvement of precision", "Our system has achieved almost 08 F1 fifth place in the coarsegrained English allwords task using a very simple set of features plus Base Level Concepts annotation"], "introduction": ["The GPLSI system in SemEvals task 7 coarse grained English allwords consists of a corpus based supervisedlearning method which uses local context information", "The system uses Base Level Concepts BLC Rosch 1977 as features", "In short BLC are synsets of WordNet WN Fell baum 1998 that are representative of a certain hyponymy subhierarchy", "The synsets that are selected to be BLC must accomplish certain conditions that will be explained in next section", "BLC This paper has been supported by the European Union under the project QALLME FP6 IST033860 and the Spanish Government under the project TextMess TIN200615265 C0601 and KNOW TIN200615049C0301 are slightly different from Base Concepts of EuroWordNet1 EWN Vossen et al 1998 Balkanet2 or Meaning Project3 because of the selection criteria but also because our method is capable to define them automatically", "This type of features helps our system to achieve 079550 F1 over the FirstSense baseline 078889 while only four systems outperformed ours being the F1 of the best one 083208", "WordNet has been widely criticised for being a sense repository that often offers too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broadcoverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Possibly building classbased classifiers would allow to avoid the data sparseness problem of the wordbased approach", "Thus some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldo van 2001 Agirre et al 2003 and on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 Ciaramita and Altun 2006", "However most of the later approaches used the original Lexico graphical Files of WN more recently called Super 1 httpwwwillcuvanlEuroWordNet 2 httpwwwceidupatrasgrBalkanet 3 httpwwwlsiupces nlpmeaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations SemEval2007 pages 157160 Prague June 2007", "Qc 2007 Association for Computational Linguistics senses as very coarsegrained sense distinctions", "However not so much attention has been paid on learning classbased classifiers from other available sensegroupings such as WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 EuroWordNet Base Concepts or Top Concept Ontology labels Atserias et al 2004", "Obviously these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD", "Possibly their combination could improve the overall results since they offer different semantic perspectives of the data", "Furthermore to our knowledge to date no comparative evaluation have been performed exploring different sensegroupings", "This paper is organized as follows", "In section 2 we present a method for deriving fully automatically a number of Base Level Concepts from any WN version", "Section 3 shows the details of the whole system and finally in section 4 some concluding remarks are provided"]}, "S10-1090": {"title": ["GPLSIIXA Using Semantic Classes to Acquire  Monosemous Training"], "abstract": ["This paper summarizes our participation in task 17 of SemEval2 Allwords WSD on a specific domain using a supervised classbased Word Sense Disambiguation system", "Basically we use Support Vector Machines SVM as learning algorithm and a set of simple features to build three different models", "Each model considers a different training corpus SemCor SC examples from monosemous words extracted automatically from background data BG and both SC and BG SCBG", "Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire classbased annotated examples from the domain text", "We use the classbased examples gathered from the domain corpus to adapt our traditional system trained on SemCor", "The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words obtaining results above the first sense baseline and the fifth best position in the competition rank"], "introduction": ["As empirically demonstrated by the last SensEval and SemEval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed", "In fact supervised wordbased WSD systems are very dependent of the corpora used for training and testing the system Escudero et al 2000", "One possible reason could be the use of inappropriate level of abstraction", "where each class corresponds to a particular synset of the word", "But WordNet WN has been widely criticized for being a sense repository that often provides too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broadcoverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldovan 2001 Agirre and LopezDeLaCalle 2003 Navigli 2006 and Snow et al 2007", "That is they provide methods for grouping senses of the same word thus producing coarser word sense groupings for better disambiguation", "In contrast some research have been focused on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 Kohomban and Lee 2005 and Ciaramita and Altun 2006", "That is grouping senses of different words into the same explicit and comprehensive semantic class", "Most of the later approaches used the original Lexico graphical Files of WN more recently called SuperSenses as very coarsegrained sense distinctions", "We suspect that selecting the appropriate level of abstraction could be on between both levels", "Thus we use the semantic classes modeled by the Basic Level Concepts1 BLC Izquierdo et al 2007", "Our previous research using BLC empirically demonstrated that this automatically derived Most supervised systems simply model each polysemous word as a classification problem 1 httpadimensiehueswebBLC 402 Proceedings of the 5th International Workshop on Semantic Evaluation ACL 2010 pages 402406 Uppsala Sweden 1516 July 2010", "Qc 2010 Association for Computational Linguistics set of meanings groups senses into an adequate level of abstraction in order to perform classbased Word Sense Disambiguation WSD Izquierdo et al 2009", "Now we also show that classbased WSD allows to successfully incorporate monosemous examples from the domain text", "In fact 3 and A has 2 so D is the first maximum", "A 2 the robustness of our classbased WSD approach is shown by our system that just uses the Sem Cor examples SC", "It performs without any kind B C D 2 3 BLC of domain adaptation as the Most Frequent Sense MFS baseline", "This paper describes our participation in SemEval2010 Task 17 Agirre et al 2010", "In section 2 semantic classes used and selection algorithm used to obtain them automatically from WordNet are described", "In section 3 the technique employed to extract monosemous examples from background data is described", "Section 4 explains the general approach of our system and the experiments designed and finally in section 5 the results and some analysis are shown"]}, "S12-1011": {"title": ["Learning  Semantics and Selectional Preference of AdjectiveNoun  Pairs"], "abstract": ["We investigate the semantic relationship between a noun and its adjectival modifiers", "We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers and adjectivenoun selectional preference", "Through a combination of novel and existing evaluations we test the degree to which adjectivenoun relationships can be cat egorised", "We analyse the effect of lexical context on these relationships and the efficacy of the latent semantic representation for disambiguating word meaning"], "introduction": ["Developing models of the meanings of words and phrases is a key challenge for computational linguistics", "Distributed representations are useful in capturing such meaning for individual words Sato et al 2008 Maas and Ng 2010 Curran 2005", "However finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic Mitchell and Lapata 2008 Baroni and Zamparelli 2010 Grefenstette and Sadrzadeh 2011", "It is in this area that our paper makes its contribution", "The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques", "However such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference", "We propose a probabilistic model of the semantic tion of noun and adjective semantics together with their compositional probabilities", "We employ this formulation to give a dual view of nounmodifier semantics the induced latent variables provide an explicit account of selectional preference while the marginal distributions of the latent variables for each word implicitly produce a distributed representation", "Most related work on selectional preference uses classbased probabilities to approximate sparse individual probabilities", "Relevant papers include O Seaghdha 2010 who evaluates several topic models adapted to learning selectional preference using cooccurence and Baroni and Zamparelli 2010 who represent nouns as vectors and adjectives as matrices thus treating them as functions over noun meaning", "Again inference is achieved using cooccurrence and dimensionality reduction"]}, "S12-1023": {"title": ["Regular polysemy A distributional model"], "abstract": ["Many types of polysemy are not word specific but are instances of general sense alternations such as ANIMALFOOD", "Despite their pervasiveness regular alternations have been mostly ignored in empirical computational semantics", "This paper presents a a general framework which grounds sense alternations in corpus data generalizes them above individual words and allows the prediction of alternations for new words and b a concrete unsupervised implementation of the framework the Centroid Attribute Model", "We evaluate this model against a set of 2400 ambiguous words and demonstrate that it outperforms two baselines"], "introduction": ["One of the biggest challenges in computational semantics is the fact that many words are polysemous", "For instance lamb can refer to an animal as in The lamb squeezed through the gap or to a food item as in Sue had lamb for lunch", "Polysemy is pervasive in human language and is a problem in almost all applications of NLP ranging from Machine Translation as word senses can translate differently to Textual Entailment as most lexical entailments are sensespecific", "The field has thus devoted a large amount of effort to the representation and modeling of word senses", "The arguably most prominent effort is Word Sense Disambiguation WSD Navigli 2009 an invitro task whose goal is to identify which of a set of predefined senses is the one used in a given context", "In work on WSD and other tasks related to pol ysemy such as word sense induction sense alternations are treated as wordspecific", "As a result a model for the meaning of lamb that accounts for the relation between the animal and food senses cannot predict that the same relation holds between instances of chicken or salmon in the same type of contexts", "A large number of studies in linguistics and cognitive science show evidence that there are regulari ties in the way words vary in their meaning Apresjan 1974 Lakoff and Johnson 1980 Copestake and Briscoe 1995 Pustejovsky 1995 Gentner et al 2001 Murphy 2002 due to general analogical processes such as regular polysemy metonymy and metaphor", "Most work in theoretical linguistics has focused on regular systematic or logical polysemy which accounts for alternations like ANIMALFOOD", "Sense alternations also arise from metaphorical use of words as dark in dark glassdark mood and also from metonymy when for instance using the name of a place for a representative as in Germany signed the treatise", "Disregarding this evidence is empirically inadequate and leads to the wellknown lexical bottleneck of current word sense models which have serious problems in achieving high coverage Navigli 2009", "We believe that empirical computational semantics could profit from a model of polysemy1 which a is applicable across individual words and thus capable of capturing general patterns and generalizing to new 1 Our work is mostly inspired in research on regular polysemy", "However given the fuzzy nature of regularity in meaning variation we extend the focus of our attention to include other types of analogical sense construction processes", "151 First Joint Conference on Lexical and Computational Semantics SEM pages 151160 Montreal Canada June 78 2012", "Qc 2012 Association for Computational Linguistics words and b is induced in an unsupervised fashion from corpus data", "This is a longterm goal with many unsolved subproblems", "The current paper presents two contributions towards this goal", "First since we are working on a relatively unexplored area we introduce a formal framework that can encompass different approaches Section 2", "Second we implement a concrete instantiation of this framework the unsupervised Centroid Attribute Model Section 3 and evaluate it on a new task namely to detect which of a set of words in stantiate a given type of polysemy Sections 4 and 5", "We finish with some conclusions and future work Section 7"]}, "S12-1040": {"title": ["UGroningen  Negation detection with Discourse Representation Structures"], "abstract": ["We use the NLP toolchain that is used to construct the Groningen Meaning Bank to address the task of detecting negation cue and scope as defined in the shared task Resolving the Scope and Focus of Negation", "This toolchain applies the CC tools for parsing using the formalism of Combinatory Categorial Grammar and applies Boxer to produce semantic representations in the form of Discourse Representation Structures DRSs", "For negation cue detection the DRSs are converted to flat nonrecursive structures called Discourse Representation Graphs DRGs", "DRGs simplify cue detection by means of edge labels representing relations", "Scope detection is done by gathering the tokens that occur within the scope of a negated DRS", "The result is a system that is fairly reliable for cue detection and scope detection", "Furthermore it provides a fairly robust algorithm for detecting the negated event or property within the scope"], "introduction": ["Nothing is more home to semantics than the phenomenon of negation", "In classical theories of meaning all states of affairs are divided in two truth values and negation plays a central role to determine which truth value is at stake for a given sentence", "Negation lies at the heart of deductive inference of which consistency checking searching for contradictions in texts is a prime example in natural language understanding", "It shouldnt therefore come as a surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics", "In this paper we present and evaluate a system that transforms texts into logical formulas  using the CC tools and Boxer Bos 2008  in the context of the shared task on recognising negation in English texts Morante and Blanco 2012", "We will first sketch the background and the basics of the formalism that we employ in our analysis of negation Section 2", "In Section 3 we explain how we detect negation cues and scope", "Finally in Section 4 we present the results obtained in the shared task and we discuss them in Section 5"]}, "S13-1002": {"title": ["11"], "abstract": ["We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks MLNs", "We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives", "We also show that distributional phrase similarity used as textual inference rules created on the fly improves its performance"], "introduction": ["Tasks in natural language semantics are very diverse and pose different requirements on the underlying formalism for representing meaning", "Some tasks require a detailed representation of the structure of complex sentences", "Some tasks require the ability to recognize nearparaphrases or degrees of similarity between sentences", "Some tasks require logical inference either exact or approximate", "Often it is necessary to handle ambiguity and vagueness in meaning", "Finally we frequently want to be able to learn relevant knowledge automatically from corpus data", "There is no single representation for natural language meaning at this time that fulfills all requirements", "But there are representations that meet some of the criteria", "Logicbased representations Montague 1970 Kamp and Reyle 1993 provide an expressive and flexible formalism to express even complex propositions and they come with standardized inference mechanisms", "Distributional mod simhamster gerbil  w gerbil hamster x hamsterx  gerbilx  f w Figure 1 Turning distributional similarity into a weighted inference rule els Turney and Pantel 2010 use contextual similarity to predict semantic similarity of words and phrases Landauer and Dumais 1997 Mitchell and Lapata 2010 and to model polysemy Schu tze 1998 Erk and Pado  2008 Thater et al 2010", "This suggests that distributional models and logic based representations of natural language meaning are complementary in their strengths Grefenstette and Sadrzadeh 2011 Garrette et al 2011 which encourages developing new techniques to combine them", "Garrette et al", "2011 2013 propose a framework for combining logic and distributional models in which logical form is the primary meaning representation", "Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form as illustrated in Figure 1", "Finally Markov Logic Networks Richardson and Domingos 2006 MLNs are used to perform weighted inference on the resulting knowledge base", "However they only employed singleword distributional similarity rules and only evaluated on a small 11 Second Joint Conference on Lexical and Computational Semantics SEM Volume 1 Proceedings of the Main Conference and the Shared Task pages 1121 Atlanta Georgia June 1314 2013", "Qc 2013 Association for Computational Linguistics set of short handcrafted test sentences", "In this paper we extend Garrette et als approach and adapt it to handle two existing semantic tasks recognizing textual entailment RTE and semantic textual similarity STS", "We show how this single semantic framework using probabilistic logical form in Markov logic can be adapted to support both of these important tasks", "This is possible because MLNs constitute a flexible programming language based on probabilistic logic Domingos and Lowd 2009 that can be easily adapted to support multiple types of linguistically useful inference", "At the word and short phrase level our approach model entailment through distributional similarity Figure 1", "If X and Y occur in similar contexts we assume that they describe similar entities and thus there is some degree of entailment between them", "At the sentence level however we hold that a stricter logicbased view of entailment is beneficial and we even model sentence similarity in STS as entail ment", "There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data", "First we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words", "In comparison to existing methods for creating textual inference rules Lin and Pantel 2001b Szpek tor and Dagan 2008 these rules are computed on the fly as needed rather than precompiled", "Second we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors", "We replace deterministic conjunction by an average combiner which encodes causal independence Natarajan et al 2010", "We show that our framework is able to handle both sentence similarity STS and textual entailment RTE by making some simple adaptations to the MLN when switching between tasks", "The framework achieves reasonable results on both tasks", "On STS we obtain a correlation of r  066 with full logic r  073 in a system with weakened variable binding and r  085 in an ensemble model", "On RTE1 we obtain an accuracy of 057", "We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS", "Al though other approaches could be adapted to handle both RTE and STS we do not know of any other methods that have been explicitly tested on both problems"]}, "W00-0202": {"title": ["Representation of Actions as an Interlingua"], "abstract": ["We present a Parameterized Action Representation PAR that provides a conceptual representation ofdifferent types of actions used to animate virtual human agents in a simulated 3D environment", "These actions involve changes of state changes of location kinematic and exertion of force dynamic", "PARSare hierarchical parameterized structures that facilitate both visual and verbal expressions", "In order to support the animation of the actions PARShave to make explicit many details that are often underspecified in the language", "This detailed level ofrepresentation also provides a suitable pivot representation for generation in other natural languages ie a form of interlingua", "We show examples of how certain divergences in machine translation can be solved by our approach focusing specifically on how verbframed and satelliteframed languages can use our representation"], "introduction": ["In this paper we describe a Parameterized Ac tion Representation PAR Badler et al 1999that provides a conceptual representation of differ ent types of actions used to animate virtual humanagents in a simulated 3D environment", "These ac tions involve changes of state changes of location kinematic and exertion of force dynamic", "PARSare hierarchical parameterized structures that fa cilitate both visual and verbal expressions Badler et al 2000", "In order to support the animation ofthe actions PARS have to make explicit many de tails that are often underspecified in the language", "This detailed level of representation is well suitedfor an interlingua for machine translation applications since the animations of actions  and therefore the PARS that control them  will be equivalent for the same actions described in different lan guages", "These representations can be incorporated into a system which uses PARbased animations asa workbench for creating accurate conceptual representations which can map to seeral different lan guages as well as produce faithful animations", "The verb classes we are currently considering in this light involve explicit physical actions such asthose expressed in the motion verb class and contact verb class Levin 1993", "Since we are employ ing PAR as an interlingual representation we willshow examples of how it can handle certain diver gences in machine translation focusing specifically on how verbframed and satelliteframed languages Talmy 1991 can yield equivalent actions in this representation"]}, "W00-0701": {"title": ["In  Proceedings  of CoNLL2000 and  LLL2000 pages 16  Lisbon  Portugal 2000"], "abstract": ["This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing", "It shows that these ap proaches can all be explained using a single dis tribution free inductive principle related to the pac model of learning", "Furthermore they all make predictions using the same simple knowl edge representation  a linear representation over a common feature space", "This is signifi cant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be ex tended to learn from more structured knowl edge intensive examples as part of a learning centered approach to higher level natural lan guage inferences"], "introduction": ["Many important natural language inferences can be viewed as problems of resolving phonetic syntactic semantics or pragmatics ambiguities based on properties of the surrounding context", "It is generally accepted that a learning compo nent must have a central role in resolving these context sensitive ambiguities and a significant amount of work has been devoted in the last few years to developing learning methods for these tasks with considerable success", "Yet our un derstanding of when and why learning works in this domain and how it can be used to support increasingly higher level tasks is still lacking", "This article summarizes work on developing a learning theory account for the major learning approaches used in NL", "While the major statistics based methods used in NLP are typically developed with a  This research is supported by NSF grants IIS9801638 SBR9873450 and IIS9984168", "Bayesian view in mind the Bayesian principle cannot directly explain the success and robust ness of these methods since their probabilistic assumptions typically do not hold in the data", "Instead we provide this explanation using a sin gle distribution free inductive principle related to the pac model of learning", "We describe the unified learning framework and show that in addition to explaining the success and robust ness of the statistics based methods it also ap plies to other machine learning methods such as rule based and memory based methods", "An important component of the view devel oped is the observation that most methods use the same simple knowledge representation", "This is a linear representation over a new feature space  a transformation of the original instance space to a higher dimensional and more expres sive space", "Methods vary mostly algorithmicly in ways they derive weights for features in this space", "This is significant both to explaining the generalization properties of these methods and to developing an understanding for how and when can these methods be extended to learn from more structured knowledge intensive ex amples perhaps hierarchically", "These issues are briefly discussed and we emphasize the impor tance of studying knowledge representation and inference in developing a learning centered ap proach to NL inferences"]}, "W01-0502": {"title": ["A Sequential Model for MultiClass Classification"], "abstract": ["Many classification problems require decisions among a large number of competing classes", "These tasks however are not handled well by general purpose learning methods and are usually addressed in an adhoc fashion", "We suggest a general approach  a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining with high probability the presence of the true outcome in the candidates set", "Some theoretical and computational properties of the model are discussed and we argue that these are important in NLPlike domains", "The advantages of the model are illustrated in an experiment in part ofspeech tagging"], "introduction": ["A large number of important natural language inferences can be viewed as problems of resolving ambiguity either semantic or syntactic based on properties of the surrounding context", "These in turn can all be viewed as classification problems in which the goal is to select a class label from among a collection of candidates", "Examples include partof speech tagging wordsense disambiguation accent restoration word choice selection in machine translation contextsensitive spelling correction word selection in speech recognition and identifying discourse markers", "Machine learning methods have become the most popular technique in a variety of classification problems of these sort and have shown significant success", "A partial list consists of Bayesian classifiers Gale et al 1993 decision lists Yarowsky 1994 Bayesian hybrids Golding 1995 HMMs Charniak 1993 inductive logic methods Zelle and Mooney 1996 memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", "based methods Zavrel et al 1997 linear classifiers Roth 1998 Roth 1999 and transformation based learning Brill 1995", "In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large  all words in words selection problems all possible tags in tagging problems etc Since general purpose learning algorithms do not handle these multiclass classification problems well see below most of the studies do not address the whole problem rather a small set of candidates typically two is first selected and the classifier is trained to choose among these", "While this approach is important in that it allows the research community to develop better learning methods and evaluate them in a range of applications it is important to realize that an important stage is missing", "This could be significant when the classification methods are to be embedded as part of a higher level NLP tasks such as machine translation or information extraction where the small set of candidates the classifier can handle may not be fixed and could be hard to determine", "In this work we develop a general approach to the study of multiclass classifiers", "We suggest a sequential learning model that utilizes almost general purpose classifiers to sequentially restrict the number of competing classes while maintaining with high probability the presence of the true outcome in the candidate set", "In our paradigm the sought after classifier has to choose a single class label or a small set of labels from among a large set of labels", "It works by sequentially applying simpler classifiers each of which outputs a probability distribution over the candidate labels", "These distributions are multiplied and thresholded resulting in that each classifier in the sequence needs to deal with a significantly smaller number of the candidate labels than the previous classifier", "The classifiers in the sequence are selected to be simple in the sense that they typically work only on part of the feature space where the decomposition of feature space is done so as to achieve statistical independence", "Simple classifier are used since they are more likely to be accurate they are chosen so that with high probability whp they have one sided error and therefore the presence of the true label in the candidate set is maintained", "The order of the sequence is determined so as to maximize the rate of decreasing the size of the candidate labels set", "Beyond increased accuracy on multiclass classification problems  our scheme improves the computation time of these problems several orders of magnitude relative to other standard schemes", "In this work we describe the approach discuss an experiment done in the context of partofspeech pos tagging and provide some theoretical justifications to the approach", "Sec", "2 provides some background on approaches to multiclass classification in machine learning and in NLP", "In Sec", "3 we describe the sequential model proposed here and in Sec", "4 we describe an experiment the exhibits some of its advantages", "Some theoretical justifications are outlined in Sec", "5"]}, "W01-0505": {"title": ["ffflfi ffffiffi"], "abstract": ["The problem of finding lexical alignments for givensentence pairs is computationally expensive", "Furthermore it is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of wordforword correspondences is lowThis paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments", "We describe how the features improve the performance of the lexical alignment model", "The structural features providethe information for the correspondences of partsofspeech POS sequences which are useful in translation", "Based on maximum entropy ME concept the structural features are incrementally selected which are later embedded in the lexical alignment modelIt turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge"], "introduction": ["Aligned bitexts are useful for the derivation ofbilingual lexical resources which are used for ma chine translation and cross languages informationretrieval", "Thus a lot of approaches have been sug gested to find sets of corresponding word tokens Brown et al 1993 Berger et al 1996 Melamed1997 phrase Shin et al 1996 noun phrase Ku piec 1993 and collocation Smadja et al 1996 in a bitext", "Some works have used lexical association measures for finding word correspondences Gale and Church1991 Fung and Church 1994", "However the associ ation measures can be misled in cases where a word in a source language frequently cooccurs with more than one word in a target language or in cases of indirect associationapos Melamed 1997", "In other works iterative parameter reestimation aposSuppose that Uk and vk are indeed mutual translation and Uk and Ukki often cooccur in text", "Then vk and 241 will also cooccur more than expected by chance  which is represented as indirect association", "techniques based on IBM model 15 2 have been employed Brown et al 1993", "They were usually incorporated in the EM algorithm Brown et al 1993 Kupiec 1993 Tillmann and Ney 2000 Och et al 2000", "However we are often faced with some difficulties as follows when the IBM modelbased approaches are directly applied to the alignment especially on bitext involving a less closely related language pair", "1", "It needs excessive iteration time for parame", "ter estimation and high decoding complexityThus most systems assumed onetoone correspondence to reduce computational complexity", "However word sequences are not trans lated literally word for word", "For example incases of collocations compound nouns and ambiguous words with different meaning depen dent on the context they require phraselevel correspondences"]}, "W02-0503-parscit130908": {"title": [""], "abstract": ["Many papers have discussed different aspects of Arabic verb morphology", "Some of them used patterns others used patterns and affixes", "But very few have discussed Arabic noun morphology particularly for nouns that are not derived from verbs", "In this paper we describe a learning system that can analyze Arabic nouns to produce their morphological information and their paradigms with respect to both gender and number using a rule base that uses suffix analysis as well as pattern analysis", "The system utilizes userfeedback to classify the noun and identify the group that it belongs to"], "introduction": ["A morphology system is the backbone of a natural language processing system", "No application in this field can survive without a good morphology system to support it", "The Arabic language has its own features that are not found in other languages", "That is why many researchers have worked in this area", "AlFedaghi and AlAnzi 1989 present an algorithm to generate the root and the pattern of a given Arabic word", "The main concept in the algorithm is to locate the position of the rootaposs letters in the pattern and examine the letters in the same position in a given word to see whether the tri graph forms a valid Arabic root or not", "AlShalabi 1998 developed a system that removes the longest possible prefix from the word where the three letters of the root must lie somewhere in the first four or five characters of the remainder", "Then he generates some combinations and checks each one of them with all the roots in the file", "AlShalabi reduced the processing but he discussed this from point of view of verbs not nouns", "Anne Roeck and Waleed AlFares 2000 developed a clustering algorithm for Arabic words sharing the same verbal root", "They used rootbased clusters to substitute for dictionaries in indexing for information retrieval", "Beesley and Karttunen 2000 described a new technique for constructing finitestate transducers that involves reapplying a regularexpression compiler to its own output", "They implementedthe system in an algorithm called compile replace", "This technique has proved useful for handling nonconcatenate phenomena and they demonstrate it on Malay fullstem reduplication and Arabic stem interdigitations", "Most verbs in the Arabic language follow clear rules that define their morphology and generate their paradigms", "Those nouns that are not derived from roots do not seem to follow a similar set of welldefined rules", "Instead there are groups showing family resemblances", "We believe that nouns in Arabic that are not derived from roots are governed not only by phonological rules but by lexical patterns that must be identified and stored for each noun", "Like irregular verbs in English their forms are determined by history and etymology not just phonology", "Among many other examples Pinker 1999 points to the survival of past forms became for become and overcame for overcome modeled on came for come while succumb with the same sound pattern has a regular past form succumbed", "The same kinds of phenomena are especially apparent for proper nouns in Arabic derived from Indian and Persian names", "Pinker uses examples like this as well as emerging research in neurophysiology to argue for the coexistence of phonological rules and lexical storage of English verb patterns", "We believe that further work in Arabic computational linguistics requires the development of a pattern bank for nouns", "This paper describes the tool that we have built for this purpose", "While the set of patterns for common nouns in Arabic may soon be established newspapers and other dynamic sources of language will always contain new proper names so we expect our tool to be a permanent part of our system even though we may need it less often as time goes on"]}, "W02-1005": {"title": ["Proceedings of the Conference on Empirical Methods in Natural"], "abstract": ["This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classification tasks", "We present a new classification correction technique that successfully addresses the problem of underestimation of infrequent classes in the training data", "We show that the mixture models are boostingfriendly and that both Adaboost and our original correction technique can improve the results of the raw model significantly achieving state oftheart performance on several standard test sets in four languages", "With substantially different output to Nave Bayes and other statistical methods the investigated models are also shown to be effective participants in classifier combination"], "introduction": ["The focus tasks of this paper are two related problems in lexical ambiguity resolution Word Sense Disambiguation WSD and Context Sensitive Spelling Correction CSSC", "Word Sense Disambiguation has a long history as a computational task Kelly and Stone 1975 and the field has recently supported largescale international system evaluation exercises in multiple languages SEN SEVA L1 Kilgarriff and Palmer 2000 and SEN SEVA L2 Edmonds and Cotton 2001", "General purpose Spelling Correction is also a longstanding task eg McIlroy 1982 traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest valid word in a dictionary or a morphological variant typically ignoring context", "Yet Kukich 1992 observed that about 2550 of the spelling errors found in modern documents are either contextinappropriate misuses or substitutions of valid words such as principal and principle which are not detected by traditional spelling cor rectors", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models Golding 1995 Winnow Golding and Roth 1996 and TransformationBased Learning Mangu and Brill 1997", "Generally both tasks involve the selection between a relatively small set of alternatives per keyword eg sense ids such as churchBU ILD IN G and churchIN STITU T IO N or commonly confused spellings such as quiet and quite and are dependent on local and longdistance collocational and syntactic patterns to resolve between the set of alternatives", "Thus both tasks can share a common feature space data representation and algorithm infrastructure", "We present a framework of doing so while investigating the use of mixture models in conjunction with a new errorcorrection technique as competitive alternatives to Bayesian models", "While several authors have observed the fundamental similarities between CSSC and WSD eg Berleant 1995 and Roth 1998 to our knowledge no previous comparative empirical study has tackled these two problems in a single unified framework"]}, "W02-1108": {"title": [""], "abstract": ["We propose a method for semiautomatic classification of verbs to Levin classes via the semantic network of WordNet", "The method involvesfirst classifying entire WordNet senses to semantic classes and then classifying individual verbs on the basis of their WordNet senses", "We report evaluation which shows that the method can be used to build a verb classification accurateenough for practical NLP use", "The WordNetLevin mapping produced as a byproduct may in turn be used to supplement WordNet with novel information"], "introduction": ["Linguistic research has shown that verbs fallinto classes distinctive in terms of their syntac tic and semantic properties Jackendoff 1990 Hale and Keyser 1993 Levin 1993 Pinker 1989", "For example verbs which share the meaning component of aposmotionapos eg fly and walk tend to behave similarly also in terms of subcategorization and can thus be grouped to a linguistically coherent class", "While the correspondence between the syntax and semantics of verbs is arguably not perfect and while the whole notion of a verb class is somewhat elusiveapos verb classifications can be constructed which provide a generalization over a range of syntactic and semantic properties of verbs", "Such classifications are particularly useful from a practical NLP point of view", "They can be used as a means of reducing redundancy inthe lexicon and for filling gaps in lexical knowl edge", "To a certain extent they enable inferring 1For example as most verbs can be characterized byseveral meaning components there is potential for crossclassification", "Therefore different equally viable classifi cation schemes can be constructedthe semantics of a word on the basis of its syn tactic behaviour and the syntax of a word on the basis of its semantic behaviour", "Verb classifications have in fact been usedto support various NLP tasks including ma chine translation language generation Dorr 1997 document classification Klavans and Kan 1998 lexicography Sanfilippo 1994 andlexical acquisition such as word sense disambiguation Dorr and Jones 1996 and subcate gorization acquisition Korhonen 2002b", "The verb classification employed most widely in NLP is Levinaposs taxonomy of verbs and their classes Levin 1993", "Levin classes are based onthe ability of a verb to occur in specific diathe sis alternations ie specific pairs of syntacticframes which are assumed to be meaning re tentive", "The classification covers a substantial number of diathesis alternations occurring in English", "It is not however exhaustive", "More work is required on extending and refining it until a comprehensive resource can be obtained suitable for largescale NLP use Dorr and Jones 1996 Dorr 1997 Korhonen 2002bPerhaps the most challenging task is to ex tend the classification with new participants", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming Levin 1993 Dang et al 1998", "Fully automatic classification on the other hand is fast but suffers from low accuracy Dorr 1997 Stevenson and Merlo 1999", "In this paper we propose combining the strengths of theseapproaches", "We present a method for semi automatic semantic classification of verbs which exploits automatic techniques but also allows for some manual interventionThe method involves classifying verbs seman tically via the semantic network of WordNet Miller 1990", "Unlike Levinaposs source Wordnet is a comprehensive lexical database", "Although it classifies verbs on a purely semantic basis the syntactic regularities studied by Levin are to some extent reflected by semantic relatedness asit is represented by WordNetaposs particular struc ture Dorr 1997 Fellbaum 1999", "Our methodmakes use of this partial overlap between WordNet and Levin classes", "It involves first classify ing entire WordNet senses to semantic classesand then classifying individual verbs on the ba sis of their WordNet sensesWe use this method to classify verbs in a num ber of WordNet files and report experimental evaluation which shows that the method is fairlyaccurate and can also be used to build a clas sification suitable for practical NLP use", "The classification built using our method can alsobe used to benefit linguistic research as knowledge about novel verb and verb class associa tions can be used to test and enrich linguistic theory eg", "Levin 1993As a byproduct the method produces a map ping between WordNet senses and Levin classes", "This mapping  once comprehensive  can actas a useful supplement to WordNet which incorporates information about diathesis alterna tions and semantic verb classes", "Currently this information is absent in WordNet", "We discuss the background for our work in section 2", "In section 3 we describe the method for classifying verbs semantically via WordNet", "The details of the experimental evaluation of our method are supplied in section 4", "Section 5 concludes with directions for future work"]}, "W03-0432": {"title": ["Named Entity Recognition Using a Characterbased Probabilistic Approach"], "abstract": ["We present a named entity recognition and classification system that uses only probabilistic characterlevel features", "Classifications by multiple orthographic tries are combined in a hidden Markov model framework to incorporate both internal and contextual evidence", "As part of the system we perform a preprocessing stage in which capitalisation is restored to sentenceinitial and allcaps words with high accuracy", "We report fvalues of 8665 and 7978 for English and 5062 and 5443 for the German datasets"], "introduction": ["Language independent NER requires the development of a metalinguistic model that is sufficiently broad to accommodate all languages yet can be trained to exploit the specific features of the target language", "Our aim in this paper is to investigate the combination of a character level model orthographic tries with a sentencelevel hidden Markov model", "The local model uses affix information from a word and its surrounds to classify each word independently and relies on the sentencelevel model to determine a correct state sequence", "Capitalisation is an oftenused discriminator for NER but can be misleading in sentenceinitial or allcaps text", "We choose to use a model that makes no assumptions about the capitalisation scheme or indeed the character set of the target language", "We solve the problem of misleading case in a novel way by removing the effects of sentenceinitial or allcaps capitalisation", "This results in a simpler language model and easier recognition of named entities while remaining strongly language independent"]}, "W04-0705": {"title": ["Applying Coreference to Improve Name Recognition"], "abstract": ["We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese", "We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger", "For names with low confidence we show how these names can be filtered using coreference features to improve accuracy", "In addition we present rules which use coreference information to correct some name tagging errors", "Finally we show how these gains can be magnified by clustering documents and using crossdocument coreference in these clusters", "These combined methods yield an absolute improvement of about 31 in tagger F score"], "introduction": ["The problem of name recognition and classification has been intensively studied since1995 when it was introduced as part of the MUC 6 Evaluation Grishman and Sundheim 1996", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al 1997 Maximum Entropy methods Borthwick et al 1998 Chieu and Ng 2002 Decision Trees Sekine et al 1998 Conditional Random Fields McCallum and Li 2003 Classbased Language Model Sun et al 2002 Agentbased Approach Ye et al 2002 and Support Vector Machines", "However the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ", "In particular most of these methods classify an instance of a name based on the information about that instance alone and very local context of that instance  typically one or 1 The best results reported for Chinese named entity recognition on the MET2 test corpus are 092 to 095 Fmeasure for the different name types Ye et al 2002", "two words preceding and following the name", "If a name has not been seen before and appears in a relatively uninformative context it becomes very hard to classify", "We propose to use more global information to improve the performance of name recognition", "Some name taggers have incorporated a name cache or similar mechanism which makes use of names previously recognized in the document", "In our approach we perform coreference analysis and then use detailed evidence from other phrases in the document which are coreferential with this name in order to disambiguate the name", "This allows us to perform a richer set of corrections than with a name cache", "We then go one step further and process similar documents containing instances of the same name and combine the evidence from these additional instances", "At each step we are able to demonstrate a small but consistent improvement in named entity recognition", "The rest of the paper is organized as follows", "Section 2 briefly describes the baseline name tagger and coreference resolver used in this paper", "Section 3 considers methods for assessing the confidence of name tagging decisions", "Section 4 examines the distribution of name errors as a motivation for using coreference information", "Section 5 shows the coreference features we use and how they are incorporated into a statistical name filter", "Section 6 describes additional rules using coreference to improve name recognition", "Section 7 provides the flow graph of the improved system", "Section 8 reports and discusses the experimental results while Section 9 summarizes the conclusions"]}, "W04-3238": {"title": [""], "abstract": ["Logs of user queries to an internet search engine provide a large amount of implicit and explicit information about language", "In this paper we investigate their use in spelling correction of search queries a task which poses many additional challenges beyond the traditional spelling correction problem", "We present an approach that uses an iterative transformation of the input query strings into other strings that correspond to more and more likely queries according to statistics extracted from internet search query logs"], "introduction": ["The task of general purpose spelling correction has a long history eg Damerau 1964 Rieseman and Hanson 1974 McIlroy 1982 traditionally focusing on resolving typographical errors such as insertions deletions substitutions and transpositions of letters that result in unknown words ie words not found in a trusted lexicon of the language", "Typical word processing spell checkers compute for each unknown word a small set of inlexicon alternatives to be proposed as possible corrections relying on information about inlexiconword frequencies and about the most common keyboard mistakes such as typing m instead of n and phoneticcognitive mistakes both at word level eg the use of acceptible instead of acceptable and at character level eg the misuse of f instead of ph", "Very few spell checkers attempt to detect and correct word substitution errors which refer to the use of inlexicon words in inappropriate contexts and can also be the result of both typographical mistakes such as typing coed instead of cord and cognitive mistakes eg principal and principle", "Some research efforts to tackle this problem have been made for example Heidorn et al", "1982 and Garside et al", "1987 developed systems that rely on syntactic patterns to detect substitution errors while Mays et al", "1991 employed word cooccurrence evidence from a large corpus to detect and correct such errors", "The former approaches were based on the impractical assumption that all possible syntactic uses of all words ie partofspeech are known and presented both recall and precision problems because many of the substitution errors are not syntactically anomalous and many unusual syntactic constructions do not contain errors", "The latter approach had very limited success under the assumptions that each sentence contains at most one misspelled word each misspelling is the result of a single point change insertion deletion substitution or transposition and the defect rate the relative number of errors in the text is known", "A different body of work eg Golding 1995 Golding and Roth 1996 Mangu and Brill 1997 focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", "Although promising results were obtained 9295 accuracy the scope of this work was very limited as it only addressed known sets of commonly confused words such as peace piece", "11 Spell Checking of Search Engine Queries", "The task of webquery spelling correction addressed in this work has many similarities to traditional spelling correction but also poses additional challenges", "Both the frequency and severity of spelling errors for search queries are significantly greater than in word processing", "Roughly 1015 of the queries sent to search engines contain errors", "Typically the validity of a query cannot be decided by lexicon lookup or by checking its gram maticality", "Because web queries are very short on average less than 3 words techniques that use a multitude of features based on relatively wide context windows such as those investigated in CSSC are difficult to apply", "Rather than being well formed sentences most queries consist of one concept or an enumeration of concepts many times containing legitimate words that are not found in any traditional lexicon", "Just defining what a valid web query is represents a difficult enterprise", "We clearly cannot use only a static trusted lexicon as many new names and concepts such as aznar blog naboo nimh nsync and shrek become popular every day and it would be extremely difficult if not impossible to maintain a highcoverage lexicon", "In addition employing very large lexicons can result in more errors surfacing as word substitutions which are very difficult to detect rather than as unknown words", "One alternative investigated in this work is to exploit the continuously evolving expertise of millions of people that use web search engines as collected in search query logs seen as histograms over the queries received by a search engine", "In some sense we could say that the validity of a word can be inferred from its frequency in what people are querying for similarly to Wittgensteins 1968 observation that the meaning of a word is its use in the language", "Such an approach has its own caveats", "For example it would be er ple it can be the ratio between the number of letters two words do not have in common and the number of letters they share1 The two most used classes of distances in spelling correction are edit distances as proposed by Damerau 1964 and Levenshtein 1965 and correlation matrix distances Cherkassky et al 1974", "In our study we use a modified version of the DamerauLev enshtein edit distance as presented in Section 3", "One flaw of the preceding formulation is that it does not take into account the frequency of words in a language", "A simple solution to this problem is to compute the probability of words in the target language as maximum likelihood estimates MLE over a large corpus and reformulate the general spellingcorrection problem as follows roneous to simply extract from webquery logs all the queries whose frequencies are above a certain value and consider them valid", "Misspelled queries Given w dist w w    L  find and Pw  w L max v Ldist  wv  such that Pv  such as britny spears are much more popular than correctly spelled queries such as bayesian nets and amd processors", "Our challenge is to try to utilize query logs to learn what queries are valid and to build a model for valid query probabilities despite the fact that a large percentage of the logged queries are misspelled and there is no trivial way to determine the valid from invalid queries"]}, "W05-0612": {"title": ["Proceedings of the 9th Conference on Computational Natural Language Learning CoNLL"], "abstract": ["We propose an unsupervised Expectation Maximization approach to pronoun resolution", "The system learns from a fixed list of potential antecedents for each pronoun", "We show that unsupervised learning is possible in this context as the performance of our system is comparable to supervised methods", "Our results indicate that a probabilistic gendernumber model determined automatically from unlabeled text is a powerful feature for this task"], "introduction": ["Coreference resolution is the process of determining which expressions in text refer to the same real world entity", "Pronoun resolution is the important yet challenging subset of coreference resolution where a system attempts to establish coreference between a pronominal anaphor such as a thirdperson pronoun like he she it or they and a preceding noun phrase called an antecedent", "In the following example a pronoun resolution system must determine the correct antecedent for the pronouns his and he 1 When the president entered the arena with his family he was serenaded by a mariachi band", "Pronoun resolution has applications across many areas of Natural Language Processing particularly in the field of information extraction", "Resolving a pronoun to a noun phrase can provide a new interpretation of a given sentence giving a Question Answering system for example more data to consider", "Our approach is a synthesis of linguistic and statistical methods", "For each pronoun a list of antecedent candidates derived from the parsed corpus is presented to the Expectation Maximization EM learner", "Special cases such as pleonastic reflexive and cataphoric pronouns are dealt with linguistically during list construction", "This allows us to train on and resolve all thirdperson pronouns in a large Question Answering corpus", "We learn lexicalized gendernumber language and antecedent probability models", "These models tied to individual words can not be learned with sufficient coverage from labeled data", "Pronouns are resolved by choosing the most likely antecedent in the candidate list according to these distributions", "The resulting resolution accuracy is comparable to supervised methods", "We gain further performance improvement by initializing EM with a gendernumber model derived from special cases in the training data", "This model is shown to perform reliably on its own", "We also demonstrate how the models learned through our unsupervised method can be used as features in a supervised pronoun resolution system"]}, "W06-0106": {"title": ["Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 4047"], "abstract": ["Coreference resolution is the process of identifying expressions that refer to the same entity", "This paper presents a clustering algorithm for unsupervised Chinese coreference resolution", "We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for phoric reference to Clinton", "  President Clinton is described as the antecedent of  he", " Clinton  President Clinton and the second  he are all mentions of the same entity that refers to former US president Bill Clinton", "12     English can be extended to Chinese", "The 3 1 proposed system exploits clustering as it has advantages over traditional classification methods such as the fact that no training data is required and it is easily extended to accommodate additional features", "We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance", "Our system is evaluated on an annotated version of the TDT3 corpus using the MUC7 scorer and obtains comparable performance", "We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resolution"], "introduction": ["Noun phrase coreference resolution is the process of detecting noun phrases NPs in a document and determining whether the NPs refer to the same entity where an entity is defined as a construct that represents an abstract identity", "The NPs that refer to the entity are known as mentions", "Mentions can be antecedents or anaphors", "An anaphor is an expression that refers back to a previous expression in a discourse", "In Figure 1  President Clinton refersto  Clinton and is described as an ana 31  12    Clinton1 said that Washington would progressively follow through on economic aid to Korea2", "Kim DaeJung3 applauded Clinton1s speech", "He1 said President Clinton1 reiterated in the talks that he1 would provide solid support for Korea2 to shake off the economic crisis", "Figure 1 An excerpt from the text with core ferring noun phrases annotated", "English translation in italics", "NP coreference resolution is an important sub task in natural language processing NLP applications such as text summarization information extraction data mining and question answering", "This task has attracted much attention in recent years Cardie and Wagstaff 1999 Harabagiu et al 2001 Soon et al 2001 Ng and Cardie 2002 Yang et al 2004 Florian et al 2004 Zhou et al 2005 and has been included as a subtask in the MUC Message Understanding Conferences and ACE Automatic Content Extraction competitions", "Coreference resolution is a difficult task for various reasons", "Firstly a list of features can play a role to support coreference resolution such as 40 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 4047 Sydney July 2006", "Qc 2006 Association for Computational Linguistics gender agreement number agreement head noun matches semantic class positional information contextual information appositive abbreviation etc Ng and Cardie 2002 found 53 features which are useful for this problem", "However no single feature is completely reliable since there are always exceptions eg the number agree ment test returns false when     this army singular is matched against  army members plural despite the two phrases being coreferential", "Secondly identifying features automatically and accurately is hard", "Features such as semantic class come from named entity recognition NER systems and ontologies and gazetteers but they are not always accurate especially where new terms are concerned", "Thirdly coreference resolution subsumes the pronoun resolution problem which is already difficult since pronouns carry limited lexical and semantic information", "In addition to the aforementioned Chinese coreference resolution is also made more difficult due to the lack of morphological and orthographic clues", "Chinese words contain less exterior information than words in many Indoeuropean languages", "For example in English number agreement can be detected through word inflections and partofspeech POS tags but there are no simple rules in Chinese to distinguish whether a word is singular or plural", "Proper name and abbreviations are identified by capitalization in English but Chinese does not use capitalization", "Moreover written Chinese does not have word boundaries so word segmentation is a crucial problem as we cannot get the true meaning of the sentence based on characters alone", "A simple sentence can be segmented in several different ways to get different meanings", "This characteristic affects the performance of all parts and leads to irrecoverable errors", "In addition there are very few Chinese coreference data sets available for research purposes none of them freely available and as a result no easily obtainable benchmark ing dataset for training and measuring performance", "Building a reasonably large coreference corpus is a laborconsuming task", "To our knowledge there have only been two Chinese coreference systems in previously published work Florian et al", "2004 which presents a statistical framework and reports experiment results on Chinese texts and Zhou et al", "2005 which proposed a unified transformation based learning framework for Chinese entity detection and tracking", "It consists of two models the detection model locates possibly coreferring NPs and the tracking model links the coreference relations", "This paper presents research performed on Chinese noun phrase coreference resolution", "Since there are no freely available Chinese coreference resources we used an unsupervised method that partially borrows from Cardie and Wagstaffs 1999 clusteringbased technique with features that are specially designed for Chinese", "In addition we perform and present the results of experiments designed to investigate the contribution of each feature"]}, "W06-0119": {"title": ["BMMbased Chinese Word Segmentor with Word Support Model for"], "abstract": ["This paper describes a Chinese word segmentor CWS for the third International Chinese Language Processing Bakeoff SIGHAN Bakeoff 2006", "We participate in the word segmentation task at the Microsoft Research MSR closed testing track", "Our CWS is based on backward maximum matching with word support model WSM and contextualbased Chinese unknown word identification", "From the scored results and our experimental results it shows WSM can improve our previous CWS which was reported at the SIGHAN Bakeoff 2005 about 1 of Fmeasure"], "introduction": ["A highperformance Chinese word segmentor CWS is a critical processing stage to produce an intermediate result for later processes such as search engines text mining word spell checking texttospeech and speech recognition etc As per Lin et al 1993 Tsai et al 2003 Tsai 2005 the bottleneck for developing a high performance CWS is to comprise of high performance Chinese unknown word identification UWI", "It is because Chinese is written without any separation between words and more than 50 words of the Chinese texts in web corpus are outofvocabulary Tsai et al 2003", "In our report for the SIGHAN Bakeoff 2005 Tsai 2005 we have shown that a highly performance of 991 Fmeasure can be achieved while a BMMbased CWS using a perfect system dictionary Tsai 2005", "A perfect system dictionary means all word types of the dictionary are extracted from training and testing gold standard corpus", "Conventionally there are four approaches to develop a CWS 1 Dictionarybased approach Cheng et al 1999 especial forward and backward maximum matching Wong and Chan 1996 2 Linguistic approach based on syntaxsemantic knowledge Chen et al 2002 3 Statistical approach based on statistical language model SLM Sproat and Shih 1990 Teahan et al 2000 Gao et al 2003 and 4 Hybrid approach trying to combine the benefits of dictionarybased linguistic and statistical approaches Tsai et al 2003 Ma and Chen 2003", "In practice statistical approaches are most widely used because their effective and reasonable performance", "To develop UWI there are three approaches 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al 2002 association strength mutual information ambiguous matching and multistatistical features for unknown word detection and extraction 2 Linguistic approach three major types of linguistic rules knowledge morphology syntax and semantics are used to identify unknown words and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", "Statistical approaches are simple and efficient whereas linguistic approaches are effective in identifying low frequency unknown words Chen et al 2002", "To develop WSD there are two major types of word segmentation ambiguities while there are no unknown word problems with them 1 Overlap Ambiguity OA", "Take string C1C2C3 130 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 130133 Sydney July 2006", "Qc 2006 Association for Computational Linguistics comprised of three Chinese characters C1 C2 and C3 as an example", "If its segmentation can be either C1C2C3 or C1C2C3 depending on context meaning the C1C2C3 is called an overlapambiguity string OAS such as a gen eraluse and to getfor military use the symbol  indicates a word bound ary", "2 Combination Ambiguity CA", "Take string C1C2 comprised of two Chinese characters C1 and C2 as an example", "If its segmentation can be either C1C2 or C1C2 depending on context meaning the C1C2 is called a combina tion ambiguity string CAS such as just can and ability Besides the OA and CA problems the other two types of word segmentation errors are caused by unknown word problems", "They are 1 Lack of unknown word LUW it means segmentation error occurred by lack of an unknown word in the system dictionary and 2 Error identified word EIW it means segmentation error occurred by an error identified unknown words", "The goal of this paper is to report the approach and experiment results of our backward maximum matchingbased BMMbased CWS with word support model WSM for the SIGHAN Bakeoff 2006", "In Tsai 2006 WSM has been shown effectively to improve Chinese input system", "In the third Bakeoff our CWS is mainly addressed on improving its performance of OACA disambiguation by WSM", "We show that WSM is able to improve our BMMbased CWS which reported at the SIGHAN Bakeoff 2005 about 1 of Fmeasure", "The remainder of this paper is arranged as follows", "In Section 2 we present the details of our BMMbased CWS comprised of WSM", "In Section 3 we present the scored results of the CWS at the Microsoft Research closed track and give our experiment results and analysis", "Finally in Section 4 we give our conclusions and future research directions"]}, "W06-0206": {"title": ["Proceedings of the Workshop on Information Extraction Beyond The Document pages 4855"], "abstract": ["We present two semisupervised learning techniques to improve a stateoftheart multilingual name tagger", "For English and Chinese the overall system obtains 17  21 improvement in Fmeasure representing a 135  174 relative reduction in the spurious missing and incorrect tags", "We also conclude that simply relying upon large corpora is not in itself sufficient we must pay attention to unlabeled data selection too", "We describe effective measures to automatically select documents and sentences"], "introduction": ["When applying machine learning approaches to natural language processing tasks it is time consuming and expensive to handlabel the large amounts of training data necessary for good performance", "Unlabeled data can be collected in much larger quantities", "Therefore a natural question is whether we can use unlabeled data to build a more accurate learner given the same amount of labeled data", "This problem is often referred to as semisupervised learning", "It significantly reduces the effort needed to develop a training set", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al 2004 semantic class extraction Lin et al 2003 chunking Ando and Zhang 2005 coreference resolution Bean and Riloff 2004 and text classification Blum and Mitchell 1998", "However it is not clear when semisupervised learning is applied to improve a learner how the system should effectively select unlabeled data and how the size and relevance of data impact the performance", "In this paper we apply two semisupervised learning algorithms to improve a stateoftheart name tagger", "We run the baseline name tagger on a large unlabeled corpus bootstrapping and the test set selftraining and automatically generate highconfidence machinelabeled sentences as additional training data", "We then iteratively retrain the model on the increased training data", "We first investigated whether we can improve the system by simply using a lot of unlabeled data", "By dramatically increasing the size of the corpus with unlabeled data we did get a significant improvement compared to the baseline system", "But we found that adding offtopic unlabeled data sometimes makes the performance worse", "Then we tried to select relevant documents from the unlabeled data in advance and got clear further improvements", "We also obtained significant improvement by selftraining boot strapping on the test data without any additional unlabeled data", "Therefore in contrast to the claim in Banko and Brill 2001 we concluded that for some applications effective use of large unlabeled corpora demands good data selection measures", "We propose and quantify some effective measures to select documents and sentences in this paper", "The rest of this paper is structured as follows", "Section 2 briefly describes the efforts made by previous researchers to use semisupervised learning as well as the work of Banko and Brill 2001", "Section 3 presents our baseline name tag ger", "Section 4 describes the motivation for our approach while Section 5 presents the details of two semisupervised learning methods", "Section 6 presents and discusses the experimental results on both English and Chinese", "Section 7 presents our conclusions and directions for future work"]}, "W06-1624": {"title": ["A Weakly Supervised Learning Approach"], "abstract": ["In this paper we present a weakly supervised learning approach for spoken language understanding in domainspecific dialogue systems", "We model the task of spoken language understanding as a successive classification problem", "The first classifier topic classifier is used to identify the topic of an input utterance", "With the restriction of the recognized target topic the second classifier semantic classifier is trained to extract the corresponding slotvalue pairs", "It is mainly datadriven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language", "Most importantly it allows the employment of weakly supervised strategies for training the two classifiers", "We first apply the training strategy of combining active learning and selftraining Tur et al 2005 for topic classifier", "Also we propose a practical method for bootstrapping the topicdependent semantic classifiers from a small amount of labeled sentences", "Experiments have been conducted in the context of Chinese public transportation information inquiry domain", "The experimental results demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce human labeling efforts significantly"], "introduction": ["Spoken Language Understanding SLU is one of the key components in spoken dialogue systems", "Its task is to identify the users goal and extract from the input utterance the information needed to complete the query", "Traditionally there are mainly two mainstreams in the SLU researches knowledgebased approaches which are based on robust parsing or template matching techniques Sneff 1992 Dowding et al 1993 Ward and Issar 1994 and datadriven approaches which are generally based on stochastic models Pieraccini and Levin 1993 Miller et al 1995", "Both approaches have their drawbacks however", "The former approach is costexpensive to develop since its grammar development is time consuming laboursome and requires linguistic skills", "It is also strictly domaindependent and hence difficult to be adapted to new domains", "On the other hand although addressing such drawbacks associated with knowledgebased approaches the latter approach often suffers the data sparseness problem and hence needs a fully annotated corpus in order to reliably estimate an accurate model", "More recently some new variation methods are proposed through certain trade offs such as the semiautomatically grammar learning approach Wang and Acero 2001 and Hidden Vector State HVS model He and Young 2005", "The two methods require only minimally annotated data only the semantic frames are annotated", "This paper proposes a novel weakly supervised spoken language understanding approach", "Our SLU framework mainly includes two successive classifiers topic classifier and semantic classifier", "The main advantage of the proposed approach is that it is mainly datadriven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language", "In particular the two classifiers are trained using weakly supervised strategies the former one is trained through the combination of active learning and selftraining Tur et al 2005 and the latter one 199 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 199207 Sydney July 2006", "Qc 2006 Association for Computational Linguistics is trained using a practical bootstrapping technique"]}, "W06-1634": {"title": ["Automatic Construction of Predicateargument Structure Patterns"], "abstract": ["This paper presents a method of automatically constructing information extraction patterns on predicateargument structures PASs obtained by full parsing from a smaller training corpus", "Because PASs represent generalized structures for syntactical variants patterns on PASs are expected to be more generalized than those on surface words", "In addition patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results", "In this paper we present experimental results and analyze them on how well proteinprotein interactions were extracted from MEDLINE abstracts", "The results demonstrated that our method improved accuracy compared to a machine learning approach using surface wordpartofspeech patterns"], "introduction": ["One primitive approach to Information Extraction IE is to manually craft numerous extraction patterns for particular applications and this is presently one of the main streams of biomedical IE Blaschke and Valencia 2002 Koike et al 2003", "Although such IE attempts have demonstrated nearpractical performance the same sets of patterns cannot be applied to different kinds of information", "A realworld task requires several kinds of IE thus manually engineering extraction Current Afliation  FUJITSU LABORATORIES LTD  Faculty of Informatics Kogakuin University patterns which is tedious and timeconsuming process is not really practical", "Techniques based on machine learning Zhou et al 2005 Hao et al 2005 Bunescu and Mooney 2006 are expected to alleviate this problem in manually crafted IE", "However in most cases the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data which requires tedious amount of manual labor to annotate text", "To systematically reduce the necessary amount of training data we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has specic properties according to the information to be extracted", "The former subtask is of full parsing ie recognizing syntactic structures of sentences and the latter subtask is of constructing specic extraction patterns ie nding clue words to extract information based on the obtained syntactic structures", "We adopted full parsing from various levels of parsing because we believe that it offers the best utility to generalize sentences into normalized syntactic relations", "We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine SVM to learn a prediction model using the matching results of extraction patterns", "As an actual IE task we extracted pairs of interacting protein names from biomedical text"]}, "W06-1667": {"title": ["Unsupervised Relation Disambiguation with Order Identification"], "abstract": ["We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts", "It works by calculating eigen vectors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors", "This method can address two difficulties encoutered in Hasegawa et al", "2004s hierarchical clustering no consideration of manifold structure in data and requirement to provide cluster number by users", "Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al", "2004s hierarchical clustering method and a plain kmeans clustering method"], "introduction": ["The task of relation extraction is to identify various semantic relations between name entities from text", "Prior work on automatic relation extraction come in three kinds supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithm Hasegawa et al 2004", "Among these methods supervised learning is usually more preferred when a large amount of la beled training data is available", "However it is timeconsuming and laborintensive to manually tag a large amount of training data", "Semisupervised learning methods have been put forward to minimize the corpus annotation requirement", "Most of semisupervised methods employ the bootstrapping framework which only need to predefine some initial seeds for any particular relation and then bootstrap from the seeds to acquire the relation", "However it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal number of them", "Compared with supervised and semisupervised methods Hasegawa et al", "2004s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels", "Hasegawa et al", "2004s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities", "However the drawback of hierarchical clustering is that it required providing cluster number by users", "Furthermore clustering is performed in original high dimensional space which may induce nonconvex clusters hard to identified", "This paper presents a novel application of spectral clustering technique to unsupervised relation extraction problem", "It works by calculating eigenvec tors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensional space and then performing cluster number estimation on a transformed space defined by the first few eigen vectors", "This method may help us find nonconvex clusters", "It also does not need to predefine the number of the context clusters or prespecify the similarity threshold for the clusters as Hasegawa et al 568 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 568575 Sydney July 2006", "Qc 2006 Association for Computational Linguistics 2004s method", "The rest of this paper is organized as follows", "Section 2 formulates unsupervised relation extraction and presents how to apply the spectral clustering technique to resolve the task", "Then section 3 reports experiments and results", "Finally we will give a conclusion about our work in section 4"]}, "W06-1670": {"title": ["BroadCoverage Sense Disambiguation and Information Extraction with"], "abstract": ["In this paper we approach word sense disambiguation and information extraction as a unified tagging problem", "The task consists of annotating text with the tagset defined by the 41 Wordnet super sense classes for nouns and verbs", "Since the tagset is directly related to Wordnet synsets the tagger returns partial word sense disambiguation", "Furthermore since the noun tags include the standard named entity detection classes  person location organization time etc  the tagger as a byproduct returns extended named entity information", "We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminativelytrained Hidden Markov Model", "Experimental evaluation on the main senseannotated datasets available ie Semcor and Senseval shows considerable improvements over the best known firstsense baseline"], "introduction": ["Named entity recognition NER is the most studied information extraction IE task", "NER typically focuses on detecting instances of person location organization names and optionally instances of miscellaneous or time categories", "The scalability of statistical NER allowed researchers to apply it successfully on large collections of newswire text in several languages and biomedical literature", "Newswire NER performance in terms of Fscore is in the upper 80s Carreras et al 2002 Florian et al 2003 while BioNER accuracy ranges between the low 70s and 80s depending on the dataset used for trainingevaluation Dingare et al 2005", "One shortcoming of NER is its oversimplified onto logical model leaving instances of other potentially informative categories unidentified", "Hence the utility of named entity information is limited", "In addition instances to be detected are mainly restricted to sequences of proper nouns", "Word sense disambiguation WSD is the task of deciding the intended sense for ambiguous words in context", "With respect to NER WSD lies at the other end of the semantic tagging spectrum since the dictionary defines tens of thousand of very specific word senses including NER categories", "Wordnet Fellbaum 19981 possibly the most used resource for WSD defines word senses for verbs common and proper nouns", "Word sense disambiguation at this level of granularity is a complex task which resisted all attempts of robust broadcoverage solutions", "Many distinctions are too subtle to be captured automatically and the magnitude of the class space  several orders larger than NERs  makes it hard to approach the problem with sophisticated but scal able machine learning methods", "Lastly even if the methods would scale up there are not enough manually tagged data at the word sense level for training a model", "The performance of state of the art WSD systems on realistic evaluations is only comparable to the first sense baseline cf", "Section 53", "Notwithstanding much research the benefits of disambiguated lexical information for language processing are still mostly speculativeThis paper presents a novel approach to broad The first author is now at Yahoo", "Research", "The tag ger described in this paper is free software and can be downloaded from httpwwwloacnritciaramitahtml", "1 When referring to Wordnet throughout the paper we", "mean Wordnet version 20", "594 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 594602 Sydney July 2006", "Qc 2006 Association for Computational Linguistics NOUNS SUPERSENSE NOUNS DENOTING SUPERSENSE NOUNS DENOTING act animal artifact attribute body cognition communication event feeling food group location motive acts or actions animals manmade objects attributes of people and objects body parts cognitive processes and contents communicative processes and contents natural events feelings and emotions foods and drinks groupings of people or objects spatial position goals object quantity phenomenon plant possession process person relation shape state substance time Tops natural objects not manmade quantities and units of measure natural phenomena plants possession and transfer of possession natural processes people relations between people or things or ideas two and three dimensional shapes stable states of affairs substances time and temporal relations abstract terms for unique beginners VERBS SUPERSENSE VERBS OF SUPERSENSE VERBS OF body change cognition communication competition consumption contact creation grooming dressing and bodily care size temperature change intensifying thinking judging analyzing doubting telling asking ordering singing fighting athletic activities eating and drinking touching hitting tying digging sewing baking painting performing emotion motion perception possession social stative weather feeling walking flying swimming seeing hearing feeling buying selling owning political and social activities and events being having spatial relations raining snowing thawing thundering Table 1", "Nouns and verbs supersense labels and short description from the Wordnet documentation", "coverage information extraction and word sense disambiguation", "Our goal is to simplify the disambiguation task for both nouns and verbs to a level at which it can be approached as any other tagging problem and can be solved with state of the art methods", "As a byproduct this task includes and extends NER", "We define a tagset based on Word nets lexicographers classes or supersenses Ciaramita and Johnson 2003 cf", "Table 1", "The size of the supersense tagset allows us to adopt a structured learning approach which takes local dependencies between labels into account", "To this extent we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model HMM based on that of Collins 2002 on the manually annotated Semcor corpus Miller et al 1993", "In two experiments we evaluate the accuracy of the tagger on the Semcor corpus itself and on the English all words Senseval 3 shared task data Snyder and Palmer 2004", "The model outperforms remarkably the best known baseline the first sense heuristic  to the best of our knowledge for the first time on the most realistic all words evaluation setting", "The paper is organized as follows", "Section 2 introduces the tagset Section 3 discusses related work and Section 4 the learning model", "Section 5 reports on experimental settings and results", "In Section 6 we summarize our contribution and consider directions for further research"]}, "W06-2709": {"title": [""], "abstract": ["We present ANNIS a linguistic database that aims at facilitating the process of exploiting richly annotated language data by naive users", "We describe the role of the database in our research project and the project requirements with a special focus on aspects of multilevel annotation", "We then illustrate the usability of the database by illustrative examples", "We also address current challenges and next steps"], "introduction": ["Until recently working with data that is annotated at multiple levels with different types of annotation required rather advanced computer skills which cannot be expected from the majority of potentially interested users", "We present ANNIS a linguistic database that aims at providing the infrastructure for supporting linguists in their work on multilevel annotations", "We describe and illustrate the current state of our work and sketch the next steps", "In sec", "2 we present the research scenario AN NIS is developed for show the role of the linguistic database therein and sketch the major requirements it aims to fulfill", "We then describe the architecture and current functionality and discuss the way difficult aspects of multidimensional annotations are treated sec", "3", "In sec", "4 we illustrate the work with the database by three exemplary approaches", "Finally we sketch our next steps"]}, "W06-2910": {"title": ["69"], "abstract": ["This paper investigates whether human associations to verbs as collected in a web experiment can help us to identify salient verb features for semantic verb classes", "Assuming that the associations model aspects of verb meaning we apply a clustering to the verbs as based on the associations and validate the resulting verb classes against standard approaches to semantic verb classes ie GermaNet and FrameNet", "Then various clusterings of the same verbs are performed on the basis of standard corpusbased types and evaluated against the associationbased clustering as well as GermaNet and FrameNet classes", "We hypothesise that the corpus based clusterings are better if the instantiations of the feature types show more overlap with the verb associations and that the associations therefore help to identify salient feature types"], "introduction": ["There are a variety of manual semantic verb classifications major frameworks are the Levin classes Levin 1993 WordNet Fellbaum 1998 and FrameNet Fontenelle 2003", "The different frameworks depend on different instantiations of semantic similarity eg Levin relies on verb similarity referring to syntaxsemantic alternation behaviour WordNet uses synonymy and FrameNet relies on situationbased agreement as defined in Fillmores frame semantics Fillmore 1982", "As an alternative to the resourceintensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg", "Merlo and Stevenson 2001 Joanis and Stevenson 2003 Korhonen et al 2003 Stevenson and Joanis 2003 Schulte im Walde 2003 Fer rer 2004", "Depending on the types of verb classes to be induced the automatic approaches vary their choice of verbs and classificationclustering algorithm", "However another central parameter for the automatic induction of semantic verb classes is the selection of verb features", "Since the target classification determines the similarity and dissimilarity of the verbs the verb feature selection should model the similarity of interest", "For example Merlo and Stevenson 2001 classify 60 English verbs which alternate between an intransitive and a transitive usage and assign them to three verb classes according to the semantic role assignment in the frames their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment", "In largerscale classifications such as Korhonen et al 2003 Stevenson and Joanis 2003 Schulte im Walde 2003 which model verb classes with similarity at the syntaxsemantics interface it is not clear which features are the most salient", "The verb features need to relate to a behavioural component modelling the syntaxsemantics interplay but the set of features which potentially influence the behaviour is large ranging from structural syntactic descriptions and argument role fillers to adverbial adjuncts", "In addition it is not clear how finegrained the features should be for example how much information is covered by lowlevel window cooccurrence vs higherorder syntactic frame fillers", "69 Proceedings of the 10th Conference on Computational Natural Language Learning CoNLLX pages 6976 New York City June 2006", "Qc 2006 Association for Computational Linguistics In this paper we investigate whether human associations to verbs can help us to identify salient verb features for semantic verb classes", "We collected associations to German verbs in a web experiment and hope that these associations represent a useful basis for a theoryindependent semantic classification of the German verbs assuming that the associations model a nonrestricted set of salient verb meaning aspects", "In a preparatory step we perform an unsupervised clustering on the experiment verbs as based on the verb associations", "We validate the resulting verb classes henceforth assocclasses by demonstrating that they show considerable overlap with standard approaches to semantic verb classes ie GermaNet and FrameNet", "In the main body of this work we compare the associations underlying the assocclasses with standard corpusbased feature types We check on how many of the associations we find among the corpusbased features such as adverbs direct object nouns etc we hypothesise that the more associations are found as instantiations in a feature set the better is a clustering as based on that feature type", "We assess our hypothesis by applying various corpusbased feature types to the experiment verbs and comparing the resulting classes henceforth corpusclasses against the assocclasses", "On the basis of the comparison we intend to answer the question whether the human associations help identify salient features to induce semantic verb classes ie do the corpusbased feature types which are identified on the basis of the associations outperform previous clustering results", "By applying the feature choices to GermaNet and FrameNet we address the question whether the same types of features are salient for different types of semantic verb classes", "In what follows the paper presents the association data in Section 2 and the associationbased classes in Section 3", "In Section 4 we compare the associations with corpusbased feature types and in Section 5 we apply the insights to induce semantic verb classes"]}, "W06-3604": {"title": ["Allword prediction as the ultimate confusable disambiguation"], "abstract": ["We present a classificationbased word prediction model based on IGTR EE a decisiontree induction algorithm with favorable scaling abilities and a functional equivalence to ngram models with back off smoothing", "Through a first series of experiments in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text we demonstrate that the system exhibits loglinear increases in prediction accuracy with increasing numbers of training examples", "Trained on 30 million words of newswire text prediction accuracies range between 126 on fictional text and 422 on newswire text", "In a second series of experiments we compare allwords prediction with confusable prediction ie the same task but specialized to predicting among limited sets of words", "Con fusable prediction yields high accuracies on nine example confusable sets in all genres of text", "The confusable approach outperforms the allwordsprediction approach but with more data the difference decreases"], "introduction": ["Word prediction is an intriguing language engineering semiproduct", "Arguably it is the archetypical prediction problem in natural language processing EvenZohar and Roth 2000", "It is usually not an engineering end in itself to predict the next word in a sequence or fill in a blankedout word in a sequence", "Yet it could be an asset in higherlevel proofing or authoring tools eg to be able to automatically discern among confusables and thereby to detect con fusable errors Golding and Roth 1999 EvenZohar and Roth 2000 Banko and Brill 2001 Huang and Powers 2001", "It could alleviate problems with low frequency and unknown words in natural language processing and information retrieval by replacing them with likely and higherfrequency alternatives that carry similar information", "And also since the task of word prediction is a direct interpretation of language modeling a word prediction system could provide useful information for to be used in speech recognition systems", "A unique aspect of the word prediction task as compared to most other tasks in natural language processing is that realworld examples abound in large amounts", "Any digitized text can be used as training material for a word prediction system capable of learning from examples and nowadays gigascale and terascale document collections are available for research purposes", "A specific type of word prediction is confus able prediction ie learn to predict among limited sets of confusable words such as totwotoo and theretheirtheyre Golding and Roth 1999 Banko and Brill 2001", "Having trained a confusable predictor on occurrences of words within a confusable set it can be applied to any new occurrence of a word from the set if its prediction based on the context deviates from the word actually present then 25 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing pages 2532 New York City New York June 2006", "Qc 2006 Association for Computational Linguistics this word might be a confusable error and the classifiers prediction might be its correction", "Confusable prediction and correction is a strong asset in proofing tools", "In this paper we generalize the word prediction task to predicting any word in context", "This is basically the task of a generic language model", "An explicit choice for the particular study on allwords prediction is to encode context only by words and not by any higherlevel linguistic nonterminals which have been investigated in related work on word prediction Wu et al 1999 EvenZohar and Roth 2000", "This choice leaves open the question how the same tasks can be learned from examples when nonterminal symbols are taken into account as well", "The choice for our algorithm a decisiontree approximation of knearestneigbor kNN based or memorybased learning is motivated by the fact that as we describe later in this paper this particular algorithm can scale up to predicting tens of thousands of words while simultaneously being able to scale up to tens of millions of examples as training material predicting words at useful rates of hundreds to thousands of words per second", "Another motivation for our choice is that our decisiontree approximation of knearest neighbor classification is functionally equivalent to backoff smoothing Zavrel and Daelemans 1997 not only does it share its performance capacities with ngram models with backoff smoothing it also shares its scaling abilities with these models while being able to handle large values of n The article is structured as follows", "In Section 2 we describe what data we selected for our experiments and we provide an overview of the experimental methodology used throughout the experiments including a description of the IGTR EE algorithm central to our study", "In Section 3 the results of the word prediction experiments are presented and the subsequent Section 4 contains the experimental results of the experiments on confusables", "We briefly relate our work to earlier work that inspired the current study in Section 5", "The results are discussed and conclusions are drawn in Section 6"]}, "W07-0722": {"title": ["Domain Adaptation in Statistical Machine Translation with Mixture"], "abstract": ["Mixture modelling is a standard technique for density estimation but its use in statistical machine translation SMT has just started to be explored", "One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset", "This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears", "In this paper we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a stateoftheart phrasebased system", "Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling"], "introduction": ["Mixture modelling is a popular approach for density estimation in many scientific areas G J McLachlan and D Peel 2000", "One of the most interesting properties of mixture modelling is its capability to model multimodal datasets by defining soft partitions on these datasets and learning specific probability distributions for each partition that better explains the general data generation process", "Work supported by the EC FEDER and the Spanish MEC under grant TIN200615694CO201 the Consellera dEmpresa Universitat i CienciaGeneralitat Valenciana under contract GV06252 the Universidad Politecnica de Valencia with ILETA project and Ministerio de Educacio n y Ciencia", "In Machine Translation MT it is common to encounter large parallel corpora devoted to heterogeneous topics", "These topics usually define sets of topicspecific lexicons that need to be translated taking into the semantic context in which they are found", "This semantic dependency problem could be overcome by learning topicdependent translation models that capture together the semantic context and the translation process", "However there have not been until very recently that the application of mixture modelling in SMT has received increasing attention", "In Zhao and Xing 2006 three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism", "These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence", "The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task", "In Civera and Juan 2006 a mixture extension of IBM model 2 along with a specific dynamic programming decoding algorithm were proposed", "This IBM2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semisynthetic task", "In this work we present a mixture extension of the wellknown HMM alignment model first proposed in Vogel and others 1996 and refined in Och and Ney 2003", "This model possesses appealing properties among which are worth mentioning the simplicity of the firstorder word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation pages 177180 Prague June 2007", "Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages and the efficient and exact computation of the Estep and Viterbi alignment by using a dynamicprogramming approach", "These properties have made this model suitable for extensions Toutanova et al 2002 and integration in a phrasebased model Deng and Byrne 2005 in the past", "3 Mixture of HMM alignment models", "Let us suppose that px  y has been generated using a Tcomponent mixture of HMM alignment models T px  y   pt  y px  y t t1 T   pt  y  px a  y t 6"]}, "W07-0802-parscit130908": {"title": ["Implementation of the Arabic Numerals and their Syntax in GF"], "abstract": ["The numeral system of Arabic is rich in its morphosyntactic variety yet suffers from the lack of a good computational resource that describes it in a reusable way", "This implies that applications that require the use of rules of the Arabic numeral system have to either reimplement them each time which implieswasted resources or use simplified imprecise rules that result in low quality applications", "A solution has been devised withGrammatical Framework GF to use language constructs and grammars as librariesthat can be written once and reused in various applications", "In this paper we describe our implementation of the Arabic numeralsystem as an example of a bigger implementation of a grammar library for Arabic", "We show that users can reuse our system by accessing a simple languageindependent API rule"], "introduction": ["11 Problem", "Language technology and software localization con sume a significant share of many companies timeand work", "Translating an operating system or an application to different languages involves in the tra ditional approach translating outofcontext strings into different languages", "This requires a languageexpert for each new language and will still involve languagerelated problems because of the difficulty in translating outofcontext strings and tak ing care of morphological and syntactic variations at the same time", "We illustrate this with an example", "A mail reader application wants to display messages like You have 1 new message You have 2 new messages You have 3 new messages You have 100 new messagesIf these are to be translated into Arabic special mor phological and syntactic considerations should bemade which include inflecting message in num ber  P risalatun   2 messages  P risalatani   310 messages PrasaVa 1199 messages    P risalatan x100 messages  P risalatin So the word messages is translated into dif ferent words in Arabic depending on the numeral counting it", "Counted nouns are an extreme example of how varied case inflection can be The case of thesingular and the dual is determined by their syntac tic function nominative in the example above", "Thisis not the case for plurals which assume the geni tive case from three to ten  P is diptote thus the   to nightynine and genitive again for plurals that aremultiples of hundred", "This is not to mention noun adjective agreement which should be taken care of when translating new messages into ArabicThe aforementioned details should not be the responsibility of the application programmer and hav 1 message     marker then accusative singular from eleven 9 Proceedings of the 5th Workshop on Important Unresolved Matters pages 916 Prague Czech Republic June 2007", "c2007 Association for Computational Linguistics ing translators do this work over and over again for each application can be costly and lead to repeated work andor poor results", "12 Solution and Contributions", "We reviewed in other works Dada and Ranta 2007 an approach that addresses problems in language technology similar but not limited to the above", "We applied this approach to Arabic thus developing aresource grammar for Arabic in which we imple ment rules that cover the orthography morphologyand syntax", "In short this approach is based on de veloping libraries of natural language constructs andrules which can be used by an application programmer who is not knowledgeable in a specific language", "The core programming language is Grammatical Framework GF Ranta 2004", "The lan guage library called a resource grammar Khegai and Ranta 2004 and comprising the linguistic rules can be reused in applications through an Application Programming Interface API by programmers thatare unaware of the details of the specific natural language", "Such a programmer uses a resource gram mar assuming it will take care of morphological andsyntactic rules", "So far we have implemented significant parts of the Arabic morphology syntax ortho graphic rules and provided a sample lexicon of 300 words based on the Swadesh list Hymes 1960", "In this paper we only describe part of the work namely the numeral system of Arabic and its syntax", "In the next section we elaborate on the approach the programming language that implements it and on Resource Grammars"]}, "W08-0329": {"title": ["Incremental Hypothesis Alignment for Building Confusion Networks with"], "abstract": ["Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation MT systems in the recent DARPA GALE and NIST Open MT evaluations", "Due to the varying word order between outputs from different MT systems the hypothesis alignment presents the biggest challenge in confusion network decoding", "This paper describes an incremental alignment method to build confusion networks based on the translation edit rate TER algorithm", "This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBNs submission to the WMT08 shared translation task"], "introduction": ["Confusion network decoding has been applied in combining outputs from multiple machine translation systems", "The earliest approach in Bangalore et al 2001 used edit distance based multiple string alignment MSA Durbin et al 1988 to build the confusion networks", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al 2006 or edit distance alignments allowing shifts Rosti et al 2007", "The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment", "The confusion networks are built around a skeleton hypothesis", "The skeleton hypothesis defines the word order of the decoding output", "Usually the 1best hypotheses from each system are considered as possible skeletons", "Using the pairwise hypothesis alignment the confusion networks are built in two steps", "First all hypotheses are aligned against the skeleton independently", "Second the confusion networks are created from the union of these alignments", "The incremental hypothesis alignment algorithm combines these two steps", "All words from the previously aligned hypotheses are available even if not present in the skeleton hypothesis when aligning the following hypotheses", "As in Rosti et al 2007 confusion networks built around all skeletons are joined into a lattice which is expanded and re scored with language models", "System weights and language model weights are tuned to optimize the quality of the decoding output on a development set", "This paper is organized as follows", "The incremental TER alignment algorithm is described in Section 2", "Experimental evaluation comparing the incremental and pairwise alignment methods are presented in Section 3 along with results on the WMT08 Europarl test sets", "Conclusions and future work are presented in Section 4"]}, "W09-0441": {"title": ["Fluency Adequacy or HTER"], "abstract": ["Automatic Machine Translation MT evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance", "Different types of human judgments such as Fluency Adequacy and HTER measure varying aspects of MT performance that can be captured by automatic MT metrics", "We explore these differences through the use of a new tunable MT metric TERPlus which extends the Translation Edit Rate evaluation metric with tun able parameters and the incorporation of morphology synonymy and paraphrases", "TERPlus was shown to be one of the top metrics in NISTs Metrics MATR 2008 Challenge having the highest average rank in terms of Pearson and Spear man correlation", "Optimizing TERPlus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits demonstrating significant differences between the types of human judgments"], "introduction": ["Since the introduction of the BLEU metric Pa pineni et al 2002 statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics", "These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality", "Numerous methods of judging MT output by humans have been used including Fluency Adequacy and more recently Humanmediated Translation Edit Rate HTER Snover et al 2006", "Fluency measures whether a translation is fluent regardless of the correct meaning while Adequacy measures whether the translation conveys the correct meaning even if the translation is not fully fluent", "Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale with their average being used as a single score of translation quality", "HTER is a more complex and semiautomatic measure in which humans do not score translations directly but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference", "This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate TER Snover et al 2006 or when used with other automatic metrics such as BLEU or METEOR Banerjee and Lavie 2005", "One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits as measured by TER between the MT output and the targeted reference creating the reference that is as close as possible to the MT output while still being adequate and fluent", "In this way only true errors in the MT output are counted", "While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy it is much more time consuming and taxing on human annotators than other types of human judgments making it difficult and expensive to use", "In addition because HTER treats all edits equally no distinction is made between serious errors errors in names or missing subjects and minor edits such as a difference in verb agreement Proceedings of the Fourth Workshop on Statistical Machine Translation  pages 259268 Athens Greece 30 March  31 March 2009", "Qc 2009 Association for Computational Linguistics or a missing determinator", "Different types of translation errors vary in importance depending on the type of human judgment being used to evaluate the translation", "For example errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent", "On the other hand deletion of content words might not lower the fluency of a translation but the adequacy would suffer", "In this paper we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric", "To study this we introduce a new evaluation metric TERPlus TERp1 that improves over the existing Translation Edit Rate TER metric Snover et al 2006 incorporating morphology synonymy and paraphrases as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments", "Section 2 summarizes the TER metric and discusses how TERp improves on it", "Correlation results with human judgments including independent results from the 2008 NIST Metrics MATR evaluation where TERp was consistently one of the top metrics are presented in Section 3 to show the utility of TERp as an evaluation metric", "The generation of paraphrases as well as the effect of varying the source of paraphrases is discussed in Section 4", "Section 5 discusses the results of tuning TERp to Fluency Adequacy and HTER and how this affects the weights of various edit types"]}, "W09-0621": {"title": ["Clustering and Matching Headlines for Automatic Paraphrase Acquisition"], "abstract": ["For developing a datadriven text rewriting algorithm for paraphrasing it is essential to have a monolingual corpus of aligned paraphrased sentences", "News article headlines are a rich source of paraphrases they tend to describe the same event in various different ways and can easily be obtained from the web", "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases one based on clustering and the other on pairwise similaritybased matching", "We show that the latter performs best on the task of aligning paraphrastic headlines"], "introduction": ["In recent years texttotext generation has received increasing attention in the field of Natural Language Generation NLG", "In contrast to traditional concepttotext systems texttotext generation systems convert source text to target text where typically the source and target text share the same meaning to some extent", "Applications of texttotext generation include sum marization Knight and Marcu 2002 question answering Lin and Pantel 2001 and machine translation", "For texttotext generation it is important to know which words and phrases are semantically close or exchangable in which contexts", "While there are various resources available that capture such knowledge at the word level eg synset knowledge in WordNet this kind of information is much harder to get by at the phrase level", "Therefore paraphrase acquisition can be considered an important technology for producing resources for texttotext generation", "Paraphrase generation has already proven to be valuable for Question Answering Lin and Pantel 2001 Riezler et al 2007 Machine Translation CallisonBurch et al 2006 and the evaluation thereof RussoLassner et al 2006 Kauchak and Barzilay 2006 Zhou et al 2006 but also for text simplification and explanation", "In the study described in this paper we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation", "News article headlines are abundant on the web and are already grouped by news aggregators such as Google News", "These services collect multiple articles covering the same event", "Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases Dolan et al 2004 Nelken and Shieber 2006", "We use this method to collect a large amount of aligned paraphrases in an automatic fashion"]}, "W09-0802": {"title": ["The Karamel System and Semitic Languages Structured MultiTiered"], "abstract": ["Karamel is a system for finitestate morphology which is multitape and uses a typed Cartesian product to relate tapes in a structured way", "It implements statically compiled feature structures", "Its language allows the use of regular expressions and Generalized Restriction rules to define multitape transducers", "Both simultaneous and successive application of local constraints are possible", "This system is interesting for describing rich and structured morphologies such as the morphology of Semitic languages"], "introduction": ["Karamel is a system for defining and executing multitape finitestate transducers where relationships between tapes are expressed using a tree structure", "This structure is obtained through embedded units which are used to analyze a tuple of strings recognized by the transducer", "For instance the units considered in an example may be affix form and sentence", "The system includes a language and an Integrated Development Environment", "The language uses extended regular expressions computations and contextual rules", "The environment provides a graphical interface to write and execute finitestate descriptions", "Karamel has many applications", "For Natural Language Processing it may be used for morphological analysis transliteration parsing etc This paper is dedicated to the application of Karamel to the morphological analysis of Semitic languages for which both multiple tapes and complex structures are useful", "Some descriptions of the morphology of Semitic Languages use several tiers", "For instance McCarthy 1981 uses four tiers one for prefixes one for the root one for the template consonant vowel pattern and the last one for the vocalizationSuch a multitiered description may be im plemented using a cascade of 2tape machines Beesley 1998 or using a multitape transducer where each tier is described by a tape and the surface form by an additional tape", "This is the approach of G A Kiraz for the Syriac language Kiraz 2000", "Karamel is designed for the later solution", "The multitape feature is also interesting for describing related dialects whenever a great part of the analysis may be shared", "A separate tape is dedicated to the surface form in each dialect", "The Semitic Morphology is strongly structured by the roots", "The basis of an analysis is the identification of the root", "Furthermore several layers of affixes may be distinguished around the core containing the roots paradigmatic prefixes affixes encoding the person gender and number clitics such as pronouns", "This structure is conveniently defined using Karamels units", "In the following section of the paper we present Karamels language its theoretical background and its syntax", "Section 3 describe the other aspects of the Karamel System its development environment its current state and future evolutions", "Then comes an example of Semitic morphology written in Karamel the description of Akkadian verbal flexion", "The last section compares Karamel to some other systems"]}, "W09-1007": {"title": ["Language models for contextual error detection and correction"], "abstract": ["The problem of identifying and correcting confusibles ie contextsensitive spelling errors in text is typically tackled using specifically trained machine learning classifiers", "For each different set of con fusibles a specific classifier is trained and tuned", "In this research we investigate a more generic approach to contextsensitive con fusible correction", "Instead of using specific classifiers we use one generic classifier based on a language model", "This measures the likelihood of sentences with different possible solutions of a confusible in place", "The advantage of this approach is that all confusible sets are handled by a single model", "Preliminary results show that the performance of the generic classifier approach is only slightly worse that that of the specific classifier approach"], "introduction": ["When writing texts people often use spelling checkers to reduce the number of spelling mistakes in their texts", "Many spelling checkers concentrate on nonword errors", "These errors can be easily identified in texts because they consist of character sequences that are not part of the language", "For example in English woord is is not part of the language hence a nonword error", "A possible correction would be word  Even when a text does not contain any non word errors there is no guarantee that the text is errorfree", "There are several types of spelling errors where the words themselves are part of the language but are used incorrectly in their context", "Note that these kinds of errors are much harder to recognize as information from the context in which they occur is required to recognize and correct these errors", "In contrast nonword errors can be recognized without context", "One class of such errors called confusibles consists of words that belong to the language but are used incorrectly with respect to their local sentential context", "For example She owns to cars contains the confusible to  Note that this word is a valid token and part of the language but used incorrectly in the context", "Considering the context a correct and very likely alternative would be the word two  Confusibles are grouped together in confusible sets", "Confusible sets are sets of words that are similar and often used incorrectly in context", "Too is the third alternative in this particular confusible set", "The research presented here is part of a larger project which focusses on contextsensitive spelling mistakes in general", "Within this project all classes of contextsensitive spelling errors are tackled", "For example in addition to confusibles a class of pragmatically incorrect words where words are incorrectly used within the document wide context is considered as well", "In this article we concentrate on the problem of confusibles where the context is only as large as a sentence"]}, "W10-1730-parscit": {"title": ["Maximum Entropy Translation Model"], "abstract": ["Maximum Entropy Principle has been used successfully in various NLP tasks", "Inthis paper we propose a forward translation model consisting of a set of maximum entropy classifiers a separate classifier is trained for each sufficiently frequent sourceside lemma", "In this way the estimates of translation probabilitiescan be sensitive to a large number of features derived from the source sentence including nonlocal features features making use of sentence syntactic structureetc", "When integrated into EnglishtoCzech dependencybased translation scenario implemented in the TectoMT framework the new translation model significantly outperforms the baseline modelMLE in terms of BLEU", "The performance is further boosted in a configurationinspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the targetlanguage dependency tree model"], "introduction": ["The principle of maximum entropy states thatgiven known constraints the probability distri bution which best represents the current state of knowledge is the one with the largest entropyMaximum entropy models based on this princi ple have been widely used in Natural Language Processing eg for tagging Ratnaparkhi 1996parsing Charniak 2000 and named entity recog nition Bender et al 2003", "Maximum entropy models have the following form where fi is a feature function Ai is its weight and Zx is the normalizing factor Zx   exp Aifix y y iIn statistical machine translation SMT trans lation model TM pts is the probability that the string t from the target language is the translation of the string s from the source language", "Typical approach in SMT is to use backward translationmodel pst according to Bayes rule and noisy channel model", "However in this paper we deal only with the forward direct model1The idea of using maximum entropy for con structing forward translation models is not new", "It naturally allows to make use of various featurespotentially important for correct choice of targetlanguage expressions", "Let us adopt a motivat ing example of such a feature from Berger et al 1996 which contains the first usage of maxenttranslation model we are aware of If house ap pears within the next three words eg the phrases in the house and in the red house then dans might be a more likely French translation of in Incorporating nonlocal features extracted fromthe source sentence into the standard noisychannel model in which only the backward trans lation model is available is not possible", "Thisdrawback of the noisychannel approach is typi cally compensated by using large targetlanguage ngram models which can  in a result  play arole similar to that of a more elaborate more context sensitive forward translation model", "How ever we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side", "1A backward translation model is used only for pruning training data in this paper", "1  pyx  Zxexp Aifix y i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR pages 201206 Uppsala Sweden 1516 July 2010", "c2010 Association for Computational Linguistics A deeper discussion on the potential advantagesof maximum entropy approach over the noisy channel approach can be found in Foster 2000and Och and Ney 2002 in which another suc cessful applications of maxent translation models are shown", "Loglinear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos 2007 and Gimpel andSmith 2009 the idea can be traced back to Pap ineni et al 1997", "What makes our approach different from the previously published works is that1", "we show how the maximum entropy trans lation model can be used in a dependencyframework we use deepsyntactic dependency trees as defined in the Prague Depen dency Treebank Hajic et al 2006 as the transfer layer2", "we combine the maximum entropy transla tion model with targetlanguage dependency tree model and use treemodified Viterbisearch for finding the optimal lemmas label ing of the targettree nodes", "The rest of the paper is structured as follows", "InSection 2 we give a brief overview of the translation framework TectoMT in which the experi ments are implemented", "In Section 3 we describehow our translation models are constructed", "Sec tion 4 summarizes the experimental results and Section 5 contains a summary"]}, "W10-1750": {"title": ["Documentlevel Automatic MT Evaluation based on Discourse Representations"], "abstract": ["This paper describes the joint submission of Universitat Politecnica de Catalunya and Universitat de Barcelona to the Metrics MaTr 2010 evaluation challenge in collaboration with ELDAELRA", "Our work is aimed at widening the scope of current automatic evaluation measures from sentence to document level", "Preliminary experiments based on an extension of the metrics by Gimenez and Marquez 2009 operating over discourse representations are presented"], "introduction": ["Current automatic similarity measures for Machine Translation MT evaluation operate all without exception at the segment level", "Translations are analyzed on a segmentbysegment1 fashion ignoring the text structure", "Document and system scores are obtained using aggregate statistics over individual segments", "This strategy presents the main disadvantage of ignoring cross sententialdiscursive phenomena", "In this work we suggest widening the scope of evaluation methods", "We have defined genuine documentlevel measures which are able to exploit the structure of text to provide more informed evaluation scores", "For that purpose we take advantage of two coincidental facts", "First test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event story or topic Przybocki et al 2008 Przybocki et al 2009 CallisonBurch et al 2009", "Second we count on automatic linguistic processors which provide very detailed discourse level representations of text Curran et al 2007", "Discourse representations allow us to focus on relevant pieces of information such as the agent 1 A segment typically consists of one or two sentences", "who location where time when and theme what which may be spread all over the text", "Counting on a means of discerning the events the individuals taking part in each of them and their role is crucial to determine the semantic equivalence between a reference document and a candidate translation", "Moreover the discourse analysis of a document is not a mere concatenation of the analyses of its individual sentences", "There are some phenomena which may go beyond the scope of a sentence and can only be explained within the context of the whole document", "For instance in a newspaper article facts and entities are progressively added to the discourse and then referred to anaphorically later on", "The following extract from the development set illustrates the importance of such a phenomenon in the discourse analysis Among the current or underlying crises in the Middle East Rod Larsen mentioned the ArabIsraeli conflict and the Iranian nuclear portfolio as well as the crisis between Lebanon and Syria", "He stated All this leads us back to crucial values and opinions which render the situation prone at any moment to getting out of control more so than it was in past days", "The subject pronoun he works as an anaphoric pronoun whose antecedent is the proper noun Rod Larson", "The anaphoric relation established between these two elements can only be identified by analyzing the text as a whole thus considering the gender agreement between the third person singular masculine subject pronoun he and the masculine proper noun Rod Larson", "However if the two sentences were analyzed separately the identification of this anaphoric relation would not be feasible due to the lack of connection between the two elements", "Discourse representations allow us to trace links across sentences between the different facts and entities appearing in them", "Therefore providing an approach to the text more similar to that of a human which implies taking into account the whole text structure instead of considering each sentence separately", "The rest of the paper is organized as follows", "Section 2 describes our evaluation methods and the linguistic theory upon which they are based", "Experimental results are reported and discussed in Section 3", "Section 4 presents the metric submitted to the evaluation challenge", "Future work is outlined in Section 5", "As an additional result documentlevel metrics generated in this study have been incorporated to the IQMT package for automatic MT evaluation2 "]}, "W10-1757": {"title": ["Nbest Reranking by Multitask Learning"], "abstract": ["We propose a new framework for Nbest reranking on sparse feature sets", "The idea is to reformulate the reranking problem as a Multitask Learning problem where each Nbest list corresponds to a distinct task", "This is motivated by the observation that Nbest lists often show significant differences in feature distributions", "Training a single reranker directly on this heteroge nous data can be difficult", "Our proposed metaalgorithm solves this challenge by using multitask learning such as 12 regularization to discover common feature representations across N best lists", "This metaalgorithm is simple to implement and its modular approach allows one to plugin different learning algorithms from existing literature", "As a proof of concept we show statistically significant improvements on a machine translation system involving millions of features"], "introduction": ["Many natural language processing applications such as machine translation MT parsing and language modeling benefit from the Nbest reranking framework Shen et al 2004 Collins and Koo 2005 Roark et al 2007", "The advantage of Nbest reranking is that it abstracts away the complexities of firstpass decoding allowing the researcher to try new features and learning algorithms with fast experimental turnover", "In the Nbest reranking scenario the training data consists of sets of hypotheses ie Nbest lists generated by a firstpass system along with their labels", "Given a new Nbest list the goal is to rerank it such that the best hypothesis appears near the top of the list", "Existing research have focused on training a single reranker directly on the entire data", "This approach is reasonable if the data is homogenous but it fails when features vary significantly across different Nbest lists", "In particular when one employs sparse feature sets one seldom finds features that are simultaneously active on multiple Nbest lists", "In this case we believe it is more advantageous to view the Nbest reranking problem as a multi task learning problem where each Nbest list corresponds to a distinct task", "Multitask learning a subfield of machine learning focuses on how to effectively train on a set of different but related datasets tasks", "Our heterogenous Nbest list data fits nicely with this assumption", "The contribution of this work is threefold 1", "We introduce the idea of viewing Nbest", "reranking as a multitask learning problem", "This view is particularly apt to any general reranking problem with sparse feature sets"]}, "W10-1761": {"title": ["Improved Translation with Source Syntax Labels"], "abstract": ["We present a new translation model that include undecorated hierarchicalstyle phrase rules decorated sourcesyntax rules and partially decorated rules", "Results show an increase in translation performance of up to 08 BLEU for GermanEnglish translation when trained on the newscommentary corpus using syntactic annotation from a source language parser", "We also experimented with annotation from shallow taggers and found this increased performance by 05 BLEU"], "introduction": ["Hierarchical decoding is usually described as a formally syntactic model without linguistic commitments in contrast with syntactic decoding which constrains rules and production with linguistically motivated labels", "However the decoding mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar", "Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field", "Both use synchronous context free grammar SCFG formalism consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence", "The most common algorithm for decoding with SCFG is currently CKY with cube pruning works for both hierarchical and syntactic systems as implemented in Hiero Chiang 2005 Joshua Li et al 2009 and Moses Hoang et al 2009 Rewrite rules in hierarchical systems have general applicability as their nonterminals are undec orated giving hierarchical system broad coverage", "However rules may be used in inappropriate situations without the labeled constraints", "The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses", "Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage", "This paper presents work on combining hierarchical and syntax translation utilizing the high coverage of hierarchical decoding and the insights that syntactic information can bring", "We seek to balance the generality of using undeco rated nonterminals with the specificity of labeled nonterminals", "Specifically we will use syntactic labels from a source language parser to label nonterminal in production rules", "However other source span information such as chunk tags can also be used", "We investigate two methods for combining the hierarchical and syntactic approach", "In the first method syntactic translation rules are used concurrently with a hierarchical phrase rules", "Each ruleset is trained independently and used concurrently to decode sentences", "However results for this method do not improve", "The second method uses one translation model containing both hierarchical and syntactic rules", "Moreover an individual rule can contain both decorated syntactic nonterminals and undeco rated hierarchicalstyle nonterminals also the lefthandside nonterminal may or may not be decorated", "This results in a 08 improvement over the hierarchical baseline and analysis suggest that longrange ordering has been improved", "We then applied the same methods but using linguistic annotation from a chunk tagger Abney 1991 instead of a parser and obtained an improvement of 05 BLEU over the hierarchical baseline showing that gains with additional source side annotation can be obtained with simpler tools"]}, "W10-4128": {"title": ["HMM Revises Low Marginal Probability by CRF for Chinese Word Segmentation"], "abstract": ["This paper presents a Chinese word segmentation system for CIPSSIGHAN 2010 Chinese language processing task", "Firstly based on Conditional Random Field CRF model with local features and global features the characterbased tagging model is designed", "Secondly Hidden Markov Models HMM is used to revise the substrings with low marginal probability by CRF", "Finally confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers", "As is well known that characterbased approach has outstanding capability of discovering outofvocabulary OOV word but external information of word lost", "HMM makes use of word information to increase invocabulary IV recall", "We participate in the simplified Chinese word segmentation both closed and open test on all four corpora which belong to different domains", "Our system achieves better performance"], "introduction": ["Chinese Word Segmentation CWS has witnessed a prominent progress in the first four SIGHAN Bakeoffs", "Since Xue 2003 used characterbased tagging this method has attracted more and more attention", "Some previous work Peng et al 2004 Tseng et al 2005 Low et al 2005 illustrated the effectiveness of using characters as tagging units while literatures Zhang et al 2006 Zhao and Kit 2007a Zhang and Clark 2007 focus on employing lexical words or subwords as tagging units", "Because the wordbased models can capture the wordlevel contextual information and IV knowledge", "Besides many strategies are proposed to balance the IV and OOV performance Wang et al 2008", "CRF has been widely used in sequence labeling tasks and has a good performance Laffertyet al 2001", "Zhao and Kit 2007b 2008 at tempt to integrate global information with local information to further improve CRFbased tagging method of CWS which provides a solid foundation for strengthening CRF learning with unsupervised learning outcomes", "In order to increase the accuracy of tagging using CRF we adopt the strategy which is if the marginal probability of characters is lower than a threshold the modified component based on HMM will be trigged combining the confidence measure the results will be regenerated"]}, "W10-4223": {"title": ["Paraphrase Generation as Monolingual  Translation  Data and Evaluation"], "abstract": ["In this paper we investigate the automatic generation and evaluation of sentential paraphrases", "We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard PhraseBased Machine Translation PBMT framework", "The output of this system is compared to a word substitution baseline", "Human judges prefer the PBMT paraphrasing system over the word substitution system", "We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence"], "introduction": ["Texttotext generation is an increasingly studied subfield in natural language processing", "In contrast with the typical natural language generation paradigm of converting concepts to text in text totext generation a source text is converted into a target text that approximates the meaning of the source text", "Texttotext generation extends to such varied tasks as summarization Knight and Marcu 2002 questionanswering Lin and Pan tel 2001 machine translation and paraphrase generation", "Sentential paraphrase generation SPG is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence but approximates its meaning", "Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input for example to generate paraphrases of questions that in their original form cannot be answered Lin and Pantel 2001 Riezler et al 2007 or to generate paraphrases of sentences that failed to translate CallisonBurch et al 2006", "Paraphrasing has also been used in the evaluation of machine translation system output RussoLassner et al 2006 Kauchak and Barzilay 2006 Zhou et al 2006", "Adding certain constraints to paraphrasing allows for additional useful applications", "When a constraint is specified that a paraphrase should be shorter than the input text paraphrasing can be used for sentence compression Knight and Marcu 2002 Barzilay and Lee 2003 as well as for text simplification for question answering or subtitle generation Daelemans et al 2004", "We regard SPG as a monolingual machine translation task where the source and target languages are the same Quirk et al 2004", "However there are two problems that have to be dealt with to make this approach work namely obtaining a sufficient amount of examples and a proper evaluation methodology", "As CallisonBurch et al", "2008 argue automatic evaluation of paraphrasing is problematic", "The essence of SPG is to generate a sentence that is structurally different from the source", "Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity while paraphrasing centers around achieving dissimilarity", "Besides the evaluation issue another problem is that for an data driven MT account of paraphrasing to work a large collection of data is required", "In this case this would have to be pairs of sentences that are paraphrases of each other", "So far paraphrasing data sets of sufficient size have been mostly lacking", "We argue that the headlines aggregated by Google News offer an attractive avenue"]}, "W11-0311": {"title": ["Improving the Impact of Subjectivity Word Sense Disambiguation  on"], "abstract": ["Subjectivity word sense disambiguation SWSD is automatically determining which word instances in a corpus are being used with subjective senses and which are being used with objective senses", "SWSD has been shown to improve the performance of contextual opinion analysis but only on a small scale and using manually developed integration rules", "In this paper we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance by successfully gathering data annotated by nonexpert annotators", "Further by improving the method for integrating SWSD into contextual opinion analysis even greater benefits from SWSD are achieved than in previous work", "We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis"], "introduction": ["Often methods for opinion sentiment and subjectivity analysis rely on lexicons of subjective opinioncarrying words eg Turney 2002 Whitelaw et al 2005 Riloff and Wiebe 2003 Yu and Hatzivassiloglou 2003 Kim and Hovy 2004 Bloom et al 2007 Andreevskaia and Bergler 2008 Agarwal et al 2009", "Examples of such words are the following in bold 1 He is a disease to every team he has gone to", "Converting to SMF is a headache", "The concert left me cold", "That guy is such a pain", "However even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity Su and Markert 2008 Gyamfi et al 2009", "That is many clues in these lexicons have both subjective and objective senses", "This ambiguity leads to errors in opinion and sentiment analysis because objective instances represent false hits of subjectivity clues", "For example the following sentence contains the keywords from 1 used with objective senses 2 Early symptoms of the disease include severe headaches red eyes fevers and cold chills body pain and vomiting", "Recently in Akkaya et al 2009 we introduced the task of subjectivity word sense disambiguation SWSD which is to automatically determine which word instances in a corpus are being used with subjective senses and which are being used with objective senses", "We developed a supervised system for SWSD and exploited the SWSD output to improve the performance of multiple contextual opinion analysis tasks", "Although the reported results are promising there are three obvious shortcomings", "First we were able to apply SWSD to contextual opinion analysis only on a very small scale due to a shortage of annotated data", "While the experiments show that SWSD improves contextual opinion analysis this was only on the small amount of opinionannotated data that was in the coverage of our system", "Two questions arise is it feasible to obtain greater amounts of the needed data and do SWSD performance improvements on contextual opinion analysis hold on a 87 Proceedings of the Fifteenth Conference on Computational Natural Language Learning pages 8796 Portland Oregon USA 2324 June 2011", "Qc 2011 Association for Computational Linguistics larger scale", "Second the annotations in Akkaya et al 2009 are piggybacked on SENSEVAL sense tagged data which are finegrained word sense annotations created by trained annotators", "A concern is that SWSD performance improvements on contextual opinion analysis can only be achieved using such finegrained expert annotations the availability of which is limited", "Third Akkaya et al 2009 uses manual rules to apply SWSD to contextual opinion analysis", "Although these rules have the advantage that they transparently show the effects of SWSD they are somewhat ad hoc", "Likely they are not optimal and are holding back the potential of SWSD to improve contextual opinion analysis", "To address these shortcomings in this paper we investigate 1 the feasibility of obtaining a substantial amount of annotated data 2 whether performance improvements on contextual opinion analysis can be realized on a larger scale and 3 whether those improvements can be realized with subjectivity sense tagged data that is not built on expert full inventory sense annotations", "In addition we explore better methods for applying SWSD to contextual opinion analysis"]}, "W11-0910": {"title": ["Incorporating Coercive Constructions into a Verb Lexicon"], "abstract": ["We take the first steps towards augmenting a lexical resource VerbNet with probabilistic information about coercive constructions", "We focus on CAUSED MOTION as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion which occurs productively and adds substantially to the relational semantics of a verb", "However through annotation we find that VerbNet fails to accurately capture all usages of the construction", "We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet", "We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages"], "introduction": ["Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles eg see Mrquez et al 2008", "Underlying these recent successes are lexical resources such as PropBank Palmer et al 2005 VerbNet Kipper et al 2008 and FrameNet Baker et al 1998 Fillmore et al 2002 which encode the relational semantics of numerous lexical items especially verbs", "However because authors and speakers use verbs productively in previously unseen ways semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources cf", "Pustejovsky  Jezek 2008", "To achieve more accurate semantic analyses we must augment such resources with knowledge of the extensibility of verbs", "Central to verb extensibility is the process of semantic and syntactic coercion", "Coercion allows a verb to be used in atypical contexts that extend its relational semantics thereby enabling expression of a novel concept or simply more fluid expression of a complex concept", "For example consider a strictly intransitive action verb such as blink", "This verb may instead be used in a construction with an object as in She blinked the snow off her lashes leading to an interpretation of the verb in which the object is causally affected and changes location the CAUSED MOTION construction Goldberg 1995", "This type of constructional coercion is common in language and underlies much extensibility of verb usages", "Understanding such coercive processes thus has significant impact on how we should represent knowledge about verbs in a lexical resource", "Importantly constructional coercion is not an allor nothing process  a word must be semantically and syntactically compatible in some respects with a context in order for its use to be extended to that context but the restrictions on compatibility are not hardandfast rules Langacker 1987 Kay  Fillmore 1999 Goldberg 2006 Goldberg to appear", "Gradience of compatibility plays an important role in coercion suggesting that a probabilistic approach may be necessary for encoding knowledge of constructional coercion in a verb lexicon cf", "Lapata  Lascarides 2003", "Our hypothesis here is that due to this gradient process of productivity existing verb lexicons do not adequately capture the actual patterns of use of extensible constructions", "In this paper we focus on the CAUSED MOTION CM construction as an initial test case", "We first annotate the classes of an extensive verb lexicon VerbNet as to whether the CM construction is allowed for all some or none of the verbs in the class noting additionally whether it is a typical or coerced usage", "We find that many of the classes that allow the construction for at least some verbs do not include the CM frame in their definition indicating a significant shortcoming in the relational knowledge encoded in the lexicon", "Next we develop probabilistic measures for determining to what degree a class is likely to admit the CM construction", "We then test our measures over corpus data manually annotated for use of the CM construction", "Finally we present preliminary work on automatic techniques for calculating the proposed measures in an unsupervised way to avoid the need for expensive manual annotation", "This work forms the preliminary steps toward empirically augmenting VerbNets predictive capabilities concerning the event semantics of verbs in coercible constructions"]}, "W11-1101": {"title": ["A Combination of Topic Models with Maxmargin Learning for Relation"], "abstract": ["This paper proposes a novel application of a supervised topic model to do entity relation detection ERD", "We adapt Maximum Entropy Discriminant Latent Dirichlet Allocation MEDLDA with mixed membership for relation detection", "The ERD task is reformulated to fit into the topic modeling framework", "Our approach combines the benefits of both maximumlikelihood estimation MLE and maxmargin estimation MME and the mixed membership formulation enables the system to incorporate heterogeneous features", "We incorporate different features into the system and perform experiments on the ACE 2005 corpus", "Our approach achieves better overall performance for precision recall and Fmeasure metrics as compared to SVMbased and LLDAbased models"], "introduction": ["Entity relation detection ERD aims at finding relations between pairs of Named Entities NEs in text", "Availability of annotated corpora NIST 2003 Doddington et al 2004 and introduction of shared tasks eg", "Farkas et al 2010 Carreras and Marquez 2005 has spurred a large amount of research in this field in recent times", "Researchers have used supervised and semisupervised approaches Hasegawa et al 2004 Mintz et al 2009 Jiang 2009 and explored rich features Kambhatla 2004 kernel design Culotta and Sorensen 2004 Zhou et al 2005 Bunescu and Mooney 2005 Qian et al 2008 and inference algorithms Chan and Roth 2011 to detect predefined relations between NEs", "In this work we explore if and how the latent semantics of the text can help in detecting entity relations", "For this we adapt the Latent Dirichlet Allocation LDA approach to solve the ERD task", "Specifically we present a ERD system based on Maximum Entropy Discriminant Latent Dirichlet Allocation MEDLDA", "MEDLDA Zhu et al 2009 is an extension of Latent Dirichlet Allocation LDA that combines capability of capturing latent semantics with the discriminative capabilities of SVM", "There are a number of challenges in employing the LDA framework for ERD", "Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA LLDA Ramage et al 2009 and supervised LDA sLDA Blei and McAuliffe 2008 are powerful generative models that capture the underlying semantics of texts", "However they have trouble discovering marginal classes and easily employing rich feature sets both of which are important for ERD", "We overcome the first drawback by employing a MEDLDA framework which integrates maximum likelihood estimation MLE and maximum margin estimation MME", "Specifically it is a combination of sLDA and support vector machines SVMs", "Further in order to employ rich and heterogeneous features we introduce a separate exponential family distribution for each feature similar to Shan et al 2009 into our MEDLDA model", "We formulate the relation detection task within the topic model framework as follows", "Pairs of NE mentions1 and the text between them is considered 1 Adopting the terminology used in the Automatic Context Extraction ACE program NIST 2003 specific NE instances are called mentions", "as minidocument", "Each minidocument has a relation type analogous to the response variable in the supervised topic model", "The topic model infers the topic relation type distribution of the mini documents", "The supervised topic model discovers a latent topic representation of the minidocuments and a response parameter distribution", "The topic representation is discovered with observed response variables during training", "During testing the topic distribution of each minidocument can form a prediction of the relation types", "We carry out experiments to measure the effectiveness of our approach and compare it to SVM based and LLDAbased models as well as to a previous work using the same corpora", "We also measure and analyze the effectiveness of incorporating different features in our model relative to other models", "Our approach exhibits better overall precision recall and Fmeasure than baseline systems", "We also find that the MEDLDAbased approach shows consistent capability for incorporation and improvement due to a variety of heterogeneous features", "The rest of the paper is organized as follows", "We describe the proposed model in Section 2 and the features that we explore in this work in Section 3", "Section 4 describes the data experiments results and analyses", "We discuss the related work in Section 5 before concluding in Section 6"]}, "W11-1604": {"title": ["Comparing  Phrasebased and Syntaxbased Paraphrase Generation"], "abstract": ["Paraphrase generation can be regarded as machine translation where source and target language are the same", "We use the Moses statistical machine translation toolkit for paraphrasing comparing phrasebased to syntaxbased approaches", "Data is derived from a recently released large scale 21M tokens paraphrase corpus for Dutch", "Preliminary results indicate that the phrasebased approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source"], "introduction": ["One of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms", "As the ability to deal with paraphrases holds great potential for improving the coverage of NLP systems a substantial body of research addressing recognition extraction and generation of paraphrases has emerged Androutsopoulos and Malakasiotis 2010 Madnani and Dorr 2010", "Paraphrase Generation can be regarded as a translation task in which source and target language are the same", "Both Paraphrase Generation and Machine Translation MT are instances of TextToText Generation which involves transforming one text into another obeying certain restrictions", "Here these restrictions are that the generated text must be grammatically wellformed and semanticallytranslationally equivalent to the source text", "Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree", "The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation", "One popular approach  arguably the most successful so far  is Statistical Phrase based Machine Translation PBMT which learns phrase translation rules from aligned bilingual text corpora Och et al 1999 Vogel et al 2000 Zens et al 2002 Koehn et al 2003", "Prior work has explored the use of PBMT for paraphrase generation Quirk et al 2004 Bannard and CallisonBurch 2005 Mad nani et al 2007 CallisonBurch 2008 Zhao et al 2009 Wubben et al 2010 However since many researchers believe that PBMT has reached a performance ceiling ongoing research looks into more structural approaches to statistical MT Marcu and Wong 2002 Och and Ney 2004 Khalilov and Fonollosa 2009", "Syntax based MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases presupposing syntactic structures for source target or both languages", "Syntactic information might lead to better results in the area of grammatical wellformedness and unlike phrase based MT that uses contiguous ngrams syntax enables the modeling of longdistance translation patterns", "While the verdict on whether or not this approach leads to any significant performance gain is still out a similar line of reasoning would suggest that syntaxbased paraphrasing may offer similar advantages over phrasebased paraphrasing", "Considering the fact that the success of PBMT can partly be attributed to the abundance of large parallel corpora 27 Workshop on Monolingual TextToText Generation pages 2733 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 2733 Portland Oregon 24 June 2011", "Oc 2011 Association for Computational Linguistics and that sufficiently large parallel corpora are still lacking for paraphrase generation using more linguistically motivated methods might prove beneficial for paraphrase generation", "At the same time automatic syntactic analysis introduces errors in the parse trees as no syntactic parser is perfect", "Likewise automatic alignment of syntactic phrases may be prone to errors", "The main contribution of this paper is a systematic comparison between phrasebased and syntaxbased paraphrase generation using an offtheshelf statistical machine translation SMT decoder namely Moses Koehn et al 2007 and the wordalignment tool GIZA Och and Ney 2003", "Training data derives from a new large scale 21M tokens paraphrase corpus for Dutch which has been recently released", "The paper is organized as follows", "Section 2 reviews the paraphrase corpus from which provides training and test data", "Next Section 3 describes the paraphrase generation methods and the experimental setup", "Results are presented in Section 4", "In Section 5 we discuss our findings and formulate our conclusions"]}, "W11-1815": {"title": ["BioNLP 2011 Task Bacteria Biotope  The Alvis system"], "abstract": ["This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope BB task of the BioNLP 2011 shared tasks", "Bacteria geographical locations and host entities were processed by a patternbased approach and domain lexical resources", "For the extraction of environment locations we propose a framework based on semantic analysis supported by an ontology of the biotope domain", "Domainspecific rules were developed for dealing with Bacteria anaphora", "Official results show that our Alvis system achieves the best performance of participating systems"], "introduction": ["Given a set of Web pages the information extraction goal of the Bacteria Biotope BB task is to precisely identify bacteria and their locations and to relate them", "The type of the predicted locations has to be selected among eight types", "Among them the host and hostpart locations have to be related by the partof relation", "Three teams participated in the challenge", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al 2010 and geographical locations Zhou et al 2005", "Locations include natural environments and hosts as well as food and medical locations", "In order to deal with this heterogeneity we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope BB terminoontology", "This mapping derives the type of location terms and filters out nonlocation terms", "Large external dictionaries of host names ie NCBI taxonomy and geographical names ie Agrovoc thesaurus complete the lexical resources", "The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty", "Our Alvis system implements an anaphora resolution algorithm that takes into consideration the anaphoric distance and the position of the antecedent in the sentence", "Alvis predicts the bacteria names and their relation to the locations with the help of handmade patterns based on linguistic analysis and lexical resources", "The methods for predicting and typing locations section 2 and bacteria section 3 are first described", "Section 4 details the method for relating them", "Section 5 comments the experimental results"]}, "W11-2139": {"title": ["The CMUARK GermanEnglish Translation System"], "abstract": ["This paper describes the GermanEnglish translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation WMT11", "We present the results of several modeling and training improvements to our core hierarchical phrasebased translation system including feature engineering to improve modeling of the derivation structure of translations better handing of OOVs and using development set translations into other languages to create additional pseudo references for training"], "introduction": ["We describe the GermanEnglish translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation WMT11 by the ARK research group at Carnegie Mellon University1 The core translation system is a hierarchical phrasebased machine translation system Chiang 2007 that has been extended in several ways described in this paper", "Some of our innovations focus on modeling", "Since German and English word orders can diverge considerably particularly in nonmatrix clauses we focused on feature engineering to improve the modeling of longdistance relationships which are poorly captured in standard hierarchical phrase based translation models", "To do so we developed features that assess the goodness of the source 1 httpwwwarkcscmuedu language parse tree under the translation grammar rather than of a linguistic grammar", "To train the feature weights we made use of a novel twophase training algorithm that incorporates a probabilistic training objective and standard minimum error training Och 2003", "These segmentation features were supplemented with a 7gram classbased language model which more directly models longdistance relationships", "Together these features provide a modest improvement over the baseline and suggest interesting directions for future work", "While our work on parse modeling was involved and required substantial changes to the training pipeline some other modeling enhancements were quite simple for example improving how outofvocabulary words are handled", "We propose a very simple change and show that it provides a small consistent gain", "On the training side we had two improvements over our baseline system", "First we were inspired by the work of Madnani 2010 who showed that when training to optimize BLEU Papineni et al 2002 overfitting is reduced by supplementing a single humangenerated reference translation with additional computergenerated references", "We generated supplementary pseudoreferences for our development set which is translated into many languages but once by using MT output from a secondary SpanishEnglish translation system", "Second following Foster and Kuhn 2009 we used a secondary development set to select from among many optimization runs which further improved generalization", "We largely sought techniques that did not require languagespecific resources eg treebanks POS 337 Proceedings of the 6th Workshop on Statistical Machine Translation pages 337343 Edinburgh Scotland UK July 3031 2011", "Qc 2011 Association for Computational Linguistics annotations morphological analyzers", "An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German", "Aside from this no further manually annotated data was used and we suspect many of the improvements described here can be had in other language pairs", "Despite avoiding languagespecific resources and using only the training data provided by the workshop an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rulebased systems that made use of languagespecific resources CallisonBurch et al 2011"]}, "W11-2408": {"title": ["Discovering Commonsense Entailment  Rules Implicit in Sentences"], "abstract": ["Reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge including expectations about the probable results of actions and the lexical entailments for many predicates", "We describe initial work to acquire such a collection of conditional ifthen knowledge by exploiting presuppositional discourse patterns such as ones involving but yet and hoping to and abstracting the matched material into general rules"], "introduction": ["We are interested ultimately in enabling an inference system to reason forward from facts as well as backward from goals using lexical knowledge together with world knowledge", "Creating appropriate collections of general world knowledge to support reasoning has long been a goal of researchers in Artificial Intelligence", "Efforts in information extraction eg Banko et al", "2007 have focused on learning base facts about specific entities such as that Barack Obama is president and work in knowledge extraction eg Van Durme and Schubert 2008 has found generalizations such as that a president may make a speech", "While the latter provides a basis for possibilistic forward inference Barack Obama probably makes a speech at least occasionally when its meaning is sharpened Gordon and Schubert 2010 these resources dont provide a basis for saying what we might expect to happen if for instance someone crashes their car", "That the driver in a car crash might be injured and the car damaged is a matter of common sense and as such is rarely stated directly", "However it can be found in sentences where this expectation is disconfirmed Sally crashed her car into a tree but she wasnt hurt We have been exploring the use of lexicosyntactic discourse patterns indicating disconfirmed expectations as well as peoples goals Joe apologized repeatedly hoping to be forgiven", "The resulting rules expressed at this point in natural language are a first step toward obtaining classes of general conditional knowledge typically not obtained by other methods"]}, "W12-0304": {"title": ["Google Books Ngram  Corpus used as a Grammar Checker"], "abstract": ["In this research we explore the possibility of using a large ngram corpus Google Books to derive lexical transition probabilities from the frequency of word ngrams and then use them to check and suggest corrections in a target text without the need for grammar rules", "We conduct several experiments in Spanish although our conclusions also reach other languages since the procedure is corpusdriven", "The paper reports on experiments involving different types of grammar errors which are conducted to test different grammarchecking procedures namely spotting possible errors deciding between different lexical possibilities and fillingin the blanks in a text"], "introduction": ["This paper discusses a series of early experiments on a methodology for the detection and correction of grammatical errors based on cooccurrence statistics using an extensive corpus of ngrams Google Books compiled by Michel et al 2011", "We start from two complementary assumptions on the one hand books are published accurately that is to say they usually go through different phases of revision and correction with high standards and thus a large proportion of these texts can be used as a reference corpus for inferring the grammar rules of a language", "On the other hand we hypothesise that with a sufficiently large corpus a high percentage of the information about these rules can be extracted with word ngrams", "Thus although there are still many grammatical errors that cannot be detected with this method there is also another important group which can be identified and corrected successfully as we will see in Section 4", "Grammatical errors are the most difficult and complex type of language errors because grammar is made up of a very extensive number of rules and exceptions", "Furthermore when grammar is observed in actual texts the panorama becomes far more complicated as the number of exceptions grows and the variety and complexity of syntactical structures increase to an extent that is not predicted by theoretical studies of grammar", "Grammar errors are extremely important and the majority of them cannot be considered to be performancebased because it is the meaning of the text and therefore the success or failure of communication that is compromised", "To our knowledge no grammar book or dictionary has yet provided a solution to all the problems a person may have when he or she writes and tries to follow the grammar rules of language", "Doubts that arise during the writing process are not always clearly associated to a lexical unit or the writer is not able to detect such an association and this makes it difficult to find the solution using a reference book", "In recent years some advances have been made in the automatic detection of grammar mistakes see Section 2", "Effective rulebased methods have been reported but at the cost of a very time consuming task and with an inherent lack of flexibility", "In contrast statistical methods are easier and faster to implement as well as being more flexible and adaptable", "The experiment we will describe in the following sections is the first part of a more extensive study", "Most probably the logical step to follow in order to continue such a study will be a hybrid approach based on both 27 Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing pages 2734 Avignon France April 23 2012", "Qc 2012 Association for Computational Linguistics statistics and rules", "Hence this paper aims to contribute to the statistical approach applied to grammar checking", "The Google Books Ngram Corpus is a database of ngrams of sequences of up to 5 words and records the frequency distribution of each unit in each year from 1500 onwards", "The bulk of the corpus however starts from 1970 and that is the year we took as a starting point for the material that we used to compile our reference corpus", "The idea of using this database as a grammar checker is to analyse an input text and detect any sequence of words that cannot be found in the ngram database which only contains ngrams with frequency equal to or greater than 40 and eventually to replace a unit in the text with one that makes a frequent ngram", "More specifically we conduct four types of operations accepting a text and spotting possible errors inflecting a lemma into the appropriate form in a given context fillingin the blanks in a text and selecting from a number of options the most probable word form for a given context", "In order to evaluate the algorithm we applied it to solve exercises from a Spanish grammar book and also tested the detection of errors in a corpus of real errors made by second language learners", "The paper is organised as follows we first offer a brief description of related work and then explain our methodology for each of the experiments", "In the next section we show the evaluation of the results in comparison to the Microsoft Word grammar checker and finally we draw some conclusions and discuss lines of future work"]}, "W12-3205": {"title": ["42"], "abstract": [], "introduction": ["Research in Natural Language Processing NLP has long benefitted from the fact that text can often be treated as simply a bag of words or a bag of sentences", "But not always Position often matters  eg It is wellknown that the first one or two sentences in a news report usually comprise its best ex tractive summary", "Order often matters  eg very different events are conveyed depending on how clauses and sentences are ordered", "1 a I said the magic words and a genie appeared", "b A genie appeared and I said the magic words", "Adjacency often matters  eg attributed material may span a sequence of adjacent sentences and contrasts are visible through sentence juxtaposition", "Context always matters  eg All languages achieve economy through minimal expressions that can only convey intended meaning when understood in context", "Position order adjacency and context are intrinsic features of discourse and research on discourse processing attempts to solve the challenges posed by contextbound expressions and the discourse structures that give rise when linearized to position order and adjacency", "But challenges are not why Language Technology LT researchers should care about discourse Rather discourse can enable LT to overcome known obstacles to better performance", "Consider automated summarization and machine translation Humans regularly judge output quality in terms that include referential clarity and coherence", "Systems can only improve here by paying attention to discourse  ie to linguistic features above the level of n grams and single sentences", "In fact we predict that as soon as cheap  ie nonmanual  methods are found for reliably assessing these features  for example using proxies like those suggested in Pitler et al 2010  they will supplant or at least complement todays common metrics Bleu and Rouge that say little about what matters to human text understanding CallisonBurch et al 2006", "Consider also work on automated text simplification One way that human editors simplify text is by reexpressing a long complex sentence as a discourse sequence of simple sentences", "Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse", "Other examples of LT applications already benefitting from recognizing and applying discourselevel information include automated assessment of student essays Burstein and Chodorow2010 summarization Thione et al 2004 infor 42 Proceedings of the ACL2012 Special Workshop on Rediscovering 50 Years of Discoveries pages 4254 Jeju Republic of Korea 10 July 2012", "Qc 2012 Association for Computational Linguistics Eales et al 2008 Maslennikov and Chua 2007 and more recently statistical machine translation Foster et al 2010", "These are described in more detail in Webber et al 2012", "Our aim here then on this occasion of ACLs 50th Annual Meeting is to briefly describe the evolution of computational approaches to discourse structure reflect on where the field currently stands and what new challenges it faces in trying to deliver on its promised benefit to Language Technology"]}, "W12-3311": {"title": ["A Generic Framework for Multiword Expressions Treatment"], "abstract": ["This paper presents an open and flexible methodological framework for the automatic acquisition of multiword expressions MWEs from monolingual textual corpora", "This research is motivated by the importance of MWEs for NLP applications", "After briefly presenting the modules of the framework the paper reports extrinsic evaluation results considering two applications computeraided lexicography and statistical machine translation", "Both applications can benefit from automatic MWE acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality", "The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into these and many other applications"], "introduction": ["Multiword expressions MWEs range over linguistic constructions such as idioms to pay an arm and a leg fixed phrases rock n roll and noun compounds dry ice", "There is no unique and widely accepted definition for the term multiword expression", "It can be an arbitrary and recurrent word combination Smadja 1993 or a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components Choueka 1988 or simply an idiosyncratic interpretation that crosses word boundaries or spaces Sag et al 2002", "MWEs lie in the fuzzy zone between lexicon and syntax thus constituting a real challenge for NLP systems", "In addition they are very pervasive occurring frequently in everyday language as well as in specialised communications", "Some common properties of MWEs are1 1 These are not binary yesno flags but values in a continuum going from flexible word combinations to prototypical fixed expressions", "SRC I paid my poor parents a visit MT Jai pay mes pauvres parents une visite REF Jai rendu visite  mes pauvres parents SRC Students pay an arm and a leg to park on campus MT Les tudiants paient un bras et une jambe pour se garer sur le campus REF Les tudiants paient les yeux de la tte pour se garer sur le campus SRC It shares the translationinvariance and homogeneity properties with the central moment MT Il partage la traductioninvariance et proprits dhomognit avec le moment central REF Il partage les proprits dinvariance par translation et dhomognit avec le moment central Table 1 Examples of SMT errors due to MWEs", " Arbitrariness sometimes valid constructions are not acceptable because people do not use them", "Smadja 1993 p 143144 illustrates this by presenting 8 different ways of referring to the Dow Jones index among which only 4 are used", " Institutionalisation MWEs are recurrent as they correspond to conventional ways of saying things", "Jackendoff 1997 estimates that they compose half of the entries of a speakers lexicon and Sag et al", "2002 point out that this may be an underestimate if we consider domainspecific MWEs", " Limited semantic variability MWEs do not undergo the same semantic compositionality rules as ordinary word combinations", "This is expressed in terms of i noncompositionality as the meaning of the whole expression often cannot be directly inferred from the meaning of the parts composing it ii nonsubstitutability as it is not possible to replace part of an MWE by a related synonymequivalent word or construction and iii no wordfor word translation", "61 Proceedings of the 2012 Student Research Workshop pages 6166 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics  Limited syntactic variability standard grammatical rules do not apply to MWEs", "This can be expressed in terms of i lexicalisation as one cannot list all MWEs in the lexicon undergeneration nor include them all in the grammar overgeneration and ii extragrammaticality as MWEs are unpredictable and seem weird for a second language learner who only knows general rules2  Heterogeneity MWEs are hard to define because they encompass a large amount of phenomena", "Thus NLP applications cannot use a unified approach and need to rely on some typology3  In this paper I adopt the definition by Calzolari et al", "2002 who define MWEs as different but related phenomena which can be described as a sequence4 of words that acts as a single unit at some level of linguistic analysis", "This generic and intentionally vague definition can be narrowed down according to the application needs", "For example for the statistical machine translation MT system5 used in the examples shown in Table 1 an MWE is any sequence of words which when not translated as a unit generates errors ungrammatical or unnatural verbal constructions sentence 1 awkward literal translations of idioms sentence 2 and problems of lexical choice and word order in specialised texts sentence 3", "These examples illustrate the importance of correctly dealing with MWEs in MT applications and more generally MWEs can speed up and help remove ambiguities in many current NLP applications for example  Lexicography Church and Hanks 1990 used a lexicographic environment as their evaluation scenario comparing manual and intuitive research with the automatic association ratio they proposed", " Word sense disambiguation MWEs tend to be less polysemous than simple words", "Finlayson and Kulkarni 2011 exemplify that the word world has 9 senses in Wordnet 16 record has 14 but world record has only 1", " POS tagging and parsing recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities Seretan 2008", " Information retrieval when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries Acosta et al 2011"]}, "W13-2101": {"title": ["Aligning Formal Meaning Representations with Surface Strings for"], "abstract": ["Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of textmeaning pairs", "Even though such corpora exist nowadays or could be constructed using robust semantic parsing the simple alignment between text and meaning representation is too coarse for developing robust statistical NLG systems", "By reformatting semantic representations as graphs finegrained alignment can be obtained", "Given a precise alignment at the word level the complete surface form of a meaning representations can be deduced using a simple declarative rule"], "introduction": ["Surface Realization is the task of producing fluent text from some kind of formal abstract representation of meaning Reiter and Dale 2000", "However while it is obvious what the output of a natural language generation component should be namely text there is little to no agreement on what its input formalism should be Evans et al 2002", "Since opendomain semantic parsers are able to produce formal semantic representations nowadays Bos 2008 Butler and Yoshimoto 2012 it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", "The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently Basile and Bos 2011 Wanner et al 2012", "The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power 1999 who derives semantic networks enriched with scope information from knowledge representations for content planning", "In this paper we take a further step towards the goal of generating text from deep semantic representations and consider the issue of aligning the representations with surface strings that capture their meaning", "First we describe the basic idea of aligning semantic representations logical forms with surface strings in a formalismindependent way Section 2", "Then we apply our method to a wellknown and widelyused semantic formalism namely Discourse Representation Theory DRT first demonstrating how to represent Discourse Representation Structures DRSs as graphs Section 3 and showing that the resulting Discourse Representation Graphs DRGs are equivalent to DRSs but are more convenient to fulfill word level alignment Section 4", "Finally in Section 5 we present a method that generates partial surface strings for each discourse referent occurring in the semantic representation of a text and composes them into a complete surface form", "All in all we think this would be a first and important step in surface realization from formal semantic representations"]}, "W13-2708": {"title": ["55"], "abstract": ["We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific complex concepts within large amounts of text", "Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for NLP", "It is of particular importance to find a common language between the different disciplines", "Therefore we use an interactive webinterface which is easily usable by nonexperts", "It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection", "Political scientists are thus enabled to use their own intuitions to find custom concepts"], "introduction": ["In this paper we give examples of how NLP methods and tools can be used to provide support for complex tasks in political sciences", "Many concepts of political science are complex and faceted they tend to come in different linguistic realizations often in complex ones many concepts are not directly identifiable by means of a small set of individual lexical items but require some interpretation", "Many researchers in political sciences either work qualitatively on small amounts of data which precise results due to a rather unspecific search as well as semantically invalid or ambigious search words", "On the other hand large amounts of eg news texts are available also over longer periods of time such that eg tendencies over time can be derived", "The corpora we are currently working on contain ca", "700000 articles from British Irish German and Austrian newspapers as well as yet unexplored material in French", "Figure 1 depicts a simple example of a quantitative analysis1 The example shows how often two terms Friedensmissionpeace operation and Auslandseinsatzforeign intervention are used in the last two decades in newspaper texts about interventions and wars", "The longterm goal of the project is to provide similar analysis for complex concepts", "An example of a complex concept is the evocation of collective identities in political contexts as indirect in the news", "Examples for such collective identities are the Europeans the French the Catholics", "The objective of the work we are going to discuss in this paper is to provide NLP methods and tools for assisting political scientists in the exploration of large data sets with a view to both a detailed qualitative analysis of text instances and a quantitative overview of trends over time at the level of corpora", "The examples discussed here have to do with possibly multiple collective identities", "Typical context of such identities tend to report communication as direct or as indirect speech", "Examples of such contexts are given in 1they interpret instancewise or if they are in 1 Die Europaer wu rden die Lu cke fu llen terested in quantitative trends they use comparatively simple tools such as keywordbased search The Europeans would the gap fill in corpora or text classification on the basis of terms only this latter approach may lead to im 1 The figure shows a screenshot of our webbased prototype", "55 Proceedings of the 7th Workshop on Language Technology for Cultural Heritage Social Sciences and Humanities pages 5564 Sofia Bulgaria August 8 2013", "Qc 2013 Association for Computational Linguistics Figure 1 The screenshot of our webbased system shows a simple quantitative analysis of the frequency of two terms in news articles over time", "While in the 90s the term Friedensmission peace operation was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz foreign intervention being now frequently used", "sagte Ru he", "lated to the infrastructural standards in use in the said Ru he", "CLARIN communit y In section 4 we exemplify The Europeans would fill the gap Ru he said The tool support is meant to be semiautomatic as the automatic tools propose candidates that need to be validated or refused by the political scientists", "We combine a chain of corpus processing tools with classifierbased tools eg for topic classifiers commentaryreport classifiers etc make the tools interoperable to ensure flexible data exchange and multiple usage scenarios and we embed the tool collection under a web service  based user interface", "The remainder of this paper is structured as follows", "In section 2 we present an outline of the architecture of our tool collection and we motivate the architecture", "Section 3 presents examples of implemented modules both from corpus processing and search and retrieval of instances of complex concepts", "We also show how our tools are re the intended use of the methods with case studies about steps necessary for identifying evocation being able to separate reports from comments and strategies for identifying indirect speech", "Section 6 is devoted to a conclusion and to the discussion of future work"]}, "W13-3208": {"title": ["Determining Compositionality of Word Expressions Using"], "abstract": ["This paper presents a comparative study of 5 different types of Word Space Models WSMs combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions", "Many combinations of WSMs and mea"], "introduction": ["before", "The study follows Biemann and Giesbrecht 2011 who attempted to find a list of expressions for which the composition ality assumption  the meaning of an expression is determined by the meaning of its constituents and their combination  does not hold", "Our results are very promising and can be appreciated by those interested in WSMs compositionality andor relevant evaluation methods", "1 Introduction", "Our understanding of WSM is in agreement with Sahlgren 2006 The word space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terms of spatial proximity", "There are many types of WSMs built by different algorithms", "WSMs are based on the Harris distributional hypothesis Harris 1954 which assumes that words are similar to the extent to which they share similar linguistic contexts", "WSM can be viewed as a set of words associated with vectors representing contexts in which the words occur", "Then similar vectors imply semantic similarity of the words and vice versa", "Consequently WSMs provide a means to find words semantically similar to a given word", "This capability of WSMs is exploited by many Natural Language Processing NLP applications as listed eg by Turney and Pantel 2010", "This study follows Biemann and Giesbrecht 2011 who attempted to find a list of non compositional expressions whose meaning is not fully determined by the meaning of its constituents and their combination", "The task turned out to be frustratingly hard Johannsen et al 2011", "Biemanns idea and motivation is that non compositional expressions could be treated as single units in many NLP applications such as Information Retrieval Acosta et al 2011 or Machine Translation Carpuat and Diab 2010", "We extend this motivation by stating that WSMs could also benefit from a set of noncompositional expressions", "Specifically WSMs could treat semantically noncompositional expressions as single units", "As an example consider kick the bucket hot dog or zebra crossing", "Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents kick bucket dog or zebra but instead to the whole expressions", "Recent works including that of Lin 1999 Baldwin et al", "2003 Biemann and Giesbrecht 2011 Johannsen et al", "2011 Reddy et al", "2011a Krcmar et al", "2012 and Krcmar et al", "2013 show the applicability of WSMs in determining the compositionality of word expressions", "The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets", "First this leads to nondirectly comparable results and second many combinations of 64 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality pages 6473 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics WSMs and measures have never before been applied to the task", "The main contribution and novelty of our study lies in systematic research of several basic and also advanced WSMs combined with all the so far to the best of our knowledge proposed WSMbased measures for determining the semantic compositionality", "The explored WSMs described in more detail in Section 2 include the Vector Space Model noted as log or cij denoted as sqrt", "The purpose of local weighting is to lower the importance of highly occurring words in the document", "The global function weights every value in row i of C by the same value calculated for row i Typically none denoted as N o Inverse Document Frequency denoted as I df  or a function referred to as Entropy Ent", "I df is calculated as 1  logndocsdf i and Ent Latent Semantic Analysis Hyperspace Analogue as 1   j pi j log pi j log ndocs where to Language Correlated Occurrence Analogue to Lexical Semantics and Random Indexing", "The measures including substitutability endocentricity compositionality and neighborsincommon based are described in detail in Section 3", "Section 4 describes our experiments performed on the manually annotated datasets  Distributional Semantics and Compositionality dataset DISCO and the dataset built by Reddy et al", "2011a", "Section 5 summarizes the results and Section 6 concludes the paper"]}, "W13-3209": {"title": ["Not not bad is not bad A distributional  account of negation"], "abstract": ["With the increasing empirical success of distributional models of compositional semantics it is timely to consider the types of textual logic that such models are capable of capturing", "In this paper we address shortcomings in the ability of current models to capture logical operations such as negation", "As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models"], "introduction": ["Distributional models of semantics characterize the meanings of words as a function of the words they cooccur with Firth 1957", "These models mathematically instantiated as sets of vectors in high dimensional vector spaces have been applied to tasks such as thesaurus extraction Grefenstette 1994 Curran 2004 wordsense discrimination Schu tze 1998 automated essay marking Landauer and Dumais 1997 and so on", "During the past few years research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences", "This move from word to sentence has yielded models applied to tasks such as paraphrase detection Mitchell and Lapata 2008 Mitchell and Lapata 2010 Grefenstette and Sadrzadeh 2011 Blacoe and Lapata 2012 sentiment analysis Socher et al 2012 Hermann and Blunsom 2013 and semantic relation classification ibid", "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations Mitchell and Lapata 2008 Mitchell and Lapata 2010 Zanzotto et al 2010 matrix or tensorbased approaches Baroni and Zamparelli 2010 Coecke et al 2010 Grefenstette et al 2013 Kartsaklis et al 2012 or through the use of recursive autoencoding Socher et al 2011 Hermann and Blunsom 2013 or neuralnetworks Socher et al 2012", "On the noncompositional front Erk and Pado 2008 keep word vectors separate using syntactic information from sentences to disambiguate words in context likewise Turney 2012 treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition", "These compositional distributional approaches often portray themselves as attempts to reconcile the empirical aspects of distributional semantics with the structured aspects of formal semantics", "However they in fact only principally coopt the syntaxsensitivity of formal semantics while mostly eschewing the logical aspects", "Expressing the effect of logical operations in high dimensional distributional semantic models is a very different task than in boolean logic", "For example whereas predicates such as red are seen in predicate calculi as functions mapping elementsof some set Mred to T and all other domain elements to  in compositional distributional mod els we give the meaning of red a vectorlike representation and devise some combination operation with noun representations to obtain the representation for an adjectivenoun pair", "Under the logical view negation of a predicate therefore yields a new truthfunction mapping elements ofthe complement of Mred to T and all other do main elements to  but the effect of negation and other logical operations in distributional models is not so sharp we expect the representation for not red to remain close to other objects of the same domain of discourse ie other colours while being sufficiently different from the representation of red in some manner", "Exactly how textual logic 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality pages 7482 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics would best be represented in a continuous vector space model remains an open problem", "In this paper we propose one possible formulation for a continuous vector space based representation of semantics", "We use this formulation as the basis for providing an account of logical operations for distributional models", "In particular we focus on the case of negation and how it might work in higher dimensional distributional models", "Our formulation separates domain value and functional representation in such a way as to allow negation to be handled naturally", "We explain the linguistic and modelrelated impacts of this mode of representation and discuss how this approach could be generalised to other semantic functions", "In Section 2 we provide an overview of work relating to that presented in this paper covering the integration of logical elements in distributional models and the integration of distributional elements in logical models", "In Section 3 we introduce and argue for a tripartite representation in distributional semantics and discuss the issues relating to providing a linguistically sensible notion of negation for such representations", "In Section 4 we present matrixvector models similar to that of Socher et al", "2012 as a good candidate for expressing this tripartite representation", "We argue for the elimination of nonlinearities from such models and thus show that negation cannot adequately be captured", "In Section 5 we present a short analysis of the limitation of these matrix vector models with regard to the task of modelling nonboolean logical operations and present an improved model bypassing these limitations in Section 6", "Finally in Section 7 we conclude by suggesting future work which will extend and build upon the theoretical foundations presented in this paper"]}, "W13-3306": {"title": ["43"], "abstract": ["The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives", "The goldstandard discourse relation annotation leads to better translation performance in ranges of 460 for some ambiguous English connectives and helps to find correct syntactical constructs in Czech for less ambiguous connectives", "Automatic scoring confirms the stability of the newly built discourse aware translation systems", "Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful"], "introduction": ["Recently research in statistical machine translation SMT has renewed interest in the fact that for a variety of linguistic phenomena one needs information from a longerrange context", "Current statistical translation models and decoding algorithms operate at the sentence andor phrase level only not considering already translated context from previous sentences", "This local distance is in many cases too restrictive to correctly model lexical cohesion referential expressions noun phrases pronouns and discourse markers all of which relate to the sentences before the one to be translated", "Discourse relations between sentences are often conveyed by explicit discourse connectives DC such as although because but since while", "DCs play a significant role in coherence and readability of a text", "Likewise if a wrong connective is used in translation the target text can be fully incomprehensible or not conveying the same meaning as was established by the discourse relations in the source text", "In English about 100 types of such explicit connectives have been annotated in the Penn Discourse TreeBank PDTB see Section 4 signaling discourse relations such as temporality or contrast between two spans of text", "Depending on the set of relations used there can be up to 130 such relations and combinations thereof", "Discourse relations can also be present implicitly inferred from the context without any explicit marker being present", "Although annotation for implicit DCs exists as well we only deal with explicit DCs in this paper", "DCs are difficult to translate mainly because a same English connective can signal different discourse relations in different contexts and when the target language has either different connectives according to the source relations signaled or uses different lexical or syntactical constructs in place of the English connective", "In this paper we present MT experiments from English EN to Czech CZ with a large amount of manually annotated DCs", "The corpus the parallel Prague CzechEnglish Dependency Treebank PCEDT Section 4 is directly usable for MT experiments the entire discourse annotation in EN is paralleled with a human CZ translation", "This means that we can build and evaluate against the CZ reference a translation system that learns from the EN gold standard discourse relations", "These then have no distortion from wrongly labeled connectives as it is given in related work Section 3 where automatic classifiers have been used to label the connectives with a certain error rate", "Furthermore we can use the sense labels for 100 types of EN connectives whereas related work only focused on a few highly ambiguous connectives that are especially problematic for translation", "The paper starts by illustrating difficult translations involving connectives Section 2 and discusses related work in Section 3", "The resources and data used are introduced in Section 4", "The MT experiments are explained in Section 5 and 43 Proceedings of the Workshop on Discourse in Machine Translation DiscoMT pages 4350 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics automatic evaluation is given in Section 6", "We further provide a detailed manual evaluation and error analysis for the CZ translations generated by our SMT systems Section 7", "Future work described in Section 8 concludes the paper"]}, "W15-0909": {"title": ["The Impact of Multiword Expression Compositionality  on Machine"], "abstract": ["In this paper we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric", "The attempt is marginally successful and we speculate on whether a largerscale attempt is likely to have greater impact"], "introduction": ["While the explicit identification of multiword expressions MWEs Sag et al", "2002 Baldwin and Kim 2009 has been shown to be useful in various NLP applications Ramisch 2012 recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval IR Acosta et al", "2011 and machine translation MT Weller et al", "2014 Carpuat and Diab 2010 and Venkatapathy and Joshi 2006", "For instance Acosta et al", "2011 showed that by considering noncompositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab 2010 showed that by adding compositionality scores to the Moses SMT system Koehn et al 2007 they could improve translation quality", "This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs", "The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations based on compositionality", "For example an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation as tower here is most naturally interpreted compositionally in the MT output but noncompositionally in the reference translation", "On the other hand a partial mismatch between traffic signal and traffic light should be rewarded as the usage of traffic is highly compositional in both cases", "That is we ask the question can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs focusing on compound nouns and the TESLA machine translation metric Liu et al 2010"]}, "W96-0108": {"title": [""], "abstract": ["This paper describes an automatic contextsensitive worderror correction system based on statistical language modeling SLM as applied to optical character recognition OCR post processing", "The system exploits information from multiple sources including letter ngrams character confusion probabilities and wordbigram probabilities", "Letter ngrams are used to index the words in the lexicon", "Given a sentence to be corrected the system decomposes each string in the sentence into letter ngrams and retrieves word candidates from the lexicon by comparing string ngrams with lexiconentry ngrams", "The retrieved candidates are ranked by the conditional probability of matches with the string given character confusion probabilities", "Finally the wordbigram model and Viterbi algorithm are used to determine the best scoring word sequence for the sentence", "The system can correct nonword errors as well as realword errors and achieves a 602 error reduction rate for real OCR text", "In addition the system can learn the character confusion probabilities for a specific OCR environment and use them in selfcalibration to achieve better performance"], "introduction": ["Word errors present problems for various text or speechbased applications such as optical char acter recognition OCR and voiceinput computer interfaces", "In particular though current OCR technology is quite refined and robust sources such as old books poorquality nthgeneration photocopies and faxes can still be difficult to process and may cause many OCR errors", "For OCR to be truly useful in a wide range of applications such as office automation and information retrieval systems OCR reliability must be improved", "A method for the automatic correction of OCR errors would be clearly beneficial", "Essentially there are two types of word errors nonword errors and realword errors", "A non word error occurs when a word in a source text is interpreted under OCR as a string that does not correspond to any valid word in a given word list or dictionary", "A realword error occurs when a sourcetext word is interpreted as a string that actually does occur in the dictionary but is not identical with the sourcetext word", "For example if the source text John found the man is rendered as John fomd he man by an OCR device then fomd is a nonword error and he is a realword error", "In general nonword errors will never correspond to any dictionary entries and will include wildly incorrect strings such as  as well as misrecognized alphanumeric sequences such as BN234 for 8N234", "However some nonword errors might become real word errors if the size of the word list or dictionary increases", "For example the word ruel1 might count as a nonword error for the sourcetext word rut if a small dictionary is used for reference but count as a realword error if an unabridged dictionary is used", "While nonword errors might be corrected without considering the context in which the error occurs a realword error can only be corrected by taking context into account", "The problems of worderror detection and correction have been studied for several decades", "A good survey in this area can be found in Kukich 1992", "Most traditional wordcorrection techniques concentrate on nonword error correction and do not consider the context in which the error appears", "Recently statistical language models SLMs and featurebased methods have been used for contextsensitive spellingerror correction", "For example Atwell and Elliittm 1987 have used a partofspeech POS tagging method to detect the realword errors in text", "Mays and colleagues 1991 have exploited word trigrams to detect and correct both the nonword and realword errors that were artificially generated from 100 sentences", "Church and Gale 1991 have used a Bayesian classifier method to improve the performance for nonword error correction", "Golding 1995 has applied a hybrid Bayesian method for realword error correction and Golding and Schabes 1996 have combined a POS trigram and Bayesian methods for the same purpose", "The goal of the work described here is to investigate the effectiveness and efficiency of SLM based methods applied to the problem of OCR error correction", "Since POSbased methods are not effective in distinguishing among candidates with the same POS tags and since methods based on wordtrigram models involve extensive training data and require that huge wordtrigram tables be available at run time we used a wordbigram SLM as the first step in our investigation", "In this paper we describe a system that uses a wordbigram SLM technique to correct OCR errors", "The system takes advantage of information from multiple sources including letter n grams character confusion probabilities and word bigram probabilities to effect contextbased word error correction", "It can correct nonword as well as realword errors", "In addition the system can learn the character confusion probability table for a specific OCR environment and use it to achieve better performance"]}, "W98-1234": {"title": ["Methods and tricks used in an attempt to pass the Turing Test"], "abstract": ["This paper describes differents methods and tricks in connection with our program which has been entered in the Loebner Prize competition that will happen on Sunday 11 January 1998 at the PowerHouse Museum in Sydney", "Of course this isnt exhaustive there are other possible techniques but we aim to give the main ideas", "Well speak about the main modules of our program  Spelling correction Different uses of WordNet and Generation of comments", "Our module used for spelling correction was developed on the basis of works by Brill 1 Brill and Marcus 2 Golding 3 Golding and Schabes 4 and Powers 5"], "introduction": ["Alan Turing was a brilliant British mathematician who played an important role in the development of partofspeech verbs nouns adjectives adverbs", "These sets are divided into semanticals categories eg synonymous for nouns", "WordNet is completely described in the URL httpwwwspeechcscmuedulcompspeechSectionl I LexicaVwoidnethtml", "3", "Architecture", "To mimic some parts of human thought we created different principal modules  Spelling Correction Disambiguation between words Generation of comments Simulating human typing", " computers and developed a test that would serve as an indicator of intelligence for machines", "A lot of researchers posed the Loebner Prize as the first formal instantiation of the Turing Test", "To participate in this competition we conceived a program that attempts to simulate the responses of a human being", "Well begin to describe WordNet which includes a classification of English words", "Afterwards well present the architecture of our system which we are programming at the moment", "In this section well briefly explain every module", "Next well give an Gecnomemraetinotnof 31", "Spelling Correction", " r J ", "example of interaction between our program and one human", "In the same section well show different processes of generating a response from the input of the user", "Finally well conclude by indicating our own position on this test using knowledge that we have acquired during only two months of work in this area"]}}