{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modules.texts import Vocab, GloVeLoader\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from modules.layers import SentenceEncoder, DocumentEncoder, ExtractorCell\n",
    "from modules.data import DocumentDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Load the Multiple Documents into the memory\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained vector file to use: /home/yhs/data/NLP/word_embeddings/GloVe/glove.6B.300d.txt\n",
      "The number of words in the pretrained vector: 400000\n",
      "The dimension of the pretrained vector: 300\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained embedding into the memory\n",
    "path_glove = os.path.join(os.path.expanduser('~'),\n",
    "             'data/NLP/word_embeddings/GloVe/glove.6B.300d.txt')\n",
    "glove = GloVeLoader(path_glove)\n",
    "\n",
    "# Load the dataset\n",
    "file = './data/Trump.txt'\n",
    "with open(file) as f:\n",
    "#     vocab = Vocab(f.read(), top_k = 50)\n",
    "    vocab = Vocab(f.read())\n",
    "\n",
    "d = 300\n",
    "emb = nn.Embedding(vocab.V, d)\n",
    "for word in vocab.word2id:\n",
    "    try:\n",
    "        emb.weight.data[vocab[word]] = torch.from_numpy(glove[word])\n",
    "    except KeyError as e:\n",
    "        # Case when pretrained embedding for a word does not exist\n",
    "        pass\n",
    "# emb.weight.requires_grad = False # suppress updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = DocumentDataset(file, vocab)\n",
    "docloader = DataLoader(doc, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    88   142   248     2    46   144     8     4     9     3   139   241\n",
      "\n",
      "Columns 13 to 25 \n",
      "   15    28    76   193   166   241   250   233     4   134   167   224   141\n",
      "\n",
      "Columns 26 to 38 \n",
      "   11     4    13     6    42    95   187     4   128   257    20    51    28\n",
      "\n",
      "Columns 39 to 42 \n",
      "  238   183     6   273\n",
      "[torch.LongTensor of size 1x43]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   248   257    46   134   241   163   270    60    47   166   203     6\n",
      "\n",
      "Columns 13 to 13 \n",
      "  273\n",
      "[torch.LongTensor of size 1x14]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128    90    27    91    79   118   241   260   217   166   241   251\n",
      "\n",
      "Columns 13 to 16 \n",
      "  166   181     6   273\n",
      "[torch.LongTensor of size 1x17]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    20   243     5   120    51     4   248   112   134   241   113   166\n",
      "\n",
      "Columns 13 to 25 \n",
      "  130   124    93    28   107   116   134   214   241   105   205    97    66\n",
      "\n",
      "Columns 26 to 27 \n",
      "    6   273\n",
      "[torch.LongTensor of size 1x28]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128   220    33    56    28   193   166   241   248   174   118    10\n",
      "\n",
      "Columns 13 to 25 \n",
      "  252   130   135    33   193   134   141    13     4   261   128    80    66\n",
      "\n",
      "Columns 26 to 36 \n",
      "  152   244   130   227    88   143     6    28    96     6   273\n",
      "[torch.LongTensor of size 1x37]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   248     1   216    50    54   196   111   169    49   172   208   167\n",
      "\n",
      "Columns 13 to 23 \n",
      "  245     4   132     4    55     4    28   121    72     6   273\n",
      "[torch.LongTensor of size 1x24]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    34   118   239     4   248    26   231   157   223   254    28    48\n",
      "\n",
      "Columns 13 to 19 \n",
      "  253   199   262   130   159     6   273\n",
      "[torch.LongTensor of size 1x20]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128   126   268   172    63     5    39   222    45     4   114   100\n",
      "\n",
      "Columns 13 to 25 \n",
      "  241    32   166   241    77     4    28   128   198    28   131   241    31\n",
      "\n",
      "Columns 26 to 32 \n",
      "  238   219   114     7   269     6   273\n",
      "[torch.LongTensor of size 1x33]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    33   166    13     4   128   257   241    17   212   182   134   241\n",
      "\n",
      "Columns 13 to 25 \n",
      "  266     4   262    27    98   162   267   166     0    14     6    16    44\n",
      "\n",
      "Columns 26 to 27 \n",
      "    6   273\n",
      "[torch.LongTensor of size 1x28]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "  272   248   125   149   101   136   134   187     6   273\n",
      "[torch.LongTensor of size 1x10]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128    94   241    12   194   204    33    20   211    28    78   225\n",
      "\n",
      "Columns 13 to 18 \n",
      "  171   134   241   195     6   273\n",
      "[torch.LongTensor of size 1x19]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    65    83   130   186   190    33   189     4   200     4    28   160\n",
      "\n",
      "Columns 13 to 14 \n",
      "    6   273\n",
      "[torch.LongTensor of size 1x15]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   130    52   206   102   117   155    74    19   153   166   130   202\n",
      "\n",
      "Columns 13 to 19 \n",
      "  232   259    69   172   104     6   273\n",
      "[torch.LongTensor of size 1x20]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   248   265   241   119    92   169   164    18     4    12     4   134\n",
      "\n",
      "Columns 13 to 22 \n",
      "   20   237   255    24    81   170   129    62     6   273\n",
      "[torch.LongTensor of size 1x23]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128    41   241   168    28   258   182    99   244    37   241   192\n",
      "\n",
      "Columns 13 to 25 \n",
      "    4   241   110   264   197   156   172   123   221     4    28   241   109\n",
      "\n",
      "Columns 26 to 37 \n",
      "  244   127   265   241    92    84   150   241   188   256     6   273\n",
      "[torch.LongTensor of size 1x38]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "  272   130    92    28   184   127   229   165   201     6   273\n",
      "[torch.LongTensor of size 1x11]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   134    87   185     4   248   126   228     4   226   106   264   234\n",
      "\n",
      "Columns 13 to 23 \n",
      "    4   244   209    28   210   241    22    53    21     6   273\n",
      "[torch.LongTensor of size 1x24]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "  272   128    30   161   122   244   241   236    73     6   273\n",
      "[torch.LongTensor of size 1x11]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   128   173    20   247    40   169    59   118   222   158     5   151\n",
      "\n",
      "Columns 13 to 25 \n",
      "   71     4    58   218    67    19   241    40   257   178   133    23   147\n",
      "\n",
      "Columns 26 to 28 \n",
      "   57     6   273\n",
      "[torch.LongTensor of size 1x29]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272   134   115   185     4   128   263   241   250   233   118   241   246\n",
      "\n",
      "Columns 13 to 25 \n",
      "    5   176   179    28   241   177    61    25     4   249   180   166   241\n",
      "\n",
      "Columns 26 to 38 \n",
      "   75   240     4    28    35   158   146   244    89   175   103   118   242\n",
      "\n",
      "Columns 39 to 46 \n",
      "   71    38   241    13   213   235     6   273\n",
      "[torch.LongTensor of size 1x47]\n",
      " \n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "  272    23   248    86   108    85   140    64     4   241   145    82    30\n",
      "\n",
      "Columns 13 to 25 \n",
      "   20   230    70   244    68   241   138   137   191   148    43   215    28\n",
      "\n",
      "Columns 26 to 35 \n",
      "  248    52    36     4    28    29   207   154     6   273\n",
      "[torch.LongTensor of size 1x36]\n",
      " \n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input, target in docloader:\n",
    "    print(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / Loss: 0.6908168\n",
      "Epoch: 10 / Loss: 0.0762508\n",
      "Epoch: 20 / Loss: 0.0151254\n",
      "Epoch: 30 / Loss: 0.0054811\n",
      "Epoch: 40 / Loss: 0.0026618\n",
      "Epoch: 50 / Loss: 0.0014242\n",
      "Epoch: 60 / Loss: 0.0009217\n",
      "Epoch: 70 / Loss: 0.0006863\n",
      "Epoch: 80 / Loss: 0.0005508\n",
      "Epoch: 90 / Loss: 0.0004623\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab.V\n",
    "emb_size = emb.weight.data.size(1)\n",
    "n_kernels = 50\n",
    "kernel_sizes = [1,2,3,4,5]\n",
    "pretrained = emb\n",
    "sent_size = len(kernel_sizes) * n_kernels\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "####WARNING: No mini-batch processing#########\n",
    "s_encoder = SentenceEncoder(vocab_size,\n",
    "                            emb_size,\n",
    "                            n_kernels,\n",
    "                            kernel_sizes,\n",
    "                            pretrained)\n",
    "d_encoder = DocumentEncoder(sent_size, hidden_size)\n",
    "ext_cell = ExtractorCell(sent_size, hidden_size)\n",
    "\n",
    "# Binary Cross-Entropy loss\n",
    "loss_fn = nn.BCELoss()\n",
    "params = list(s_encoder.parameters()) + list(d_encoder.parameters()) + list(ext_cell.parameters())\n",
    "optimizer = optim.Adam(params, lr = .005)\n",
    "\n",
    "def run_epoch(docloader):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Encode the sentences in a document\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for input_raw, target_raw in docloader:\n",
    "        input_raw = Variable(input_raw).cuda()\n",
    "        inputs.append(s_encoder(input_raw))\n",
    "        targets.append(target.cuda())\n",
    "    \n",
    "    # Build the document representation using encoded sentences\n",
    "    d_encoded = torch.cat(inputs, dim = 0)\n",
    "    targets = Variable(torch.cat(targets, dim = 0).type(torch.FloatTensor).view(-1)).cuda()\n",
    "    #### WARNING: \"BEGINNING OF THE SENTENCE\" embedding was initialized to zero ####\n",
    "    init_sent = Variable(torch.zeros(1, d_encoded.size(1))).cuda()\n",
    "    d_final = torch.cat([init_sent, d_encoded[:-1]], dim = 0)\n",
    "    d_final = d_final.view(d_final.size(0),1,d_final.size(1))\n",
    "\n",
    "    # Initialize the d_encoder\n",
    "    h, c = d_encoder.init_h0c0(batch_size)\n",
    "    h0 = Variable(h.data)\n",
    "\n",
    "    # An input goes through the d_encoder\n",
    "    output, hn, cn = d_encoder(d_final, h, c)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    ## calculate p0, h_bar0, c_bar0\n",
    "    h_ = hn.squeeze(0)\n",
    "    c_ = cn.squeeze(0)\n",
    "    p = ext_cell.init_p(h0.squeeze(0), h_)\n",
    "\n",
    "    ## calculate p_t, h_bar_t, c_bar_t\n",
    "    d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "    extract_probs = Variable(torch.zeros(len(inputs))).cuda()\n",
    "    for i, (s, h) in enumerate(zip(inputs, d_encoder_hiddens)):\n",
    "        h_, c_, p = ext_cell(s, h, h_, c_, p)\n",
    "        extract_probs[i] = p\n",
    "\n",
    "    optimizer.zero_grad() # flush the gradients\n",
    "    loss = loss_fn(extract_probs, targets)\n",
    "    epoch_loss += loss.data.cpu().numpy()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def train(docloader, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = run_epoch(docloader)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, epoch_loss))\n",
    "        \n",
    "# Initial Training\n",
    "train(docloader, n_epochs = 100, print_every = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test(docloader):\n",
    "    # Encode the sentences in a document\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for input_raw, target_raw in docloader:\n",
    "        input_raw = Variable(input_raw).cuda()\n",
    "        inputs.append(s_encoder(input_raw))\n",
    "        targets.append(target.cuda())\n",
    "    \n",
    "    # Build the document representation using encoded sentences\n",
    "    d_encoded = torch.cat(inputs, dim = 0)\n",
    "    targets = Variable(torch.cat(targets, dim = 0).type(torch.FloatTensor).view(-1)).cuda()\n",
    "    #### WARNING: \"BEGINNING OF THE SENTENCE\" embedding was initialized to zero ####\n",
    "    init_sent = Variable(torch.zeros(1, d_encoded.size(1))).cuda()\n",
    "    d_final = torch.cat([init_sent, d_encoded[:-1]], dim = 0)\n",
    "    d_final = d_final.view(d_final.size(0),1,d_final.size(1))\n",
    "\n",
    "    # Initialize the d_encoder\n",
    "    h, c = d_encoder.init_h0c0(batch_size)\n",
    "    h0 = Variable(h.data)\n",
    "\n",
    "    # An input goes through the d_encoder\n",
    "    output, hn, cn = d_encoder(d_final, h, c)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    ## calculate p0, h_bar0, c_bar0\n",
    "    h_ = hn.squeeze(0)\n",
    "    c_ = cn.squeeze(0)\n",
    "    p = ext_cell.init_p(h0.squeeze(0), h_)\n",
    "\n",
    "    ## calculate p_t, h_bar_t, c_bar_t\n",
    "    d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "    extract_probs = Variable(torch.zeros(len(inputs))).cuda()\n",
    "    for i, (s, h) in enumerate(zip(inputs, d_encoder_hiddens)):\n",
    "        h_, c_, p = ext_cell(s, h, h_, c_, p)\n",
    "        extract_probs[i] = p\n",
    "    \n",
    "    extract_probs = extract_probs\n",
    "    extract_probs = extract_probs.data.cpu().numpy()\n",
    "    targets = targets.data.cpu().numpy()\n",
    "    preds = np.array([1 if p > 0.5 else 0 for p in extract_probs])\n",
    "    accuracy = np.mean(preds == targets)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "preds = test(docloader)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_size = vocab.V\n",
    "# emb_size = emb.weight.data.size(1)\n",
    "# n_kernels = 50\n",
    "# kernel_sizes = [1,2,3,4,5]\n",
    "# input_size = vocab.V\n",
    "# hidden_size = 100\n",
    "# batch_size = 1\n",
    "\n",
    "# ####WARNING: No mini-batch processing#########\n",
    "# s_encoder = SentenceEncoder(vocab_size,\n",
    "#                             emb_size,\n",
    "#                             n_kernels,\n",
    "#                             kernel_sizes)\n",
    "# d_encoder = DocumentEncoder(emb_size, hidden_size)\n",
    "# ext_cell = ExtractorCell(input_size, hidden_size)\n",
    "\n",
    "# # Binary Cross-Entropy loss\n",
    "# loss_fn = nn.BCELoss()\n",
    "# params = list(s_encoder.parameters()) + list(d_encoder.parameters()) + list(ext_cell.parameters())\n",
    "# optimizer = optim.Adam(params, lr = .005)\n",
    "\n",
    "# def run_epoch(input_docs, target_docs):\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "    \n",
    "#     # Train over the whole document\n",
    "#     for input, target in zip(input_docs, target_docs):\n",
    "#         # flush the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         input = Variable(input).view(input.size(0),1,input.size(1)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(target)).cuda()\n",
    "\n",
    "#         # Initialize the d_encoder\n",
    "#         h, c = d_encoder.init_h0c0(batch_size)\n",
    "#         h0 = Variable(h.data)\n",
    "\n",
    "#         # An input goes through the d_encoder\n",
    "#         output, hn, cn = d_encoder(input, h, c)\n",
    "\n",
    "#         # Initialize the decoder\n",
    "#         ## calculate p0, h_bar0, c_bar0\n",
    "#         h_ = hn.squeeze(0)\n",
    "#         c_ = cn.squeeze(0)\n",
    "#         p = ext_cell.init_p(h0.squeeze(0), h_)\n",
    "\n",
    "#         ## calculate p_t, h_bar_t, c_bar_t\n",
    "#         d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "#         extract_probs = Variable(torch.zeros(input.size(0))).cuda()\n",
    "#         for i, (s, h) in enumerate(zip(input, d_encoder_hiddens)):\n",
    "#             h_, c_, p = ext_cell(s, h, h_, c_, p)\n",
    "#             extract_probs[i] = p\n",
    "#         loss = loss_fn(extract_probs, target)\n",
    "#         epoch_loss += loss.data.cpu().numpy()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     return epoch_loss\n",
    "\n",
    "# def train(input_docs, target_docs, n_epochs = 100, print_every = 10):\n",
    "#     total_loss = 0.0\n",
    "#     for epoch in range(n_epochs):\n",
    "#         epoch_loss = run_epoch(input_docs, target_docs)\n",
    "#         if epoch % print_every == 0:\n",
    "#             print('Epoch: %2i / Loss: %.7f' % (epoch, epoch_loss))\n",
    "        \n",
    "# # Initial Training\n",
    "# train(input_docs, target_docs, n_epochs = 100, print_every = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
