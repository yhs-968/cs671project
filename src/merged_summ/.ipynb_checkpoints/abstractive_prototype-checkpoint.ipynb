{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modules.texts import Vocab, GloVeLoader\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import modules.extractive as ext\n",
    "import modules.abstractive as abs\n",
    "from modules.data import Documents\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained vector file to use: /home/yhs/data/NLP/word_embeddings/GloVe/glove.6B.200d.txt\n",
      "The number of words in the pretrained vector: 400000\n",
      "The dimension of the pretrained vector: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the pretrained embedding into the memory\n",
    "path_glove = os.path.join(os.path.expanduser('~'),\n",
    "             'data/NLP/word_embeddings/GloVe/glove.6B.200d.txt')\n",
    "glove = GloVeLoader(path_glove)\n",
    "\n",
    "# Load the dataset\n",
    "doc_file = './data/kaggle_news_rouge1.pkl'\n",
    "docs = Documents(doc_file, vocab_size = 30000)\n",
    "docs.set_doc_classes(np.random.randint(2, size = len(docs)).tolist()) # attach random document labels\n",
    "vocab = docs.vocab\n",
    "\n",
    "d = 200\n",
    "emb = nn.Embedding(vocab.V, d)\n",
    "\n",
    "def init_emb(emb, vocab):\n",
    "    for word in vocab.word2id:\n",
    "        try:\n",
    "            emb.weight.data[vocab[word]] = torch.from_numpy(glove[word])\n",
    "        except KeyError as e:\n",
    "            # Case when pretrained embedding for a word does not exist\n",
    "            pass\n",
    "#     emb.weight.requires_grad = False # suppress updates\n",
    "    print('Initialized the word embeddings.')\n",
    "\n",
    "init_emb(emb, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "vocab_size = vocab.V\n",
    "emb_size = emb.weight.data.size(1)\n",
    "hidden_size = 400\n",
    "num_layers = 1\n",
    "batch_size = 1\n",
    "\n",
    "enc = abs.EncoderRNN(vocab_size, emb_size, hidden_size, num_layers, emb).cuda()\n",
    "dec = abs.AttnDecoderRNN(vocab_size, emb_size, hidden_size * 2, num_layers, emb).cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = Adam(params, lr = .005)\n",
    "\n",
    "def run_epoch(docs):\n",
    "    epoch_loss = 0\n",
    "    for doc in docs:\n",
    "        docloader = DataLoader(doc, batch_size = batch_size, shuffle = False)\n",
    "        total_loss = 0\n",
    "        for sent, ext_label in docloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            # Run through the encoder\n",
    "            sent = Variable(sent.t()).cuda()\n",
    "            enc_hidden = enc.init_hidden(batch_size)\n",
    "            enc_output, enc_hidden = enc(sent, enc_hidden)\n",
    "\n",
    "            # Run through the decoder\n",
    "            dec_hidden = dec.init_hidden(batch_size)\n",
    "            for target in docloader.dataset.summ:\n",
    "                target = Variable(torch.LongTensor([target]).unsqueeze(1)).cuda()\n",
    "                dec_output, dec_hidden, attn_weights = dec(target, dec_hidden, enc_output)\n",
    "                loss += loss_fn(dec_output, target.squeeze(1))\n",
    "\n",
    "            epoch_loss += loss.data.cpu().numpy()[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    epoch_loss /= len(docs)\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def train(docs, n_epochs = 10, print_every = 1):\n",
    "    import time\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = run_epoch(docs)\n",
    "        end_time = time.time()\n",
    "        wall_clock = (end_time - start_time) / 60\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch:%2i / Loss:(%.3f/%.3f) / Accuracy:(%.3f/%.3f) / TrainingTime:%.3f(min)' %\n",
    "                  (epoch, ext_loss, dclass_loss, ext_acc, dclass_acc, wall_clock))\n",
    "\n",
    "train(docs, n_epochs = 3, print_every = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
