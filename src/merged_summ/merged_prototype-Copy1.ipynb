{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modules.texts import Vocab, GloVeLoader\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import modules.extractive as ext\n",
    "import modules.abstractive as abs\n",
    "import modules.beam_search as bs\n",
    "from modules.data import Documents\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained vector file to use: /home/yhs/data/NLP/word_embeddings/GloVe/glove.6B.200d.txt\n",
      "The number of words in the pretrained vector: 400000\n",
      "The dimension of the pretrained vector: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the pretrained embedding into the memory\n",
    "path_glove = os.path.join(os.path.expanduser('~'),\n",
    "             'data/NLP/word_embeddings/GloVe/glove.6B.200d.txt')\n",
    "glove = GloVeLoader(path_glove)\n",
    "\n",
    "# Load the dataset\n",
    "doc_file = './data/kaggle_news_rouge1.pkl'\n",
    "docs = Documents(doc_file, n_samples=100, vocab_size = 30000)\n",
    "vocab = docs.vocab\n",
    "\n",
    "d = 200\n",
    "emb = nn.Embedding(vocab.V, d)\n",
    "\n",
    "def init_emb(emb, vocab):\n",
    "    for word in vocab.word2id:\n",
    "        try:\n",
    "            emb.weight.data[vocab[word]] = torch.from_numpy(glove[word])\n",
    "        except KeyError as e:\n",
    "            # Case when pretrained embedding for a word does not exist\n",
    "            pass\n",
    "#     emb.weight.requires_grad = False # suppress updates\n",
    "    print('Initialized the word embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the word embeddings.\n",
      "Epoch: 0 / Loss:(69.154/171.778/6974.253) / Accuracy:(0.507) / TrainingTime:0.137(min)\n",
      "Epoch: 1 / Loss:(68.526/165.057/7867.030) / Accuracy:(0.595) / TrainingTime:0.176(min)\n",
      "Epoch: 2 / Loss:(68.147/160.722/7178.985) / Accuracy:(0.595) / TrainingTime:0.177(min)\n",
      "Epoch: 3 / Loss:(67.833/156.720/6649.148) / Accuracy:(0.595) / TrainingTime:0.176(min)\n",
      "Epoch: 4 / Loss:(67.557/152.730/6201.312) / Accuracy:(0.595) / TrainingTime:0.177(min)\n",
      "Epoch: 5 / Loss:(67.305/148.548/5821.301) / Accuracy:(0.595) / TrainingTime:0.176(min)\n",
      "Epoch: 6 / Loss:(67.073/144.021/5469.786) / Accuracy:(0.595) / TrainingTime:0.177(min)\n",
      "Epoch: 7 / Loss:(66.858/139.105/5119.098) / Accuracy:(0.595) / TrainingTime:0.176(min)\n",
      "Epoch: 8 / Loss:(66.651/133.773/4763.871) / Accuracy:(0.595) / TrainingTime:0.177(min)\n",
      "Epoch: 9 / Loss:(66.453/128.054/4403.646) / Accuracy:(0.595) / TrainingTime:0.176(min)\n",
      "Epoch:10 / Loss:(66.262/121.971/4037.107) / Accuracy:(0.595) / TrainingTime:0.177(min)\n",
      "Epoch:11 / Loss:(66.078/115.600/3663.783) / Accuracy:(0.596) / TrainingTime:0.176(min)\n",
      "Epoch:12 / Loss:(65.895/109.075/3285.561) / Accuracy:(0.598) / TrainingTime:0.177(min)\n",
      "Epoch:13 / Loss:(65.723/102.507/2907.500) / Accuracy:(0.599) / TrainingTime:0.175(min)\n",
      "Epoch:14 / Loss:(65.553/95.989/2533.006) / Accuracy:(0.603) / TrainingTime:0.175(min)\n",
      "Epoch:15 / Loss:(65.389/89.702/2165.366) / Accuracy:(0.611) / TrainingTime:0.173(min)\n",
      "Epoch:16 / Loss:(65.230/83.704/1812.668) / Accuracy:(0.605) / TrainingTime:0.173(min)\n",
      "Epoch:17 / Loss:(65.082/78.075/1484.365) / Accuracy:(0.602) / TrainingTime:0.170(min)\n",
      "Epoch:18 / Loss:(64.940/72.827/1190.562) / Accuracy:(0.608) / TrainingTime:0.167(min)\n",
      "Epoch:19 / Loss:(64.806/67.957/939.569) / Accuracy:(0.611) / TrainingTime:0.166(min)\n",
      "Epoch:20 / Loss:(64.679/63.481/736.345) / Accuracy:(0.610) / TrainingTime:0.164(min)\n",
      "Epoch:21 / Loss:(64.556/59.368/580.262) / Accuracy:(0.617) / TrainingTime:0.163(min)\n",
      "Epoch:22 / Loss:(64.436/55.614/464.367) / Accuracy:(0.611) / TrainingTime:0.161(min)\n",
      "Epoch:23 / Loss:(64.320/52.184/379.224) / Accuracy:(0.613) / TrainingTime:0.160(min)\n",
      "Epoch:24 / Loss:(64.205/49.031/316.050) / Accuracy:(0.613) / TrainingTime:0.159(min)\n",
      "Epoch:25 / Loss:(64.089/46.138/268.557) / Accuracy:(0.614) / TrainingTime:0.159(min)\n",
      "Epoch:26 / Loss:(63.974/43.484/232.082) / Accuracy:(0.617) / TrainingTime:0.158(min)\n",
      "Epoch:27 / Loss:(63.855/41.036/203.395) / Accuracy:(0.620) / TrainingTime:0.158(min)\n",
      "Epoch:28 / Loss:(63.733/38.777/180.446) / Accuracy:(0.621) / TrainingTime:0.158(min)\n",
      "Epoch:29 / Loss:(63.609/36.681/161.811) / Accuracy:(0.622) / TrainingTime:0.158(min)\n",
      "Epoch:30 / Loss:(63.482/34.747/146.426) / Accuracy:(0.621) / TrainingTime:0.157(min)\n",
      "Epoch:31 / Loss:(63.351/32.932/133.552) / Accuracy:(0.618) / TrainingTime:0.157(min)\n",
      "Epoch:32 / Loss:(63.217/31.259/122.634) / Accuracy:(0.623) / TrainingTime:0.157(min)\n",
      "Epoch:33 / Loss:(63.078/29.690/113.275) / Accuracy:(0.626) / TrainingTime:0.157(min)\n",
      "Epoch:34 / Loss:(62.935/28.233/105.158) / Accuracy:(0.627) / TrainingTime:0.157(min)\n",
      "Epoch:35 / Loss:(62.786/26.872/98.080) / Accuracy:(0.628) / TrainingTime:0.157(min)\n",
      "Epoch:36 / Loss:(62.635/25.586/91.833) / Accuracy:(0.631) / TrainingTime:0.156(min)\n",
      "Epoch:37 / Loss:(62.478/24.405/86.305) / Accuracy:(0.633) / TrainingTime:0.157(min)\n",
      "Epoch:38 / Loss:(62.317/23.284/81.354) / Accuracy:(0.636) / TrainingTime:0.155(min)\n",
      "Epoch:39 / Loss:(62.148/22.239/76.909) / Accuracy:(0.638) / TrainingTime:0.156(min)\n",
      "Epoch:40 / Loss:(61.976/21.260/72.921) / Accuracy:(0.640) / TrainingTime:0.155(min)\n",
      "Epoch:41 / Loss:(61.797/20.341/69.279) / Accuracy:(0.645) / TrainingTime:0.156(min)\n",
      "Epoch:42 / Loss:(61.613/19.481/65.962) / Accuracy:(0.648) / TrainingTime:0.155(min)\n",
      "Epoch:43 / Loss:(61.424/18.669/62.928) / Accuracy:(0.653) / TrainingTime:0.156(min)\n",
      "Epoch:44 / Loss:(61.229/17.910/60.145) / Accuracy:(0.652) / TrainingTime:0.155(min)\n",
      "Epoch:45 / Loss:(61.026/17.192/57.581) / Accuracy:(0.654) / TrainingTime:0.156(min)\n",
      "Epoch:46 / Loss:(60.818/16.515/55.212) / Accuracy:(0.656) / TrainingTime:0.155(min)\n",
      "Epoch:47 / Loss:(60.602/15.873/53.025) / Accuracy:(0.659) / TrainingTime:0.155(min)\n",
      "Epoch:48 / Loss:(60.376/15.269/50.984) / Accuracy:(0.660) / TrainingTime:0.155(min)\n",
      "Epoch:49 / Loss:(60.144/14.698/49.088) / Accuracy:(0.662) / TrainingTime:0.155(min)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from copy import deepcopy\n",
    "from torch import optim\n",
    "import time\n",
    "from itertools import chain\n",
    "\n",
    "vocab_size = vocab.V\n",
    "emb_size = emb.weight.data.size(1)\n",
    "n_kernels = 50\n",
    "kernel_sizes = [1,2,3,4,5]\n",
    "pretrained = emb\n",
    "sent_size = len(kernel_sizes) * n_kernels\n",
    "hidden_size = 400\n",
    "num_layers = 1\n",
    "n_classes = len(docs.dclass2id)\n",
    "batch_size = 1\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed(7)\n",
    "\n",
    "init_emb(emb, vocab)\n",
    "ext_s_enc = ext.SentenceEncoder(vocab_size, emb_size,\n",
    "                                   n_kernels, kernel_sizes, pretrained)\n",
    "ext_d_enc = ext.DocumentEncoder(sent_size, hidden_size)\n",
    "ext_extc = ext.ExtractorCell(sent_size, hidden_size)\n",
    "ext_d_classifier = ext.DocumentClassifier(sent_size, n_classes)\n",
    "abs_enc = abs.EncoderRNN(emb, hidden_size, num_layers)\n",
    "abs_dec = abs.AttnDecoderRNN(emb, hidden_size * 2, num_layers)\n",
    "\n",
    "models = [ext_s_enc, ext_d_enc, ext_extc, ext_d_classifier,\n",
    "         abs_enc, abs_dec]\n",
    "params = list(chain(*[model.parameters() for model in models]))\n",
    "optimizer = optim.SGD(params, lr = .005)\n",
    "\n",
    "loss_fn_ext = nn.BCELoss()\n",
    "loss_fn_dclass = nn.NLLLoss()\n",
    "loss_fn_abs = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_accuracy(probs, targets, verbose = False):   \n",
    "    '''\n",
    "    Calculates the accuracy for the extractor\n",
    "\n",
    "    Args:\n",
    "        probs: extraction probability\n",
    "        targets: ground truth labels for extraction\n",
    "    '''\n",
    "    import numpy as np\n",
    "    preds = np.array([1 if p > 0.5 else 0 for p in probs])\n",
    "    if verbose:\n",
    "        print(preds)\n",
    "    accuracy = np.mean(preds == targets)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# class RougeScorer:\n",
    "#     def __init__(self):\n",
    "#         from rouge import Rouge\n",
    "#         self.rouge = Rouge()\n",
    "#     def score(self, reference, generated, type = 1):\n",
    "#         score = self.rouge.get_scores(reference, generated, avg=True)\n",
    "#         score = score['rouge-%s' % type]['f']\n",
    "#         return score\n",
    "\n",
    "# rouge = RougeScorer()\n",
    "    \n",
    "def run_epoch(docs):\n",
    "    \n",
    "    epoch_loss_abs = 0\n",
    "    epoch_loss_ext = 0\n",
    "    epoch_loss_dclass = 0\n",
    "    epoch_accuracy_ext = 0\n",
    "\n",
    "    for doc in docs:\n",
    "        optimizer.zero_grad()\n",
    "        docloader = DataLoader(doc, batch_size=1, shuffle=False)\n",
    "        # Encode the sentences in a document\n",
    "        sents_raw = []\n",
    "        sents_encoded = []\n",
    "        ext_labels = []\n",
    "        doc_class = Variable(torch.LongTensor([doc.doc_class])).cuda()\n",
    "        for sent, ext_label in docloader:\n",
    "            # only accept sentences that conforms the maximum kernel sizes\n",
    "            if sent.size(1) < max(kernel_sizes):\n",
    "                continue\n",
    "            sent = Variable(sent).cuda()\n",
    "            sents_raw.append(sent)\n",
    "            sents_encoded.append(ext_s_enc(sent))\n",
    "            ext_labels.append(ext_label.cuda())\n",
    "        # Ignore if the content is a single sentence(no need to train)\n",
    "        if len(sents_raw) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Build the document representation using encoded sentences\n",
    "        d_encoded = torch.cat(sents_encoded, dim = 0).unsqueeze(1)\n",
    "        ext_labels = Variable(torch.cat(ext_labels, dim = 0).type(torch.FloatTensor).view(-1)).cuda()\n",
    "        init_sent = ext_s_enc.init_sent(batch_size)\n",
    "        d_ext = torch.cat([init_sent, d_encoded[:-1]], dim = 0)\n",
    "\n",
    "        # Extractive Summarizer\n",
    "        ## Initialize the d_encoder\n",
    "        h, c = ext_d_enc.init_h0c0(batch_size)\n",
    "        h0 = Variable(h.data)\n",
    "        ## An input goes through the document encoder\n",
    "        output, hn, cn = ext_d_enc(d_ext, h, c)\n",
    "        ## Initialize the decoder\n",
    "        ### calculate p0, h_bar0, c_bar0\n",
    "        h_ = hn.squeeze(0)\n",
    "        c_ = cn.squeeze(0)\n",
    "        p = ext_extc.init_p(h0.squeeze(0), h_)\n",
    "        ### calculate p_t, h_bar_t, c_bar_t\n",
    "        d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "        extract_probs = Variable(torch.zeros(len(sents_encoded))).cuda()\n",
    "        for i, (s, h) in enumerate(zip(sents_encoded, d_encoder_hiddens)):\n",
    "            h_, c_, p = ext_extc(s, h, h_, c_, p)\n",
    "            extract_probs[i] = p\n",
    "        ## Document Classifier\n",
    "        q = ext_d_classifier(extract_probs.view(-1,1), d_encoded.squeeze(1))\n",
    "        \n",
    "        ## Optimize over the extractive examples\n",
    "        loss_ext = loss_fn_ext(extract_probs, ext_labels)\n",
    "        loss_dclass = loss_fn_dclass(q.view(1,-1), doc_class)\n",
    "        epoch_loss_ext += loss_ext.data.cpu().numpy()[0]\n",
    "        epoch_loss_dclass += loss_dclass.data.cpu().numpy()[0]\n",
    "        torch.autograd.backward([loss_ext, loss_dclass])\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## Measure the accuracy\n",
    "        p_cpu = extract_probs.data.cpu().numpy()\n",
    "        t_cpu = ext_labels.data.cpu().numpy()\n",
    "        q_cpu = q.data.cpu().numpy()\n",
    "        c_cpu = doc_class.data.cpu().numpy()\n",
    "        epoch_accuracy_ext += get_accuracy(p_cpu, t_cpu)\n",
    "\n",
    "        # Abstractive Summarizer\n",
    "        optimizer.zero_grad()\n",
    "        loss_abs = 0\n",
    "        ## Run through the encoder\n",
    "#         words = torch.cat(sents_ext, dim=1).t()\n",
    "        sents_ext = [sent for i,sent in enumerate(sents_raw)\n",
    "                     if extract_probs[i].data.cpu().numpy() > 0.5]\n",
    "        \n",
    "        # skip if no sentences are selected as summaries\n",
    "        if len(sents_ext) == 0:\n",
    "            continue\n",
    "        words = torch.cat(sents_ext, dim=1).t()\n",
    "\n",
    "        abs_enc_hidden = abs_enc.init_hidden(batch_size)\n",
    "        abs_enc_output, abs_enc_hidden = abs_enc(words, abs_enc_hidden)\n",
    "        ## Remove to too long documents to tackle memory overflow\n",
    "        if len(abs_enc_output) > 6000:\n",
    "            continue\n",
    "        ## Run through the decoder\n",
    "        abs_dec_hidden = abs_dec.init_hidden(batch_size)\n",
    "        for target in doc.head:\n",
    "            target = Variable(torch.LongTensor([target]).unsqueeze(1)).cuda()\n",
    "            abs_dec_output, abs_dec_hidden, attn_weights = abs_dec(target, abs_dec_hidden, abs_enc_output)\n",
    "            loss_abs += loss_fn_abs(abs_dec_output, target.squeeze(1))\n",
    "\n",
    "        epoch_loss_abs += loss_abs.data.cpu().numpy()[0]\n",
    "        loss_abs.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc_ext = epoch_accuracy_ext / len(docs)\n",
    "    \n",
    "    return epoch_loss_ext, epoch_loss_dclass, epoch_loss_abs, acc_ext\n",
    "\n",
    "def train(docs, n_epochs = 10, print_every = 1):\n",
    "    import time\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        ext_loss, dclass_loss, abs_loss, ext_acc = run_epoch(docs)\n",
    "        end_time = time.time()\n",
    "        wall_clock = (end_time - start_time) / 60\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch:%2i / Loss:(%.3f/%.3f/%.3f) / Accuracy:(%.3f) / TrainingTime:%.3f(min)' %\n",
    "                  (epoch, ext_loss, dclass_loss, abs_loss, ext_acc, wall_clock))\n",
    "\n",
    "import os\n",
    "from os.path import join            \n",
    "# Training\n",
    "train(docs, n_epochs = 50, print_every = 1)\n",
    "# for n in range(5):\n",
    "#     train(docs, n_epochs = 10, print_every = 1)\n",
    "#     print('Epoch %2i finished.' % ((n+1)*10))\n",
    "#     model_dict = dict()\n",
    "#     model_dict['emb'] = emb\n",
    "#     model_dict['ext_s_enc'] = ext_s_enc\n",
    "#     model_dict['ext_d_enc'] = ext_d_enc\n",
    "#     model_dict['ext_extc'] = ext_extc\n",
    "#     model_dict['ext_d_classifier'] = ext_d_classifier\n",
    "#     model_dict['abs_enc'] = abs_dec\n",
    "#     model_dict['abs_dec'] = abs_dec\n",
    "\n",
    "#     data_dir = join(os.path.expanduser('~'), 'cs671-large')\n",
    "#     for name, model in model_dict.items():\n",
    "#         torch.save(model.state_dict(), join(data_dir, name + '_epoch_%2i' % ((n+1)*10)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import modules.texts as texts\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BeamTree : \n",
    "    def __init__(self, log_prob, word_idx, parent_node = None) : \n",
    "        self.log_prob = log_prob\n",
    "        self.word_idx = word_idx\n",
    "        self.parent_node = parent_node\n",
    "        # log probs from the root (all log_probs added)\n",
    "        if parent_node is None : self.total_log_prob = 0\n",
    "        else : self.total_log_prob = parent_node.log_prob + log_prob\n",
    "        # whether this node is EOS\n",
    "        self.is_done = False\n",
    "\n",
    "    def __repr__(self) : \n",
    "        return '[%d, %.2f, %.2f, %s]' % (self.word_idx, self.log_prob, self.total_log_prob, self.is_done)\n",
    "    \n",
    "# select topk(beam_size) nodes for each batch\n",
    "def select_topk(beam_size, tmp_beam_batch):\n",
    "    beam_batch = []\n",
    "    for tmp_beam in tmp_beam_batch:\n",
    "        beam = []\n",
    "        # firstly select previously EOS node\n",
    "        for node in tmp_beam: \n",
    "            if node.is_done: beam.append(node)\n",
    "        \n",
    "        # remove selected node from tmp_beam_batch\n",
    "        for node in beam: tmp_beam.remove(node)\n",
    "\n",
    "        # sort by total log probability\n",
    "        sorted_tmp_beam = sorted(tmp_beam, key = lambda BeamTree : -BeamTree.total_log_prob)\n",
    "        for beam_idx in range(beam_size - len(beam)) : \n",
    "            beam.append(sorted_tmp_beam[beam_idx])\n",
    "            if sorted_tmp_beam[beam_idx].word_idx == texts.EOS_token : sorted_tmp_beam[beam_idx].is_done = True\n",
    "        beam_batch.append(beam)\n",
    "    \n",
    "    return beam_batch\n",
    "\n",
    "def check_EOS(beam_batch):\n",
    "    '''\n",
    "    Checks if all nodes in the beam_batch are EOS\n",
    "    '''\n",
    "    for beam in beam_batch:\n",
    "        for node in beam:\n",
    "            if not node.is_done: return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def beam2seq(beam_batch):\n",
    "    seqs_batch = []\n",
    "    top1_batch = []\n",
    "    for beam in beam_batch:\n",
    "        # sort each beam\n",
    "        sorted_beam = sorted(beam, key = lambda BeamTree: -BeamTree.total_log_prob)\n",
    "        # get strings from node\n",
    "        seqs = []\n",
    "        for node in sorted_beam:\n",
    "            seqs.append([node2seq(node), node.total_log_prob])\n",
    "        seqs_batch.append(seqs)\n",
    "        top1_batch.append(seqs[0])\n",
    "        \n",
    "    return top1_batch, seqs_batch\n",
    "\n",
    "def node2seq(leaf):\n",
    "    '''\n",
    "    Get a \n",
    "    '''\n",
    "    seq = []\n",
    "    cur = leaf\n",
    "    while True:\n",
    "        seq.append(cur.word_idx)\n",
    "        cur = cur.parent_node\n",
    "        if cur.word_idx == texts.SOS_token: break\n",
    "    seq.reverse()\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    1\n",
      " 6200\n",
      " 1800\n",
      "  629\n",
      " 2079\n",
      "  458\n",
      " 4402\n",
      " 6717\n",
      " 6788\n",
      "  335\n",
      " 1391\n",
      " 6199\n",
      "  750\n",
      " 6797\n",
      " 5880\n",
      " 6282\n",
      " 6259\n",
      " 5027\n",
      " 4402\n",
      " 3840\n",
      " 1464\n",
      "  495\n",
      " 6200\n",
      " 4433\n",
      " 6382\n",
      "  335\n",
      "  857\n",
      " 2693\n",
      " 2264\n",
      "  629\n",
      " 6689\n",
      " 5340\n",
      "  670\n",
      " 4402\n",
      " 5773\n",
      " 3933\n",
      "   46\n",
      " 6200\n",
      " 6478\n",
      " 6182\n",
      "  315\n",
      " 5421\n",
      "  458\n",
      " 6689\n",
      " 2646\n",
      " 6282\n",
      " 5300\n",
      " 6789\n",
      "  191\n",
      " 3078\n",
      " 4376\n",
      " 3360\n",
      " 6200\n",
      " 1391\n",
      " 6199\n",
      " 3802\n",
      " 3361\n",
      " 1543\n",
      " 2643\n",
      " 3365\n",
      " 5880\n",
      " 6282\n",
      " 1282\n",
      " 5028\n",
      "  774\n",
      " 6809\n",
      "   46\n",
      "    2\n",
      "    1\n",
      "  315\n",
      " 3361\n",
      " 2945\n",
      "  951\n",
      " 1842\n",
      " 6282\n",
      " 1282\n",
      " 6200\n",
      " 2541\n",
      " 4376\n",
      " 5028\n",
      " 4402\n",
      "  801\n",
      "  274\n",
      "   46\n",
      "    2\n",
      "    1\n",
      " 3194\n",
      " 6229\n",
      " 1579\n",
      "   40\n",
      "  563\n",
      " 4387\n",
      "   61\n",
      " 1911\n",
      " 5628\n",
      " 5191\n",
      " 4412\n",
      "  629\n",
      " 1282\n",
      " 6200\n",
      " 2541\n",
      " 1469\n",
      "  774\n",
      "  335\n",
      " 6025\n",
      " 6267\n",
      " 6743\n",
      "  563\n",
      " 6200\n",
      " 3604\n",
      " 5880\n",
      " 5628\n",
      " 6259\n",
      " 5027\n",
      " 6282\n",
      " 6206\n",
      " 1464\n",
      "   42\n",
      "    2\n",
      "    1\n",
      " 6706\n",
      " 5671\n",
      " 3824\n",
      " 6200\n",
      " 4876\n",
      " 4376\n",
      "  335\n",
      " 6809\n",
      "  315\n",
      "    2\n",
      "    1\n",
      "  619\n",
      " 4388\n",
      " 6289\n",
      " 3022\n",
      " 6268\n",
      " 2182\n",
      " 3194\n",
      " 6200\n",
      " 1817\n",
      "   46\n",
      "    2\n",
      "    1\n",
      " 5639\n",
      " 5146\n",
      " 6282\n",
      "  933\n",
      " 3138\n",
      "   46\n",
      " 6200\n",
      " 4324\n",
      " 6689\n",
      " 3358\n",
      " 4402\n",
      " 1800\n",
      "  629\n",
      " 2079\n",
      "  460\n",
      "  629\n",
      " 2661\n",
      " 2877\n",
      " 3048\n",
      " 4005\n",
      " 4784\n",
      " 3572\n",
      " 4569\n",
      "  315\n",
      " 5421\n",
      " 2017\n",
      "   40\n",
      " 5812\n",
      " 5436\n",
      "   46\n",
      " 5028\n",
      "   40\n",
      "  335\n",
      " 1284\n",
      " 4376\n",
      " 6200\n",
      " 1086\n",
      "  986\n",
      " 1157\n",
      "  629\n",
      " 5727\n",
      "   40\n",
      " 3345\n",
      " 4404\n",
      " 4376\n",
      " 5605\n",
      " 3021\n",
      " 2543\n",
      "  629\n",
      " 5347\n",
      " 6199\n",
      "  714\n",
      " 4296\n",
      " 3761\n",
      " 1563\n",
      " 4376\n",
      " 4851\n",
      "   40\n",
      " 2476\n",
      "  483\n",
      " 1187\n",
      " 2953\n",
      "  946\n",
      " 6306\n",
      " 6282\n",
      " 4960\n",
      " 4728\n",
      "  550\n",
      " 3142\n",
      "   46\n",
      " 3194\n",
      "  165\n",
      "   40\n",
      " 6200\n",
      " 6843\n",
      " 1038\n",
      " 5952\n",
      " 6282\n",
      " 4773\n",
      "  774\n",
      " 6200\n",
      " 1295\n",
      "   40\n",
      " 5053\n",
      " 6083\n",
      " 5465\n",
      "   24\n",
      " 5397\n",
      "   26\n",
      " 1359\n",
      " 4061\n",
      "  991\n",
      " 5436\n",
      " 6200\n",
      " 2541\n",
      " 2895\n",
      "  315\n",
      " 4205\n",
      " 5700\n",
      "  315\n",
      "    2\n",
      "    1\n",
      "  629\n",
      " 5671\n",
      "  933\n",
      " 1283\n",
      " 6760\n",
      "  315\n",
      " 6282\n",
      " 4913\n",
      " 3021\n",
      " 1769\n",
      "  629\n",
      " 3736\n",
      " 1192\n",
      " 6200\n",
      " 6547\n",
      " 2295\n",
      " 3194\n",
      " 3361\n",
      "  320\n",
      "    2\n",
      "[torch.cuda.LongTensor of size 265x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      "    1  2162  4043  3192  6035\n",
      "[torch.cuda.LongTensor of size 1x5 (GPU 0)]\n",
      "\n",
      "[[[1, -0.00, -0.00, False], [2162, -10.13, -10.13, False], [4043, -10.36, -10.36, False], [3192, -10.69, -10.69, False], [6035, -10.87, -10.87, False]]]\n",
      "Lengths of the generated [5]\n",
      "_BEGIN_\n",
      "_BEGIN_ daman & diu revokes mandatory rakshabandhan in offices order _END_\n",
      "0.16666666513888892\n",
      "[[[[1], -0.028980255126953125], [[6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035, 6035], -0.21643447875976562], [[3192, 3192, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744], -0.8643798828125], [[3192, 3192, 3192, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744], -1.765472412109375], [[3192, 3192, 3192, 3192, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744, 5744], -3.04685115814209]]]\n"
     ]
    }
   ],
   "source": [
    "def generate_title(doc_sents, beam_size, max_kernel_size, models, max_target_length = 100, batch_size = 1):\n",
    "    '''\n",
    "    Args:\n",
    "        doc_sents: list of torch.LongTensors, where each elements can\n",
    "        have variable length.\n",
    "        beam_size (int)\n",
    "        models (list): encoders and decoders\n",
    "        max_kernel_size (int): maximum kernel size of the CNN sentence encoder\n",
    "        max_target_length (int): maximum length that a sentence can have.\n",
    "    '''\n",
    "    assert len(models) == 6\n",
    "    \n",
    "    ext_s_enc = models[0]\n",
    "    ext_d_enc = models[1]\n",
    "    ext_extc = models[2]\n",
    "    ext_d_classifier = models[3]\n",
    "    abs_enc = models[4]\n",
    "    abs_dec = models[5]\n",
    "    \n",
    "    # Encode the sentences in a document\n",
    "    if len(doc_sents) <= 1:\n",
    "        print('Error: The length of the document is %i.' % 1)\n",
    "        return\n",
    "    \n",
    "    sents_raw = []\n",
    "    sents_encoded = []\n",
    "    for sent in doc_sents:\n",
    "        if sent.size(1) < max_kernel_size:\n",
    "            continue\n",
    "        sent = Variable(sent).cuda()\n",
    "        sents_raw.append(sent)\n",
    "        sents_encoded.append(ext_s_enc(sent))\n",
    "    \n",
    "    # Build the document representation using encoded sentences\n",
    "    d_encoded = torch.cat(sents_encoded, dim = 0).unsqueeze(1)\n",
    "    init_sent = ext_s_enc.init_sent(batch_size)\n",
    "    d_ext = torch.cat([init_sent, d_encoded[:-1]], dim = 0)\n",
    "    \n",
    "    # Extractive Summarizer\n",
    "    ## Initialize the d_encoder\n",
    "    h, c = ext_d_enc.init_h0c0(batch_size)\n",
    "    h0 = Variable(h.data)\n",
    "    ## An input goes through the document encoder\n",
    "    output, hn, cn = ext_d_enc(d_ext, h, c)\n",
    "    ## Initialize the decoder\n",
    "    ### calculate p0, h_bar0, c_bar0\n",
    "    h_ = hn.squeeze(0)\n",
    "    c_ = cn.squeeze(0)\n",
    "    p = ext_extc.init_p(h0.squeeze(0), h_)\n",
    "    ### calculate p_t, h_bar_t, c_bar_t\n",
    "    d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "    extract_probs = Variable(torch.zeros(len(sents_encoded))).cuda()\n",
    "    for i, (s, h) in enumerate(zip(sents_encoded, d_encoder_hiddens)):\n",
    "        h_, c_, p = ext_extc(s, h, h_, c_, p)\n",
    "        extract_probs[i] = p.squeeze(0)\n",
    "    ## Document Classifier\n",
    "    q = ext_d_classifier(extract_probs.view(-1,1), d_encoded.squeeze(1))\n",
    "    \n",
    "    # Abstractive Summarizer\n",
    "    sents_ext = [sent for i,sent in enumerate(sents_raw)\n",
    "                 if extract_probs[i].data[0] > 0.5]\n",
    "#     print(sents_ext)\n",
    "    ## skip if no sentences are selected as summaries\n",
    "    if len(sents_ext) == 0:\n",
    "        print(\"No sentences are selected\")\n",
    "        return\n",
    "    words = torch.cat(sents_ext, dim=1).t()\n",
    "    print(words)\n",
    "    abs_enc_hidden = abs_enc.init_hidden(batch_size)\n",
    "    abs_enc_output, abs_enc_hidden = abs_enc(words, abs_enc_hidden)\n",
    "    ## Remove to too long documents to tackle memory overflow\n",
    "    if len(abs_enc_output) > 6000:\n",
    "        print('Out of memory')\n",
    "        return\n",
    "    abs_dec_hidden = abs_dec.init_hidden(batch_size)\n",
    "    abs_dec_input = Variable(torch.LongTensor([texts.SOS_token]).unsqueeze(1)).cuda()\n",
    "    \n",
    "    beam_batch = [[] for i in range(batch_size)]\n",
    "    \n",
    "    for t in range(max_target_length):\n",
    "        if t == 0:\n",
    "            print(abs_dec_input)\n",
    "            abs_dec_output, abs_dec_hidden, _ = abs_dec(abs_dec_input, abs_dec_hidden, abs_enc_output)\n",
    "            # (B = 1, V = vocab_size)\n",
    "            abs_dec_prob = F.log_softmax(abs_dec_output)\n",
    "            # print(abs_dec_prob.size())\n",
    "            # Get top-k(beam size) values\n",
    "            top_values, top_idxs = abs_dec_prob.data.topk(beam_size, dim = -1)\n",
    "#             print(top_values)\n",
    "            print(top_idxs)\n",
    "            for batch_idx in range(batch_size):\n",
    "                log_prob = 0 # p = 1\n",
    "                root = BeamTree(log_prob, texts.SOS_token)\n",
    "                for beam_idx in range(beam_size):\n",
    "                    log_prob = top_values[batch_idx][beam_idx]\n",
    "                    word_idx = top_idxs[batch_idx][beam_idx]\n",
    "                    beam_batch[batch_idx].append(BeamTree(log_prob, word_idx, root))\n",
    "            print(beam_batch)\n",
    "        else:\n",
    "            tmp_beam_batch = [[] for i in range(batch_size)]\n",
    "            for beam_idx in range(beam_size): \n",
    "                abs_dec_input = []\n",
    "                for batch_idx in range(batch_size): \n",
    "                    # decoder inputs are words in current beam\n",
    "                    abs_dec_input.append(beam_batch[batch_idx][beam_idx].word_idx)\n",
    "                \n",
    "                # Regard each beams as seperate batches\n",
    "                # (1, beam_size)\n",
    "                abs_dec_input = Variable(torch.LongTensor(abs_dec_input).view(1,-1)).cuda()\n",
    "                abs_dec_output, abs_dec_hidden, _ = abs_dec(abs_dec_input, abs_dec_hidden, abs_enc_output)\n",
    "                # (B, V)\n",
    "                \n",
    "                abs_dec_prob = F.log_softmax(abs_dec_output)\n",
    "\n",
    "                # get top k(beam size) values\n",
    "                top_values, top_idxs = abs_dec_prob.data.topk(beam_size, dim = -1)\n",
    "#                 print(top_idxs)\n",
    "                for batch_idx in range(batch_size):\n",
    "                    for tmp_beam_idx in range(beam_size): \n",
    "                        # if current word is EOS, add it to tmp_beam_batch instead of its children\n",
    "                        if beam_batch[batch_idx][beam_idx].word_idx == texts.EOS_token: \n",
    "                            tmp_beam_batch[batch_idx].append(beam_batch[batch_idx][beam_idx])\n",
    "                            break\n",
    "                        log_prob = top_values[batch_idx][tmp_beam_idx]\n",
    "                        word_idx = top_idxs[batch_idx][tmp_beam_idx]\n",
    "                        tmp_beam_batch[batch_idx].append(BeamTree(log_prob, word_idx, beam_batch[batch_idx][beam_idx]))\n",
    "\n",
    "            # get the new beam\n",
    "            beam_batch = select_topk(beam_size, tmp_beam_batch)\n",
    "            # check if all nodes in the beam_batch are EOS\n",
    "            if check_EOS(beam_batch): break\n",
    "    \n",
    "    top1_batch, seqs_batch = beam2seq(beam_batch)\n",
    "    \n",
    "    return top1_batch, seqs_batch\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "doc_i = 0\n",
    "models = [ext_s_enc, ext_d_enc, ext_extc, ext_d_classifier, abs_enc, abs_dec]\n",
    "test_input = [torch.LongTensor(sent).view(1,-1) for sent in docs[doc_i].sents]\n",
    "ref_input = torch.LongTensor(docs[doc_i].head).view(1,-1)\n",
    "top1_batch, seqs_batch = generate_title(doc_sents = test_input,\n",
    "                                           beam_size = 5,\n",
    "                                           models = models,\n",
    "                                           max_kernel_size = 5)\n",
    "\n",
    "print('Lengths of the generated %s' % str(list(map(len, seqs_batch))))\n",
    "\n",
    "rouge = Rouge()\n",
    "total_rouge = 0\n",
    "for i in range(len(top1_batch)):\n",
    "    generated = vocab.id2sents([top1_batch[i][0]])\n",
    "    reference = vocab.id2sents([ref_input[i]])\n",
    "    print(generated)\n",
    "    print(reference)\n",
    "    total_rouge += rouge.get_scores(generated, reference)[0]['rouge-1']['f']\n",
    "\n",
    "print(total_rouge / len(top1_batch))\n",
    "print(seqs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of the generated [5]\n",
      "_BEGIN_ food regulator planning leftover banks to feed hungry people _END_\n",
      "0.16666666513888892\n",
      "[[[[1], -0.00043487548828125], [[1], -0.00043487548828125], [[1], -0.00043487548828125], [[1], -0.00043487548828125], [[2162], -10.48104190826416]]]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "doc_i = 10\n",
    "models = [ext_s_enc, ext_d_enc, ext_extc, ext_d_classifier, abs_enc, abs_dec]\n",
    "test_input = [torch.LongTensor(sent).view(1,-1) for sent in docs[doc_i].sents]\n",
    "ref_input = torch.LongTensor(docs[doc_i].head).view(1,-1)\n",
    "top1_batch, seqs_batch = generate_title(doc_sents = test_input,\n",
    "                                           beam_size = 5,\n",
    "                                           models = models,\n",
    "                                           max_kernel_size = 5)\n",
    "\n",
    "print('Lengths of the generated %s' % str(list(map(len, seqs_batch))))\n",
    "\n",
    "rouge = Rouge()\n",
    "total_rouge = 0\n",
    "for i in range(len(top1_batch)):\n",
    "    generated = vocab.id2sents([top1_batch[i][0]])\n",
    "    reference = vocab.id2sents([ref_input[i]])\n",
    "    print(reference)\n",
    "    total_rouge += rouge.get_scores(generated, reference)[0]['rouge-1']['f']\n",
    "\n",
    "print(total_rouge / len(top1_batch))\n",
    "print(seqs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the word embeddings.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9dfad64228d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %2i finished.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m#     model_dict = dict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-9dfad64228d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(docs, n_epochs, print_every)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mext_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdclass_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mext_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdclass_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdclass_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-9dfad64228d5>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mepoch_loss_abs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_abs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mloss_abs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs671/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs671/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Test\n",
    "# from copy import deepcopy\n",
    "# from torch import optim\n",
    "# import time\n",
    "# from itertools import chain\n",
    "\n",
    "# vocab_size = vocab.V\n",
    "# emb_size = emb.weight.data.size(1)\n",
    "# n_kernels = 50\n",
    "# kernel_sizes = [1,2,3,4,5]\n",
    "# pretrained = emb\n",
    "# sent_size = len(kernel_sizes) * n_kernels\n",
    "# hidden_size = 400\n",
    "# num_layers = 1\n",
    "# n_classes = len(docs.dclass2id)\n",
    "# batch_size = 1\n",
    "# torch.manual_seed(7)\n",
    "# torch.cuda.manual_seed(7)\n",
    "\n",
    "# init_emb(emb, vocab)\n",
    "# ext_s_enc = ext.SentenceEncoder(vocab_size, emb_size,\n",
    "#                                    n_kernels, kernel_sizes, pretrained)\n",
    "# ext_d_enc = ext.DocumentEncoder(sent_size, hidden_size)\n",
    "# ext_extc = ext.ExtractorCell(sent_size, hid\\den_size)\n",
    "# ext_d_classifier = ext.DocumentClassifier(sent_size, n_classes)\n",
    "# abs_enc = abs.EncoderRNN(emb, hidden_size, num_layers)\n",
    "# abs_dec = abs.AttnDecoderRNN(emb, hidden_size * 2, num_layers)\n",
    "\n",
    "# models = [ext_s_enc, ext_d_enc, ext_extc, ext_d_classifier,\n",
    "#          abs_enc, abs_dec]\n",
    "# params = list(chain(*[model.parameters() for model in models]))\n",
    "# optimizer = optim.SGD(params, lr = .005)\n",
    "\n",
    "# loss_fn_ext = nn.BCELoss()\n",
    "# loss_fn_dclass = nn.NLLLoss()\n",
    "# loss_fn_abs = nn.CrossEntropyLoss()\n",
    "\n",
    "# def get_accuracy(probs, targets, verbose = False):   \n",
    "#     '''\n",
    "#     Calculates the accuracy for the extractor\n",
    "\n",
    "#     Args:\n",
    "#         probs: extraction probability\n",
    "#         targets: ground truth labels for extraction\n",
    "#     '''\n",
    "#     import numpy as np\n",
    "#     preds = probs > .5\n",
    "#     if verbose:\n",
    "#         print(preds)\n",
    "#     matches = preds.type(torch.cuda.FloatTensor) == targets\n",
    "#     accuracy = torch.mean(matches.type(torch.cuda.FloatTensor))\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "    \n",
    "# def run_epoch(docs):\n",
    "    \n",
    "#     epoch_loss_abs = 0\n",
    "#     epoch_loss_ext = 0\n",
    "#     epoch_loss_dclass = 0\n",
    "#     epoch_accuracy_ext = 0\n",
    "\n",
    "#     for doc in docs:\n",
    "#         optimizer.zero_grad()\n",
    "#         docloader = DataLoader(doc, batch_size=1, shuffle=False)\n",
    "#         # Encode the sentences in a document\n",
    "#         sents_raw = []\n",
    "#         sents_encoded = []\n",
    "#         ext_labels = []\n",
    "#         doc_class = Variable(torch.LongTensor([doc.doc_class])).cuda()\n",
    "#         for sent, ext_label in docloader:\n",
    "#             # only accept sentences that conforms the maximum kernel sizes\n",
    "#             if sent.size(1) < max(kernel_sizes):\n",
    "#                 continue\n",
    "#             sent = Variable(sent).cuda()\n",
    "#             sents_raw.append(sent)\n",
    "#             sents_encoded.append(ext_s_enc(sent))\n",
    "#             ext_labels.append(ext_label.cuda())\n",
    "#         # Ignore if the content is a single sentence(no need to train)\n",
    "#         if len(sents_raw) <= 1:\n",
    "#             continue\n",
    "\n",
    "#         # Build the document representation using encoded sentences\n",
    "#         d_encoded = torch.cat(sents_encoded, dim = 0).unsqueeze(1)\n",
    "#         ext_labels = Variable(torch.cat(ext_labels, dim = 0).type(torch.FloatTensor).view(-1)).cuda()\n",
    "#         init_sent = ext_s_enc.init_sent(batch_size)\n",
    "#         d_ext = torch.cat([init_sent, d_encoded[:-1]], dim = 0)\n",
    "\n",
    "#         # Extractive Summarizer\n",
    "#         ## Initialize the d_encoder\n",
    "#         h, c = ext_d_enc.init_h0c0(batch_size)\n",
    "#         h0 = Variable(h.data)\n",
    "#         ## An input goes through the document encoder\n",
    "#         output, hn, cn = ext_d_enc(d_ext, h, c)\n",
    "#         ## Initialize the decoder\n",
    "#         ### calculate p0, h_bar0, c_bar0\n",
    "#         h_ = hn.squeeze(0)\n",
    "#         c_ = cn.squeeze(0)\n",
    "#         p = ext_extc.init_p(h0.squeeze(0), h_)\n",
    "#         ### calculate p_t, h_bar_t, c_bar_t\n",
    "#         d_encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "#         extract_probs = Variable(torch.zeros(len(sents_encoded))).cuda()\n",
    "#         for i, (s, h) in enumerate(zip(sents_encoded, d_encoder_hiddens)):\n",
    "#             h_, c_, p = ext_extc(s, h, h_, c_, p)\n",
    "#             extract_probs[i] = p\n",
    "#         ## Document Classifier\n",
    "#         q = ext_d_classifier(extract_probs.view(-1,1), d_encoded.squeeze(1))\n",
    "        \n",
    "#         ## Optimize over the extractive examples\n",
    "#         loss_ext = loss_fn_ext(extract_probs, ext_labels)\n",
    "#         loss_dclass = loss_fn_dclass(q.view(1,-1), doc_class)\n",
    "#         epoch_loss_ext += loss_ext.data\n",
    "#         epoch_loss_dclass += loss_dclass.data\n",
    "#         torch.autograd.backward([loss_ext, loss_dclass])\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         ## Measure the accuracy\n",
    "# #         q_cpu = q.data\n",
    "# #         c_cpu = doc_class.data\n",
    "#         epoch_accuracy_ext += get_accuracy(extract_probs.data, ext_labels.data)\n",
    "\n",
    "#         # Abstractive Summarizer\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_abs = 0\n",
    "#         ## Run through the encoder\n",
    "# #         words = torch.cat(sents_ext, dim=1).t()\n",
    "#         sents_ext = [sent for i,sent in enumerate(sents_raw)\n",
    "#                      if extract_probs[i].data[0] > 0.5]\n",
    "        \n",
    "#         # skip if no sentences are selected as summaries\n",
    "#         if len(sents_ext) == 0:\n",
    "#             continue\n",
    "#         words = torch.cat(sents_ext, dim=1).t()\n",
    "\n",
    "#         abs_enc_hidden = abs_enc.init_hidden(batch_size)\n",
    "#         abs_enc_output, abs_enc_hidden = abs_enc(words, abs_enc_hidden)\n",
    "#         ## Remove to too long documents to tackle memory overflow\n",
    "#         if len(abs_enc_output) > 6000:\n",
    "#             continue\n",
    "#         ## Run through the decoder\n",
    "#         abs_dec_hidden = abs_dec.init_hidden(batch_size)\n",
    "#         for target in doc.head:\n",
    "#             target = Variable(torch.LongTensor([target]).unsqueeze(1)).cuda()\n",
    "#             abs_dec_output, abs_dec_hidden, attn_weights = abs_dec(target, abs_dec_hidden, abs_enc_output)\n",
    "#             loss_abs += loss_fn_abs(abs_dec_output, target.squeeze(1))\n",
    "\n",
    "#         epoch_loss_abs += loss_abs.data\n",
    "#         loss_abs.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     acc_ext = epoch_accuracy_ext / len(docs)\n",
    "    \n",
    "#     return epoch_loss_ext, epoch_loss_dclass, epoch_loss_abs, acc_ext\n",
    "\n",
    "# def train(docs, n_epochs = 10, print_every = 1):\n",
    "#     import time\n",
    "    \n",
    "#     for epoch in range(n_epochs):\n",
    "#         start_time = time.time()\n",
    "#         ext_loss, dclass_loss, abs_loss, ext_acc = run_epoch(docs)\n",
    "#         ext_loss = ext_loss.cpu().numpy()[0]\n",
    "#         dclass_loss = dclass_loss.cpu().numpy()[0]\n",
    "#         abs_loss = abs_loss.cpu().numpy()[0]\n",
    "#         end_time = time.time()\n",
    "#         wall_clock = (end_time - start_time) / 60\n",
    "#         if epoch % print_every == 0:\n",
    "#             print('Epoch: %i',epoch)\n",
    "#             print('Extractive Loss: ',ext_loss)\n",
    "#             print('Classification Loss: ',dclass_loss)\n",
    "#             print('Abstractive Loss: ',abs_loss)\n",
    "#             print('Extractive Accuracy: ', ext_acc)\n",
    "#             print('Training Time: %.3f(min)' % wall_clock)\n",
    "\n",
    "# import os\n",
    "# from os.path import join            \n",
    "# # Training\n",
    "# for n in range(5):\n",
    "#     train(docs, n_epochs = 1, print_every = 1)\n",
    "#     print('Epoch %2i finished.' % ((n+1)*10))\n",
    "# #     model_dict = dict()\n",
    "# #     model_dict['emb'] = emb\n",
    "# #     model_dict['ext_s_enc'] = ext_s_enc\n",
    "# #     model_dict['ext_d_enc'] = ext_d_enc\n",
    "# #     model_dict['ext_extc'] = ext_extc\n",
    "# #     model_dict['ext_d_classifier'] = ext_d_classifier\n",
    "# #     model_dict['abs_enc'] = abs_enc\n",
    "# #     model_dict['abs_dec'] = abs_dec\n",
    "\n",
    "# #     data_dir = join(os.path.expanduser('~'), 'cs671-large')\n",
    "# #     for name, model in model_dict.items():\n",
    "# #         torch.save(model.state_dict(), join(data_dir, name + '_epoch_%2i' % ((n+1)*10)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
