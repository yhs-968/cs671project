{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents(variable-length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "from custom_utils.preprocess import Vocab\n",
    "with open('./data/dl_history.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab = Vocab(text, top_k = 50)\n",
    "\n",
    "print(vocab.V)\n",
    "\n",
    "sents = vocab.sents2id(text)\n",
    "# print(sents)\n",
    "# print(vocab.id2sents(sents))\n",
    "# print(vocab[0])\n",
    "# print(vocab[vocab[0]])\n",
    "\n",
    "onehot = vocab.sent2onehot(sents[0])\n",
    "# print(onehot.size())\n",
    "# print(vocab.onehot2sent(onehot))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Build the Training dataset\n",
    "input_docs = [vocab.sent2onehot(sent) for sent in sents]\n",
    "# Build inputs / targets as lists of tensors\n",
    "np.random.seed(0)\n",
    "target_docs = [np.random.randint(2, size=len(sent)).tolist()\n",
    "               for sent in input_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / Loss: 4.1718683\n",
      "Epoch: 10 / Loss: 2.2915561\n",
      "Epoch: 20 / Loss: 2.2757397\n",
      "Epoch: 30 / Loss: 0.5868411\n",
      "Epoch: 40 / Loss: 0.0598319\n",
      "Epoch: 50 / Loss: 0.0094373\n",
      "Epoch: 60 / Loss: 0.0044017\n",
      "Epoch: 70 / Loss: 0.0027340\n",
      "Epoch: 80 / Loss: 0.0018196\n",
      "Epoch: 90 / Loss: 0.0013290\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from modules.layers import DocumentEncoder, ExtractorCell\n",
    "\n",
    "input_size = vocab.V\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "####WARNING: No mini-batch processing#########\n",
    "\n",
    "encoder = DocumentEncoder(input_size, hidden_size)\n",
    "extc = ExtractorCell(input_size, hidden_size)\n",
    "\n",
    "# Binary Cross-Entropy loss\n",
    "loss_fn = nn.BCELoss()\n",
    "params = list(encoder.parameters()) + list(extc.parameters())\n",
    "optimizer = optim.Adam(params, lr = .005)\n",
    "\n",
    "def run_epoch(input_docs, target_docs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Train over the whole document\n",
    "    for input, target in zip(input_docs, target_docs):\n",
    "        # flush the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = Variable(input).view(input.size(0),1,input.size(1)).cuda()\n",
    "        target = Variable(torch.FloatTensor(target)).cuda()\n",
    "\n",
    "        # Initialize the encoder\n",
    "        h, c = encoder.init_h0c0(batch_size)\n",
    "        h0 = Variable(h.data)\n",
    "\n",
    "        # An input goes through the encoder\n",
    "        output, hn, cn = encoder(input, h, c)\n",
    "\n",
    "        # Initialize the decoder\n",
    "        ## calculate p0, h_bar0, c_bar0\n",
    "        h_ = hn.squeeze(0)\n",
    "        c_ = cn.squeeze(0)\n",
    "        p = extc.init_p(h0.squeeze(0), h_)\n",
    "\n",
    "        ## calculate p_t, h_bar_t, c_bar_t\n",
    "        encoder_hiddens = torch.cat((h0, output[:-1]), 0) #h0 ~ h_{n-1}\n",
    "        extract_probs = Variable(torch.zeros(input.size(0))).cuda()\n",
    "        for i, (s, h) in enumerate(zip(input, encoder_hiddens)):\n",
    "            h_, c_, p = extc(s, h, h_, c_, p)\n",
    "            extract_probs[i] = p\n",
    "        loss = loss_fn(extract_probs, target)\n",
    "        epoch_loss += loss.data.cpu().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def train(input_docs, target_docs, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = run_epoch(input_docs, target_docs)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, epoch_loss))\n",
    "        \n",
    "# Initial Training\n",
    "train(input_docs, target_docs, n_epochs = 100, print_every = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
