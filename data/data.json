{"A00-1020": {"title": ["Multilingual Coreference Resolution"], "abstract": ["In this paper we present a new multi lingual datadriven method for coreference resolution as implemented in the SWIZZLE system", "The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts outperformed coreference resolution in each of the indi vidual languages"], "inroduction": ["The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing", "Most of the research has focused on bilingual terminology identification either as par allel multiwords forms eg the Champollion sys tem Smadja et al1996 technical terminology eg the Termight system Dagan and Church 1994 or broadcoverage translation lexicons eg the SABLE system Resnik and Melamed 1997", "In addition the Multilingual Entity Task MET from the TIP STER program1 httpwwwnlpirnistgovjrelated projectsjtipsterjmethtm challenged the partici pants in the Message Understanding Conference MUC to extract named entities across several for eign language corpora such as Chinese Japanese and Spanish", "In this paper we present a new application of aligned multilingual texts", "Since coreference reso lution is a pervasive discourse phenomenon causing performance impediments in current IE systems we considered a corpus of aligned English and Roma nian texts to identify coreferring expressions", "Our task focused on the same kind of coreference as considered in the past MUC competitions namely 1The TIPSTER Text Program was a DARPAled government effort to advance the state of the art in text processing technologies", "Steven J Maiorano IPO Washington DC 20505 maiorano caiscom the identity coreference", "Identity coreference links nouns pronouns and noun phrases including proper names to their corresponding antecedents", "We created our bilingual collection by translating the MUC6 and MUC7 coreference training texts into Romanian using native speakers", "The train ing data set for Romanian coreference used wher ever possible the same coreference identifiers as the English data and incorporated additional tags as needed", "Our claim is that by adding the wealth of coreferential features provided by multilingual data new powerful heuristics for coreference resolu tion can be developed that outperform monolingual coreference resolution systems", "For both languages we resolved coreference by using SWIZZLE our implementation of a bilingual coreference resolver", "SWIZZLE is a multilingual en hancement of COCKTAIL Harabagiu and Maiorano 1999 a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information2  When COCKTAIL was applied separately on the English and the Ro manian texts coreferring links were identified for each English and Romanian document respectively", "When aligned referential expressions corefer with nonaligned anaphors SWIZZLE derived new heuris tics for coreference", "Our experiments show that SWIZZLE outperformed COCKTAIL on both English and Romanian test documents", "The rest of the paper is organized as follows", "Sec tion 2 presents COCKTAIL a monolingual coreference resolution system used separately on both the En glish and Romanian texts", "Section 3 details the datadriven approach used in SWIZZLE and presents some of its resources", "Section 4 reports and discusses the experimental results", "Section 5 summarizes the 2 The name of COCKTAIL is a pun on CogNIAC be cause COCKTAIL combines a larger number of heuristics than those reported in Baldwin 1997", "SWIZZLE more over adds new heuristics discovered from the bilingual aligned corpus", "conclusions"]}, "A00-1024": {"title": ["Categorizing Unknown Words Using Decision Trees to Identify"], "abstract": ["This paper introduces a system for categorizing un known words", "The system is based on a multi component architecture where each component is re sponsible for identifying one class of unknown words", "The focus of this paper is the components that iden tify names and spelling errors", "Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word", "The sys tem is evaluated using data from live closed captions  a genre replete with a wide variety of unknown words"], "inroduction": ["In any real world use a Natural Language Process ing NLP system will encounter words that are not in its lexicon what we term unknown words", "Un known words are problematic because a NLP system will perform well only if it recognizes the words that it is meant to analyze or translate the more words a system does not recognize the more the systems per formance will degrade", "Even when unknown words are infrequent they can have a disproportionate ef fect on system quality", "For example Min 1996 found that while only 06 of words in 300 emails were misspelled this meant that 12 of the sen tences contained an error discussed in Min and Wilson 1998", "Words may be unknown for many reasons the word may be a proper name a misspelling an ab breviation a number a morphological variant of a known word eg receared  or missing from the dictionary", "The first step in dealing with unknown words is to identify the class of the unknown word whether it is a misspelling a proper name an ab breviation etc Once this is known the proper ac tion can be taken misspellings can be corrected ab breviations can be expanded and so on as deemed necessary by the particular text processing applica tion", "In this paper we introduce a system for cat egorizing unknown words", "The system is based on a multi component architecture where each compo nent is responsible for identifying one category of unknown words", "The main focus of this paper is the components that identify names and spelling errors", "Both components use a decision tree architecture to combine multiple types of evidence about the un known word", "Results from the two components are combined using a weighted voting procedure", "The system is evaluated using data from live closed cap tions  a genre replete with a wide variety of un known words", "This paper is organized as follows", "In section 2 we outline the overall architecture of the unknown word categorizer", "The name identifier and the mis spelling identifier are introduced in section 3", "Perfor mance and evaluation issues are discussed in section 4", "Section 5 considers portability issues", "Section 6", "compares the current system with relevant preced ing research", "Concluding comments can be found in section 6"]}, "A00-2019": {"title": ["An Unsupervised Method for Detecting Grammatical Errors"], "abstract": ["We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora", "The system was developed and tested using essaylength responses to prompts on the Test of English as a Foreign Language TOEFL", "The error recognition system ALEK performs with about 80 precision and 20 recall"], "inroduction": ["A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence Miller and Gildea 1987", "Much information about usage can be obtained from quite a limited context Choueka and Lusignan 1985 found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it", "Statisticallybased computer programs have been able to do the same with a high level of accuracy Kilgarriff and Palmer 2000", "The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word", "We have developed a statistical system ALEK Assessing Lexical Knowledge that uses statistical analysis for this purpose", "A major objective of this research is to avoid the laborious and costly process of collecting errors or negative evidence for each word that we wish to evaluate", "Instead we train ALEK on a general corpus of English and on edited text containing example uses of the target word", "The system identifies inappropriate usage based on differences between the words local context cues in an essay and the models of context it has derived from the corpora of wellformed sentences", "Claudia Leacock Educational Testing Service Rosedale Road Princeton NJ cleacocketsorg A requirement for ALEK has been that all steps in the process be automated beyond choosing the words to be tested and assessing the results", "Once a target word is chosen preprocessing building a model ofthe words appropriate usage and identifying usage errors in essays is performed without manual intervention", "ALEK has been developed using the Test of English as a Foreign Language TOEFL administered by the Educational Testing Service", "TOEFL is taken by foreign students who are applying to US undergraduate and graduatelevel programs", "1 Background", "Approaches to detecting errors by nonnative writers typically produce grammars that look for specific expected error types Schneider and McCoy 1998 Park Palmer and Washburn 1997", "Under this approach essays written by ESL students are collected and examined for errors", "Parsers are then adapted to identify those error types that were found in the essay collection", "We take a different approach initially viewing error detection as an extension of the word sense disambiguation WSD problem", "Corpusbased WSD systems identify the intended sense of a polysemous word by 1 collecting a set of example sentences for each of its various senses and 2 extracting salient contextual cues from these sets to 3 build a statistical model for each sense", "They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model eg Leacock Chodorow and Miller 1998 Yarowsky 1993", "Golding 1995 showed how methods used for WSD decision lists and Bayesian classifiers could be adapted to detect errors resulting from common spelling confusions among sets such as there their and theyre", "He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context", "However most grammatical errors are not the result of simple word confusions", "This complicates the task of building a model of incorrect usage", "One approach we considered was to proceed without such a model represent appropriate word usage across senses in a single model and compare a novel example to that model", "The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck", "All occurrences ofthe word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage", "Inappropriate usage would be signaled by contextual cues that do not occur in training", "Unfortunately this approach was not effective for error detection", "An example of a word usage error is often very similar to the model of appropriate usage", "An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element", "The problem of error detection does not entail finding similarities to appropriate usage rather it requires identifYing one element among the contextual cues that simply does not fit"]}, "A00-2032": {"title": ["MostlyUnsupervised Statistical Segmentation of Japanese Applications to Kanji Rie Kubota Ando and Lillian Lee Department of Computer Science Cornell University Ithaca NY 148537501 kubotar llee cscornelledu Abstract Given the lack of word delimiters in written Japanese word segmentation is generally considered a crucial first step in processing Japanese texts Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on presegmented data In contrast we introduce a novel statistical method utilizing unsegmented training data with performance on kanji sequences comparable to and sometimes surpassing that of morphological nalyzers over a variety of error metrics I Introduction Because Japanese is written without delimiters between words accurate word segmentation to recover the lexical items is a key step in Japanese text processing Proposed applications of segmentation technology include extracting new technical terms indexing documents for information retrieval and correcting optical character recognition OCR errors Wu and Tseng 1993 Nagao and Mori 1994 Nagata 1996a Nagata 1996b Sproat et al 1996 Fung 1998 Typically Japanese word segmentation is performed by morphological nalysis based on lexical and grammatical knowledge This analysis is aided by the fact that there are three types of Japanese characters kanji hiragana and katakana changes in character type often indicate word boundaries although using this heuristic alone achieves less than 60 accuracy Nagata 1997 Character sequences consisting solely of kanji pose a challenge to morphologicallybased segreenters for several reasons First and most importantly kanji sequences often contain domain terms and proper nouns Fung 1998 notes that 5085 of the terms in various technical dictioThe analogous situation i English would be if words were written without spaces between them Sequence l ngth  of characters  of corpus 1  3 kanji 20405486 256 4  6 kanji 12743177 161 more than 6 kanji 3966408 51 Total 37115071 468 Figure 1 Statistics from 1993 Japanese newswire NIKKEI 79326406 characters total naries are composed at least partly of kanji Such words tend to be missing from general purpose lexicons causing an unknown word problem for morphological nalyzers yet these terms are quite important for information retrieval information extraction and text summarization making correct segmentation f these terms critical Second kanji sequences often consist of compound nouns so grammatical constraints are not applicable For instance the sequence shachohlkenlgyohmulbuchoh presidentlandlbusinesslgeneral m nager  a president as well as a general manager of business could be incorrectly segmented as shachohlkengyohlmulbuchoh presidentl subsidiary businessTsutomu a namegeneral manager since both alternatives are fournoun sequences they cannot be distinguished by partofspeech information alone Finally heuristics based on changes in character type obviously do not apply to kanjionly sequences Although kanji sequences are difficult to segment they can comprise a significant portion of Japanese text as shown in Figure 1 Since sequences of more than 3 kanji generally consist of more than one word at least 212 of 1993 Nikkei newswire consists of kanji sequences requiring segmentation Thus accuracy on kanji sequences i an important aspect of the total segmentation process As an alternative to lexico grammatical and supervised approaches we propose a simple effi241 cient segmentation method which learns mostly from very large amounts of unsegmented training data thus avoiding the costs of building a lexicon or grammar or hand segmenting large amounts of training data Some key advantages of this method are  No  Japanesespecific rules are employed en hancing portability to other languages  A very small number of presegmented training examples as few as 5 in our experiments are needed for good performance as long as large amounts of unsegmented data are available  For long kanji strings the method produces results rivalling those produced by Juman 361 Kurohashi and Nagao 1998 and Chasen 10 Matsumoto et al 1997 two morphological analyzers in widespread use For instance we achieve 5 higher word precision and 6 better morpheme r call 2 A lgor i thm Our algorithm employs counts of character ngrams in an unsegmented corpus to make segmentation de cisions We illustrate its use with an example see Figure 2 Let A B C D W X Y Z represent an eightkanji sequence To decide whether there should be a word boundary between D and W we check whether ngrams that are adjacent to the proposed boundary such as the 4grams l A B C D and 82 W X Y Z tend to be more frequent than ngrams that straddle it such as the 4gram tl  B C D W If so we have evidence of a word boundary between D and W since there seems to be relatively little cohesion between the characters on opposite sides of this gap The ngram orders used as evidence in the segmentation decision are specified by the set N For instance if N  4 in our example then we pose the six questions of the form Is s  t j where x  denotes the number of occurrences of x in the unsegmented training corpus If N  24 then two more questions Is C D  D W  and Is W X  O W are added More formally let s and 8 be the nonstraddling ngrams just to the left and right of location k respectively and let t be the straddling ngram with j characters to the right of location k s  I i ABCbWXYZ t    Figure 2 Collecting evidence for a word boundary"], "abstract": ["Given the lack of word delimiters in written Japanese word segmentation is generally consid ered a crucial first step in processing Japanese texts", "Typical Japanese segmentation algorithms rely ei ther on a lexicon and grammar or on presegmented data", "In contrast we introduce a novel statistical method utilizing unsegmentd training data with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyz ers over a variety of error metrics"], "inroduction": ["Because Japanese is written without delimiters be tween words 1 accurate word segmentation to re cover the lexical items is a key step in Japanese text processing", "Proposed applications of segmentation technology include extracting new technical terms indexing documents for information retrieval and correcting optical character recognition OCR er rors Wu and Tseng 1993 Nagao and Mori 1994 Nagata 1996a Nagata 1996b Sproat et al 1996 Fung 1998", "Typically Japanese word segmentation is per formed by morphological analysis based on lexical and grammatical knowledge", "This analysis is aided by the fact that there are three types of Japanese characters kanji hiragana and katakana changes in character type often indicate word boundaries al though using this heuristic alone achieves less than 60 accuracy Nagata 1997", "Character sequences consisting solely of kanji pose a challenge to morphologicallybased seg menters for several reasons", "First and most importantly kanji sequences often contain domain terms and proper nouns Fung 1998 notes that5085 of the terms in various technical dictio 1The analogous situation in English would be if words were written without spaces between them", "Figure 1 Statistics from 1993 Japanese newswire NIKKEI 79326406 characters total", "naries are composed at least partly of kanji", "Such words tend to be missing from generalpurpose lexicons causing an unknown word problem for morphological analyzers yet these terms are quite important for information retrieval information extraction and text summarization making correct segmentation of these terms critical", "Second kanji sequences often consist of compound nouns so grammatical constraints are not applicable", "For instance the sequence shachohjkenjgyohmujbu choh presidentjandjbusinessjgeneral manager  a president as well as a general manager of business could be incorrectly segmented as sha chohjkengyohjmujbuchoh presidentjsubsidiary businessTsutomu a namejgeneral manager since both alternatives are fournoun sequences they cannot be distinguished by partofspeech information alone", "Finally heuristics based on changes in character type obviously do not apply to kanjionly sequences", "Although kanji sequences are difficult to seg ment they can comprise a significant portion of Japanese text as shown in Figure 1", "Since se quences of more than 3 kanji generally consist of more than one word at least 212 of 1993 Nikkei newswire consists of kanji sequences requiring seg mentation", "Thus accuracy on kanji sequences is an important aspect of the total segmentation process", "As an alternative to lexicagrammatical and supervised approaches we propose a simple effi cient segmentation method which learns mostly from very large amounts of unsegmented training data thus avoiding the costs of building a lexicon or grammar or handsegmenting large amounts of training data", "Some key advantages of this method are  No Japanesespecific rules are employed en hancing portability to other languages", " A very small number of presegmented train ing examples as few as 5 in our experiments are needed for good performance as long as large amounts of unsegmented data are avail able", " For long kanji strings the method produces re sults rivalling those produced by Juman 361 Kurohashi and Nagao 1998 and Chasen 10 Matsumoto et al 1997 two morphological analyzers in widespread use", "For instance we achieve 5 higher word precision and 6 bet ter morpheme recall", "Figure 2 Collecting evidence for a word boundary  are the nonstraddling ngrams 8 1 and 82 more frequent than the straddling ngrams t 1 t2 and t3", "Let I y z be an indicator function that is 1 when y  z and 0 otherwise2 In order to compensate for the fact that there are more ngram questions than n 1gram questions we calculate the fraction of affirmative answers separately for each n in N 2 n1"]}, "A00-2034": {"title": ["Using  Semantic Preferences to  Identify Verbal  Participation  in"], "abstract": ["We propose a method for identifying diathesis alter nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms", "The method uses selectional pref erences acquired as probability distributions over WordNet", "Preferences for the target slots are com pared using a measure of distributional similarity", "The method is evaluated on the causative and cona tive alternations but is generally applicable and does not require a priori knowledge specific to the alternation"], "inroduction": ["Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically", "The syntactic changes are sometimes accompanied by slight changes in the meaning of the verb", "An ex ample of the causative alternation is given in 1 be low", "In this alternation the object of the transitive variant can also appear as the subject of the intransi tive variant", "In the conative alternation the transi tive form alternates with a prepositional phrase con struction involving either at or on", "An example of the conative alternation is given in 2", "1", "The boy broke the window ", "The window", "broke"]}, "A92-1018": {"title": ["A Practical PartofSpeech Tagger"], "abstract": ["We present an implementation of a partofspeech tagger based on a hidden Markov model", "The methodology enables robust and accurate tagging with few resource requirements", "Only a lexicon and some unlabeled training text are required", "Accuracy exceeds 96", "We describe implementation strategies and optimizations which result in highspeed operation", "Three applications for tagging are described phrase recognition word sense disambiguation and grammatical function assignment", "1 Desiderata Many words are ambiguous in their part of speech", "For example tag can be a noun or a verb", "However when a word appears in the context of other words the ambiguity is often reduced in  tag is a partofspeech label the word tag can only be a noun", "A partofspeech tagger is a system that uses context to assign parts of speech to words", "Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora", "Partofspeech information facilitates higherlevel analysis such as recognizing noun phrases and other patterns in text", "For a tagger to function as a practical component in a language processing system we believe that a tagger must be Robust Text corpora contain ungrammatical constructions isolated phrases such as titles and non linguistic data such as tables", "Corpora are also likely to contain words that are unknown to the tagger", "It is desirable that a tagger deal gracefully with these situations", "Efficient If a tagger is to be used to analyze arbitrarily large corpora it must be efficientperforming in time linear in the number of words tagged", "Any training required should also be fast enabling rapid turnaround with new corpora and new text genres", "Accurate A tagger should attempt to assign the correct partofspeech tag to every word encountered", "Tunab le A tagger should be able to take advantage of linguistic insights", "One should be able to correct systematic errors by supplying appropriate a priori hints", "It should be possible to give different hints for different corpora", "Reusable The effort required to retarget a tagger to new corpora new tagsets and new languages should be minimal"], "inroduction": ["Several different approaches have been used for building text taggers", "Greene and Rubin used a rulebased approach in the TAGGIT program Greene and Rubin 1971 which was an aid in tagging the Brown corpus Francis and Kuera 1982", "TAGGIT disambiguated 77 of the corpus the rest was done manually over a period of several years", "More recently Koskenniemi also used a rulebased approach implemented with finitestate machines Koskenniemi 1990", "Statistical methods have also been used eg DeRose 1988 Garside et al 1987", "These provide the capability of resolving ambiguity on the basis of most likely interpretation", "A form of Markov model has been widely used that assumes that a word depends probabilistically on just its partofspeech category which in turn depends solely on the categories of the preceding two words", "Two types of training ie parameter estimation have been used with this model", "The first makes use of a tagged training corpus", "Derouault and Merialdo use a bootstrap method for training Derouault and Merialdo 1986", "At first a relatively small amount of text is manually tagged and used to train a partially accurate model", "The model is then used to tag more text and the tags are manually corrected and then used to retrain the model", "Church uses the tagged Brown corpus for training Church 1988", "These models involve probabilities for each word in the lexicon so large tagged corpora are required for reliable estimation", "The second method of training does not require a tagged training corpus", "In this situation the BaumWelch algorithm also known as the forwardbackward algorithm can be used Baum 1972", "Under this regime the model is called a hidden Markov model HMM as state transitions ie partofspeech categories are assumed to be unobservable", "Jelinek has used this method for training a text tagger Jelinek 1985", "Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from second and firstorder models and a uniform probability distribution Jelinek and Mercer 1980", "Kupiec used word equivalence classes referred to here as ambiguity classes based on parts of speech to pool data from individual words Kupiec 1989b", "The most common words are still represented individually as sufficient data exist for robust estimation", "133 However all other words are represented according to the set of possible categories they can assume", "In this manner the vocabulary of 50000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes Kupiec 1992", "To further reduce the number of parameters a firstorder model can be employed this assumes that a words category depends only on the immediately preceding words category", "In Kupiec 1989a networks are used to selectively augment the context in a basic first order model rather than using uniformly secondorder dependencies", "22 Our approach", "We next describe how our choice of techniques satisfies the criteria listed in section 1", "The use of an HMM permits complete flexibility in the choice of training corpora", "Text from any desired domain can be used and a tagger can be tailored for use with a particular text database by training on a portion of that database", "Lexicons containing alternative tag sets can be easily accommodated without any need for relabeling the training corpus affording further flexibility in the use of specialized tags", "As the resources required are simply a lexicon and a suitably large sample of ordinary text taggers can be built with minimal effort even for other languages such as French eg Kupiec 1992", "The use of ambiguity classes and a firstorder model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section 5", "This also enables a tagger to be reliably trained using only moderate amounts of text", "We have produced reasonable results training on as few as 3000 sentences", "Fewer parameters also reduce the time required for training", "Relatively few ambiguity classes are sufficient for wide coverage so it is unlikely that adding new words to the lexicon requires retraining as their ambiguity classes are already accommodated", "Vocabulary independence is achieved by predicting categories for words not in the lexicon using both context and suffix information", "Probabilities corresponding to category sequences that never occurred in the training data are assigned small nonzero values ensuring that the model will accept any sequence of tokens while still providing the most likely tagging", "By using the fact that words are typically associated with only a few partof speech categories and carefully ordering the computation the algorithms have linear complexity section 33", "3 H idden Markov Mode l ing The hidden Markov modeling component of our tagger is implemented as an independent module following the specification given in Levinson et al 1983 with special attention to space and time efficiency issues", "Only firstorder modeling is addressed and will be presumed for the remainder of this discussion", "31 Formalism", "In brief an HMM is a doubly stochastic process that generates sequence of symbols S   Si S2ST Si E W I iT where W is some finite set of possible symbols by composing an underlying Markov process with a statedependent symbol generator ie a Markov process with noise i Th Markov process captures the notion of sequence depen dency and is described by a set of N states a matrix c transition probabilities A  aij 1  i j  N where ai is the probability of moving from state i to state j and vector of initial probabilities H  rq 1  i  N where is the probability of starting in state i The symbol ger erator is a statedependent measure on V described by matrix of symbol probabilities B  bjk 1  j  N an 1  k  M where M  IWI and bjk is the probability generating symbol s given that the Markov process is i state j2 In partofspeech tagging we will model word order d pendency through an underlying Markov process that ot crates in terms of lexical tags yet we will only be ab to observe the sets of tags or ambiguity classes that aJ possible for individual words", "The ambiguity class of eac word is the set of its permitted parts of speech only or of which is correct in context", "Given the parameters A and H hidden Markov modeling allows us to compute tt most probable sequence of state transitions and hence tt mostly likely sequence of lexical tags corresponding to sequence of ambiguity classes", "In the following N can identified with the number of possibletags and W wit the set of all ambiguity classes", "Applying an HMM consists of two tasks estimating tt model parameters A B and H from a training set ar computing the most likely sequence of underlying sta transitions given new observations", "Maximum likeliho estimates that is estimates that maximize the probabili of the training set can be found through application of ternating expectation in a procedure known as the Baur Welch or forwardbackward algorithm Baum 1972", "proceeds by recursively defining two sets of probabiliti the forward probabilities atiJ  atiaiii bSti 1 t Tl   where at i   ribiSi for all i and the backward prob bilities N  Ti  t  i   j i where 3Tj  1 for all j The forward probabili ati is the joint probability of the sequence up to tir t Si S2   ", "St  and the event that the Markov pr cess is in state i at time t Similarly the backwa probability 3tj is the probability of seeing the sequen Sti St2 ", " ST given that the Markov process is state i at time t It follows that the probability of t entire sequence is N N P  E E tiaibJSti3tij imi j i for any t in the range lt T 1a iFor an introduction to hidden Markov modeling see l biner and Juang 1986", "2In the following we will write hiSt  for bjk if St  s", "3This is most conveniently evaluated at t  T  1 in whi ca e P  134 Given an initial choice for the parameters A B and II the expected number of transitions 7ij from state i to state j conditioned on the observation sequence S may be computed as follows T1 1 7ij  fi E atiaijbjStltlj", "t l Hence we can estimate aij by  ET 5i  N Ejl 7ij ETI 1 atiti Similarly bj and 7ri can be estimated as follows bjk  Ets   atjtj ETI atjZt j and 3 4 1 i Ot1i1i", "5 In summary to find maximum likelihood estimates for A B and II via the BaumWelch algorithm one chooses some starting values applies equations 35 to compute new values and then iterates until convergence", "It can be shown that this algorithm will converge although possibly to a nonglobal maximum Baum 1972", "Once a model has been estimated selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S An efficient dynamic programming procedure known as the Viterbi algorithm Viterbi 1967 arranges for this computation to proceed in time proportional to T Suppose V  vt 1  t  T is a state sequence that generates S then the probability that V generates S is T Pv  ub1S1 H at1tbtSt", "t2  To find the most probable such sequence we start by defining 1i  ribS1 for 1  i  N and then perform the recursion et j  axtliaijbjSt 6 and Ctj  maxtCt l  i  IiN for 2  t  T and i  j  N The crucial observation isthatfor each time t and each state i one need only consider the most probable sequence arriving at state i at time t The probability of the most probable sequence is maxliNTi while the sequence itself can be reconstructed by defining vT  maxllig eTi and vtI  et q t  for T  t  2", "32 Numer ica l Stab i l i ty", "The BaumWelch algorithm equations 15 and the Viterbi algorithm equation 6 involve operations on products of numbers constrained to be between 0 and 1", "Since these products can easily underflow measures must be taken to rescale", "One approach premultiplies the a and 13 probabilities with an accumulating product depending on t Levin son et al 1983", "Let 51i  ali and define ct  5t i ltT Now define ti  ctKti and use a in place of a in equation 1 to define  for the next iteration 5tl j   tiaij bjStl ltT1  Note that E inl ti  1 for 1  t  T Similarly let Ti  Ti and define 3ti  ctti for T  t  1 where N ti  E aiJ bjStl3tlj jl Tltl  The scaled backward and forward probabilities 5 and  can be exchanged for the unscaled probabilities in equations 35 without affecting the value of the ratios", "To see this note that ati  Cati and ti  tiCl where J CHct  Now in terms of the scaled probabilities equation 5 for example can be seen to be unchanged  1 ifl i  ENI aTi El CTaTi  i", "A slight difficulty occurs in equation 3 that can be cured by the addition of a new term ctl in each product of the upper sum T1     tl atzaijbjSt tlJCtl  ETll t it i  aj Numerical instability in the Viterbi algorithm can be ameliorated by operating on a logarithmic scale Levinson et al 1983", "That is one maximizes the log probability of each sequence of state transitions logPv   logb 1Sl  T E logat 1t  logbtSt", "t2 Hence equation 6 is replaced by etJ  max t1i  logao  logbjSt", "I iN Care must be taken with zero probabilities", "However this can be elegantly handled through the use of IEEE negative infinity P754 1981", "135 33 Reduc ing T ime Complex i ty", "As can be seen from equations 15 the time cost of training is OTN", "Similarly as given in equation 6 the Viterbi algorithm is also OTN2", "However in partofspeech tagging the problem structure dictates that the matrix of symbol probabilities B is sparsely populated", "That is bij 3 0 iff the ambiguity class corresponding to symbol j includes the partofspeech tag associated with state i In practice the degree of overlap between ambiguity classes is relatively low some tokens are assigned unique tags and hence have only one nonzero symbol probability", "The sparseness of B leads one to consider restructuring equations 16 so a check for zero symbol probability can obviate the need for further computation", "Equation 1 is already conveniently factored so that the dependence on bjStl is outside the inner sum", "Hence ifk is the average number of nonzero entries in each row of B the cost of computing equation 1 can be reduced to OkTN", "Equations 24 can be similarly reduced by switching the order of iteration", "For example in equation 2 rather than for a given t computing3ti for each i one at a time one can accumulate terms for all i in parallel", "The net effect of this rewriting is to place a bjStl  0 check outside the innermost iteration", "Equations 3 and 4 submit to a similar approach", "Equation 5 is already only ON", "Hence the overall cost of training can be reduced to OkTN which in our experience amounts to an order of magnitude speed upfl The time complexity of the Viterbi algorithm can also be reduced to OkTN by noting that bjSt can be factored out of the maximization of equation 6", "34 Controlling Space Complexity Adding up the sizes of the probabil ity matrices A B and H it is easy to see that the storage cost for directly representing one model is proportional to NN t M  1", "Running the BaumWelch algorithm requires storage for the sequence of observations the a and 3 probabilities the vector ci and copies of the A and B matrices since the originals cannot be overwritten until the end of each iteration", "Hence the grand total of space required for training is proportional to T q 2NT q N  M  1", "Since N and M are fixed by the model the only parameter that can be varied to reduce storage costs is T Now adequate training requires processing from tens of thousands to hundreds of thousands of tokens Kupiec 1989a", "The training set can be considered one long sequence it which case T is very large indeed or it can be broken up into a number of smaller sequences at convenient boundaries", "In firstorder hidden Markov modeling the stochastic process effectively restarts at unambiguous tokens such as sentence and paragraph markers hence these tokens are convenient points at which to break the training set", "If the BaumWeleh algorithm is run separately from the same starting point on each piece the resulting trained models must be recombined in some way", "One obvious approach is simply to average", "However this fails if any two 4An equivalent approach maintains a mapping from states i to nonzero symbol probabilities and simply avoids in the inner iteration computing products which must be zero Kupiec 1992", "states are indistinguishable in the sense that they had the same transition probabilities and the same symbol probabilities at start because states are then not matched across trained models", "It is therefore important that each state have a distinguished role which is relatively easy to achieve in partofspeech tagging", "Our implementation of the BaumWelch algorithm breaks up the input into fixedsized pieces of training text", "The BaumWelch algorithm is then run separately on each piece and the results are averaged together", "Running the Viterbi algorithm requires storage for the sequence of observations a vector of current maxes a scratch array of the same size and a matrix of  indices for a total proportional to T  N2  T and a grand total including the model of T t NN H M  T  3", "Again N and M are fixed", "However T need not be longer than a single sentence since as was observed above the HMM and hence the Viterbi algorithm restarts at sentence boundaries", "35 Mode l Tun ing", "An HMM for partofspeech tagging can be tuned in a variety of ways", "First the choice of tagset and lexicon determines the initial model", "Second empirical and a priori information can influence the choice of starting values for the BaumWelch algorithm", "For example counting instances of ambiguity classes in running text allows one to assign nonuniform starting probabilities in A for a particular tags realization as a particular ambiguity class", "Alternatively one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags", "For example if an ambiguity class consisting of the open class tags is used for unknown words one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in B Another biasing of starting values can arises from noting that some tags are unlikely to be followed by others", "For example the lexical item to maps to an ambiguity class containing two tags infinitivemarker and toas preposition neither of which occurs in any other ambiguity class", "If nothing more were stated the HMM would have two states which were indistinguishable", "This can be remedied by setting the initial transition probabilities from infinitivemarker to strongly favor transitions to such states as verbuninflected and adverb", "Our implementation allows for two sorts of biasing of starting values ambiguity classes can be annotated with favored tags and states can be annotated with favored transitions", "These biases may be specified either as sets or as set complements", "Biases are implemented by replacing the disfavored probabilities with a small constant machine epsilon and redistributing mass to the other possibilities", "This has the effect of disfavoring the indicated outcomes without disallowing them sufficient converse data can rehabilitate these values", "4 Arch i tec ture", "In support of this and other work we have developed a system architecture for text access Cutting et al 1991", "This architecture defines five components for such systems 136 Search Index Analysis Corpus 4    ", "     oO oo further analysis  stem tag l  Tagging  Training    t ra inedHMM ambiguityelassstemtag ambigu i tyc lass Lexicon token l Tokenizer  character ooQ            ", "  Figure 1 Tagger Modules in System Context corpus which provides text in a generic manner analysis which extracts terms from the text index which stores term occurrence statistics and search which utilizes these statistics to resolve queries", "The partofspeech tagger described here is implemented as an analysis module", "Figure 1 illustrates the overall architecture showing the tagger analysis implementation in detail", "The tagger itself has a modular architecture isolating behind standard protocols those elements which may vary enabling easy substitution of alternate implementations", "Also illustrated here are the data types which flow between tagger components", "As an analysis implementation the tagger must generate terms from text", "In this context a term is a word stem annotated with part of speech", "Text enters the analysis subsystem where the first processing module it encounters is the tokenizer whose duty is to convert text a sequence of characters into a sequence of tokens", "Sentence boundaries are also identified by the tokenizer and are passed as reserved tokens", "The tokenizer subsequently passes tokens to the lexicon", "Here tokens are converted into a set of stems each annotated with a partofspeech tag", "The set of tags identifies an ambiguity class", "The identification of these classes is also the responsibility of the lexicon", "Thus the lexicon delivers a set of stems paired with tags and an ambiguity c lass  The training module takes long sequences of ambiguity classes as input", "It uses the BaumWelch algorithm to produce a trained HMM an input to the tagging module", "Training is typically performed on a sample of the corpus at hand with the trained HMM being saved for subsequent use on the corpus at large", "The tagging module buffers sequences of ambiguity classes between sentence boundaries", "These sequences are disambiguated by computing the maximal path through the HMM with the Viterbi algorithm", "Operating at sentence granularity provides fast throughput without loss of accuracy as sentence boundaries are unambiguous", "The resulting sequence of tags is used to select the appropriate stems", "Pairs of stems and tags are subsequently emitted", "The tagger may function as a complete analysis component providing tagged text to search and indexing components or as a subsystem of a more elaborate analysis such as phrase recognition", "41 Token izer Implementation The problem of tokenization has been well addressed by much work in compilation of programming languages", "The accepted approach is to specify token classes with regular expressions", "These may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens Aho et al 1986 Lesk 1975", "In the context of tagging we require at least two token classes sentence boundary and word", "Other classes may include numbers paragraph boundaries and various sorts of punctuation eg braces of various types commas", "However for simplicity we will henceforth assume only words and sentence boundaries are extracted", "Just as with programming languages with text it is not always possible to unambiguously specify the required token classes with regular expressions", "However the addition of a simple lookahead mechanism which allows specification of right context ameliorates this Aho et al 1986 Lesk 1975", "For example a sentence boundary in English text might be identified by a period followed by white space followed by an uppercase letter", "However the up 137 percase letter must not be consumed as it is the first component of the next token", "A lookahead mechanism allows us to specify in the sentenceboundary regular expression that the final character matched should not be considered a part of the token", "This method meets our stated goals for the overall system", "It is efficient requiring that each character be examined only once modulo lookahead", "It is easily parameter izable providing the expressive power to concisely define accurate and robust token classes", "42 Lex icon Imp lementat ion", "The lexicon module is responsible for enumerating parts of speech and their associated stems for each word it is given", "For the English word does the lexicon might return do verb and doe pluralnoun", "It is also responsible for identifying ambiguity classes based upon sets of tags", "We have employed a threestage implementation First we consult a manuallyconstructed lexicon to find stems and parts of speech", "Exhaustive lexicons of this sort are expensive if not impossible to produce", "Fortunately a small set of words accounts for the vast majority of word occurences", "Thus high coverage can be obtained without prohibitive effort", "Words not found in the manually constructed lexicon are generally both open class and regularly inflected", "As a second stage a languagespecific method can be employed to guess ambiguity classes for unknown words", "For many languages eg English and French word suffixes provide strong cues to words possible categories", "Probabalistic predictions of a words category can be made by analyzing suffixes in untagged text Kupiec 1992 Meteer e al 1991", "As a final stage if a word is not in the manually constructed lexicon and its suffix is not recognized a default ambiguity class is used", "This class typically contains all the open class categories in the language", "Dictionaries and suffix tables are both efficiently implementable as letter trees or tries Knuth 1973 which require that each character of a word be examined only once during a lookup", "5 Per fo rmance", "In this section we detail how our tagger meets the desiderata that we outlined in section 1", "51 Efficient The system is implemented in Common Lisp Steele 1990", "All timings reported are for a Sun SPARCStation2", "The English lexicon used contains 38 tags M  38 and 174 ambiguity classes N  174", "Training was performed on 25000 words in articles selected randomly from Groliers Encyclopedia", "Five iterations of training were performed in a total time of 115 CPU seconds", "Following is a time breakdown by component Training average seconds per token tokenizer lexicon 1 iteration 5 iterations total 640 400 680 3400 4600 Tagging was performed on 115822 words in a collection of articles by the journalist Dave Barry", "This required a total of of 143 CPU seconds", "The time breakdown for this was as follows Tagging average seconds per token tokenizer lexicon Viterbi total 604 388 233 1235 It can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes and that tens of megabytes of text may then be tagged per hour", "52 Accurate and Robust", "When using a lexicon and tagset built from the tagged text of the Brown corpus Francis and Kuera 1982 training on one half of the corpus about 500000 words and tagging the other 96 of word instances were assigned the correct tag", "Eight iterations of training were used", "This level of accuracy is comparable to the best achieved by other taggers Church 1988 Merialdo 1991", "The Brown Corpus contains fragments and ungrammaticalities thus providing a good demonstration of robustness", "53 Tunable and Reusable", "A tagger should be tunable so that systematic tagging errors and anomalies can be addressed", "Similarly it is important that it be fast and easy to target the tagger to new genres and languages and to experiment with different tagsets reflecting different insights into the linguistic phenomena found in text", "In section 35 we describe how the HMM implementation itself supports tuning", "In addition our implementation supports a number of explicit parameters to facilitate tuning and reuse including specification of lexicon and training corpus", "There is also support for a flexible tagset", "For example if we want to collapse distinctions in the lexicon such as those between positive comparative and superlative adjectives we only have to make a small change in the mapping from lexicon to tagset", "Similarly if we wish to make finer grain distinctions than those available in the lexicon such as case marking on pronouns there is a simple way to note such exceptions", "6 Applications", "We have used the tagger in a number of applications", "Wc describe three applications here phrase recognition word sense disambiguation and grammatical function assignment", "These projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text", "61 Phrase Recognition", "We have constructed a system that recognizes simpl phrases when given as input the sequence of tags for a sentence", "There are recognizers for noun phrases verb groups adverbial phrases and prepositional phrases", "Each of thes phrases comprises a contiguous sequence of tags that satis", "ties a simple grammar", "For example a noun phrase can b a unary sequence containing a pronoun tag or an arbitrar", "ily long sequence of noun and adjective tags possibly pre", "ceded by a determiner tag and possibly with an embeddec possessive marker", "The longest possible sequence is fount eg the program committee but not the program 138 Conjunctions are not recognized as part of any phrase for example in the fragment the cats and dogs the cats and dogs will be recognized as two noun phrases", "Prepositional phrase attachment is not performed at this stage of processing", "This approach to phrase recognition in some cases captures only parts of some phrases however our approach minimizes false positives so that we can rely on the recognizers results", "62 Word Sense Disamblguatlon Partofspeech tagging in and of itself is a useful tool in lexical disambiguation for example knowing that dig is being used as a noun rather than as a verb indicates the words appropriate meaning", "But many words have multiple meanings even while occupying the same part of speech", "To this end the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm Hearst 1991", "The algorithm known as CatchWord performs supervised training over a large text corpus gathering lexical orthographic and simple syntactic evidence for each sense of the ambiguous noun", "After a period of training CatchWord classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found", "Because the sense distinctions made are coarse the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms", "Initial tests resulted in accuracies of around 90 for nouns with strongly distinct senses", "This algorithm uses the tagger in two ways i to determine the part of speech of the target word filtering out the nonnoun usages and ii as a step in the phrase recognition analysis of the context surrounding the noun", "63 Grammatical Function Assignment The phrase recognizers also provide input to a system Sopa Sibun 1991 which recognizes nominal arguments of verbs specifically Subject Object and Predicative Arguments", "Sopa does not rely on information such as arity or voice specific to the particular verbs involved", "The first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases", "The phrase types include those mentioned in section 61 additional types to account for conjunctions complementizers and indicators of sentence boundaries and an unknown type", "After a sentence has been partitioned each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right", "On the basis of this local context and a set of rules the noun phrase is marked as a syntactic Subject Object Predicative or is not marked at all", "A label of Predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb eg a form of be", "Because this cannot always be determined some Predicatives are labeled Objects", "If a noun phrase is labeled it is also annotated as to whether the governing verb is the closest verb group to the right or to the left", "The algorithm has an accuracy of approximately 800o in assigning grammatical functions"]}, "A97-1025": {"title": [""], "abstract": ["Contextual spelling errors are defined as the use of an incorrect though valid word in a particular sentence or context", "Tra ditional spelling checkers flag misspelled words but they do not typically attempt to identify words that are used incorrectly in a sentence", "We explore the use of Latent Se mantic Analysis for correcting these incor rectly used words and the results are com pared to earlier work based on a Bayesian classifier"], "inroduction": ["Spelling checkers are now available for all major word processing systems", "However these spelling checkers only catch errors that result in misspelled words", "If an error results in a different but incor rect word it will go undetected", "For example quite may easily be mistyped as quiet", "Another type of er ror occurs when a writer simply doesnt know which word of a set of homophones 1 or near homophones is the proper one for a particular context", "For ex ample the usage of affect and effect is commonly confused", "Though the cause is different for the two types of errors we can treat them similarly by examining the contexts in which they appear", "Consequently no effort is made to distinguish between the two er ror types and both are called contextual spelling er rors", "Kukich 1992a 1992b reports that 40 to 45 of observed spelling errors are contextual er rors", "Sets of words which are frequently misused or mistyped for one another are identified as confusion sets", "Thus from our earlier examples quiet quite and affect effect are two separate confusion sets", "In this paper we introduce Latent Semantic Anal ysis LSA as a method for correcting contextual spelling errors for a given collection of confusion sets", "1Homophones are words that sound the same but are spelled differently", "LSA was originally developed as a model for infor mation retrieval Dumais et al 1988 Deerwester et al 1990 but it has proven useful in other tasks too", "Some examples include an expert Expert lo cator Streeter and Lochbaum 1988 and a confer ence proceedings indexer Foltz 1995 which per forms better than a simple keywordbased index", "Recently LSA has been proposed as a theory of se mantic learning Landauer and Dumais In press", "Our motivation in using LSA was to test its effec tiveness at predicting words based on a given sen tence and to compare it to a Bayesian classifier", "LSA makes predictions by building a highdimensional semantic space which is used to compare the sim ilarity of the words from a confusion set to a given context", "The experimental results from LSA predic tion are then compared to both a baseline predic tor and a hybrid predictor based on trigrams and a Bayesian classifier"]}, "C00-1081": {"title": ["A Stochastic Parser Based on a Structural Word Prediction Model"], "abstract": ["In this paper we present a stochastic language model using dependency", "This model considers a sentence as a word sequence and predicts each wordfrom left to right", "The history at each step of prediction is a sequence of partial parse trees covering the preceding words", "First our model predicts the partial parse trees which have a dependency relation with the next word among them and then predictsthe next word from only the trees which have a dependency relation with the next word", "Our model is a generative stochastic model thus this can be used not only as a parser but also as a language modelof a speech recognizer", "In our experiment we prepared about 1000 syntactically annotated Japanese sentences extracted from a nancial newspaper and estimated the parameters of our model", "We built aparser based on our model and tested it on approximately 100 sentences of the same newspaper", "The accuracy of the dependency relation was 899 thehighest accuracy level obtained by Japanese stochastic parsers"], "inroduction": ["The stochastic language modeling imported from the speech recognition area is one of the successful methodologies of natural language processing", "In fact all language models for speech recognition are as far as we know based on an ngram model and most practical partofspeech POS taggers are alsobased on a word or POS ngram model or its exten sion Church 1988 Cutting et al 1992 Merialdo1994 Dermatas and Kokkinakis 1995", "POS tagging is the rst step of natural language process ing and stochastic taggers have solved this problem with satisfying accuracy for many applications", "The next step is parsing or that is to say discovering the structure of a given sentence", "Recently many parsers based on the stochastic approach have been proposed", "Although their reported accuracies arehigh they are not accurate enough for many appli cations at this stage and more attempts have to be made to improve them further", "One of the major applications of a parser is toparse the spoken text recognized by a speech rec ognizer", "This attempt is clearly aiming at spokenlanguage understanding", "If we consider how to com bine a parser and a speech recognizer it is better if the parser is based on a generative stochastic modelas required for the language model of a speech rec ognizer", "Here generative means that the sum of probabilities over all possible sentences is equal to or less than 1", "If the language model is generative it allows a seamless combination of the parser and the speech recognizer", "This means that the speech recognizer has the stochastic parser as its languagemodel and benets richer information than a normal ngram model", "Even though such a combination is not possible in practices  the recognizer out puts N best sentences with their probabilities and the parser taking them as input parses all of them and outputs the sentence with its parse tree thathas the highest probability of all possible combina tions", "As a result a parser based on a generativestochastic language model may help a speech rec ognizer to select the most syntactically reasonable sentence among candidates", "Therefore it is better if the language model of a parser is generativeIn this paper taking Japanese as the object lan guage we propose a generative stochastic language model and a parser based on it", "This model treats a sentence as a word sequence and predicts each wordfrom left to right", "The history at each step of predic tion is a sequence of partial parse trees covering the preceding words", "To predict a word our model rst predicts which of the partial parse trees at this stage have dependency relation with the word and then predicts the word from the selected partial parsetrees", "In Japanese each word depends on a subse quent word that is to say each dependency relationis left to right it is not necessary to predict the di rection of each dependency relation", "So in order to extend our model to other languages the model may have to predict the direction of each dependency", "Webuilt a parser based on this model whose parameters are estimated from 1072 sentences in a nan cial newspaper and tested it on 119 sentences fromthe same newspaper", "The accuracy of the depen dency relation was 899 the highest obtained by any Japanese stochastic parsers"]}, "C00-2095": {"title": ["A Formalism for Universal Segmentat ion of Text J u l ien  Qu in t GETACLIPS IMAG  BP 53 F38041 Grenoble Cedex 9 France Xerox Research Centre Europe 6 chemin de Maupertuis  F38240 Meylan France email ju l ien   quinciraagfr Abstract Sumo is a formalism for universal segmentation of text Its purpose is to provide a franlework for the creation of segmentation applications It is called universal as tile formalism itself is independent of the language of the documents to process and independent of the levels of segmentation eg words sentences paragraphs nlorphemes considered by the target application This framework relies on a layered structure representing the possible segmentations of the document This structure and the tools to manipulate it are described followed by detailed examples highlighting some features of Sumo Int roduct ion Tokenization or word segmentation is a fundamental task of ahnost all NLP systems In languages that use word separators in their writing tokenization seenls easy every sequence of characters between two whitespaces or punctuation marks is a word This works reasonably well but exceptions are handled in a cumbersome way On the other hand there are languages that do not use word separators A much nlore complicated processing is needed closer to morphological nalysis or partofspeech tagging Tokenizers designed for those languages are generally very tied to a given system and language Itowever the gap becomes maller when we look at sentence segmentation a simplistic ap proach would not be sufficient because of the ambiguity of punctuation signs And if we consider the segmentation of a document into higherlevel units such as paragraphs ections and so on we can notice that language becomes less relevant These observations lead to the definition of our formalism for segmentation  ot just tokenization that considers tile process independently flom the language By describing a segmentation systenl formally a clean distinction can be made between tile processing itself and tile linguistic data it uses This entails the ability to develop a truly multilingual system by using a common segmentation e gine br the various languages of the system conversely one can imagine evaluating several segmentation  ethods by using the same set of data with different strategies Sumo is the name of the proposed formalisnl evolving from initial work by Quint 1999 Quint 2000 Some theoretical works from the literature also support this approach Guo 1997 shows that sonle segmentation techniques can be generalized to any language regardless of their writing systenl The sentence segmenter of Pahner and Hearst 1997 and the issues raised by Habert et al 1998 prove that even in ln glish or French segmentation is not so trivial Lastly AtMokhtar 1997 handles all kinds of presyntactic processing in one step arguing that there are strong interactions between segnlentation and morphology 1 The  F ramework for  Segmentat ion"], "abstract": ["Sumo is a formalism for universal segmentation of text", "Its purpose is to provide a framework for the creation of segmentation applications", "It is called universal as the formalism itself is independent of the language of the documents to process and independent of the levels of seg mentation eg words sentences paragraphs morphemes", "considered by the target applica tion", "This framework relies on a layered struc ture representing the possible segmentations of the document", "This structure and the tools to manipulate it arc described followed by detailed examples highlighting some features of Sumo"], "inroduction": ["Tokenization or word segmentation is a fun damental task of almost all NLP systems", "In languages that use word separators in their writ ing tokenization seems easy every sequence of characters between two whitespaces or punctu ation marks is a word", "This works reasonably well but exceptions are handled in a cumber some way", "On the other hand there arc lan guages that do not use word separators", "A much more complicated processing is needed closer to morphological analysis or partofspeech tag ging", "Tokenizers designed for those languages arc generally very tied to a given system and language", "However the gap becomes smaller when we look at sentence segmentation a simplistic ap proach would not be sufficient because of the ambiguity of punctuation signs", "And if we consider the segmentation of a document into higherlevel units such as paragraphs sections and so on we can notice that language becomes less relevant", "These observations lead to the definition of our formalism for segmentation not just tok enization that considers the process indepen dently from the language", "By describing a seg mentation system formally a clean distinction can be made between the processing itself and the linguistic data it uses", "This entails the abil ity to develop a truly multilingual system by us ing a common segmentation engine for the vari ous languages of the system conversely one can imagine evaluating several segmentation meth ods by using the same set of data with different strategies", "Sumo is the name of the proposed formal ism evolving from initial work by Quint 1999 Quint 2000", "Some theoretical works from the literature also support this approach Guo 1997 shows that some segmentation techniques can be generalized to any language regardless of their writing system", "The sentence segmenter of Palmer and Hearst 1997 and the issues raisecl by Habert et a 1998 prove that even in En glish or French segmentation is not so trivial", "Lastly AltMokhtar 1997 handles all kinds of presyntactic processing in one step arguing that there are strong interactions between segmenta tion and morphology", "1 The Framework for Segmentation", "11 Overview", "fhe framework revolves around the document representation chosen for Sumo which is a layered structure each layer being a view of the document at a given level of segmentation", "These layers are introduced by the author of the segmentation application as needed and are not imposed by Sumo", "The example in section 31 uses a twolayer structure figure 4 correspond ing to two levels of segmentation characters and words", "To extend this to a sentence segmenter a third level for sentences is addedThese levels of segmentation can have a lin guistic or structural level but artificial levels can be introduced as well when needed", "lt is also interesting to note that several layers can belong to the same level", "Jn the example of section 33 the result structure can have an indefinite num ber of levels and all levels arc of the same kind", "We call item the segmentation unit of a doc ument at a given scgmcntttion level eg items of the word level are words", "The document is then representee", "at every segmentation level in terms of its items because segmentation is usu ally ambiguous item gmphs arc used to factorize all the possible segmentations", "Ambiguity issues anfurther addressed in section 23", "The main processing paradigms of Sumo arc identification ancltmnsfonnalion", "With identifi cation new item graphs are built by identifying items from a source graph using a segmentation resounc fhcse graphs arc then modified by transformation processes", "Section 2 gives the details about both identification and transfor mation", "12 Item Graphs", "The item graphs arc directed acyclic graphs they arc similar to the word graphs of Amtrup et al l99G or the string graphs of Colmcr auer 1970", "They arc actually represented by means of finitestate automata sec section 21", "11 order to facilitate tlwir manipulation two ad ditional properties arc enforced thcsatttomata always have a single startstate and finitestate and no dangling arcs this is verified by pruning the automata after modifications", "The exam ple of scctiou 3 sJow various item graphs", "An item is an arc in the automaton", "An arc is a complex structure containing a label gen erally the surface form of tile item named at tributes and relations", "Attributes arc used to hold information on the item like part of speech tags sec section 32", "These attributes can also be viewed as annotations in the same sense as the annotation graphs of Bird ct al 2000", "13 Relations Relations are links between levels", "Items from a given graph arc linked to items of the graph from which they were identified", "We call the first graph the lower graph and the graph that was the source for the identification the upper graph", "Relations exist bctvvcen a path in the upper graph and either a path or a subgraph in the lower graph", "Figure 1 illustrates the first kind of relation called palh relation", "This example in French is a relation between the two characters of the word clu which is really a contraction of de lc", "Figure 1 A patJ relation Figme 2 illustrates the other kind of relation called subgmph relation", "J n this example the sentence A BCDEFC ", "we can imagine that i through G arc Chinese characters is related to scvcral possible segmentations", "Figure 2 A graph relation The interested reader may refer to Planas 1998 for a comparable stntctunmultiple lay ers of a document and relations used in trans lation memory"]}, "C00-2118": {"title": ["Autotnatic Lexical Acquisition  Based on  Statistical Distributions"], "abstract": ["We automatically classify verbs into lexical se rnantic classes based on distributions of indiet tors or verb alternations extracted from a very large annotated corpus", "Ve address a problem which is particularly difficult because the verb classes although semantically diflerr nt how sim ilar surface syntactic behavior", "Five prantmatical feattJres arc suflicient to reduce error rate by more thw 50 over dance we achieve almost 70 accuracy in a task whose baseline performance is 11 and whosexpertbased upptr bound we cal culated at 8G51L vVe conclude that corpudriven extraction of grannuatical featurcs is a promiing methodology for fin grained verb classification"], "inroduction": ["Detailcd information about verbs is critical to a broad range of NLI and IR tasks yet its Jnau ual determination for large numbers of verbs is difficult and resourc intensive", "Hmearch 011 tit automatic acquisition of verbbas d krtowledg has succeded in pleaning syntactic proprtis of v rbs such as subcategorization frames from on line resources lhmt 1993 Briscoe and Carroll 1997 Dorr 1997 Manning 1993", "Recently researchers have investigated statistical corpus based methods for lexical semantic classification from syntactic properties of verb usage one and Ivlclcc 199G Lapata aud Brew 1999 Schulte i111 Walde 199H Stevenson and Ivlerlo 1999 Steven son ct al 1999 McCarthy 2000", "Corpusbased approaches to lexical semantic classification in particular have drawn on Levins hypothesis Levin 1993 that verbs can be classi fied according to the diathesis alternations alter nations in he syntactic expressions or arguments in which they participatefor example whether a  fhis research was partly sponsored hy US N W prauts070211 aml  li5J8J22 Swiss NSF fellowship 8210 1650 Information Sciences Cmmcil of Hutgers University and HCS U  of Pennsylvania", "This research was cml clncted while the first author was at Hutgers University", "Paola Merlo LATL  Department of Linguistics University of Geneva", "2 rue de Caudollc 1211 Gcncvc tlSuisse", "merlolettresunigech verb occurs in the dativeprepositional phrase al ternation in English", "One diagnostic for diathesis alternations is the subcategorization alternatives of a verb", "However some classes exhibit the same subcaegorization possibilities but difler in their argument structures ie the content of the the tnatic roles assigned to the arguments of the verb", "This type of situation constitutes a particularly difficult case for corpusbased classiltcation meth ods", "Jn this paper we apply corpusbased lexica", "acquisition tucthodology to distinguish classes of verbs which allow the sante su bcatcgorizations but difrer in thematic roles", "Ve first assume that one can atttontatically restrict the choice of classes to those that participate in the relevant su bcaic gorizations d", "Lapata", "and Brew ID99", "Our proposal is then to use statistics over diathesis alternants as a way to furthcr distinguish those verbs w h iclt allow tlw sante su bcat gorizations achivinp fin graincd classification within that St", "Our work focuses 011 dekrtnining the bestS  lllantic class for a verb lypr the set of usages of a verb across a document or corpusrather than for a single verb lokrn in a single local context", "In this way we can exploit the broad behavior of the verb across the corpus to determine its most likely class overall", "VVe investigate the proposed approach in an in depth case study of the three major classes of op tionally intransitive verbs in English utwrgative unaccusativc and objectdrop", "Nlore specifically according to Levins classification  lA vin 1993 the unergatives arc manner of motion verbs such as iump and march the unaccusatives arc verbs of change of state such as open and explode the objectdrop verbs are unexpressed object alterna tion verbs such as played and painlcd", "These classes all support both transitive and intransi tive subcategorizations but arc distinguished by the pattern of thematic role assignments to sub jed and object position", "Ve au tom atically clas sify these verbs on the basis of statistical ap proximations to syntactic indicators of the under lying argument structures using numerical fea tures collected from a large syntactically anno tated tagged or parsed corpus", "We apply ma chine learning techniques to determine whether the frequency distributions of the features in dividually or in combination support automatic classification of the verbs", "To preview our re sults we demonstrate that combining only fivc numerical indicators is sufficient to reduce the er ror rate in this classification task by more than 50 over chance", "Specifically we achieve almost 70 accuracy in a task whose baseline chance performance is 31 and whose expertbased up per bound is calculated at 865", "We conclude that a distributionbased method for lexical se mantic verb classification is a promising avenue of research"]}, "C00-2148": {"title": ["Integrating compositional sentantics into a verb lexicon"], "abstract": ["We present a classbased approach to building a verb lexicon that makes explicit the close asso ciation between syntax and semantics for Levin classes", "We have used Lexicalized Tree Adjoin ing Grammars to capture the syntax associated with each verb class and have augmented the trees to in clude sclcctional restrictions", "In addition semantic predicates arc associated with each tree which al low for a compositional interpretation"], "inroduction": ["The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within wellclefinccl subdomains", "Despite many different lexicon clcvclopmcnt ap proaches Melcuk 1988 Copestakc and Sanfil ippo 1993 Lowe et al 1997 the field has yet to develop a clear consensus on guidelines for a computational lexicon", "One of the most controver sial areas in building such a lexicon is polysemy how senses can be computationally distinguished and characterized", "We address this problem by em ploying compositional semantics and the adjunction of syntactic phrases to support regular verb sense extensions", "This differs from the Lexical Concep tual Structure LCS approach exemplified by Voss 1996 which requires a separate LCS representa tion for each possible sense extension", "In this pa per we describe the construction of VcrbNet a verb lexicon with explicitly stated syntactic and seman tic information for individual lexical items using Levin verb classes Levin 1993 to systematically construct lexical entries", "We use Lexicalizecl Tree Adjoining Grammar LTAG Joshi 1987 Schabes 1990 to capture the syntax for each verb class and associate semantic predicates with each tree", "Although similar ideas have been explored for verb sense extension Pustejovsky 1995 Goldberg 1995 our approach of applying LTAG to the prob lem of composing and extending verb senses is novel", "LTAGs have an extended domain of local ity that captures the arguments of a verb in a local manner", "The association of semantic predicates to a tree yields a complete semantics for the verb", "More over the operation of adjunction in LTAGs provides a mechanism for extending verb senses"]}, "C02-1027": {"title": ["Shallow language processing architecture  for Bulgarian"], "abstract": ["This paper describes LINGUA  an architecture for text processing in Bulgarian", "First the preprocessing modules for tokenisation sentence splitting paragraph segmentation part ofspeech tagging clause chunking and noun phrase extraction are outlined", "Next the paper proceeds to describe in more detail the anaphora resolution module", "Evaluation results are reported for each processing task"], "inroduction": ["The state of the art of todays full parsing and knowledgebased automatic analysis still falls short of providing a reliable processing framework for robust realworld applications such as automatic abstracting or information extraction", "The problem is especially acute for languages which do not benefit from a wide range of processing programs such as Bulgarian", "There have been various projects which address different aspects of the automatic analysis in Bulgarian such as morphological analysis Krushkov 1997 Simov et al 1992 morphological disambiguation Simov et al 1992 and parsing Avgustinova et al 1989 but no previous work has pursued the development of a knowledgepoor robust processing environment with a high level of component integrity", "This paper reports the development and implementation of a robust architecture for language processing in Bulgarian referred to as LINGUA which includes modules for POS tagging sentence splitting clause segmentation parsing and anaphora resolution", "Our text processing framework builds on the basis of considerably shallower linguistic analysis of the input thus trading off depth of interpretation for breadth of coverage and workable robust solution", "LINGUA uses knowledge poor heuristi cally based algorithms for language analysis in this way getting round the lack of resources for Bulgarian"]}, "C02-1033": {"title": [""], "abstract": ["We present in this paper a method for achieving in an integrated way two tasks of topic analysis segmentation and link detection", "This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches", "We report an evaluation of our method for segmentation on two corpora one in French and one in English and we propose an evaluation measure that specifically suits that kind of systems"], "inroduction": ["Topic analysis which aims at identifying the topics of a text delimiting their extend and finding the relations between the resulting segments has recently raised an important interest", "The largest part of it was dedicated to topic segmentation also called linear text segmentation and to the TDT Topic Detection and Tracking initiative Fiscus et al 1999 which addresses all the tasks we have mentioned but from a domaindependent viewpoint and not necessarily in an integrated way", "Systems that implement this work can be categorized according to what kind of knowledge they use", "Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts word distribution as in Hearst 1997 Choi 2000 and Utiyama and Isahara 2001 or linguistic cues as in Passonneau and Litman 1997", "They can be applied without restriction about domains but have low results when a text doesnt characterize its topical structure by surface clues", "Some systems exploit domainindependent knowl edge about lexical cohesion a network of words built from a dictionary in Kozima 1993 a large set of collocations collected from a corpus in Fer ret 1998 Kaufmann 1999 and Choi 2001", "To some extend this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains", "The last main type of systems relies on knowledge about the topics they may encounter in the texts they process", "This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts", "The work of Bigi Bigi et al 1998 stands in the same perspective but focuses on much larger topics than TDT", "These systems have a limited scope due to their topic representations but they are also more precise for the same reason", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination Jobbins and Evett 1998 combined word recurrence collocations and a thesaurus Beeferman et al 1999 relied on both collocations and linguistic cues", "The topic analysis we propose implements such a hybrid approach it relies on a general language resource a collocation network but exploits it together with word recurrence in texts", "Moreover it simultaneously achieves topic segmentation and link detection ie determining whether two segments discuss the same topic", "We detail in this paper the implementation of this analysis by the TOPICOLL system we reportevaluations of its capabilities concerning segmen tation for two languages French and English and finally we propose an evaluation measure that integrates both segmentation and link detection"]}, "C02-1039": {"title": [""], "abstract": ["We describe an algorithm for Word Sense Disambiguation WSD that relies on a lazy learner improved with automatic feature selection", "The algorithm was implemented in a system that achieves excellent performance on the set ofdata released during the SENSEVAL2 competition", "We present the results obtained and discuss the performance of various features in the context of supervised learning algorithms for WSD"], "inroduction": ["The task of Word Sense Disambiguation consists in assigning the most appropriate meaning to a polysemous word within a given con text", "A large range of applications includingmachine translation knowledge acquisition in formation retrieval information extraction andothers require knowledge about word mean ings and therefore WSD algorithms represent anecessary step in all these applications", "Start ing with SENSEVAL1 in 1999 WSD has received growing attention from the Natural LanguageProcessing community and motivates a continuously increasing number of researchers to de velop WSD systems and devote time for finding solutions to this challenging problem", "The SENSEVAL1 competitions provided a goodenvironment for the development of super vised WSD systems making freely available large amounts of sense tagged data", "During SENSEVAL1 in 1999 data for 35 words wasmade available adding up to about 20000 examples tagged with respect to the Hector dic tionary", "The size of the tagged corpus increasedwith SENSEVAL2 in 2001 when 13000 additional examples were released for 73 polyse lhttpwwwitribtonacukeventssensevalmous words", "This time the semantic annota tions were performed with respect to WordNet", "The experiments and results reported in thispaper pertain to the SENSEVAL2 data", "How ever similar experiments were performed on the SENSEVAL1 data with comparable resultsMost of the efforts in the WSD field were concentrated so far towards supervised learning algorithms and these are the methods that usu ally achieve the best performance at the cost of low recall", "Each sense tagged occurrence of a particular word is transformed into a featurevector suitable for an automatic learning pro cess", "Two main decisions need to be made in the design of such a system the set of features to be used and the learning algorithm", "Commonly used features include surrounding words and their part of speechBruce and Wiebe 1999context keywords Ng and Lee 1996 or context bigrams Pedersen 2001 various syntac tic properties Fellbaum et al 2001 etc As for the learning methodology a large range ofalgorithms have been employed including neu ral networks Leacock et al 1998 decision trees Pedersen 2001 decision lists Yarowsky 2000 memory based learning Veenstra et al2000 and others", "An experimental comparison of seven learning algorithms used to disambiguate the meaning of the word line is pre sented in Mooney 1996", "We investigate in this paper the use of a lazy learner namely instance based learning to solve the semantic ambiguity of words in contextThe main advantage of instance based learn ers is the fact that they consider every single training example when making a classification decision", "This characteristic proves particularly useful for NLP problems where training datais usually expensive and exceptions are impor tant", "On the other side lazy learners including instance based learners have the disadvantage of being easily misled by irrelevant features", "Inthe algorithm described in this paper this draw back is solved by improving the learner with a scheme for automatic feature selection", "The methodology presented here is integralpart of a larger system that has the capabil ity of performing both supervised and opentext WSD Mihalcea 2002", "For reasons of clarity and space we focus in this paper only on the description of the supervised component", "To our knowledge instance based learning with per word automatic feature selection is a new approach in the WSD field and we show that it leads to very good results", "Previous work has considered the application of instance based learning with automatic feature selection for the problem of pronoun resolution Cardie 1996", "In WSD the work that is closest to ours was reported by Bruce and Wiebe 1999 where decomposable probabilistic models are used in combination with eager Naive Bayes algorithms"]}, "C02-1050": {"title": ["Bidirectional Decoding for Statistical Machine Translation Taro WATANABE   and Eiichiro SUMITA  tarowatanabe eiichirosumitaatrcojp  ATR Spoken Language Translation Research Laboratories  Department of Information Science 222 Hikaridai Seikacho Kyoto University Sorakugun Kyoto 6190288 JAPAN Sakyoku Kyoto 6068501 JAPAN"], "abstract": ["This paper describes the righttoleft decoding method which translates an input string by generating in righttoleft direction", "In addition presented is the bidirectional decoding method that can take both of the advantages of lefttoright and righttoleft decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions", "The experimental results on Japanese and English translation showed that the righttoleft was better for EnglithtoJapanese translation while the lefttoright was suitable for JapanesetoEnglish translation", "It was also observed that the bidirectional method was better for EnglishtoJapanese translation"], "inroduction": ["The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text", "According to the Bayes Rule the problem is transformed into the noisy channel model paradigm where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text Brown et al 1993", "Although there exists efficient algorithms to estimate the parameters for the statistical machine translation SMT one of the problems of SMT is the search algorithms for the translation given a sequence of words", "There exists stack decoding algorithm Berger et al 1996 A search algorithm Och et al 2001 Wang and Waibel 1997 and dynamicprogramming algorithms Tillmann and Ney 2000 GarciaVarea and Casacuberta 2001 and all translate a given input string wordbyword and render the translation in lefttoright with pruning technologies assuming almost linearly aligned translation source and target texts", "The algorithms proposed above cannot deal with drastically different word correspondence such as Japanese and English translation where Japanese is SOV while SVO in English", "Germann et al", "2001 suggested greedy method and integer programming decoding though the first method suffer from the similar problem as described above and the second is impractical for the realworld application", "This paper presents two decoding methods one is the righttoleft decoding based on the leftto right beam search algorithm which generates outputs from the end of a sentence", "The second one is the bidirectional decoding method which decodes in both of the lefttoright and righttoleft directions and merges the two hypothesized partial sentences into one", "The experimental results of Japanese and English translation indicated that the righttoleft decoding was better for EnglishtoJapanese translation while the lefttoright decoding was better for JapanesetoEnglish decoding", "The above results could be justified by the structural difference of Japanese and English where English takes the prefix structure that places emphasis at the beginning of a sentence hence prefers lefttoright decoding", "On the other hand Japanese takes postfix structure setting attention around the end of a sentence therefore favors righttoleft decoding", "The bidirectional decoding which can take both of the benefits of decoding method was superior to mono directional decoding methods", "The next section briefly describes the SMT focusing on the IBM Model 4", "Then the Section 3 presents decoding algorithms in three direction left toright righttoleft and bidirection", "The Section 4 presents the results of Japanese and English translation followed by discussions"]}, "C04-1074": {"title": ["Optimizing Algorithms for Pronoun Resolution"], "abstract": ["The paper aims at a deeper understanding of several wellknown algorithms and proposes ways to optimize them", "It describes and discusses factors and strategies of factor interaction used in the algorithms", "The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information Negra Skut et al 1997", "A common format for pronoun resolution algorithms with several open parameters is proposed and the parameter settings optimal on the evaluation data are given"], "inroduction": ["In recent years a variety of approaches to pronoun resolution have been proposed", "Some of them are based on centering theory Strube 1998 Strube and Hahn 1999 Tetreault 2001 others on Machine Learning Aone and Bennett 1995 Ge et al 1998 Soon et al 2001 Ng and Cardie 2002 Yang et al 2003", "They supplement older heuristic approaches Hobbs 1978 Lappin and Leass 1994", "Unfortunately most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible", "Appreciation of the new insights is quite hard", "Evaluation differs not only with regard to size and genre of corpora but also along the following lines", "Scope of application Some approaches only deal with personal and possessive pronouns centering and heuristic while others consider coreference links in general Soon et al 2001 Ng and Cardie 2002 Yang et al 2003", "A drawback of this latter view is that it mixes problems on different lev els of difficulty", "It remains unclear how much of the success is due to the virtues of the approach and how much is due to the distribution of hard and easy problems in the corpus", "In this paper we will only deal with coreferential pronouns ie possessive demonstrative and third person pronouns", "My thanks go to Melvin Wurster for help in annotation and to Ciprian Gerstenberger for discussion", "Quality of linguistic input Some proposals were evaluated on hand annotated Strube and Hahn 1999 or tree bank input Ge et al 1998 Tetreault 2001", "Other proposals provide a more realistic picture in that they work as a backend to a parser Lappin and Leass 1994 or noun chunker Mitkov 1998 Soon et al 2001 Ng and Cardie 2002", "In evaluation of applications presupposing parsing it is helpful to separate errors due to parsing from intrinsic errors", "On the other hand one would also like to gauge the endtoend performance of a system", "Thus we will provide performance figures for both ideal handannotated input and realistic automatically generated input", "Language Most approaches were evaluated on English where large resources are available both in terms of preannotated data MUC6 and MUC7 data and lexical information WordNet", "This paper deals with German", "Arguably the free wordorder of German arguably leads to a clearer distinction between grammatical function surface order and information status Strube and Hahn 1999", "The paper is organized as follows", "Section 2 describes the evaluation corpus", "Section 3 describes several factors relevant to pronoun resolution", "It assesses these factors against the corpus measuring their precision and restrictiveness", "Section 4 describes and evaluates six algorithms on the basis of these factors", "It also captures the algorithms as para metric systems and proposes parameter settings optimal on the evaluation data", "Section 5 concludes"]}, "C04-1075": {"title": ["A HighPerformance Coreference Resolution System"], "abstract": ["This paper presents a constraintbased multi agent strategy to coreference resolution of general noun phrases in unrestricted English text", "For a given anaphor and all the preceding referring expressions as the antecedent candidates a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge", "Then according to the type of the anaphor a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge", "Finally a simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates based on the proximity principle", "One interesting observation is that the most recent antecedent of an anaphor in the coreferential chain is sometimes indirectly linked to the anaphor via some other antecedents in the chain", "In this case we find that the most recent antecedent always contains little information to directly determine the coreference relationship with the anaphor", "Therefore for a given anaphor the corresponding special constraint agent can always safely filter out these less informative antecedent candidates", "In this way rather than finding the most recent antecedent for an anaphor our system tries to find the most direct and informative antecedent", "Evaluation shows that our system achieves Precision  Recall  Fmeasures of 847 658  739 and 828  557  665 on MUC6 and MUC7 English coreference tasks respectively", "This means that our system achieves significantly better precision rates by about 8 percent over the bestreported systems while keeping recall rates"], "inroduction": ["Coreference accounts for cohesion in texts", "Especially a coreference denotes an identity of reference and holds between two expressions which can be named entities definite noun phrases pronouns and so on", "Coreference resolution is the process of determining whether two referring expressions refer to the same entity in the world", "The ability to link referring expressions both within and across the sentence is critical to discourse and language understanding in general", "For example coreference resolution is a key task in natural language interfaces machine translation text summarization information extraction and question answering", "In particular information extraction systems like those built in the DARPA Message Understanding Conferences MUC have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC6 1995 and MUC7 1998", "There is a long tradition of work on coreference resolution within computational linguistics", "Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge Carter 1987 Rich and LuperFoy 1988 Carbonell and Brown 1988", "However the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledgepoor strategies Dagan and Itai 1990 Lappin and Leass 1994 Mitkov 1998 Soon Ng and Lim 2001 Ng and Cardie 2002 which was further motivated by the emergence of cheaper and more reliable corpus based NLP tools such as partofspeech taggers and shallow parsers alongside the increasing availability of corpora and other resources eg ontology", "Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements ccommand constraints semantic consistency syntactic parallelism semantic parallelism salience proximity etc These factors can be either constraints which discard invalid ones from the set of possible candidates such as gender and number agreements ccommand constraints semantic consistency or preferences which gives more preference to certain candidates and less to others such as syntactic parallelism semantic parallelism salience proximity", "While a number of approaches use a similar set of factors the computational strategies the way antecedents are determined ie the algorithm and formula for assigning antecedents may differ ie from simple cooccurrence rules Dagan and Itai 1990 to decision trees Soon Ng and Lim 2001 Ng and Cardie 2002 to pattern induced rules Ng and Cardie 2002 to centering algorithms Grosz and Sidner 1986 Brennan Friedman and Pollard 1987 Strube 1998 Tetreault 2001", "This paper proposes a simple constraintbased multiagent system to coreference resolution of general noun phrases in unrestricted English text", "For a given anaphor and all the preceding referring expressions as the antecedent candidates a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge", "Then according to the type of the anaphor a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge", "Finally a simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates based on the proximity principle", "One interesting observation is that the most recent antecedent of an anaphor in the coreferential chain is sometimes indirectly linked to the anaphor via some other antecedents in the chain", "In this case we find that the most recent antecedent always contains little information to directly determine the coreference relationship with the anaphor", "Therefore for a given anaphor the corresponding special constraint agent can always safely filter out these less informative antecedent candidates", "In this way rather than finding the most recent antecedent for an anaphor our system tries to find the most direct and informative antecedent", "In this paper we focus on the task of determining coreference relations as defined in MUC6 1995 and MUC7 1998", "In order to evaluate the performance of our approach on coreference resolution we utilize the annotated corpus and the scoring programs from MUC6 and MUC7", "For MUC6 30 dryrun documents annotated with coreference information are used as the training data", "There are also 30 annotated training documents from MUC7", "The total size of 30 training documents is close 12400 words for MUC6 and 19000 for MUC7", "For testing we utilize the 30 standard test documents from MUC 6 and the 20 standard test documents from MUC7", "The layout of this paper is as follows in Section 2 we briefly describe the preprocessing determination of referring expressions", "In Section 3 we differentiate coreference types and discuss how to restrict possible types of direct and informative antecedent candidates according to anaphor types", "In Section 4 we describe the constraintbased multiagent system", "In Section 5 we evaluate the multiagent algorithm", "Finally we present our conclusions"]}, "C04-1091": {"title": ["An Algorithmic Framework for the Decoding Problem in Statistical Machine Translation Raghavendra Udupa U Tanveer A Faruquie IBM India Research Lab Block1A IIT Hauz Khas New Delhi  110 016 India uraghave ftanveerinibmcom Hemanta K Maji Dept of Computer Science and Engineering IIT Kanpur Kanpur  208 016 India hkmajiiitkacin Abstract The decoding problem in Statistical Machine Translation SMT is a computationally hard combinatorial optimization problem In this paper we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility In the new algorithmic framework the decoding problem can be solved both exactly and approximately The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations Our first algorithm is a provably linear time search algorithm We use this algorithm as a subroutine in the other algorithms We believe that decoding algorithms derived from our framework can be of practical significance 1 Introduction Decoding is one of the three fundamental problems in classical SMT translation model and language model being the other two as proposed by IBM in the early 1990 s Brown et al 1993 In the decoding problem we are given the language and translation models and a source language sentence and are asked to find the most probable translation for the sentence Decoding is a discrete optimization problem whose search space is prohibitively large The challenge is therefore in devising a scheme to efficiently search the solution space for the solution Decoding is known to belong to a class of computational problems popularly known as NPhard problems Knight 1999 NPhard problems are known to be computationally hard and have eluded polynomial time algorithms Garey and Johnson 1979 The first algorithms for the decoding problem were based on what is known among the speech recognition community as stackbased search Jelinek 1969 The original IBM solution to the decoding problem employed a restricted stackbased search Berger et al 1996 This idea was further explored by Wang and Waibel Wang and Waibel 1997 who developed a faster stackbased search algorithm In perhaps the first work on the computational complexity of Decoding Kevin Knight showed that the problem is closely related to the more famous Traveling Salesman problem TSP Independently Christoph Till man adapted the Held Karp dynamic programming algorithm for TSP Held and Karp 1962 to Decoding Tillman 2001 The original HeldKarp algorithm for TSP is an exponential time dynamic programming algorithm and Tillman s adaptation to Decoding has a prohibitive complexity of O  l3m22m   O m52m where m and l are the lengths of the source and target sentences respectively Tillman and Ney showed how to improve the complexity of the HeldKarp algorithm for restricted word reordering and gave a O  l3m4   O m7 algorithm for FrenchEnglish translation Tillman and Ney 2000 An optimal decoder based on the wellknown A heuristic was implemented and benchmarked in Och et al 2001 Since optimal solution can not be computed for practical problem instances in a reasonable amount of time much of recent work has focused on good quality suboptimal solutions An O  m6  greedy search algorithm was developed Germann et al 2003 whose complexity was reduced further to O  m2  Germann 2003 In this paper we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework We model the search problem as an alternating search problem The search therefore alternates between two subproblems both of which are much easier to solve in practice By breaking the decoding problem into two simpler search problems we are able to provide handles for solving the problem efficiently The solutions of the subproblems can be combined easily to arrive at a solution for the original problem The first subproblem fixes an alignment and seeks the best translation with that alignment Starting with an initial alignment between the source sentence and its translation the second subproblem asks for an improved alignment We show that both of these problems are easy to solve and provide efficient solutions for them In an iterative search for a local optimal solution we alternate between the two algorithms and refine our solution The algorithmic framework provides handles for solving the decoding problem at several levels of complexity At one extreme the framework yields an algorithm for solving the decoding problem optimally At the other extreme it yields a provably linear time algorithm for finding suboptimal solutions to the problem We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility The rest of the paper is devoted to the development and discussion of our framework We start with a mathematical formulation of the decoding problem Section 2 We then develop the alternating search paradigm and use it to develop several decoding algorithms Section 3 Next we demonstrate the practical utility of our algorithms with the help of results from our initial experiments Section 5 2 Decoding The decoding problem in SMT is one of finding the most probable translation e in the target language of a given source language sentence f in accordance with the Fundamental Equation of SMT Brown et al 1993 e  argmaxe Prf ePre 1 In the remainder of this paper we will refer to the search problem specified by Equation 1 as STRICT DECODING Rewriting the translation model Prf e as  a Prf ae where a denotes an alignment between the source sentence and the target sentence the problem can be restated as e  argmaxe  a Prf aePre 2 Even when the translation model Prf e is as simple as IBM Model 1 and the language model Pre is a bigram language model the decoding problem is NPhard Knight 1999 Unless P  NP there is no hope of an efficient algorithm for the decoding problem Since the Fundamental Equation of SMT does not yield an easy handle to design a solution exact or even an approximate one for the problem most researchers have instead worked on solving the following relatively simpler problem Germann et al 2003 e  a    argmaxea Prf aePre 3 We call the search problem specified by Equation 3 as RELAXED DECODING Note that RELAXED DECODING relaxes STRICT DECODING to a joint optimization problem The search in RELAXED DECODING is for a pair e a While RELAXED DECODING is simpler than STRICT DECODING it is also unfortunately NP hard for even IBM Model 1 and Bigram language model Therefore all practical solutions to RELAXED DECODING have focused on finding suboptimal solutions The challenge is in devising fast search strategies to find good suboptimal solutions Table 1 lists the combinatorial optimization problems in the domain of decoding In the remainder of the paperm and l denote the length of the source language sentence and its translation respectively 3 Framework for Decoding We begin with a couple of useful observations about the decoding problem Although deceptively simple these observations are very crucial for developing our framework They are the source for algorithmic handles for breaking the decoding problem into two relatively easier search problems The first of these observations concerns with solving the problem when we know in advance the mapping between the source and target sentences This leads to the development of an extremely simple algorithm for decoding when the alignment is known or Problem Search STRICT DECODING e  argmaxePrf ePre RELAXED DECODING e a  argmaxeaPrf aePre FIXED ALIGNMENT DECODING e  argmaxePrf  a  ePre VITERBI ALIGNMENT a  argmaxaPrf ae Table 1 Combinatorial Search Problems in Decoding can be guessed Our second observation is on finding a better alignment between the source and target sentences starting with an initial possibly suboptimal alignment The insight provided by the two observations are employed in building a powerful algorithmic framework"], "abstract": ["The decoding problem in Statistical Machine Translation SMT is a computationally hard combinatorial optimization problem", "In this paper we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility", "In the new algorithmic framework the decoding problem can be solved both exactly and approximately", "The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems", "We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem", "A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations", "Our first algorithm is a provably linear time search algorithm", "We use this algorithm as a subroutine in the other algorithms", "We believe that decoding algorithms derived from our framework can be of practical significance"], "inroduction": ["Decoding is one of the three fundamental problems in classical SMT translation model and language model being the other two as proposed by IBM in the early 1990s Brown et al 1993", "In the decoding problem we are given the language and translation models and a source language sentence and are asked to find themost probable translation for the sentence", "De coding is a discrete optimization problem whose search space is prohibitively large", "The challenge is therefore in devising a scheme to efficiently search the solution space for the solution", "Decoding is known to belong to a class of computational problems popularly known as NP hard problems Knight 1999", "NPhard problems are known to be computationally hard and have eluded polynomial time algorithms Garey and Johnson 1979", "The first algorithms for the decoding problem were based on what is known among the speech recognition community as stackbased search Jelinek 1969", "The original IBM solution to the decoding problem employed a restricted stackbased search Berger et al 1996", "This idea was further explored by Wang and Waibel Wang and Waibel 1997 who developed a faster stackbased search algorithm", "In perhaps the first work on the computational complexity of Decoding Kevin Knight showed that the problem is closely related to the more famous Traveling Salesman problem TSP", "Independently Christoph Tillman adapted the HeldKarp dynamic programming algorithm for TSP Held and Karp 1962 to Decoding Tillman 2001", "The original Held Karp algorithm for TSP is an exponential time dynamic programming algorithm and Tillmans adaptation to Decoding has a prohibitive com plexity of O l3m2 2m   O m5 2m  where mand l are the lengths of the source and tar get sentences respectively", "Tillman and Ney showed how to improve the complexity of the HeldKarp algorithm for restricted word reordering and gave a O l3m4  O m7 algo rithm for FrenchEnglish translation Tillman and Ney 2000", "An optimal decoder based on the wellknown A heuristic was implemented and benchmarked in Och et al 2001", "Since optimal solution can not be computed for practical problem instances in a reasonable amount of time much of recent work has focused on good quality suboptimal solutions", "An O m6 greedy search algorithm was developed Germann et al 2003 whose complexity was re duced further to O m2 Germann 2003", "In this paper we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework", "We model the search problem as an alternating search problem", "The search therefore alternates between two subproblems both of which are much easier to solve in practice", "By breaking the decoding problem into two simpler search problems we are able to provide handles for solving the problem efficiently", "The solutions of the subproblems can be combined easily to arrive at a solution for the original problem", "The first subproblem fixes an alignment and seeks the best translation with that alignment", "Starting with an initial alignment between the source sentence and its translation the second subproblem asks for an improved alignment", "We show that both of these problems are easy to solve and provide efficient solutions for them", "In an iterative search for a local optimal solution we alternate between the two algorithms and refine our solution", "The algorithmic framework provides handles for solving the decoding problem at several levels of complexity", "At one extreme the framework yields an algorithm for solving the decoding problem optimally", "At the other extreme it yields a provably linear time algorithm for finding suboptimal solutions to the problem", "We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations", "Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility", "The rest of the paper is devoted to the development and discussion of our framework", "We start with a mathematical formulation of the decoding problem Section 2", "We then develop the alternating search paradigm and use it to develop several decoding algorithms Section 3", "Next we demonstrate the practical utility of our algorithms with the help of results from our initial experiments Section 5"]}, "C04-1131": {"title": ["Word sense disambiguation criteria a systematic study"], "abstract": ["This article describes the results of a systematic in depth study of the criteria used for word sense disambiguation", "Our study is based on 60 target words 20 nouns 20 adjectives and 20 verbs", "Our results are not always in line with some practices in the field", "For example we show that omitting non content words decreases performance and that bigrams yield better results than unigrams"], "inroduction": ["The task of word sense disambiguation WSD is to identify the correct sense of a word in context", "WSD is usually performed by matching information from the context in which the word occurs with disambiguation knowledge source", "Our approach uses supervised machinelearning techniques to automatically acquire such disambiguation knowledge from sensetagged corpora", "At present this type of approach is widely used and seems to yield the best results Kilgarriff Rosenzweig 2000 Ng 1997b", "Information conveyed by context words morphological form is enriched with further annotations partofspeech tag lemma etc Each individual piece of information is called a feature", "A good feature should capture an important source of knowledge that is critical in determining the sense of the word to be disambiguated", "The choice of the appropriate set of features is an important issue for WSD Bruce Wiebe Perdersen 1996 Ng Zelle 1997 Pedersen 2001", "Thus this paper describes the results of a systematic and indepth study of homogenous criteria ie set of features that can be used for WSD"]}, "C04-1143": {"title": ["FASIL Email Summarisation System"], "abstract": ["Email summarisation presents a unique set of requirements that are different from general text summarisation", "This work describes the implementation of an email summarisation system for use in a voicebased Virtual Personal Assistant developed for the EU FASiL Project", "Evaluation results from the first integrated version of the project are presented"], "inroduction": ["Email is one of the most ubiquitous applications used on a daily basis by millions of people worldwide traditionally accessed over a fixed terminal or laptop computer", "In the past years there has been an increasing demand for email access over mobile phones", "Our work has focused on creating an email summarisation service that provides quality summaries adaptively and quickly enough to cater for the tight constrains imposed by a real time texttospeech system", "This work has been done as part of the European Union FASiL project which aims to aims to construct a conversationally intelligent Virtual Personal Assistant VPA designed to manage the users personal and business information through a voicebased interface accessible over mobile phones", "As the quality of life and productivity is to improved in an increasingly information dominated society people need access to information anywhere anytime", "The Adaptive Information Management AIM service in the FASiL VPA seeks to automatically prioritise and present information that is most pertinent to the mobile users and adapt to different user preferences", "The AIM service is comprised of three main parts an email sum mariser email categoriser calendar schedulingPIM interaction and an adaptive prioritisation service that optimizes the sequence in which information is presented keeping the overall duration of the voicebased dialogue to a minimum"]}, "C08-1002": {"title": ["A Supervised Algorithm for Verb Disambiguation into VerbNet Classes"], "abstract": ["VerbNet VN is a major largescale English verb lexicon", "Mapping verb instances to their VN classes has been proven useful for several NLP tasks", "However verbs are polysemous with respect to their VN classes", "We introduce a novel supervised learning model for mapping verb instances to VN classes using rich syntactic features and class membership constraints", "We evaluate the algorithm in both indomain and corpus adaptation scenarios", "In both cases we use the manually tagged Sem link WSJ corpus as training data", "For in domain testing on Semlink WSJ data we achieve 959 accuracy 351 error reduction ER over a strong baseline", "For adaptation we test on the GENIA corpus and achieve 724 accuracy with 107 ER", "This is the first largescale experimentation with automatic algorithms for this task"], "inroduction": ["The organization of verbs into classes whose members exhibit similar syntactic and semantic behavior has been discussed extensively in the linguistics literature see eg", "Levin and Rappaport Hovav 2005 Levin 1993", "Such an organization helps in avoiding lexicon representation redundancy and enables generalizations across similar verbs", "It can also be of great practical use eg in compensating NLP statistical models for data sparseness", "Indeed Levins seminal work had motivated Qc 2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "much research aimed at automatic discovery of verb classes see Section 2", "VerbNet VN Kipper et al 2000 Kipper Schuler 2005 is a large scale publicly available domain independent verb lexicon that builds on Levin classes and extends them with new verbs new classes and additional information such as semantic roles and selectional restrictions", "VN classes were proven beneficial for Semantic Role Labeling SRL Swier and Stevenson 2005 Semantic Parsing Shi and Mihalcea 2005 and building conceptual graphs Hensman and Dun nion 2004", "Levininspired classes have been used in several NLP tasks such as Machine Translation Dorr 1997 and Document Classification Klavans and Kan 1998", "Many applications that use VN need to map verb instances onto their VN classes", "However verbs are polysemous with respect to VN classes", "Sem link Loper et al 2007 is a dataset that maps each verb instance in the WSJ Penn Treebank to its VN class", "The mapping has been created using a combination of automatic and manual methods", "Yi et al", "2007 have used Semlink to improve SRL", "In this paper we present the first largescale experimentation with a supervised machine learning classification algorithm for disambiguating verb instances to their VN classes", "We use rich syntactic features extracted from a treebankstyle parse tree and utilize a learning algorithm capable of imposing class membership constraints thus taking advantage of the nature of our task", "We use Semlink as the training set", "We evaluate our algorithm in both indomain and corpus adaptation scenarios", "In the former we test on the WSJ using Semlink again obtaining 959 accuracy with 351 error reduction ER over a strong baseline most frequent 9 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 916 Manchester August 2008 class when using a modern statistical parser", "In the corpus adaptation scenario we disambiguate verbs in sentences taken from outside the training domain", "Since the manual annotation of new corpora is costly and since VN is designed to be a domain independent resource adaptation results are important to the usability in NLP in practice", "We manually annotated 400 sentences from GENIA Kim et al 2003 a medical domain corpus1  Testing on these we achieved 724 accuracy with 107 ER", "Our adaptation scenario is complete in the sense that the parser we use was also trained on a different corpus WSJ", "We also report experiments done using goldstandard manually created parses", "The most relevant previous works addressing verb instance class classification are Lapata and Brew 2004 Li and Brew 2007 Girju et al 2005", "The former two do not use VerbNet and their experiments were narrower than ours so we cannot compare to their results", "The latter mapped to VN but used a preliminary highly restricted setup where most instances were monosemous", "For completeness we compared our method to theirs2  achieving similar results", "We review related work in Section 2 and discuss the task in Section 3", "Section 4 introduces the model Section 5 describes the experimental setup and Section 6 presents our results"]}, "C08-1014": {"title": ["Regenerating Hypotheses for Statistical Machine Translation"], "abstract": ["This paper studies three techniques that improve the quality of Nbest hypotheses through additional regeneration process", "Unlike the multisystem consensus approach where multiple translation systems are used our improvement is achieved through the expansion of the N best hypotheses from a single system", "We explore three different methods to implement the regeneration process re decoding ngram expansion and confusion networkbased regeneration", "Experiments on ChinesetoEnglish NIST and IWSLT tasks show that all three methods obtain consistent improvements", "Moreover the combination of the three strategies achieves further improvements and outperforms the baseline by 081 BLEUscore on IWSLT06 057 on NIST03 061 on NIST05 test set respectively"], "inroduction": ["Stateoftheart Statistical Machine Translation SMT systems usually adopt a twopass search strategy Och 2003 Koehn et al 2003 as shown in Figure 1", "In the first pass a decoding algorithm is applied to generate an Nbest list of translation hypotheses while in the second pass the final translation is selected by rescoring and reranking the Nbest translations through additional feature functions", "The fundamental assumption behind using a second pass is that the generated Nbest list may contain better transla  2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbync sa30", "Some rights reserved", "tions than the best choice found by the decoder", "Therefore the performance of a twopass SMT system can be improved from two aspects ie scoring models and the quality of the Nbest hypotheses", "Rescoring pass improves the performance of machine translation by enhancing the scoring models with more global sophisticated and dis criminative feature functions", "The idea for applying two passes instead of one is that some global feature functions cannot be easily decomposed into local scores and computed during decoding", "Furthermore rescoring allows some feature functions such as word and ngram posterior probabilities to be estimated on the Nbest list Ueffing 2003 Chen et al 2005 Zens and Ney 2006", "In this twopass method translation performance hinges on the Nbest hypotheses that are generated in the first pass since rescoring occurs on these so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance", "This technique is called system combination Bangalore et al 2001 Matusov et al 2006 Sim et al 2007 Rosti et al 2007a Rosti et al 2007b", "We have instead chosen to regenerate new hypotheses from the original Nbest list a technique which we call regeneration", "Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2", "Given the original Nbest list Nbest1 generated by the decoder this regeneration pass creates new translation hypotheses from this list to form another Nbest list Nbest2", "These two Nbest lists are then combined and given to the rescoring pass to derive the best translation", "We implement three methods to regenerate new hypotheses redecoding ngram expansion and confusion network", "Redecoding Rosti et al 2007a based regeneration redecodes the source sentence using original LM as well as new trans 105 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 105112 Manchester August 2008 Figure 1 Structure of a typical twopass machine translation system", "Nbest translations are generated by the decoder and the 1best translation is returned after rescored with additional feature functions", "didates from the other MT systems"]}, "C08-1088": {"title": ["Exploiting Constituent Dependencies for Tree Kernelbased Semantic"], "abstract": ["This paper proposes a new approach to dynamically determine the tree span for tree kernelbased semantic relation extraction", "It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities while removing the noisy information from the syntactic parse tree eventually leading to a dynamic syntactic parse tree", "This paper also explores entity features and their combined features in a unified parse and semantic tree which integrates both structured syntactic parse information and entityrelated semantic information", "Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans and the composite kernel combining this tree kernel with a linear stateoftheart featurebased kernel achieves the so far best performance"], "inroduction": ["Information extraction is one of the key tasks in natural language processing", "It attempts to identify relevant information from a large amount of natural language text documents", "Of three sub tasks defined by the ACE program1 this paper focuses exclusively on Relation Detection and Characterization RDC task which detects and classifies semantic relationships between predefined types of entities in the ACE corpus", "For  2008", "Licensed under the Creative Commons Attribution NoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "1 httpwwwldcupenneduProjectsACE example the sentence Microsoft Corp is based in Redmond WA conveys the relation GPEAFFBased between Microsoft Corp ORG and Redmond GPE", "Due to limited accuracy in stateoftheart syntactic and semantic parsing reliably extracting semantic relationships between named entities in natural language documents is still a difficult unresolved problem", "In the literature featurebased methods have dominated the research in semantic relation extraction", "Featuredbased methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features such as lexical knowledge entityrelated information syntactic parse trees and deep semantic information", "However detailed research Zhou et al 2005 shows that its difficult to extract new effective features to further improve the extraction accuracy", "Therefore researchers turn to kernelbased methods which avoids the burden of feature engineering through computing the similarity of two discrete objects eg parse trees directly", "From prior work Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005 to current research Zhang et al 2006 Zhou et al 2007 kernel methods have been showing more and more potential in relation extraction", "The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances", "While kernel methods using the dependency tree Culotta and Sorensen 2004 and the shortest dependency path Bunescu andMooney 2005 suffer from low recall perform ance convolution tree kernels Zhang et al 2006 Zhou et al 2007 over syntactic parse trees achieve comparable or even better performance than featurebased methods", "However there still exist two problems regarding currently widely used tree spans", "Zhang et al", "2006 discover that the Shortest Path 697 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 697704 Manchester August 2008 enclosed Tree SPT achieves the best performance", "Zhou et al", "2007 further extend it to ContextSensitive Shortest Pathenclosed Tree CS SPT which dynamically includes necessary predicatelinked path information", "One problem with both SPT and CSSPT is that they may still contain unnecessary information", "The other problem is that a considerable number of useful contextsensitive information is also missing from SPTCSSPT although CSSPT includes some contextual information relating to predicate linked path", "This paper proposes a new approach to dynamically determine the tree span for relation extraction by exploiting constituent dependencies to remove the noisy information as well as keep the necessary information in the parse tree", "Our motivation is to integrate dependency information which has been proven very useful to relation extraction with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction", "Moreover we also explore interesting combined entity features for relation extraction via a unified parse and semantic tree", "The other sections in this paper are organized as follows", "Previous work is first reviewed in Section 2", "Then Section 3 proposes a dynamic syntactic parse tree while the entityrelated semantic tree is described in Section 4", "Evaluation on the ACE RDC corpus is given in Section 5", "Finally we conclude our work in Section 6"]}, "C08-1107": {"title": ["Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 849856 Manchester August 2008 Learning Entailment Rules for Unary Templates Idan Szpektor Department of Computer Science BarIlan University Ramat Gan Israel szpektimacsbiuacil Ido Dagan Department of Computer Science BarIlan University Ramat Gan Israel daganmacsbiuacil Abstract Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables ignoring unary rules  entailment rules between templates with a single variable In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method The results show that the learned unary rulesets outperform the binary ruleset In addition a novel directional similarity measure for learning entailment termed BalancedInclusion is the best performing measure 1 Introduction In many NLP applications such as Question Answering QA and Information Extraction IE it is crucial to recognize whether a specific target meaning is inferred from a text For example a QA system has to deduce that  SCO sued IBM is inferred from  SCO won a lawsuit against IBM to answer  Whom did SCO sue   This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework Giampiccolo et al 2007 An important type of knowledge needed for such inference is entailment rules An entailment rule specifies a directional inference relation be tween two templates text patterns with variables such as X win lawsuit against Y  X sue Y   Applying this rule by matching  X win lawsuit against Y  in the above text allows a QA system to"], "abstract": ["Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables ignoring unary rules  entailment rules between templates with a single variable", "In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method", "The results show that the learned unary rulesets outperform the binary ruleset", "In addition a novel directional similarity measure for learning entailment termed BalancedInclusion is the best performing measure"], "inroduction": ["In many NLP applications such as Question Answering QA and Information Extraction IE it is crucial to recognize whether a specific target meaning is inferred from a text", "For example a QA system has to deduce that SCO sued IBM is inferred from SCO won a lawsuit against IBM to answer Whom did SCO sue", "This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entail ment framework Giampiccolo et al 2007", "An important type of knowledge needed for such inference is entailment rules", "An entailment rule specifies a directional inference relation between two templates text patterns with variables such as X win lawsuit against Y  X sue Y ", "Applying this rule by matching X win lawsuit against Y  in the above text allows a QA system to Qc 2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "infer X sue Y  and identify IBM Y s instantiation as the answer for the above question", "Entail ment rules capture linguistic and worldknowledge inferences and are used as an important building block within different applications eg", "Romano et al 2006", "One reason for the limited performance of generic semantic inference systems is the lack of broadscale knowledgebases of entailment rules in analog to lexical resources such as WordNet", "Supervised learning of broad coverage rulesets is an arduous task", "This sparked intensive research on unsupervised acquisition of entailment rules and similarly paraphrases eg", "Lin and Pantel 2001 Szpektor et al 2004 Sekine 2005", "Most unsupervised entailment rule acquisition methods learn binary rules rules between templates with two variables ignoring unary rules rules between unary templates templates with only one variable", "However a predicate quite often appears in the text with just a single variable eg intransitive verbs or passives where infer ence requires unary rules eg X take a nap  X sleep further motivations in Section 31", "In this paper we focus on unsupervised learning of unary entailment rules", "Two learning approaches are proposed", "In our main approach rules are learned by measuring how similar the variable instantiations of two templates in a corpus are", "In addition to adapting stateoftheart similarity measures for unary rule learning we propose a new measure termed BalancedInclusion which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity", "In a second approach unary rules are derived from binary rules learned by stateofthe art binary rule learning methods", "We tested the various unsupervised unary rule 849 Proceedings of the 22nd International Conference on Computational Linguistics Coling 2008 pages 849856 Manchester August 2008 learning methods as well as a binary rule learning method on a test set derived from a standard IE benchmark", "This provides the first comparison between the performance of unary and binary rule sets", "Several results rise from our evaluation a while most work on unsupervised learning ignored unary rules all tested unary methods outperformed the binary method b it is better to learn unary rules directly than to derive them from a binary rulebase c our proposed BalancedInclusion measure outperformed all other tested methods in terms of F1 measure", "Moreover only Balanced Inclusion improved F1 score over a baseline inference that does not use entailment rules at all "]}, "C08-2009": {"title": ["Underspecified Modelling  of Complex Discourse Constraints"], "abstract": ["We introduce a new type of discourse constraints for the interaction of discourse relations with the configuration of discourse segments", "We examine corpusextracted examples as soft constraints", "We show how to use Regular Tree Gramamrs to process such constraints and how the representation of some constraints depends on the expressive power of this formalism"], "inroduction": ["Discourse structures cannot always be described completely either because they are ambiguous Stede 2004 or because a discourse parser fails to analyse them completely", "In either case underspecification formalisms UFs can be used to represent partial information on discourse structure", "UFs are used in semantics to model structural ambiguity without disjunctive enumeration of the readings van Deemter and Peters 1996", "Underspecified descriptions of discourse must handle two kinds of incomplete information on the configuration of discourse segments how they combine into larger units and on the discourse relations that bring about this configuration Our corpus studies on the RST Discourse Treebank Carlson et al 2002 showed interdependencies between relations and configuration a phenomenon first noted by CorstonOliver 1998", "These interdependencies can be formulated as constraints that contribute to the disambiguation of underspecified descriptions of discourse structure", "Eg in discourse segments constituted by the relation Condition the premiss tends to be a dis Qc 2008", "Licensed under the Creative Commons AttributionNoncommercialShare Alike 30 Unported license httpcreativecommonsorglicensesbyncsa30", "Some rights reserved", "course atom or at least maximally short1 Similarly there is evidence for an interdependency constraint for the relation Purpose1 2", "In most cases Purpose1 has a discourse atom as its nucleus", "The corpus evaluation furthermore shows that those patterns never occur exclusively but only as tendencies", "Realised as soft constraints such tendencies can help to sort the set of readings according to the established preferences which allows to focus on the best reading or the nbest readings", "This is of high value for an UFbased approach to discourse structure which must cope with extremely high numbers of readings", "To model interdependency constraints we will use Regular Tree Grammars RTGs Comon et al 2007", "RTGs can straightforwardly be extended to weighted Regular Tree Grammars wRTGs which can represent both soft and hard constraints", "Apart from our corpusextracted examples we also consider a hard interdependency constraint similar to the Right Frontier Constraint", "We show that we can integrate this attachment constraint with our formalism and how its representation depends on the expressiveness of RTGs"]}, "C10-1018": {"title": ["Exploiting Background Knowledge for Relation Extraction"], "abstract": ["Relation extraction is the task of recognizing semantic relations among entities", "Given a particular sentence supervised approaches to Relation Extraction employed feature or kernel functions which usually have a single sentence in their scope", "The overall aim of this paper is to propose methods for using knowledge and resources that are external to the target sentence as a way to improve relation extraction", "We demonstrate this by exploiting background knowledge such as relationships among the target relations as well as by considering how target relations relate to some existing knowledge resources", "Our methods are general and we suggest that some of them could be applied to other NLP tasks"], "inroduction": ["Relation extraction RE is the task of detecting and characterizing semantic relations expressed between entities in text", "For instance given the sentence Cone a Kansas City native was originally signed by the Royals and broke into the majors with the team one of the relations we might want to extract is the employment relation between the pair of entity mentions Cone and Royals", "RE is important for many NLP applications such as building an ontology of entities biomedical information extraction and question answering", "Prior work have employed diverse approaches towards resolving the task", "One approach is to build supervised RE systems using sentences annotated with entity mentions and predefined target relations", "When given a new sentence the RE system has to detect and disambiguate the presence of any predefined relations that might exist between each of the mention pairs in the sentence", "In building these systems researchers used a wide variety of features Kambhatla 2004 Zhou et al 2005 Jiang and Zhai 2007", "Some of the common features used to analyze the target sentence include the words appearing in the sentence their partof speech POS tags the syntactic parse of the sentence and the dependency path between the pair of mentions", "In a related line of work researchers have also proposed various kernel functions based on different structured representations eg dependency or syntactic tree parses of the target sentences Bunescu and Mooney 2005 Zhou et al 2007 Zelenko et al 2003 Zhang et al 2006", "Additionally researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia Weld et al 2008 and databases Mintz et al 2009 or attempted open information extraction IE Banko et al 2007 to extract all possible relations", "In this work we focus on supervised RE", "In prior work the feature and kernel functions employed are usually restricted to being defined on the various representations eg lexical or structural of the target sentences", "However in recognizing relations humans are not thus constrained and rely on an abundance of implicit world knowledge or background information", "What quantifies as world or background knowledge is rarely explored in the RE literature and we do not attempt to provide complete nor precise definitions in this paper", "However we show that by considering the relationship between our relations of interest as 152 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 152160 Beijing August 2010 well as how they relate to some existing knowledge resources we improve the performance of RE", "Specifically the contributions of this paper are the following  When our relations of interest are clustered or organized in a hierarchical ontology we show how to use this information to improve performance", "By defining appropriate con straints between the predictions of relations at different levels of the hierarchy we obtain globally coherent predictions as well as improved performance", " Coreference is a generic relationship that might exists among entity mentions and we show how to exploit this information by assuming that coreferring mentions have no other interesting relations", "We capture this intuition by using coreference information to constraint the predictions of a RE system", " When characterizing the relationship between a pair of mentions one can use a large encyclopedia such as Wikipedia to infer more knowledge about the two mentions", "In this work after probabilistically mapping mentions to their respective Wikipedia pages we check whether the mentions are related", "Another generic relationship that might exists between a pair of mentions is whether they have a parentchild relation and we use this as additional information", " The sparsity of features especially lexical features is a common problem for supervised systems", "In this work we show that one can make fruitful use of unlabeled data by using word clusters automatically gathered from unlabeled texts as a way of generalizing the lexical features", " We combine the various relational predictions and background knowledge through a global inference procedure which we formalize via an Integer Linear Programming ILP framework as a constraint optimization problem Roth and Yih 2007", "This allows us to easily incorporate various constraints that encode the background knowledge", "Roth and Yih 2004 develop a relation extraction approach that exploits constraints among entity types and the relations allowed among them", "We extend this view significantly within a similar computational framework to exploit relations among target relations background information and world knowledge as a way to improve relation extraction and make globally coherent predictions", "In the rest of this paper we first describe the features used in our basic RE system in Section 2", "We then describe how we make use of background knowledge in Section 3", "In Section 4 we show our experimental results and perform analysis in Section 5", "In Section 6 we discuss related work before concluding in Section 7"]}, "C10-1064": {"title": ["A Crosslingual Annotation Projection Approach"], "abstract": ["While extensive studies on relation extraction have been conducted in the last decade statistical systems based on supervised learning are still limited because they require large amounts of training data to achieve high performance", "In this paper we develop a crosslingual annotation projection method that leverages parallel corpora to bootstrap a relation detector without significant annotation efforts for a resourcepoor language", "In order to make our method more reliable we introduce three simple projection noise reduction methods", "The merit of our method is demonstrated through a novel Korean relation detection task"], "inroduction": ["Relation extraction aims to identify semantic relations of entities in a document", "Many relation extraction studies have followed the Relation Detection and Characterization RDC task organized by the Automatic Content Extraction project Doddington et al 2004 to make multilingual corpora of English Chinese and Arabic", "Although these datasets encourage the development and evaluation of statistical relation extractors for such languages there would be a scarcity of labeled training samples when learning a new system for another language such as Korean", "Since manual annotation of entities and their relations for such resourcepoor languages is very expensive we would like to consider instead a weaklysupervised learning technique in order to learn the relation extractor without significant annotation efforts", "To do this we propose to leverage parallel corpora to project the relation annotation on the source language eg English to the target eg Korean", "While many supervised machine learning approaches have been successfully applied to the RDC task Kambhatla 2004 Zhou et al 2005 Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005 Zhang et al 2006 few have focused on weaklysupervised relation extraction", "For example Zhang 2004 and Chen et al 2006 utilized weaklysupervised learning techniques for relation extraction but they did not consider weak supervision in the context of crosslingual relation extraction", "Our key hypothesis on the use of parallel corpora for learning the relation extraction system is referred to as crosslingual annotation projection", "Early studies of crosslingual annotation projection were accomplished for lexicallybased tasks for example partofspeech tagging Yarowsky and Ngai 2001 namedentity tagging Yarowsky et al 2001 and verb classification Merlo et al 2002", "Recently there has been increasing interest in applications of annotation projection such as dependency parsing Hwa et al 2005 mention detection Zitouni and Florian 2008 and semantic role labeling Pado and Lapata 2009", "However to the best of our knowledge no work has reported on the RDC task", "In this paper we apply a crosslingual annotation projection approach to binary relation detection a task of identifying the relation between two entities", "A simple projection method propagates the relations in source language sentences to 564 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 564571 Beijing August 2010 wordaligned target sentences and a target relation detector can bootstrap from projected annotation", "However this automatic annotation is unreliable because of misclassification of source text and word alignment errors so it causes a critical fallingoff in annotation projection quality", "To alleviate this problem we present three noise reduction strategies a heuristic filtering an alignment correction with dictionary and an instance selection based on assessment and combine these to yield a better result", "We provide a quantitive evaluation of our method on a new Korean RDC dataset", "In our experiment we leverage an EnglishKorean parallel corpus collected from the Web and demonstrate that the annotation projection approach and noise reduction method are beneficial to build an initial Korean relation detection system", "For example the combined model of three noise reduction methods achieves F1scores of 369 598 precision and 267 recall favorably comparing with the 305 shown by the supervised baseline1 The remainder of this paper is structured as follows", "In Section 2 we describe our crosslingual annotation projection approach to relation detection task", "Then we present the noise reduction methods in Section 3", "Our experiment on the proposed Korean RDC evaluation set is shown in Section 4 and Section 5 and we conclude this paper in Section 6"]}, "C10-1070": {"title": ["Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 617 625 Beijing August 2010 Revisiting Contextbased Projection Methods for TermTranslation Spotting in Comparable Corpora Audrey Laroche OLST  De p de linguistique et de traduction Universite de Montre  al audreylarocheumontrealca Philippe Langlais RALI  DIRO Universite de Montre  al felipeiroumontrealca Abstract Contextbased projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community eg Fung 1998 Rapp 1999 Surprisingly none of those works have systematically investigated the impact of the many parameters controlling their approach The present study aims at doing just this As a testcase we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice 1 Introduction Identifying translations of terms in comparable corpora is a challenge that has attracted many researchers A popular idea that emerged for solving this problem is based on the assumption that the context of a term and its translation share sim ilarities that can be used to rank translation candidates Fung 1998 Rapp 1999 Many variants of this idea have been implemented While a few studies have investigated pattern matching approaches to compare source and target contexts Fung 1995 Diab and Finch 2000 Yu and Tsujii 2009 most variants make use of a bilingual lexicon in order to translate the words of the context of a term often called seed words De jean et al 2005 instead use a bilingual thesaurus for translating these Another distinction between approaches lies in the way the context is defined The most common practice the socalled windowbased ap proach defines the context words as those cooccuring significantly with the source term within windows centered around the term1 Some studies have reported gains by considering syntactically motivated cooccurrences Yu and Tsujii 2009 propose a resourceintensive strategy which requires both source and target dependency parsers while Otero 2007 investigates a lighter approach where a few hand coded regular expressions based on POS tags simulate source parsing The latter approach only requires a POS tagger of the source and the target languages as well as a small parallel corpus in order to project the source regular expressions Naturally studies differ in the way each cooccurrence either window or syntaxbased is weighted and a plethora of association scores have been investigated and compared the likelihood score Dunning 1993 being among the most popular Also different similarity measures have been proposed for ranking target context vectors among which the popular cosine measure The goal of the different authors who investigate contextprojection approaches also varies Some studies are tackling the problem of identifying the translation of general words Rapp 1999 Otero 2007 Yu and Tsujii 2009 while others are addressing the translation of domain specific terms Among the latter many are trans lating singleword terms Chiao and Zweigenbaum 2002 Dejean et al 2005 Prochasson et 1A stoplist is typically used in order to prevent function words from populating the context vectors 617 al 2009 while others are tackling the translation of multiword terms Daille and Morin 2005 The type of discourse might as well be of con cern in some of the studies dedicated to bilingual terminology mining For instance Morin et al 2007 distinguish popular science versus scientific terms while Saralegi et al 2008 target popular science terms only The present discussion only focuses on a few number of representative studies Still it is already striking that a direct comparison of them is difficult if not impossible Differences in resources being used in quantities in domains etc in technical choices made similarity measures context vector computation etc and in objectives general versus terminological dictionary extraction prevent one from establishing a clear landscape of the various approaches Indeed many studies provide some figures that help to appreciate the influence of some parameters in a given experimental setting Notably Otero 2008 studies no less than 7 similarity measures for ranking context vectors while comparing window and syntaxbased methods Morin et al 2007 consider both the loglikelihood and the mutual information association scores as well as the Jaccard and the cosine similarity measures Ideally a benchmark on which researchers could run their translation finder would ease the comparison of the different approaches However designing such a benchmark that would satisfy the evaluation purposes of all the researchers is far too ambitious a goal for this contribution Instead we investigate the impact of some major factors influencing projectionbased approaches on a task of translating 5000 terms of the medical domain the most studied domain making use of French and English Wikipedia pages extracted monolingually thanks to an information retrieval engine While the present work does not investigate all the parameters that could potentially impact results we believe it constitutes the most complete and systematic comparison made so far with variants of the contextbased projection approach In the remainder of this paper we describe the projectionbased approach to translation spotting in Section 2 and detail the parameters that directly influence its performance The experimental protocol we followed is described in Section 3 and we analyze our results in Section 4 We discuss the main results in the light of previous work and propose some future avenues in Section 5 2 Projectionbased variants The approach we investigate for identifying term translations in comparable corpora is similar to Rapp 1999 and many others We describe in the following the different steps it encompasses and the parameters we are considering in the light of typical choices made in the literature"], "abstract": ["Contextbased projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community eg", "Fung 1998 Rapp 1999", "Surprisingly none of those works have systematically investigated the impact of the many parameters controlling their approach", "The present study aims at doing just this", "As a test case we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia", "One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice"], "inroduction": ["Identifying translations of terms in comparable corpora is a challenge that has attracted many researchers", "A popular idea that emerged for solving this problem is based on the assumption that the context of a term and its translation share similarities that can be used to rank translation candidates Fung 1998 Rapp 1999", "Many variants of this idea have been implemented", "While a few studies have investigated pattern matching approaches to compare source and target contexts Fung 1995 Diab and Finch 2000 Yu and Tsujii 2009 most variants make use of a bilingual lexicon in order to translate the words of the context of a term often called seed words", "Dejean et al", "2005 instead use a bilingual thesaurus for translating these", "Another distinction between approaches lies in the way the context is defined", "The most common practice the socalled windowbased approach defines the context words as those cooc curing significantly with the source term within windows centered around the term1 Some studies have reported gains by considering syntactically motivated cooccurrences", "Yu and Tsujii 2009 propose a resourceintensive strategy which requires both source and target dependency parsers while Otero 2007 investigates a lighter approach where a few hand coded regular expressions based on POS tags simulate source parsing", "The latter approach only requires a POS tagger of the source and the target languages as well as a small parallel corpus in order to project the source regular expressions", "Naturally studies differ in the way each co occurrence either window or syntaxbased is weighted and a plethora of association scores have been investigated and compared the likelihood score Dunning 1993 being among the most popular", "Also different similarity measures have been proposed for ranking target context vectors among which the popular cosine measure", "The goal of the different authors who investigate contextprojection approaches also varies", "Some studies are tackling the problem of identifying the translation of general words Rapp 1999 Otero 2007 Yu and Tsujii 2009 while others are addressing the translation of domain specific terms", "Among the latter many are translating singleword terms Chiao and Zweigenbaum 2002 Dejean et al 2005 Prochasson et 1 A stoplist is typically used in order to prevent function words from populating the context vectors", "617 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 617625 Beijing August 2010 al 2009 while others are tackling the translation of multiword terms Daille and Morin 2005", "The type of discourse might as well be of concern in some of the studies dedicated to bilingual terminology mining", "For instance Morin et al", "2007 distinguish popular science versus scientific terms while Saralegi et al", "2008 target popular science terms only", "The present discussion only focuses on a few number of representative studies", "Still it is already striking that a direct comparison of them is difficult if not impossible", "Differences in resources being used in quantities in domains etc in technical choices made similarity measures context vector computation etc and in objectives general versus terminological dictionary extraction prevent one from establishing a clear landscape of the various approaches", "Indeed many studies provide some figures that help to appreciate the influence of some parameters in a given experimental setting", "Notably Otero 2008 studies no less than 7 similarity measures for ranking context vectors while comparing window and syntaxbased methods", "Morin et al", "2007 consider both the loglikelihood and the mutual information association scores as well as the Jaccard and the cosine similarity measures", "Ideally a benchmark on which researchers could run their translation finder would ease the comparison of the different approaches", "However designing such a benchmark that would satisfy the evaluation purposes of all the researchers is far too ambitious a goal for this contribution", "Instead we investigate the impact of some major factors influencing projectionbased approaches on a task of translating 5000 terms of the medical domain the most studied domain making use of French and English Wikipedia pages extracted monolingually thanks to an information retrieval engine", "While the present work does not investigate all the parameters that could potentially impact results we believe it constitutes the most complete and systematic comparison made so far with variants of the contextbased projection approach", "In the remainder of this paper we describe the projectionbased approach to translation spotting in Section 2 and detail the parameters that directly influence its performance", "The experimental pro tocol we followed is described in Section 3 and we analyze our results in Section 4", "We discuss the main results in the light of previous work and propose some future avenues in Section 5"]}, "C10-1081": {"title": ["Semantic Role Features for Machine Translation"], "abstract": ["We propose semantic role features for a TreetoString transducer to model the reorderingdeletion of sourceside semantic roles", "These semantic features as well as the TreetoString templates are trained based on a conditional loglinear model and are shown to significantly outperform systems trained based on MaxLikelihood and EM", "We also show significant improvement in sentence fluency by using the semantic role features in the loglinear model based on manual evaluation"], "inroduction": ["Syntaxbased statistical machine translation SSMT has achieved significant progress during recent years Galley et al 2006 May and Knight 2007 Liu et al 2006 Huang et al 2006 showing that deep linguistic knowledge if used properly can improve MT performance", "Semanticsbased SMT as a natural extension to SSMT has begun to receive more attention from researchers Liu and Gildea 2008 Wu and Fung 2009", "Semantic structures have two major advantages over syntactic structures in terms of helping machine translation", "First of all semantic roles tend to agree better between two languages than syntactic constituents Fung et al 2006", "This property motivates the approach of using the consistency of semantic roles to select MT outputs Wu and Fung 2009", "Secondly the set of semantic roles of a predicate models the skeleton of a sentence which is crucial to the readability of MT output", "By skeleton we mean the main structure of a sentence including the verbs and their arguments", "In spite of the theoretical potential of the semantic roles there has not been much success in using them to improve SMT systems", "Liu and Gildea 2008 proposed a semantic role based TreetoString TTS transducer by adding semantic roles to the TTS templates", "Their approach did not differentiate the semantic roles of different predicates and did not always improve the TTS transducers performance", "Wu and Fung 2009 took the output of a phrasebased SMT system Moses Koehn et al 2007 and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence", "This approach shows the positive effect of applying semantic role constraints but it requires retagging semantic roles for every permuted MT output and does not scale well to longer sentences", "This paper explores ways of tightly integrating semantic role features SRFs into an MT system rather than using them in postprocessing or n best reranking", "Semantic role labeling SRL systems usually use sentencewide features Xue and Palmer 2004 Pradhan et al 2004 Toutanova et al 2005 thus it is difficult to compute target side semantic roles incrementally during decoding", "Noticing that the source side semantic roles are easy to compute we apply a compromise approach where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences", "Since this approach does not perform true SRL on the target string it cannot fully evaluate whether the source and target semantic structures are consistent", "However the approach does capture the semanticlevel reordering of the sentences", "We assume here that the MT system is capable of providing word alignment or equivalent information during decoding which is generally true for current statistical MT systems", "Specifically two types of semantic role features are proposed in this paper a semantic role reordering feature designed to capture the skeleton level permutation and a semantic role deletion fea 716 Proceedings of the 23rd International Conference on Computational Linguistics Coling 2010 pages 716724 Beijing August 2010 ture designed to penalize missing semantic roles in the target sentence", "To use these features during decoding we need to keep track of the semantic role sequences SRS for partial translations which can be generated based on the sourceside semantic role sequence and the corresponding word alignments", "Since the SRL system and the MT system are separate a translation rule eg a phrase pair in phrasebased SMT could cover two partial sourceside semantic roles", "In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs", "Dealing with this problem will increase the complexity of the decoding algorithm", "Fortunately Treeto String transducer based MT systems Liu et al 2006 Huang et al 2006 can avoid this problem by using the same syntax tree for both SRL and MT Such an arrangement guarantees that a TTS template either covers parts of one sourceside semantic role or a few complete semantic roles", "This advantage motivates us to use a TTS transducer as the MT system with which to demonstrate the use of the proposed semantic role features", "Since it is hard to design a generative model to combine both the semantic role features and the TTS templates we use a loglinear model to estimate the feature weights by maximizing the conditional probabilities of the target strings given the source syntax trees", "The loglinear model with latent variables has been discussed by Blunsom et al", "2008 we apply this technique to combine the TTS templates and the semantic role features", "The remainder of the paper is organized as follows Section 2 describes the semantic role features proposed for machine translation Section 3 describes how semantic role features are used and trained in a TTS transducer Section 4 presents the experimental results and Section 5 gives the conclusion"]}, "C10-2017": {"title": ["Coling 2010 Poster Volume pages 144 152 Beijing August 2010 The True Score of Statistical Paraphrase Generation Jonathan Chevelu12 Ghislain Putois2 Yves Lepage3"], "abstract": ["This article delves into the scoring function of the statistical paraphrase generation model", "It presents an algorithm for exact computation and two applicative experiments", "The first experiment analyses the behaviour of a statistical paraphrase generation decoder and raises some issues with the ordering of nbest outputs", "The second experiment shows that a major boost of performance can be obtained by embedding a true score computation inside a MonteCarlo sampling based paraphrase generator"], "inroduction": ["A paraphrase generator is a program which given a source sentence produces a new sentence with almost the same meaning", "The modification place is not imposed but the paraphrase has to differ from the original sentence", "Paraphrase generation is useful in applications where it is needed to choose between different forms to keep the most fit", "For instance automatic summary can be seen as a particular paraphrasing task Barzilay and Lee 2003 by selecting the shortest paraphrase", "They can help human writers by proposing alternatives and having them choose the most appropriate Max and Zock 2008", "Paraphrases can also be used to improve natural language processing NLP systems", "In this direction CallisonBurch et al 2006 tried to improve machine translations by enlarging the coverage of patterns that can be translated", "In the same way most NLP systems like information retrieval Sekine 2005 or question answering Duclaye et al 2003 based on pattern recognition can be improved by a paraphrase generator", "Most of these applications need a nbest set of solutions in order to rerank them according to a taskspecific criterion", "In order to produce the paraphrases a promising approach is to see the paraphrase generation problem as a statistical translation problem", "In that approach the target language becomes the same as the source language Quirk et al 2004 Bannard and CallisonBurch 2005 Max and Zock 2008", "The first difficulty of this approach is the need of a paraphrase table", "A paraphrase table is a monolingual version of a translation table in the statistical machine translation SMT field", "In this field the difficulty is basically overcome by using huge aligned bilingual corpora like the Europarl Koehn 2005 corpus", "In the paraphrase generation field one needs a huge aligned monolingual corpus to build a paraphrase table", "The low availability of such monolingual corpora nurtures researches in order to find heuristics to produce them Barzilay and Lee 2003 Quirk et al 2004", "On the other hand an interesting method proposed by Bannard and CallisonBurch 2005 tries to make a paraphrase table using a translation table learned on bilingual corpora", "The method uses a wellknown heuristic Lepage and Denoual 2005 which says that if two sentences have the same translation then they should be paraphrases of each others", "Another aspect less studied is the generation process of paraphrases ie the decoding process in SMT", "This process is subject to combinatorial 144 Coling 2010 Poster Volume pages 144152 Beijing August 2010 explosions", "Heuristics are then frequently used to drive the exploration process in the a priori intractable high dimensional spaces", "On the one hand these heuristics are used to build a para source and target languages ie the paraphrase table", "This can be decomposed into t  arg max P t TI P sI I phrase step by step according to the paraphrase tI iI i ti  B table", "On the other hand they try to evaluate the where I is a partition of the source sentence andrelevance of a step according to the global para phrase generation model", "The SMT model score xI the ith segment in the sentence x For a given is related to the path followed to generate a paraphrase", "Because of the stepbystep computation different ways can produce the same paraphrase but with different scores", "Amongst these scores the best one is the true score of a paraphrase according to the SMT model", "Most paraphrase generators use some standard SMT decoding algorithms Quirk et al 2004 or some offtheshelf decoding tools like MOSES", "The goal of these decoders is to find the best path in the lattice produced by the paraphrase table", "This is basically achieved by using dynamic programming  especially the Viterbi algorithm  and beam searching Koehn et al 2007", "The best paraphrase proposed by these programs is known not to be the optimal paraphrase", "One can even question if the score returned is the true score", "We first show in Section 2 that in the particular domain of statistical paraphrase generation one can compute true a posteriori scores of generated paraphrases", "We then explore some applications of the true score algorithm in the paraphrase generation field", "In Section 3 we show that scores returned by SMT decoders are not always true scores and they plague the ranking of output nbest solutions", "In Section 4 we show that the true score can give a major boost for holistic paraphrases generators which do not rely on decoding approaches"]}, "C10-2023": {"title": ["Coling 2010 Poster Volume pages 197 205 Beijing August 2010 Improving Reordering with Linguistically Informed Bilingual ngrams Josep Maria Crego LIMSICNRS jmcregolimsifr Franc ois Yvon LIMSICNRS  Univ Paris Sud yvonlimsifr Abstract We present a new reordering model estimated as a standard ngram language model with units built from morphosyntactic information of the source and target languages It can be seen as a model that translates the morphosyntactic structure of the input sentence in contrast to standard translation models which take care of the surface word forms We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual con text that is considered during the trans lation process thus effectively accounting for midrange reorderings Empirical results on FrenchEnglish and GermanEnglish translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model 1 Introduction Word ordering is one of the major issues in statistical machine translation SMT due to the many word order peculiarities of each language It is widely accepted that there is a need for structural information to account for such differences Structural information such as Partofspeech POS tags chunks or constituencydependency parse trees offers a greater potential to learn generalizations about relationships between languages than models based on word surface forms because such  surfacist models fail to infer generalizations from the training data The word ordering problem is typically decomposed in a number of related problems which can be further explained by a variety of linguistic phe nomena Accordingly we can sort out the reordering problems into three categories based on the kind of linguistic units involved andor the typical distortion distance they imply Roughly speaking we face shortrange reorderings when single words are reordered within a relatively small window distance It consist of the easiest case as typically the use of phrases in the sense of translation units of the phrasebased approach to SMT is believed to adequately perform such reorderings Midrange reorderings involve reorderings between two or more phrases translation units which are closely positioned typically within a window of about 6 words Many alternatives have been proposed to tackle midrange reorderings through the introduction of linguistic information in MT systems To the best of our knowledge the authors of Xia and McCord 2004 were the first to address this problem in the statistical MT paradigm They automatically build a set of linguistically grounded rewrite rules aimed at reordering the source sentence so as to match the word order of the target side Similarly Collins et al2005 and Popovic and Ney 2006 reorder the source sentence using a small set of handcrafted rules for GermanEnglish translation Crego and Marin  o 2007 show that the ordering problem can be more accurately solved by building a sourcesentence word lattice containing the most promising reordering hypotheses allowing the decoder to decide for the best word order hypothesis Word lattices are built by means of rewrite rules operating on POS tags such rules are automatically extracted from the training bitext Zhang et al2007 introduce shallow parse chunk information to reorder the source sentence aiming at extending the scope of their rewrite rules encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder These studies tackle midrange reorderings by predicting more or less accurate reordering hypotheses However none 197 of them introduce a reordering model to be used in decoding time Nowadays most of SMT systems implement the well known lexicalized reordering model Tillman 2004 Basically for each translation unit it estimates the probability of being translated monotone swapped or placed discontiguous with respect to its previous translation unit Integrated within the Moses Koehn et al2007 decoder the model achieves stateoftheart results for many translation tasks One of the main reasons that explains the success of the model is that it considers information of the source and targetside surface forms while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the sourceside words Finally longrange reorderings imply reorderings in the structure of the sentence Such reorderings are necessary to model the translation for pairs like ArabicEnglish as English typically follows the SVO order while Arabic sentences have different structures Even if several attempts exist which follow the above idea of making the ordering of the source sentence similar to the target sentence before decoding Niehues and Kolss 2009 longrange reorderings are typically better addressed by syntaxbased and hierarchical Chiang 2007 models In Zollmann et al 2008 an interesting comparison between phrasebased hierarchical and syntaxaugmented models is carried out concluding that hierarchical and syntaxbased models slightly outperform phrasebased models under large data conditions and for sufficiently nonmonotonic language pairs Encouraged by the work reported in Hoang and Koehn 2009 we tackle the midrange reordering problem in SMT by introducing a ngram language model of bilingual units built from POS information The rationale behind such a model is double on the one hand we aim at introducing morphosyntactic information into the reordering model as we believe it plays an important role for predicting systematic word ordering differences between language pairs at the same time that it drastically reduces the sparseness problem of standard translation units built from surface forms On the other hand ngram language modeling is a robust approach that enables to account for arbitrary large sequences of units Hence the proposed model takes care of the translation adequacy of the structural information present in translation hypotheses here introduced in the form of POS tags We also show how the new model compares to a widely used lexicalized reordering model which we have also implemented in our particular bilingual ngram approach to SMT as well as to the widely known Moses SMT decoder a stateoftheart decoder performing lexicalized reordering The remaining of this paper is as follows In Section 2 we briefly describe the bilingual ngram SMT system Section 3 details the bilingual ngram reordering model the main contribution of this paper and introduces additional well known reordering models In Section 4 we analyze the reordering needs of the language pairs considered in this work and we carry out evaluation experiments Finally we conclude and outline further work in Section 5 2 Bilingual ngram SMT Our SMT system defines a translation hypothesis t given a source sentence s as the sentence which maximizes a linear combination of feature functions t I1  argmaxtI1  M m1 mhmsJ1  tI1 "], "abstract": ["We present a new reordering model estimated as a standard ngram language model with units built from morpho syntactic information of the source and target languages", "It can be seen as a model that translates the morphosyntactic structure of the input sentence in contrast to standard translation models which take care of the surface word forms", "We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process thus effectively accounting for midrange reorderings", "Empirical results on FrenchEnglish and GermanEnglish translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model"], "inroduction": ["Word ordering is one of the major issues in statistical machine translation SMT due to the many word order peculiarities of each language", "It is widely accepted that there is a need for structural information to account for such differences", "Structural information such as Partofspeech POS tags chunks or constituencydependency parse trees offers a greater potential to learn generalizations about relationships between languages than models based on word surface forms because such surfacist models fail to infer generalizations from the training data", "The word ordering problem is typically decomposed in a number of related problems which can be further explained by a variety of linguistic phenomena", "Accordingly we can sort out the reordering problems into three categories based on the kind of linguistic units involved andor the typical distortion distance they imply", "Roughly speaking we face shortrange reorderings when single words are reordered within a relatively small window distance", "It consist of the easiest case as typically the use of phrases in the sense of translation units of the phrasebased approach to SMT is believed to adequately perform such reorderings", "Midrange reorderings involve reorderings between two or more phrases translation units which are closely positioned typically within a window of about 6 words", "Many alternatives have been proposed to tackle mid range reorderings through the introduction of linguistic information in MT systems", "To the best of our knowledge the authors of Xia and Mc Cord 2004 were the first to address this problem in the statistical MT paradigm", "They automatically build a set of linguistically grounded rewrite rules aimed at reordering the source sentence so as to match the word order of the target side", "Similarly Collins et al 2005 and Popovic and Ney 2006 reorder the source sentence using a small set of handcrafted rules for GermanEnglish translation", "Crego and Marin o 2007 show that the ordering problem can be more accurately solved by building a sourcesentence word lattice containing the most promising reordering hypotheses allowing the decoder to decide for the best word order hypothesis", "Word lattices are built by means of rewrite rules operating on POS tags such rules are automatically extracted from the training bitext", "Zhang et al 2007 introduce shallow parse chunk information to reorder the source sentence aiming at extending the scope of their rewrite rules encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder", "These studies tackle midrange reorderings by predicting more or less accurate reordering hypotheses", "However none 197 Coling 2010 Poster Volume pages 197205 Beijing August 2010 of them introduce a reordering model to be used in decoding time", "Nowadays most of SMT systems implement the well known lexicalized reordering model Tillman 2004", "Basically for each translation unit it estimates the probability of being translated monotone swapped or placed discontiguous with respect to its previous translation unit", "Integrated within the Moses Koehn et al 2007 decoder the model achieves stateof theart results for many translation tasks", "One of the main reasons that explains the success of the model is that it considers information of the source and targetside surface forms while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the sourceside words", "Finally longrange reorderings imply reorder ings in the structure of the sentence", "Such reorderings are necessary to model the translation for pairs like ArabicEnglish as English typically follows the SVO order while Arabic sentences have different structures", "Even if several attempts exist which follow the above idea of making the ordering of the source sentence similar to the target sentence before decoding Niehues and Kolss 2009 longrange reorderings are typically better addressed by syntaxbased and hierarchical Chi ang 2007 models", "In Zollmann et al 2008 ables to account for arbitrary large sequences of units", "Hence the proposed model takes care of the translation adequacy of the structural information present in translation hypotheses here introduced in the form of POS tags", "We also show how the new model compares to a widely used lexical ized reordering model which we have also implemented in our particular bilingual ngram approach to SMT as well as to the widely known Moses SMT decoder a stateoftheart decoder performing lexicalized reordering", "The remaining of this paper is as follows", "In Section 2 we briefly describe the bilingual ngram SMT system", "Section 3 details the bilingual n gram reordering model the main contribution of this paper and introduces additional well known reordering models", "In Section 4 we analyze the reordering needs of the language pairs considered in this work and we carry out evaluation experiments", "Finally we conclude and outline further work in Section 5"]}, "C10-2052": {"title": ["Whats in a Preposition"], "abstract": ["Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments", "We explore this idea for prepositions an often overlooked word class", "We examine the parameters that must be considered in preposition disambiguation namely context features and granularity", "Doing so delivers an increased performance that significantly improves over two stateof theart systems and shows potential for improving other word sense disambiguation tasks", "We report accuracies of 918 and 848 for coarse and finegrained preposition sense disambiguation respectively"], "inroduction": ["Ambiguity is one of the central topics in NLP", "A substantial amount of work has been devoted to disambiguating prepositional attachment words and names", "Prepositions as with most other word types are ambiguous", "For example the word in can assume both temporal in May and spatial in the US meanings as well as others less easily classifiable in that vein", "Prepositions typically have more senses than nouns or verbs Litkowski and Hargraves 2005 making them difficult to disambiguate", "Preposition sense disambiguation PSD has many potential uses", "For example due to the relational nature of prepositions disambiguating their senses can help with allword sense disambiguation", "In machine translation different senses of the same English preposition often correspond to different translations in the foreign language", "Thus disambiguating prepositions correctly may help improve translation quality1 Coarsegrained PSD can also be valuable for information extraction where the sense acts as a label", "In a recent study Hwang et al", "2010 identified preposition related features among them the coarsegrained PP labels used here as the most informative feature in identifying causedmotion constructions", "Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing one of the most frequent sources of parse errors", "Several papers have successfully addressed PSD with a variety of different approaches Rudzicz and Mokhov 2003 OHara and Wiebe 2003 Ye and Baldwin 2007 OHara and Wiebe 2009 Tratz and Hovy 2009", "However while it is often possible to increase accuracy by using a different classifier andor more features adding more features creates two problems a it can lead to overfitting and b while possibly improving accuracy it is not always clear where this improvement comes from and which features are actually informative", "While parameter studies exist for general word sense disambiguation WSD tasks Yarowsky and Florian 2002 and PSD accuracy has been steadily increasing there has been no exploration of the parameters of prepositions to guide engineering decisions", "We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative", "We explore the different options for context and feature se 1 See Chan et al 2007 for the relevance of word sense disambiguation and Chiang et al 2009 for the role of prepositions in MT 454 Coling 2010 Poster Volume pages 454462 Beijing August 2010 lection the influence of different preprocessing methods and different levels of sense granularity", "Using the resulting parameters in a Maximum Entropy classifier we are able to improve significantly over existing results", "The general outline we present can potentially be extended to other word classes and improve WSD in general"]}, "C10-2087": {"title": ["DependencyDriven Featurebased Learning for Extracting"], "abstract": ["Recent kernelbased PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information but at the expense of computational complexity", "This paper incorporates dependency information as well as other lexical and syntactic knowledge in a featurebased framework", "Our motivation is that considering the large amount of biomedical literature being archived daily featurebased methods with comparable performance are more suitable for practical applications", "Additionally we explore the difference of lexical characteristics between biomedical and newswire domains", "Experimental evaluation on the AIMed corpus shows that our system achieves comparable performance of 547 in F1Score with other stateoftheart PPI extraction systems yet the best performance among all the featurebased ones"], "inroduction": ["In recent years automatically extracting biomedical information has been the subject of significant research efforts due to the rapid growth in biomedical development and discovery", "A wide concern is how to characterize protein interaction partners since it is crucial to understand not only the functional role of individual proteins but also the organization of the entire biological process", "However manual collection of relevant ProteinProtein Interaction PPI information from thousands of research papers published every day is so timeconsuming that automatic extraction approaches with the help of Natural Language Processing NLP techniques become necessary", "Various machine learning approaches for relation extraction have been applied to the biomedical domain which can be classified into two categories featurebased methods Mitsumori et al 2006 Giuliano et al 2006 Stre et al 2007 and kernelbased methods Bunescu et al 2005 Erkan et al 2007 Airola et al 2008 Kim et al 2010", "Provided a largescale manually annotated corpus the task of PPI extraction can be formulated as a classification problem", "Typically for featuredbased learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names", "Early studies identify the existence of protein interactions by using bagofwords features usually unigram or bigram around the protein names as well as various kinds of shallow linguistic information such as POS tag lemma and orthographical features", "However these systems do not achieve promising results since they disregard any syntactic or semantic information altogether which are very useful for the task of relation extraction in the newswire domain Zhao and Grishman 2005 Zhou et al 2005", "Furthermore featurebased methods fail to effectively capture the structural information which is essential to  Corresponding author 757 Coling 2010 Poster Volume pages 757765 Beijing August 2010 With the wide application of kernelbased methods to many NLP tasks various kernels such as subsequence kernels Bunescu and Mooney 2005 and tree kernels Li et al 2008 are also applied to PPI detection", "Particularly dependencybased kernels such as edit distance kernels Erkan et al 2007 and graph kernels Airola et al 2008 Kim et al 2010 show some promising results for PPI extraction", "This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories Culotta and Sorensen 2004", "In order to appreciate the advantages of both featurebased methods and kernelbased methods composite kernels Miyao et al 2008 Miwa et al 2009a Miwa et al 2009b are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction", "However one critical challenge for kernelbased methods is their computation complexity which prevents them from being widely deployed in realworld applications regarding the large amount of biomedical literature being archived everyday", "Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernelbased methods one may naturally ask the question Can the essential dependency information be maximally exploited in featuredbased PPI extraction so as to enhance the performance without loss of efficiency If the answer is Yes then How This paper addresses these problems focusing on the application of dependency information to featurebased PPI extraction", "Starting from a baseline system in which common lexical and syntactic features are incorporated using Support Vector Machines SVM we further augment the baseline with various features related to dependency information including predicates in the dependency tree", "Moreover in order to reveal the linguistic difference between distinct domains we also compare the effects of various features on PPI extraction from biomedical texts with those on relation extraction from newswire narratives", "Evaluation on the AIMed and other PPI cor The rest of the paper is organized as follows", "A featurebased PPI extraction baseline system is given in Section 2 while Section 3 describes our dependencydriven method", "We report our experiments in Section 4 and compare our work with the related ones in Section 5", "Section 6 concludes this paper and gives some future directions"]}, "C10-2101": {"title": ["Semantic Classification of Automatically Acquired Nouns"], "abstract": ["In this paper we present a twostage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them", "We first acquire unknown morphemes only making a morphology level distinction and then apply semantic classification to acquired nouns", "One advantage of this approach is that at the second stage we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition we can rely on automatic parsing", "Japanese semantic classification poses an interesting challenge proper nouns need to be distinguished from common nouns", "It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them", "We explore lexicosyntactic clues that are extracted from automatically parsed text and investigate their effects"], "inroduction": ["A dictionary plays an important role in Japanese morphological analysis or the joint task of segmentation and partofspeech POS tagging Kurohashi et al 1994 Asahara and Mat sumoto 2000 Kudo et al 2004", "Like Chinese and Thai Japanese does not delimit words by whitespace", "This makes the first step of natural language processing more ambiguous than simple POS tagging", "Accordingly morphemes in a predefined dictionary compactly represent our knowledge about both segmentation and POS", "One obvious problem with the dictionarybased approach is caused by unknown morphemes or morphemes not defined in the dictionary", "Even though historically extensive human resources were used to build highcoverage dictionaries Yokoi 1995 texts other than newspaper articles in particular web pages contain a large number of unknown morphemes", "These unknown morphemes often cause segmentation errors", "For example morphological analyzer JU MAN 601 wrongly segments the phrase   saQporo eki Sapporo Station where   saQporo is an unknown morpheme as follows  sa nouncommon difference  Q UNK  po UNK  ro nouncommon sumac and  eki nouncommon station where UNK refers to unknown morphemes automatically identified by the analyzer", "Such an erroneous sequence has disastrous effects on applications of morphological analysis", "For example it can hardly be identified as a LOCATION in named entity recognition", "One solution to the unknown morpheme problem is unknown morpheme acquisition Mori and Nagao 1996 Murawaki and Kurohashi 2008", "It is the task of automatically augmenting the dictionary by acquiring unknown morphemes from text", "In the above example the goal is to acquire the morpheme  saQporo with the POS tag nounlocation name However unknown morpheme acquisition usually adopts a coarser POS tagset that only represents the morphology level distinction among noun verb and adjective", "This means that  saQporo is acquired as just a noun and that the semantic label location name remains to be assigned", "The reason only the morphology level distinction is made is 1 httpnlpkueekyotouacjp nlresourcejumanehtml 876 Coling 2010 Poster Volume pages 876884 Beijing August 2010 that the semantic level distinction cannot easily be captured with morphological clues that are exploited in unknown morpheme acquisition", "In this paper we investigate the remaining problem and introduce the new task of semantic classification that is to be applied to automatically acquired nouns", "In this task we can exploit syntactic clues in addition to morphological ones because as a result of acquisition we can now rely on automatic parsing", "For exam ple since text containing  saQporo noununclassified is correctly segmented we can extract not only the phrase saQporo station but the tree fragment  go to saQporo and we can determine its semantic label", "Japanese semantic classification poses an interesting challenge proper nouns need to be distinguished from common nouns", "Like Chinese and Thai Japanese has no orthographic distinction between common and proper nouns as there is no such thing as capitalization", "In addition there seems no morphosyntactic ie grammatical distinction between them", "In this paper we explore lexicosyntactic clues that can be extracted from automatically parsed text", "We train a classification model on manually registered nouns and apply it to automatically acquired nouns", "We then investigate the effects of lexicosyntactic clues"]}, "C10-2104": {"title": ["Kernelbased Reranking for NamedEntity Extraction"], "abstract": ["We present novel kernels based on structured and unstructured features for reranking the Nbest hypotheses of conditional random fields CRFs applied to entity extraction", "The former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples", "The experiments on two standard corpora in two languages ie the Italian EVALITA 2009 and the English CoNLL 2003 datasets show a large improvement on CRFs in Fmeasure ie from 8034 to 8433 and from 8486 to 8816 respectively", "Our analysis reveals that both kernels provide a comparable improvement over the CRFs baseline", "Additionally their combination improves CRFs much more than the sum of the individual contributions suggesting an interesting kernel synergy"], "inroduction": ["Reranking is a promising computational framework which has drawn special attention in the Natural Language Processing NLP community", "Basically this method first employs a probabilistic model to generate a list of topn candidates and then reranks this nbest list with additional features", "One appeal of this approach is its flexibility of incorporating arbitrary features into a model", "These features help in discriminating good from bad hypotheses and consequently their automatic learning", "Various algorithms have been applied for reranking in NLP applications Huang 2008 Shen et al 2004 Collins 2002b Collins and Koo 2000 including parsing name tagging and machine translation", "This work has exploited the disciminative property as one of the key criterion of the reranking algorithm", "Reranking appears extremely interesting if coupled with kernel methods Dinarelli et al 2009 Moschitti 2004 Collins and Duffy 2001 as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies", "Indeed while featurebased learning algorithms involve only the dotproduct between feature vectors kernel methods allow for a higher generalization by replacing the dot product with a function between pairs of linguistic objects", "Such functions are a kind of similarity measure satisfying certain properties", "An example is the tree kernel Collins and Duffy 2001 where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees", "Similarly sequence kernels Lodhi et al 2002 count the number of common subsequences shared by two input strings", "Namedentities NEs are essential for defining the semantics of a document", "NEs are objects that can be referred by names Chinchor and Robinson 1998 such as people organizations and locations", "The research on NER has been promoted by the Message Understanding Conferences MUCs 19871998 the shared task of the Conference on Natural Language Learning CoNLL 20022003 and the Automatic Content Extraction program ACE 20022005", "In the literature there exist various learning approaches to extract namedentities from text", "A NER sys 901 Coling 2010 Poster Volume pages 901909 Beijing August 2010 tem often builds some generativediscriminative model then either uses only one classifier Car reras et al 2002 or combines many classifiers using some heuristics Florian et al 2003", "To the best of our knowledge reranking has not been applied to NER except for the reranking algorithms defined in Collins 2002b Collins 2002a which only targeted the entity detection and not entity classification task", "Besides since kernel methods offer a natural way to exploit linguistic properties applying kernels for NE reranking is worthwhile", "In this paper we describe how kernel methods can be applied for reranking ie detection and classification of namedentities in standard corpora for Italian and English", "The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences", "For this purpose we apply tree kernels to a tree structure encoding NE tags of a sentence and combined them with a polynomial kernel which efficiently exploits global features", "Our main contribution is to show that a tree kernels can be used to define general features not merely syntactic and b using appropriate algorithms and features reranking can be very effective for namedentity recognition", "Our study demonstrates that the composite kernel is very effective for reranking namedentity sequences", "Without the need of producing and heuristically combining learning models like previous work on NER the composite kernel not only captures most of the flat features but also efficiently exploits structured features", "More interestingly this kernel yields significant improvement when applied to two corpora of two different languages", "The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the stateoftheart"]}, "C10-2139": {"title": ["Coling 2010 Poster Volume pages 1211 1219 Beijing August 2010 WordBased and CharacterBased Word Segmentation Models Comparison and Combination Weiwei Sun Department of Computational Linguistics Saarland University German Research Center for Artificial Intelligence DFKI wsuncoliunisaarlandde Abstract We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation wordbased models and characterbased models We show that in spite of similar performance overall the two models produce different distribution of segmentation errors in a way that can be explained by theoretical properties of the two models The analysis is further exploited to improve segmentation accuracy by integrating a wordbased segmenter and a characterbased segmenter A Bootstrap Aggregating model is proposed By letting multiple segmenters vote our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff 1 Introduction To find the basic language units ie words segmentation is a necessary initial step for Chi nese language processing There are two dominant models for Chinese word segmentation The first one is what we call wordbased approach where the basic predicting units are words themselves This kind of segmenters sequentially decides whether the local sequence of characters make up a word This wordbyword ap proach ranges from naive maximum matching Chen and Liu 1992 to complex solution based on semiMarkov conditional random fields CRF Andrew 2006 The second is characterbased approach where basic processing units are characters which compose words Segmentation is formulated as a classification problem to predict whether a character locates at the beginning of inside or at the end of a word This characterbycharacter method was first proposed in Xue 2003 and a number of sequence labeling algorithms have been exploited This paper is concerned with the behavior of different segmentation models in general We present a theoretical and empirical comparative analysis of the two dominant approaches Theoretically these approaches are different The wordbased models do prediction on a dynamic sequence of possible words while characterbased models on a static character sequence The former models have a stronger ability to represent word token features for disambiguation while the latter models can better induce a word from its internal structure For empirical analysis we implement two segmenters both using the PassiveAggressive algorithm Crammer et al 2006 to estimate parameters Our experiments indicate that despite similar performance in terms of overall Fscore the two models produce different types of errors in a way that can be explained by theoretical properties We will present a detailed analysis that reveals important differences of the two methods in Sec 4 The two types of approaches exhibit different behaviors and each segmentation model has strengths and weaknesses We further consider integrating wordbased and characterbased models in order to exploit their complementary strengths and thereby improve segmentation accuracy be yond what is possible by either model in isolation We present a Bootstrap Aggregating model to combine multiple segmentation systems By 1211 letting multiple segmenters vote our combination model improves accuracy consistently on all the four different segmentation data sets from the second SIGHAN bakeoff We also compare our integrating system to the stateoftheart segmentation systems Our system obtains the highest reported Fscores on three data sets 2 Two Methods for Word Segmentation First of all we distinguish two kinds of  words  "], "abstract": ["We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation wordbased models and characterbased models", "We show that in spite of similar performance overall the two models produce different distribution of segmentation errors in a way that can be explained by theoretical properties of the two models", "The analysis is further exploited to improve segmentation accuracy by integrating a wordbased segmenter and a characterbased segmenter", "A Bootstrap Aggregating model is proposed", "By letting multiple segmenters vote our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff"], "inroduction": ["To find the basic language units ie words segmentation is a necessary initial step for Chinese language processing", "There are two dominant models for Chinese word segmentation", "The first one is what we call wordbased approach where the basic predicting units are words themselves", "This kind of segmenters sequentially decides whether the local sequence of characters make up a word", "This wordbyword approach ranges from naive maximum matching Chen and Liu 1992 to complex solution based on semiMarkov conditional random fields CRF Andrew 2006", "The second is characterbased approach where basic processing units are characters which compose words", "Segmentation is formulated as a classification problem to predict whether a character locates at the beginning of inside or at the end of a word", "This character bycharacter method was first proposed in Xue 2003 and a number of sequence labeling algorithms have been exploited", "This paper is concerned with the behavior of different segmentation models in general", "We present a theoretical and empirical comparative analysis of the two dominant approaches", "Theoretically these approaches are different", "The wordbased models do prediction on a dynamic sequence of possible words while character based models on a static character sequence", "The former models have a stronger ability to represent word token features for disambiguation while the latter models can better induce a word from its internal structure", "For empirical analysis we implement two segmenters both using the Passive Aggressive algorithm Crammer et al 2006 to estimate parameters", "Our experiments indicate that despite similar performance in terms of overall Fscore the two models produce different types of errors in a way that can be explained by theoretical properties", "We will present a detailed analysis that reveals important differences of the two methods in Sec", "4", "The two types of approaches exhibit different behaviors and each segmentation model has strengths and weaknesses", "We further consider integrating wordbased and characterbased models in order to exploit their complementary strengths and thereby improve segmentation accuracy beyond what is possible by either model in isolation", "We present a Bootstrap Aggregating model to combine multiple segmentation systems", "By 1211 Coling 2010 Poster Volume pages 12111219 Beijing August 2010 letting multiple segmenters vote our combination model improves accuracy consistently on all the four different segmentation data sets from the second SIGHAN bakeoff", "We also compare our integrating system to the stateoftheart segmentation and a function GEN that enumerates a set of segmentation candidates GENc for c In general a segmenter solves the following argmax problem systems", "Our system obtains the highest reported Fscores on three data sets"]}, "C10-2164": {"title": ["Coling 2010 Poster Volume pages 1435 1443 Beijing August 2010 Fusion of Multiple Features and Ranking SVM for Webbased EnglishChinese OOV Term Translation Yuejie Zhang Yang Wang Lei Cen Yanxia Su Cheng Jin Xiangyang Xue School of Computer Science Shanghai Key Laboratory of Intelligent Information Processing Fudan University yjzhang072021176082024072 09210240074jcxyxuefudaneducn Jianping Fan Department of Computer Science The University of North Carolina at Charlotte jfanunccedu"], "abstract": ["This paper focuses on the Webbased EnglishChinese OOV term translation pattern and emphasizes particularly on the translation selection strategy based on the fusion of multiple features and the ranking mechanism based on Ranking Support Vector Machine Ranking SVM", "By utilizing the CoNLL2003 corpus for the English Named Entity Recognition NER task and selected new terms the experiments based on different data sources show the consistent results", "Our OOV term translation model can filter the most possible translation candidates with better ability", "From the experimental results for combining our OOV term translation model with EnglishChinese Cross Language Information Retrieval CLIR on the data sets of Text Retrieval Evaluation Conference TREC it can be found that the obvious performance improvement for both query translation and retrieval can also be obtained"], "inroduction": ["In CrossLanguage Information Retrieval CLIR most of users queries are generally composed of short terms in which there are many OutofVocabulary OOV terms like Named Entities NEs new words terminologies and so on", "The translation quality of OOV term directly influences the precision of querying relevant multilingual information", "Therefore OOV term translation has become a very important and challenging issue in CLIR", "With the increasing growth of Web information which includes multilingual hypertext resources with abundant topics it appears that Web information can mitigate the problem of the restricted OOV term translation accuracy Lu and Chien 2002", "However how to select the correct translations from Web information and locate the appropriate translation resources rapidly is still the main goal for OOV term translation", "Hence finding the effective feature representation and the optimal ranking pattern for translation candidates is the core part for the Webbased OOV term translation", "This paper focuses on the Webbased EnglishChinese OOV term translation pattern and emphasizes particularly on the translation selection strategy based on the fusion of multiple features and the translation ranking mechanism based on Ranking Support Vector Machine Ranking SVM", "By utilizing the CoNLL2003 corpus for the English Named Entity Recognition NER task and manually selected new terms in various fields the established OOV term translation model can filter the most possible translation candidates with better ability", "This paper also attempts to apply the OOV term translation mechanism above in EnglishChinese CLIR", "It can be observed from the experimental results on the data sets of Text Retrieval Evaluation Conference TREC that the obvious performance improvement for query translation can be obtained which is very beneficial to CLIR and can improve the whole retrieval performance"]}, "C10-2167": {"title": ["Extracting and Ranking  Product Features"], "abstract": ["An important task of opinion mining is to extract peoples opinions on features of an entity", "For example the sentence I love the GPS function of Motorola Droid expresses a positive opinion on the GPS function of the Motorola phone", "GPS function is the feature", "This paper focuses on mining features", "Double propagation is a stateoftheart technique for solving the problem", "It works well for mediumsize corpora", "However for large and small corpora it can result in low precision and low recall", "To deal with these two problems two improvements based on partwhole and no patterns are introduced to increase the recall", "Then feature ranking is applied to the extracted feature candidates to improve the precision of the topranked candidates", "We rank feature candidates by feature importance which is determined by two factors feature relevance and feature frequency", "The problem is formulated as a bipartite graph and the wellknown web page ranking algorithm HITS is used to find important features and rank them high", "Experiments on diverse reallife datasets show promising results"], "inroduction": ["In recent years opinion mining or sentiment analysis Liu 2010 Pang and Lee 2008 has been an active research area in NLP", "One task is to extract peoples opinions expressed on features of entities Hu and Liu 2004", "For example the sentence The picture of this camera is amazing expresses a positive opinion on the picture of the camera", "picture is the feature", "How to extract features from a corpus is an important problem", "There are several studies on feature extraction eg Hu and Liu 2004 Popescu and Etzioni 2005 Kobayashi et al 2007 Scaffidi et al 2007 Stoyanov and Cardie", "2008 Wong et al 2008 Qiu et al 2009", "However this problem is far from being solved", "Double Propagation Qiu et al 2009 is a stateoftheart unsupervised technique for solving the problem", "It mainly extracts noun features and works well for mediumsize corpora", "But for large corpora this method can introduce a great deal of noise low precision and for small corpora it can miss important features", "To deal with these two problems we propose a new feature mining method which enhances that in Qiu et al 2009", "Firstly two improvements based on partwhole patterns and no patterns are introduced to increase recall", "Partwhole or meronymy is an important semantic relation in NLP which indicates that one or more objects are parts of another object", "1462 Coling 2010 Poster Volume pages 14621470 Beijing August 2010 For example the phrase the engine of the car contains the partwhole relation that engine is part of car", "This relation is very useful for feature extraction because if we know one object is part of a product class this object should be a feature", "no pattern is another extraction pattern", "Its basic form is the word no followed by a nounnoun phrase for instance no noise", "People often express their short comments or opinions on features using this pattern", "Both types of patterns can help find features missed by double propagation", "As for the low precision problem we present a feature ranking approach to tackle it", "We rank feature candidates based on their importance which consists of two factors feature relevance and feature frequency", "The basic idea of feature importance ranking is that if a feature candidate is correct and frequently mentioned in a corpus it should be ranked high otherwise it should be ranked low in the final result", "Feature frequency is the occurrence frequency of a feature in a corpus which is easy to obtain", "However assessing feature relevance is challenging", "We model the problem as a bipartite graph and use the wellknown web page ranking algorithm HITS Kleinberg 1999 to find important features and rank them high", "Our experimental results show superior performances", "In practical applications we believe that ranking is also important for feature mining because ranking can help users to discover important features from the extracted hundreds of finegrained candidate features efficiently"]}, "C16-1060": {"title": ["A Bayesian model for joint word alignment and partofspeech transfer"], "abstract": ["Current methods for word alignment require considerable amounts of parallel text to deliver accurate results a requirement which is met only for a small minority of the worlds approximately 7000 languages", "We show that by jointly performing word alignment and annotation transfer in a novel Bayesian model alignment accuracy can be improved for language pairs where annotations are available for only one of the languagesa finding which could facilitate the study and processing of a vast number of lowresource languages", "We also present an evaluation where our method is used to perform singlesource and multisource partofspeech transfer with 22 translations of the same text in four different languages", "This allows us to quantify the considerable variation in accuracy depending on the specific source texts used even with different translations into the same language"], "inroduction": ["Word alignment is the problem of identifying translationally equivalent words across the languages of a parallel text", "It has found widespread use for enabling applications such as statistical machine translation Brown et al 1993 Koehn et al 2003 annotation transfer Yarowsky et al 2001 word sense disambiguation Diab and Resnik 2002 and lexicon extraction Wu and Xia 1994", "Although many types of algorithms have been explored the main line of research through the last couple of decades has been based on the generative IBM models introduced by Brown et al", "1993", "What these models have in common is that they are unsupervised asymmetric models assuming one of the languages in a bitext the source language generates the corresponding text in the other language the target language word by word", "Most often a variant of the ExpectationMaximization algorithm Dempster et al 1977 has been used for inference in these models but recently there has been some work using Bayesian alignment models using Gibbs sampling for inference DeNero et al 2008 Mermer and Saraclar 2011 Gal and Blunsom 2013", "The incorporation of Bayesian priors into these models has been shown to improve accuracy since they provide a flexible way of biasing the model towards empirical observations about language most importantly that a given word type tends to have a very limited number of translations", "While the basic word alignment models use only lexical cooccurrence and word order lexical data tends to be sparse and a number of authors have explored the usefulness of other information sources", "Toutanova et al", "2002 showed that Part of Speech PoS tags can be integrated into the IBM models to improve word alignment accuracy and others have reported similar results for dependency Cherry and Lin 2003 Wang and Zong 2013 and phrasestructure Yamada and Knight 2001 parse trees and for lemmatized texts Bojar and Prokopov 2006", "In addition to the studies just mentioned that showed how various types of linguistic annotation can be used to guide word alignment there has been research showing that the reverse also holds wordaligned parallel texts can be used to transfer annotations and models from languages where those resources exist to languages where they do not", "Pioneering work by Yarowsky et al", "2001 explored tasks such as PoS This work is licenced under a Creative Commons Attribution 40 International License", "License details http creativecommonsorglicensesby40 620 Proceedings of COLING 2016 the 26th International Conference on Computational Linguistics Technical Papers pages 620629 Osaka Japan December 1117 2016", "tagging shallow parsing and lemmatization which was followed by eg dependency parsing Hwa et al 2005", "The present work combines these previous lines of work by exploring joint models of word alignment and annotation transfer of PoS tags within a Bayesian framework", "The source code of our implementation is available at httpwwwlingsusespacos"]}, "C92-1059": {"title": ["A TREATMENT OF NEGATIVE DESCRIPTIONS OF"], "abstract": ["A formal treatment of typed feature structuresTFSs is developed to augment TFSs so that negative descriptions of them can be treated", "Negative descriptions of TFSs can make linguistic descriptionscompact and thus easy to understand", "Negative descriptions can be classified into three primitive negative descriptions 1 negations of type symbols 2 negations of feature existences and 3 negations of featureaddress value agreements", "The formalizationproposed in this paper is based on AitKaciaposs complex terms", "The first description is treated by extending type symbol lattices to include complement typesymbols", "The second and third are treated by augmenting terns structures with structures representingthese negations", "Algorithms for augmentedWS unification have been developed using graph unificationand programs using these algorithms have been written in Common Lisp"], "inroduction": ["In unificationbased or informationbased linguisticframeworks the most important objects are struc tures called aposfeature structuresapos FSs which are usedto describe linguistic objects and phenomena", "A fea ture structure is either atomic or complex an atomic FS is denoted by an atomic symbol a complex FS consists of a set of featurevalue pairs each of which describes an aspect of an object", "Partial information on an object is merged by applying the unification operation to FSs", "Research on unificationbased linguistic theories has been accompanied by research on FSs themselves", "Several extensions on FSs or on feature descriptions and formal treatments of the extensions have been proposed", "Disjunctive and negative descriptions on FSs help make the linguistic descriptions simple compact andthus easy to understand", "For disjunctive feature descriptions Kay14 introduces them into FUG Functional Unification Grammar and gives the procedural semantics", "Karttunen111 also proposes proce dural treatments of disjunctions in conjunction with relatively simple negations", "Rounds and Kasper19 13 propose a logicbased formalismfeature logicwhich uses automata to model FSs and can treat disjunctive feature descriptions and they obtain impor tant results", "For negative descriptions of FSs one of the mostfundamental properties of FSs the partiality of in formation they carry makes its insufficient to adopt relatively simple treatments", "Classical interpretation of negation for example does not allow evaluation of negations to be freely interleaved with unification", "Moshier and Rounds171 propose a formal framework which treats negative feature descriptions on the basis of intuitionistic logic", "However their formalism has trouble treating double negations", "Dawar5 proposes a formal treatment based on threevalued logic", "In order to treat feature domains of complex FSsand to treat taxonomic hierarchies of symbolic feature values type or sort hierarchies have been in troduced allowing definition of typed or sorted featurestructures Ms", "A TIaposS consists of a type symbol from a lattice and a set of featurevalue pairs", "A TFS can be seen as a generalized concept of bothatomic and complex FSs", "Pollard and Sag I8 intro duce sorts into II PSG Headdriven Phrase Structtire Grammar and use sorted Pis to describe linguistic objects", "AHKacill proposes an algebraic framework usingthe sittypes and ctypes one of promising formalizations of 1TSs based on lattice theory", "This formalization was originally aimed at formalizing and in tegrating various kinds of knowledge representation frameworks in Al In this approach types are defined as equivalence classes of complex term structures", "Asubsumption relation is defined on these term structures", "The join and meet operations on them correspond to the generalization and an opera tions on TFSs respectively", "This approach essentially adopts apostypeassetapos semantics", "Subtype relationships on type correspond to subsumption relationships on denotations of types", "Based on this frau aapos work an extension to Prolog LOGIN 2 has been developed", "Smolka20 proposes a feature logic with subsoilsIn this approach negative descriptions can be decoin posed into three kinds of primitive negations namely negations of sorts or complement sorts which denotethe complements of sets that positive counterparts de note negations of feature existences and negationsof featureaddress agreement or featureaddress dis agreement", "Smolka extends feature descriptions buta featurestructure interpretation of an extended de scription does not include negative in fort nation and corresponds to a simple TIaposSSome WSbased natural language processing systems have been developed7 24 12 15 8 22", "Car penter and Pollard 4 propose an interface to build type lattices", "Formalizations of extended FSs and of extended featuredescriptions described above are classified into two classes 1 extensions of FSs themselves and 2 extensions not of Pis themselves hut of featuredescriptions", "Previous attempts to introducetype hierarchies fall into the former class while previous treatments of disjunctive and negative descrip tions mainly fall into the latter", "Acres us COLINGt92 NANTES 2328 AmYr 1992 380 PROC", "OF COLING92 NANTES AuG 2328 1992 This paper proposes an extension to AithaciapossVapos type that incorporates three kinds of the primitive negative descriptions described below into the 0type", "AitKaciaposs 0type formalization uses ter in structures", "In this paper both these type structures and the type symbol lattice on which term structures are definedare extended to treat negative descriptions", "Nega tions of type symbols are treated by extending type symbol lattices and negations of feature existwicesand featureaddress disagreements are treated by ex tending term structures", "This extension can be seen as intuitionistic", "The extension is classified into class 1 aboveBased on this papers formalization unification al gorithms have been developed using graph unificationtechniques23 16", "Programs based on these algo rithms have been implemented in Common Lisp", "bodyText sectionHeader confidence0650502 genericHeadermethod"]}, "C92-1060": {"title": ["An Algorithm for Estimating the Parameters of Unrestricted Hidden Stochastic ContextFree Grammars"], "abstract": ["A new algorithm is presented for estimating the parameters of a stochastic contextfree grammar SCFG from ordinary unparsed text", "Unlike the InsideOutsideIO algorithm which requires a grammar to be spee fled in Chomsky normal form the new algorithm canestimate an arbitrary SCFG without ally need for transformation", "The algorithm has worstcase cubic complexity in the length of a sentence and the number of nonterminais in the grammar", "Instead of the binary branching tree structure used by the iO algorithm thenew algorithm makes use of a trellis structure for computation", "The trellis is a generalization of that used bythe BaumWelcb algorithm which is used for estimating hidden stochastic regular grammars", "Tile paper describes tile relationship between the trellis and the more typical parse tree representation"], "inroduction": ["This paper describes an iterative method for estimating the parameters of a hidden stochastic context free grammar SCFG", "The quothiddenquot aspect arises from the fact that ome information is not available when the grammar is trained", "When a parsed corpus is used for training production probabilities can be estimated by counting the number of times each production is used in the parsed corpus", "In the case of a hidden SCFG the characteristic grammar is defined but the parse trees associated with the training corpus are notavailable", "To proceed in this circumstance some initim prohabilitie are assigned which are iteratively re estimated from their current values and the training corpus", "They are adjusted to locally maximize the likelihood of generating the training corpus", "The EM algorithm Dempster 1977 embodies the approach justmentioned the new algorithm can be viewed as its ap plication to arbitrary SCFGs", "The use of unparsed training corpora is desirable because changes in thegrammar rules could conceivably require manually re ACRESDECOLING92 NANIES2328 AOr 1992parsing the training corpus several times during grammar development", "Stochastic grammarsenable ambigu ity resolution to performed on the rational basis of niostlikely interpretation", "They also acconnnodate the development of more robust grammars having high cover age where the attendant ambiguity is generally higher", "Previous approaches to the problem of estimating hidden SCFGs include parsing schemes ill which MI derivations of all sentences in the training corpus are enumerated Fujisaki et al 1989 Chitrao  Grishman 1990", "An efficient alternative is the InsideOutsideIO algorithm Baker 1979 which like the new algorithm is limited to cubic complexity in both the num ber of nonterminais and length of a entence", "The IO algorithm requires that tile grammar be in Chonmky normal form CNF", "Tile new algorithm hal the samecomplexity but does not have this restriction dispens ing with the need to transform to and from GNF", "TERMINOLOGY The training corpus can be conwmiently segmented into sentences for puposes of training each sentence comprisinga sequence of words", "A typical one may consist o f Y  1 words indexed from O to Y The lookup function Wy returns the index k of the vocabulary entry vk matching tile word w at position y ill tile sentenceThe algorithm uses a extension of the representation and terminology used for quothidden Markov mod eishidden stochastic regular grammars for which the BaumWelch algorithm Baum 1972 is applicable and which is also called the ForwardBackward FBalgo rithm", "Grammar rules are represented as networks and illustrated graphically maintaining a correspondence 387 IROC", "OFCOLING92 NANTESAUG 2328", "1992 Network NP ou 02 Det ADJP Noun 02 Figure 1 Networks for Lexical Ruh with the trellis structure on which the computation can be conveniently reprented", "The terminology is closely related to t h a t of Levinson Rabiner  Sondhi 1983 and also Lari  Young 1990", "A set o f A f different nonterminals are represented byA networks", "A component network for the nontermi nal labeled n has a parameter set A B I NF Top n", "To uniquely identify an element of the parameter set requires t h a t it be a function of its nonterminal label eg An ln etc", "However this notation has been topped to make formulae less cumbersome", "A network labeled N P is shown in Figure 1 which represents the following rules NP  NP  N o u n 02 Dee N o u n 02 N P  Dee A D J P N o u n 02 N P  A D J P N o u n 04 N o u n  quot c a t quot  0  0 0 2  N o u n  quot d o g quot  0  0 0 1  D e t  quot t h e quot  0  5  Dee  quotaquot 02 The rule N P  N o u n 02 means t h a t if the NPrule is used the probability is 02 t h a t it produces a sin gle N o u n  In Figure 1 states are represented by circles with numbers inside to index them", "NonierminMstates  re shown with double circles and represent references to other networks such as ADJP", "States marked with single circles ate called terminal states and represent partofspeech categories", "When a transition is made to a terminal state a word of the current training sentence is generated", "The word must have the same category as the state t h a t generated it", "Rules of the form N o u n  quot c a t quot  0  0 0 2  and N o u n  quot d o g quot  0  0 0 1  are collapsed into a statedependent probability vector bj ACT DE COLING92 NTS 2328 Aotr 1992 Network NP ADJP Noun 04 F 04 04 Det Figure 2 Equivalent Network l  10 l  og at 007 F0o0 0007 i Ithe os1 tho  j Noun Figure 3 Reprelentatlon for Terminal Productions which is an element of the output matrix B Elementsof the vector such as bj Wyrepresent the probabil ity of seeing word wy in terminal state j A transition to a nonterminal state does not in itself generate any words however terminal states within the referenced network will do so", "The parameter N is a matrix whichindicates the label eg n NP ADJP of the network t h a t a nonterminal state refers to", "The proba bility of making a transition from state i to state j is labeled ai j and collectively these probabilities form the transition matrix A The initial matrix I contains the production probabilities for rules t h a t are modelledby the network", "They are indicated in Figure 1 as num bers beneath the state if they are nonzero li can beequivalently viewed as the probability t h a t some sub sequence of n is started at state i The parameter F is the set of final states any sequence accepted by thenetwork must terminate on a final state", "In Figure 1 fi nal states are designated with the annotation quot F   The boolean value Top indicates whether the network is the toplevel network", "Only one network m a y be assignedas the toplevel network which models productions in 388 Paoc", "oF COLING92 NArrEs AuG 2328 1992", "02 volving the root symbol of a g r a m m a r  An equivalent network for the same set of rules is shown in Figure 2", "The lexical rules can be writtencompactly as networks with fewer states", "The transi tions from the determiner state each have probability 05 ie a1 2  a13  05", "It should be noted t h a t the algorithm can operate on either network", "Network NP a TRELLIS DIAGRAM quotlYellis diarsans conveniently relate computationalquantities to the network structure and a training sen tence", "Each network u has a set of Y  1 trellises for subsequences of a sentence Wowy starting at each different position and ending at subsequent ones", "A single trellis spanning positions 02 is shown ill Figure 4 for network NP", "Nonterminal states are associated with a row of start nodes indicating where daughter constituents m a y start and a row of end nodes t h a t indicate where they end", "A pair of s t a r t  e n d nodes thus refer to a daughter nonterminal constituent", "In Figure 4 the A D J P network is referenced via the start state at position O An adjective is then generated by a terminal state in the trellis for the A D J P network followed by a transition and another adjective", "The A D J P network is left at position 1 and a transition ismade to the noun state where the word atquot is gen erated", "Terminal states are associated with a singlerow of nodes in the trellis they represent terminal pro ductions t h a t span only a single position", "The p a t h taken through the trellis is shown with broken a line", "A path through different trellises has a corresponding unique tree representation as exemplified in Figure 5", "In cases where p a r a  are ambiguous several paths exist corresponding to the alternative derivations", "We shall next consider the c o m p u t a t i o n of the probabilities of the paths", "Two basic quantities are involved namelyalpha and beta probabilities", "Loosely speaking the al phas represent probabilities of subtrees associated with nonterminals while the betas refer to the rest of the treestructure external to the subtrees", "Subsequently prod ucts of these quantities will be formed which representthe probabilities of productions being used in generat ing a sentence", "These are summed over all sentences in the training corpus to obtain the expected number of times each production is used based on the current production probabilities and the training corpus", "These are used like frequency counts would be for a parsed corpus to form ratios t h a t represent new estimates of the production probabilities", "The procedure is iterated several times until the estimates do not change between iterations ie the overall likelihood of producing the training corpus no longer increasesThe algorithm makes use of one set of trellis dia grams to compute alpha probabilities and another forbeta probabilities", "These are both split into terminal nonterminalstart and nontermiualend probabili ties corresponding to the three different types of nodes in the trellis diagram", "The alpha set are labeled a t ct and ante respectively", "The algorithm was origi nally formulated using solely the trellis representation Kupiec 1991 however the definitions that follow will ADJP  and   Det  Noun Q big black cat Figure 4 A Path through a Trelli Diagram NP ADJP  AdJ big AdJ black Noun cat Figure 5 The EquivMent Tree ACRESDECOLING92 NANTES2328 Aor 1992 389 PROC", "OFCOLING92 NANTES AUG 2328 1992 also be related to the consituent structures used in the equivalent parse trees", "In the following equations three sets will be mentioned 1", "Termn The set of terminal states in network n"]}, "D07-1012": {"title": ["A Comparative Evaluation of Deep and Shallow Approaches to the"], "abstract": ["This paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically well formed or illformed", "The deep processing approach uses the XLE LFG parser and English grammar two versions are presented one which uses the XLE directly to perform the classification and another one which uses a decision tree trained on features consisting of the XLEs output statistics", "The shallow processing approach predicts gram maticality based on ngram frequency statistics we present two versions one which uses frequency thresholds and one which uses a decision tree trained on the frequencies of the rarest ngrams in the input sentence", "We find that the use of a decision tree improves on the basic approach only for the deep parserbased approach", "We also show that combining both the shallow and deep decision tree features is effective", "Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences", "The ungrammatical test set is generated automatically by inserting grammatical errors into wellformed BNC sentences"], "inroduction": ["This paper is concerned with the task of predicting whether a sentence contains a grammatical error", "An accurate method for carrying out automatic Also affiliated to IBM CAS Dublin", "grammaticality judgements has uses in the areas of computerassisted language learning and grammar checking", "Comparative evaluation of existing error detection approaches has been hampered by a lack of large and commonly used evaluation error corpora", "We attempt to overcome this by automatically creating a large error corpus containing four different types of frequently occurring grammatical errors", "We use this corpus to evaluate the performance of two approaches to the task of automatic error detection", "One approach uses lowlevel detection techniques based on POS ngrams", "The other approach is a novel parserbased method which employs deep linguistic processing to discriminate grammatical input from ungrammatical", "For both approaches we implement a basic solution and then attempt to improve upon this solution using a decision tree classifier", "We show that combining both methods improves upon the individual methods", "Ngrambased approaches to the problem of error detection have been proposed and implemented in various forms by Atwell1987 Bigert and Knutsson 2002 and Chodorow and Leacock 2000 amongst others", "Existing approaches are hard to compare since they are evaluated on different test sets which vary in size and error density", "Furthermore most of these approaches concentrate on one type of grammatical error only namely contextsensitive or real word spelling errors", "We implement a vanilla n grambased approach which is tested on a very large test set containing four different types of error", "The idea behind the parserbased approach to error detection is to use a broadcoverage handcrafted precision grammar to detect ungrammatical sen 112 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "112121 Prague June 2007", "Qc 2007 Association for Computational Linguistics tences", "This approach exploits the fact that a precision grammar is designed in the traditional generative grammar sense Chomsky 1957 to distinguish grammatical sentences from ungrammatical sentences", "This is in contrast to treebankbased grammars which tend to massively overgenerate and do not generally aim to discriminate between the two", "In order for our approach to work the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences and for this reason we choose the XLE Maxwell and Kaplan 1996 an efficient and robust parsing system for Lexical Functional Grammar LFG Kaplan and Bresnan 1982 and the ParGram English grammar Butt et al 2002 for our experiments", "This system employs robustness techniques some borrowed from Optimality Theory OT Prince and Smolen sky 1993 to parse extragrammatical input Frank et al 1998 but crucially still distinguishes between optimal and suboptimal solutions", "The evaluation corpus is a subset of an ungrammatical version of the British National Corpus BNC a 100 million word balanced corpus of British English Burnard 2000", "This corpus is obtained by automatically inserting grammatical errors into the original BNC sentences based on an analysis of a manually compiled real error corpus", "This paper makes the following contributions to the task of automatic error detection 1", "A novel deep processing XLEbased approach"]}, "D07-1018": {"title": ["Modelling Polysemy in Adjective Classes by MultiLabel Classification"], "abstract": ["This paper assesses the role of multilabel classification in modelling polysemy for language acquisition tasks", "We focus on the acquisition of semantic classes for Catalan adjectives and show that polysemy acquisition naturally suits architectures used for multi label classification", "Furthermore we explore the performance of information drawn from different levels of linguistic description using feature sets based on morphology syntax semantics and ngram distribution", "Finally we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence a simple majority voting ensemble classifier improves the accuracy from 625 best single classifier to 84"], "inroduction": ["This paper reports on a series of experiments to explore the automatic acquisition of semantic classes for Catalan adjectives", "The most important challenge of the classification task is to model the assignment of polysemous lexical instances to multiple semantic classes combining a a stateoftheart Machine Learning architecture for Multilabel Classification Schapire and Singer 2000 Ghamrawi and McCallum 2005 and an Ensemble Classifier Dietterich 2002 with b the definition of features at various levels of linguistic description", "A proper treatment of polysemy is essential in the area of lexical acquisition since polysemy repre sents a pervasive phenomenon in natural language", "However previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem cf", "Merlo and Stevenson 2001 and Stevenson and Joanis 2003 for English semantic verb classes or Schulte im Walde 2006 for German semantic verb classes", "There are a few exceptions to this tradition such as Pereira et al", "1993 Rooth et al", "1999 Korhonen et al", "2003 who used soft clustering methods for multiple assignment to verb semantic classes", "Our work addresses the lack of methodology in modelling a polysemous classification", "We implement a multilabel classification architecture to handle polysemy", "This paper concentrates on the classification of Catalan adjectives but the general nature of the architecture should allow related tasks to profit from our insights", "As target classification for the experiments a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic classes", "We deliberately decided in favour of a smallscale broad classification", "So far there is little work on the semantic classification of adjectives as opposed to verbal semantic classification", "The semantic classification we propose is a first step in characterising adjectival meaning and can be refined and extended in subsequent work", "The experiments also provide a thorough comparison of feature sets based on different levels of linguistic description morphology syntax semantics", "A set of features is defined for each level of description and its performance is assessed within the series of experiments", "An ensemble classifier comple 171 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "171180 Prague June 2007", "Qc 2007 Association for Computational Linguistics ments the classification architecture by optimising the combination of these different types of linguistic evidence", "Our task is motivated by the fact that adjectives play an important role in sentential semantics they are crucial in determining the reference of NPs and in defining properties of entities", "Even using only three different classes the information acquired could be applied to eg identify referents in a given context in Dialog or Question Answering systems and to induce properties of objects within Information Extraction tasks", "Furthermore with the semantic classes corresponding to broad sense representations they can be exploited for Word Sense Disambiguation", "The remainder of this paper is organised as follows", "Section 2 provides background on Catalan adjectives and Section 3 presents the Gold Standard classification", "Section 4 introduces the methodology of the multilabel classification experiments Section 5 discusses the results and the improved ensemble classifier is presented in Section 6"]}, "D07-1076": {"title": ["Tree Kernelbased Relation Extraction"], "abstract": ["This paper proposes a tree kernel with context sensitive structured parse tree information for relation extraction", "It resolves two critical problems in previous tree kernels for relation extraction in two ways", "First it automatically determines a dynamic contextsensitive tree span for relation extraction by extending the widely used Shortest Pathenclosed Tree SPT to include necessary context information outside SPT", "Second it proposes a contextsensitive convolution tree kernel which enumerates both contextfree and context sensitive subtrees by consid ering their ancestor node paths as their contexts", "Moreover this paper evaluates the complementary nature between our tree kernel and a stateoftheart linear kernel", "Evaluation on the ACE RDC corpora shows that our dynamic contextsensitive tree span is much more s uitable for relation extraction than SPT and our tree kernel outperforms the stateoftheart Collins and Duffys convolution tree kernel", "It also shows that our tree kernel achieves much better performance than the stateoftheart linear kernels  Finally it shows that featurebased and tree kernelbased methods much complement each other and the composite kernel can well integrate both flat and structured features"], "inroduction": ["Relation extraction is to find various predefined semantic relations between pairs of entities in text", "The research in relation extraction has been pr omoted by the Message Understanding Conferences MUCs MUC 19871998 and the NIST Automatic Content Extraction ACE program ACE 20022005", "According to the ACE Program an entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities", "For example the sentence Bill Gates is the chairman and chief software architect of Microsoft Corporation c onveys the ACEstyle relation EMPLOYMENTexec between the entities Bill Gates person name and Microsoft Corporation organization name", "Extraction of semantic relations between entities can be very useful in many applic a tions such as question answering eg to answer the query Who is the president of the United States and information retrieval eg to expand the query George W Bushwith the pres ident of the United Statesvia his relationship with the United States", "Many researches have been done in relation extraction", "Among them featurebased methods Kambhatla 2004 Zhou et al 2005 achieve certain success by employing a large amount of diverse linguistic features varying from lexical knowledge entity related information to syntactic parse trees dependency trees and semantic information  How ever it is difficult for them to effectively capture struc tured parse tree information Zhou et al 2005 which is critical for further performance improvement in relation extraction", "As an alternative to featurebased methods tree kernelbased methods provide an elegant solution to explore implic itly structured features by directly computing the simila rity between two trees", "Although earlier researches Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005a only achieve success on simple tasks and fail on complex tasks such as the ACE RDC task tree kernelbased methods achieve much progress recently", "As the stateoftheart Zhang et al 2006 applied the c onvolution tree kernel Collins and Duffy 2001 and achieved comparable performance with a stateofthe art linear kernel Zhou et al 2005 on the 5 relation types in the ACE RDC 2003 corpus", "However there are two problems in Collins and Duffys convolution tree kernel for relation extrac tion", "The first is that the subtrees enumerated in the tree kernel computation are contextfree", "That is each subtree enumerated in the tree kernel computation 728 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pp", "728736 Prague June 2007", "Qc 2007 Association for Computational Linguistics does not consider the context information outside the subtree", "The second is to dec ide a proper tree span in relation extraction", "Zhang et al 2006 explored five tree spans in relation extraction and it was a bit surprising to find that the Shortest Pathenclosed Tree SPT ie the subtree enclosed by the shortest path linking two involved entities in the parse tree performed best", "This is contrast to our intuition", "For example got married is critical to determine the relationship between John and Maryin the sentence John and Mary got married  as shown in Figure 1e", "It is obvious that the information contained in SPT John and Marry is not enough to determine their relationshipThis paper proposes a contextsensitive convolu tion tree kernel for relation extraction to resolve the above two problems", "It first automatically determines a dynamic contextsensitive tree span for relation extraction by extending the Shortest Pathenclosed Tree SPT to include necessary context information outside SPT", "Then it proposes a context sensitive convolution tree kernel whic h not only enumerates context free subtrees but also context sensitive subtrees by considering their ancestor node paths as their contexts", "Moreover this paper evaluates the complementary nature of different linear kernels and tree kernels via a composite kernel", "The layout of this paper is as follows", "In Section 2 we review related work in more details", "Then the dynamic contextsensitive tree span and the context sensitive convolution tree ker nel are proposed in Section 3 while Section 4 shows the experimental results", "Finally we conclude our work in Sec tion 5"]}, "D09-1025": {"title": ["Entity Extraction via Ensemble Semantics"], "abstract": ["Combining information extraction systems yields significantly higher quality resources than each system in isolation", "In this paper we generalize such a mixing of sources and features in a framework called Ensemble Semantics", "We show very large gains in entity extraction by combining stateoftheart distributional and pattern based systems with a large set of features from a webcrawl query logs and Wikipedia", "Experimental results on a web scale extraction of actors athletes and musicians show significantly higher mean average precision scores 29 gain compared with the current state of the art"], "inroduction": ["Mounting evidence shows that combining information sources and information extraction algorithms leads to improvements in several tasks such as fact extraction Pasca et al 2006 open domain IE Talukdar et al 2008 and entailment rule acquisition Mirkin et al 2006", "In this paper we show large gains in entity extraction by combining stateoftheart distributional and pattern based systems with a large set of features from a 600 million document webcrawl one year of query logs and a snapshot of Wikipedia", "Further we generalize such a mixing of sources and features in a framework called Ensemble Semantics", "Distributional and patternbased extraction algorithms capture aspects of paradigmatic and syntagmatic dimensions of semantics respectively and are believed to be quite complementary", "Pasca et al", "2006 showed that filtering facts extracted by a patternbased system according to their arguments distributional similarity with seed facts yielded large precision gains", "Mirkin et al", "2006 showed similar gains on the task of acquiring lexical entailment rules by exploring a supervised combination of distributional and patternbased algorithms using an MLbased SVM classifier", "This paper builds on the above work by studying the impact of various sources of features external to distributional and patternbased algorithms on the task of entity extraction", "Mirkin et als results are corroborated on this task and large and significant gains over this baseline are obtained by incorporating 402 features from a webcrawl query logs and Wikipedia", "We extracted candidate entities for the classes Actors Athletes and Musicians from a webcrawl using a variant of Pasca et als 2006 patternbased engine and Pantel et als 2009 distributional extraction system", "A gradient boosted decision tree is used to learn a regression function over the feature space for ranking the candidate entities", "Experimental results show 29 gains 19 nominal in mean average precision over Mirkin et als method and 34 gains 22 nominal in mean average precision over an unsupervised baseline similar to Pasca et als method", "Below we summarize the contributions of this paper  We explore the hypothesis that although dis tributional and patternbased algorithms are complementary they do not exhaust the semantic space other sources of evidence can be leveraged to better combine them  We model the mixing of knowledge sourcesand features in a novel and general informa tion extraction framework called Ensemble Semantics and  Experiments over an entity extraction taskshow that our model achieves large and sig nificant gains over stateoftheart extractors", "A detailed analysis of feature correlations and interactions shows that query log and we bcrawl features yield the highest gains but easily accessible Wikipedia features also improve over current stateoftheart systems", "238 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 238247 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP Figure 1 The Ensemble Semantics framework for information extraction", "The remainder of this paper is organized as follows", "In the next section we present our Ensemble Semantics framework and outline how various information extraction systems can be cast into the framework", "Section 3 then presents our entity extraction system as an instance of Ensemble Semantics comparing and contrasting it with previous information extraction systems", "Our experimental methodology and analysis is described in Section 4 and shows empirical evidence that our extractor significantly outperforms prior art", "Finally Section 5 concludes with a discussion and future work"]}, "D09-1086": {"title": ["Parser Adaptation and Projection"], "abstract": ["We connect two scenarios in structured learning adapting a parser trained on one corpus to another annotation style and projecting syntactic annotations from one language to another", "We propose quasi synchronous grammar QG features for these structured learning tasks", "That is we score a aligned pair of source and target trees based on local features of the trees and the alignment", "Our quasisynchronous model assigns positive probability to any alignment of any trees in contrast to a synchronous grammar which would insist on some form of structural parallelism", "In monolingual dependency parser adaptation we achieve high accuracy in translating among multiple annotation styles for the same sentence", "On the more difficult problem of crosslingual parser projection we learn a dependency parser for a target language by using bilingual text an English parser and automatic word alignments", "Our experiments show that unsupervised QG projection improves on parses trained using only high precision projected annotations and far outperforms by more than 35 absolute dependency accuracy learning an unsupervised parser from raw targetlanguage text alone", "When a few targetlanguage parse trees are available projection gives a boost equivalent to doubling the number of targetlanguage trees", "The first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst", "We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper"], "inroduction": ["11 Parser Adaptation", "Consider the problem of learning a dependency parser which must produce a directed tree whose vertices are the words of a given sentence", "There are many differing conventions for representing syntactic relations in dependency trees", "Say that we wish to output parses in the Prague style and so have annotated a small target corpuseg 100 sentenceswith those conventions", "A parser trained on those hundred sentences will achieve mediocre dependency accuracy the proportion of words that attach to their correct parent", "But what if we also had a large number of trees in the CoNLL style the source corpus", "Ideally they should help train our parser", "But unfortunately a parser that learned to produce perfect CoNLLstyle trees would for example get both links wrong when its coordination constructions were evaluated against a Praguestyle gold standard Figure 1", "If it were just a matter of this one construction the obvious solution would be to write a few rules by hand to transform the large source training corpus into the target style", "Suppose however that there were many more ways that our corpora differed", "Then we would like to learn a statistical model to transform one style of tree into another", "We may not possess handannotated training data for this treetotree transformation task", "That would require the two corpora to annotate some of the same sentences in different styles", "But fortunately we can automatically obtain a noisy form of the necessary pairedtree training data", "A parser trained on the source corpus can parse the sentences in our target corpus yielding trees or more generally probability distributions over trees in the source style", "We will then learn a tree transformation model relating these noisy source trees to our known trees in the target style", "822 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 822831 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP     now or never now or never Prague Melcuk now     or never now or never CoNLL MALT Figure 1 Four of the five logically possible schemes for annotating coordination show up in humanproduced dependency treebanks", "The other possibility is a reverse Melcuk scheme", "These treebanks also differ on other conventions", "This model should enable us to convert the original large source corpus to target style giving us additional training data in the target style", "12 Parser Projection", "For many target languages however we do not have the luxury of a large parsed source corpus in the language even one in a different style or domain as above", "Thus we may seek other forms of data to augment our small target corpus", "One option would be to leverage unannotated text McClosky et al 2006 Smith and Eisner 2007", "But we can also try to transfer syntactic information from a parsed source corpus in another language", "This is an extreme case of outofdomain data", "This leads to the second task of this paper learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation", "Tree transformations are often modeled with synchronous grammars", "Suppose we are given a sentence wl in the source language and its translation w into the target language", "Their syntactic parses tl and t are presumably not independent but will tend to have some parallel or at least correlated structure", "So we could jointly model the parses tl t and the alignment a between them with a model of the form pt a tl  w wlSuch a joint model captures how t a tl mu tually constrain each other so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text", "This idea underlies a number of recent papers on syntaxbased alignment using t and tl to better recover a grammar induction from bitext using a to better recover t and tl parser projection using tl and a to better Figure 2 With the English tree and alignment provided by a parser and aligner at test time the Chinese parser finds the correct dependencies see 6", "A monolingual parsers incorrect edges are shown with dashed lines", "recover t as well as full joint parsing Smith and Smith 2004 Burkett and Klein 2008", "In this paper we condition on the 1best source tree tl", "As for the alignment a our models either condition on the 1best alignment or integrate the alignment out", "Our models are thus of the form pt  w wl tl a or in the generative case pw t a  wl tl", "We intend to consider other for mulations in future work", "So far this is very similar to the monolingual parser adaptation scenario but there are a few key differences", "Since the source and target sentences in the bitext are in different languages there is no longer a trivial alignment between the words of the source and target trees", "Given word alignments we could simply try to project dependency links in the source tree onto the target text", "A linkby link projection however could result in invalid trees on the target side with cycles or disconnected words", "Instead our models learn the necessary transformations that align and transform a source tree into a target tree by means of quasi synchronous grammar QG features", "Figure 2 shows an example of bitext helping disambiguation when a parser is trained with only a small number of Chinese trees", "With the help of the English tree and alignment the parser is able to recover the correct Chinese dependencies using QG features", "Incorrect edges from the monolingual parser are shown with dashed lines", "The bilingual parser corrects additional errors in the second half of this sentence which has been removed to improve legibility", "The parser is able to recover the longdistance dependency from the first Chinese word China to the last begun while skipping over the intervening noun phrase that confused the undertrained monolingual parser", "Although due to the auxiliary verb China and begun are siblings in English and not in direct dependency the QG features still leverage this indirect projection", "13 Plan of the Paper", "We start by describing the features we use to augment conditional and generative parsers when scoring pairs of trees 2", "Then we discuss in turn monolingual 3 and crosslingual 4 parser adaptation", "Finally we present experiments on crosslingual parser projection in conditions when no target language trees are available for training 5 and when some trees are available 6"]}, "D09-1101": {"title": ["Supervised Models for Coreference Resolution"], "abstract": ["Traditional learningbased coreference re solvers operate by training a mention pair classifier for determining whether two mentions are coreferent or not", "Two independent lines of recent research have attempted to improve these mentionpair classifiers one by learning a mention ranking model to rank preceding mentions for a given anaphor and the other by training an entitymention classifier to determine whether a preceding cluster is coreferent with a given mention", "We propose a clusterranking approach to coreference resolution that combines the strengths of mention rankers and entity mention models", "We additionally show how our clusterranking framework naturally allows discoursenew entity detection to be learned jointly with coreference resolution", "Experimental results on the ACE data sets demonstrate its superior performance to competing approaches"], "inroduction": ["Noun phrase NP coreference resolution is the task of identifying which NPs or mentions refer to the same realworld entity or concept", "Traditional learningbased coreference resolvers operate by training a model for classifying whether two mentions are coreferring or not eg Soon et al", "2001 Ng and Cardie 2002b Kehler et al", "2004 Ponzetto and Strube 2006", "Despite their initial successes these mentionpair models have at least two major weaknesses", "First since each candidate antecedent for a mention to be resolved henceforth an active mention is considered independently of the others these models only determine how good a candidate antecedent is relative to the active mention but not how good a candidate antecedent is relative to other candidates", "In other words they fail to answer the critical question of which candidate antecedent is most probable", "Second they have limitations in their expressiveness the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision especially if the candidate antecedent is a pronoun which is semantically empty or a mention that lacks descriptive information such as gender eg Clinton", "To address the first weakness researchers have attempted to train a mentionranking model for determining which candidate antecedent is most probable given an active mention eg Denis and Baldridge 2008", "Ranking is arguably a more natural reformulation of coreference resolution than classification as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them", "Another desirable consequence is that there exists a natural resolution strategy for a ranking approach a mention is resolved to the candidate antecedent that has the highest rank", "This contrasts with classificationbased approaches where many clustering algorithms have been employed to coordinate the pairwise coreference decisions because it is unclear which one is the best", "To address the second weakness researchers have investigated the acquisition of entitymention coreference models eg Luo et al", "2004 Yang et al", "2004", "Unlike mentionpair models these entitymention models are trained to determine whether an active mention belongs to a preceding possibly partiallyformed coreference cluster", "Hence they can employ clusterlevel features ie features that are defined over any subset of mentions in a preceding cluster which makes them more expressive than mentionpair models", "Motivated in part by these recently developed models we propose in this paper a cluster ranking approach to coreference resolution that combines the strengths of mentionranking mod 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 968977 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP els and entitymention models", "Specifically we recast coreference as the problem of determining which of a set of preceding coreference clusters is the best to link to an active mention using a learned cluster ranker", "In addition we show how discoursenew detection ie the task of determining whether a mention introduces a new entity in a discourse can be learned jointly with coreference resolution in our clusterranking framework", "It is worth noting that researchers typically adopt a pipeline coreference architecture performing discoursenew detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discoursenew see Poesio et al", "2004 for an overview", "As a result errors in discoursenew detection could be propagated to the resolver possibly leading to a deterioration of coreference performance see Ng and Cardie 2002a", "Jointly learning discourse new detection and coreference resolution can potentially address this errorpropagation problem", "In sum we believe our work makes three main contributions to coreference resolution Proposing a simple yet effective coreference model", "Our work advances the stateoftheart in coreference resolution by bringing learning based coreference systems to the next level of performance", "When evaluated on the ACE 2005 coreference data sets cluster rankers outperform three competing models  mentionpair entity mention and mentionranking models  by a large margin", "Also our jointlearning approach to discoursenew detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture", "Equally importantly cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other unlike relational coreference models see McCallum and Wellner 2004", "Bridging the gap between machinelearning approaches and linguisticallymotivated approaches to coreference resolution", "While machine learning approaches to coreference resolution have received a lot of attention since the mid 90s popular learningbased coreference frameworks such as the mentionpair model are arguably rather unsatisfactory from a linguistic point of view", "In particular they have not leveraged advances in discoursebased anaphora resolution research in the 70s and 80s", "Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Leasss 1994 heuristicbased pronoun resolver which in turn was motivated by classic saliencebased approaches to anaphora resolution", "Revealing the importance of adopting the right model", "While entitymention models have previously been shown to be worse or at best marginally better than their mentionpair counterparts Luo et al 2004 Yang et al 2008 our clusterranking models which are a natural extension of entitymention models significantly outperformed all competing approaches", "This suggests that the use of an appropriate learning framework can bring us a long way towards high performance coreference resolution", "The rest of the paper is structured as follows", "Section 2 discusses related work", "Section 3 describes our baseline coreference models mention pair entitymention and mentionranking", "We discuss our clusterranking approach in Section 4 evaluate it in Section 5 and conclude in Section 6"]}, "D09-1115": {"title": ["Latticebased System Combination for Statistical Machine Translation"], "abstract": ["Current system combination methods usually use confusion networks to find consensus translations among different systems", "Requiring onetoone mappings between the words in candidate translations confusion networks have difficulty in handling more general situations in which several words are connected to another several words", "Instead we propose a latticebased system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations", "Experiments show that our approach achieves significant improvements over the stateof theart baseline system on ChinesetoEnglish translation test sets"], "inroduction": ["System combination aims to find consensus translations among different machine translation systems", "It has been proven that such consensus translations are usually better than the output of individual systems Frederking and Nirenburg 1994", "In recent several years the system combination methods based on confusion networks developed rapidly Bangalore et al 2001 Matusov et al 2006 Sim et al 2007 Rosti et al 2007a Rosti et al 2007b Rosti et al 2008 He et al 2008 which show stateoftheart performance in benchmarks", "A confusion network consists of a sequence of sets of candidate words", "Each candidate word is associated with a score", "The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score", "To construct a confusion network one first need to choose one of the hypotheses ie candidate translations as the backbone also called skeleton in the literature and then decide the word alignments of other hypotheses to the backbone", "Hypothesis alignment plays a crucial role in confusion networkbased system combination because it has a direct effect on selecting consensus translations", "However a confusion network is restricted in such a way that only 1to1 mappings are allowed in hypothesis alignment", "This is not the fact even for word alignments between the same languages", "It is more common that several words are connected to another several words", "For example be capable of and be able to have the same meaning", "Although confusionnetworkbased approaches resort to inserting null words to alleviate this problem they face the risk of producing degenerate translations such as be capable to and be able of", "In this paper we propose a new system combination method based on lattices", "As a more general form of confusion network a lattice is capable of describing arbitrary mappings in hypothesis alignment", "In a lattice each edge is associated with a sequence of words rather than a single word", "Therefore we select phrases instead of words in each candidate set and minimize the chance to produce unexpected translations such as be capable to", "We compared our approach with the stateoftheart confusionnetworkbased system He et al 2008 and achieved a significant absolute improvement of 123 BLEU points on the NIST 2005 Chineseto", "English test set and 093 BLEU point on the NIST 2008 ChinesetoEnglish test set", "1105 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 11051113 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP He feels like apples He prefer apples He feels like apples He is fond of apples a unidirectional alignments He feels like apples He prefer apples He feels like apples He is fond of apples b bidirectional alignments lations", "Note that the phrase is fond of is attached to an edge", "Now it is unlikely to obtain a translation like He is like of apples", "A lattice G  V E is a directed acyclic graph formally a weighted finite state automation FSA where V is the set of nodes and E is the set of edges", "The nodes in a lattice are usually labeled according  prefer of He feels like  apples is fond c confusion network  prefer he feels like apples is fond of d lattice Figure 1 Comparison of a confusion network and a lattice"]}, "D09-1138": {"title": ["Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes"], "abstract": ["The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes", "We intend for the probabilistic model to provide a probability distribution of verbclass associations over known and unknown verbs including pol ysemous words", "In our approach training instances are obtained from an existing lexicon andor from an annotated corpus while the features which represent syntactic frames semantic similarity and selectional preferences are extracted from unannotated corpora", "Our model is evaluated in typelevel verb classification tasks we measure the prediction accuracy of VerbNet classes for unknown verbs and also measure the dissimilarity between the learned and observed probability distributions", "We empirically compare several settings for model learning while we vary the use of features source corpora for feature extraction and disambiguated corpora", "In the task of verb classification into all VerbNet classes our best model achieved a 1069 error reduction in the classification accuracy over the previously proposed model"], "inroduction": ["Lexicons are invaluable resources for semantic processing", "In many cases lexicons are necessary to restrict a set of semantic classes to be assigned to a word", "In fact a considerable number of works on semantic processing implicitly or explicitly presupposes the availability of a lexicon such as in word sense disambiguation WSD McCarthy et al 2004 and in tokenlevel verb class disambiguation Lapata and Brew 2004 Girju et al 2005 Li and Brew 2007 Abend et al 2008", "In other words those methods are heavily dependent on the availability of a semantic lexicon", "Therefore recent research efforts have invested in developing semantic resources such as WordNet Fellbaum 1998 FrameNet Baker et al 1998 and VerbNet Kipper et al 2000 KipperSchuler 2005 which greatly advanced research in semantic processing", "However the construction of such resources is expensive and it is unrealistic to presuppose the availability of fullcoverage lexicons this is the case because unknown words always appear in real texts and wordsemantics associations may vary Abend et al 2008", "This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon", "We target the automatic classification of arbitrary verbs including polysemous verbs into all VerbNet classes further we target the estimation of a probabilistic model which represents the saliences of verbclass associations for polysemous verbs", "In our approach an existing lexicon andor an annotated corpus are used as the training data", "Since VerbNet classes are designed to represent the distinctions in the syntactic frames that verbs can take features representing the statistics of syntactic frames are extracted from the unannotated corpora", "Additionally as the classes represent semantic commonalities semantically inspired features like distributionally similar words are used", "These features can be considered as a generalized representation of verbs and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words", "Our model is evaluated in two tasks of type level verb classification one is the classification of monosemous verbs into a small subset of the classes which was studied in some previous works Joanis and Stevenson 2003 Joanis et al 2008", "The other task is the classification of all verbs into the full set of VerbNet classes which has not yet 1328 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 13281337 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP been attempted", "In the experiments training instances are obtained from VerbNet andor Sem Link Loper et al 2007 while features are extracted from the British National Corpus or from Wall Street Journal", "We empirically compare several settings for model learning by varying the set of features the source domain and the size of a corpus for feature extraction and the use of the tokenlevel statistics obtained from a manually disambiguated corpus", "We also provide the analysis of the remaining errors which will lead us to further improve the supervised learning of a probabilistic semantic lexicon", "Supervised methods for automatic verb classification have been extensively investigated Stevenson et al 1999 Stevenson and Merlo 1999 Merlo and Stevenson 2001 Stevenson and Joanis 2003 Joanis and Stevenson 2003 Joanis et al 2008", "However their focus has been limited to a small subset of verb classes and a limited number of monosemous verbs", "The main contributions of the present work are i to provide empirical results for the automatic classification of all verbs including polysemous ones into all VerbNet classes and ii to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes"]}, "D09-1149": {"title": ["SemiSupervised Learning for Semantic Relation Classification using"], "abstract": ["This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrappingbased semisupervised learning for semantic relation classification", "First the training data is partitioned into several strata according to relation typessubtypes then relation instances are randomly sampled from each stratum to form the initial seed set", "We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance", "Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself", "This suggests that a proper sampling strategy is critical in semisupervised learning"], "inroduction": ["With the dramatic increase in the amount of textual information available in digital archives and the WWW there has been growing interest in techniques for automatically extracting information from text documents", "Information Extraction IE is such a technology that IE systems are expected to identify relevant information usually of predefined types from text documents in a certain domain and put them in a structured format", "According to the scope of the NIST AutomaticContent Extraction ACE program ACE 2000 2007 current research in IE has three main objectives Entity Detection and Tracking EDT Relation Detection and Characterization RDC and Event Detection and Characterization EDC", "This paper focuses on the ACE RDC subtask where many machine learning methods have been proposed including supervised methods Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 Zhang et al 2006 Qian et al 2008 semisupervised methods Brin 1998 Agichtein and Gravano 2000 Zhang 2004 Chen et al 2006 Zhou et al 2008 and unsupervised methods Hasegawa et al 2004 Zhang et al 2005  Current work on semantic relation extraction task mainly uses supervised learning methods since it achieves relatively better performance", "However this method requires a large amount of manually labeled relation instances which is both timeconsuming and laborious", "In the contrast unsupervised methods do not need definitions of relation types and handtagged data but it is difficult to evaluate their performance since there are no criteria for evaluation", "Therefore semisupervised learning has received more and more attention as it can balance the advantages and disadvantages between supervised and unsupervised methods", "With the plenitude of unlabeled natural language data at hand semisupervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance", "Specifically a bootstrapping algorithm chooses the unlabeled instances with the highest probability of being correctly labeled and use them to augment labeled training data iteratively", "Although previous work Yarowsky 1995 Blum and Mitchell 1998 Abney 2000 Zhang 2004 has tackled the bootstrapping approach from both the theoretical and practical point of view many key problems still remain unresolved such as the selection of initial seed set", "Since the size of the initial seed set is usually small eg 1437 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 14371445 Singapore 67 August 2009", "Qc 2009 ACL and AFNLP 100 instances the imbalance of relation types or manifold structure cluster structure in it will severely weaken the strength of bootstrapping", "Therefore it is critical for a bootstrapping approach to select the most appropriate initial seed set", "However current systems Zhang 2004 Chen et al 2006 use a randomly sampling strategy which fails to explore the affinity nature among the training instances", "Alternatively Zhou et al", "2008 bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM", "Nevertheless the initial labeled data is still randomly generated only to ensure that there are at least 5 instances for every relation subtype", "This paper presents a new approach to selecting the initial seed set based on stratified sampling strategy in the bootstrapping procedure for semisupervised semantic relation classification", "The motivation behind the stratified sampling is that every relation type should be as much as possible represented in the initial seed set thus leading to more instances with diverse structures being added to the labeled set", "In addition we also explore different strategies to augment reliably classified instances to the labeled data iteratively and attempt to find a stoppage criterion for the iteration procedure to greatly decrease the training time other than using up all the unlabeled set", "The rest of this paper is organized as follows", "First Section 2 reviews related work on semi supervised relation extraction", "Then we present an underlying supervised learner in Section 3", "Section 4 details various key aspects of the bootstrapping procedure including the stratified sampling strategy", "Experimental results are reported in Section 5", "Finally we conclude our work in Section 6"]}, "D10-1005": {"title": ["  Holistic Sentiment Analysis Across Languages"], "abstract": [], "inroduction": ["As its name suggests MLSLDA is an extension of Latent Dirichlet allocation LDA Blei et al 2003 a modeling approach that takes a corpus of unannotated documents as input and produces two outputs a set of topics and assignments of documents to topics", "Both the topics and the assignments are probabilistic a topic is represented as a probability distribution over words in the corpus and each document is assigned a probability distribution over all the topics", "Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentiment bearing words and because topic distributions are a parsimonious way to represent a document1 LDA has been used to discover latent structure in text eg for discourse segmentation Purver et al 2006 and authorship RosenZvi et al 2004", "MLSLDA extends the approach by ensuring that this latent structure  the underlying topics  is consistent across languages", "We discuss multilingual topic modeling in Section 11 and in Section 12 we show how this enables supervised regression regardless of a documents language", "11 Capturing Semantic Correlations", "Topic models posit a straightforward generative process that creates an observed corpus", "For each document d some distribution d over unobserved topics is chosen", "Then for each word position in the document a topic z is selected", "Finally the word for that position is generated by selecting from the topic indexed by z", "Recall that in LDA a topic is a distribution over words", "In monolingual topic models the topic distribution is usually drawn from a Dirichlet distribution", "Using Dirichlet distributions makes it easy to specify sparse priors and it also simplifies posterior inference because Dirichlet distributions are conjugate to multinomial distributions", "However drawing topics from Dirichlet distributions will not suffice if our vocabulary includes multiple languages", "If we are working with English German and Chinese at the same time a Dirichlet prior has no way to fa vor distributions z such that pgoodz pgutz and 1 The latter property has also made LDA popular for information retrieval Wei and Croft 2006", "phaoz all tend to be high at the same time or low at the same time", "More generally the structure of our model must encourage topics to be consistent across languages and Dirichlet distributions cannot encode correlations between elements", "One possible solution to this problem is to use the multivariate normal distribution which can produce correlated multinomials Blei and Lafferty 2005 in place of the Dirichlet distribution", "This has been done successfully in multilingual settings Cohen and Smith 2009", "However such models complicate inference by not being conjugate", "Instead we appeal to treebased extensions of the Dirichlet distribution which has been used to induce correlation in semantic ontologies BoydGraber et al 2007 and to encode clustering constraints An drzejewski et al 2009", "The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree", "For con creteness in this section we will use WordNet Miller 1990 as the representation of this multilingual semantic bridge since it is well known offers convenient and intuitive terminology and demonstrates the full flexibility of our approach", "However the model we describe generalizes to any treestructured representation of multilingual knowledge we discuss some alternatives in Section 3", "WordNet organizes a vocabulary into a rooted directed acyclic graph of nodes called synsets short for synonym sets A synset is a child of another synset if it satisfies a hyponomy relationship each child is a more specific instantiation of its parent concept thus hyponomy is often called an isa relationship", "For example a dog is a canine is an animal is a living thing etc As an approximation it is not unreasonable to assume that WordNets structure of meaning is language independent ie the concept encoded by a synset can be realized using terms in different languages that share the same meaning", "In practice this organization has been used to create many alignments of international WordNets to the original English WordNet Ordan and Wintner 2007 Sagot and Fiser 2008 Isahara et al 2008", "Using the structure of WordNet we can now describe a generative process that produces a distribution over a multilingual vocabulary which encourages correlations between words with similar mean ings regardless of what language each word is in", "For each synset h we create a multilingual word distribution for that synset as follows 1", "Draw transition probabilities h  Dir h "]}, "D10-1034": {"title": ["Clusteringbased Stratified Seed Sampling for SemiSupervised Relation"], "abstract": ["Seed sampling is critical in semisupervised learning", "This paper proposes a clustering based stratified seed sampling approach to semisupervised learning", "First various clustering algorithms are explored to partition the unlabeled instances into different strata with each stratum represented by a center", "Then diversitymotivated intrastratum sampling is adopted to choose the center and additional instances from each stratum to form the unlabeled seed set for an oracle to annotate", "Finally the labeled seed set is fed into a bootstrapping procedure as the initial labeled data", "We systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ACE RDC Relation Detection and Classification task", "In particular we compare various clustering algorithms on the stratified bootstrapping performance", "Experimental results on the ACE RDC 2004 corpus show that our clustering based stratified bootstrapping approach achieves the best F1score of 759 on the sub task of semantic relation classification approaching the one with golden clustering"], "inroduction": ["Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text", "Many machine learning approaches have been proposed to attack this problem including supervised Miller et al 2000 Zelenko et al 2003 Culotta and Soresen 2004 Kambhatla 2004 Zhao and Grishman 2005 Zhou et al 2005 Zhang et al 2006 Zhou and Zhang 2007 Zhou et al 2007 Qian et al 2008 Zhou et al 2010 semi supervised Brin 1998 Agichtein and Gravano 2000 Zhang 2004 Chen et al 2006 Qian et al 2009 Zhou et al 2009 and unsupervised methods Hasegawa et al 2004 Zhang et al 2005 Chen et al 2005", "Current work on relation extraction mainly adopts supervised learning methods since they achieve much better performance", "However theynormally require a large number of manually la beled relation instances whose acquisition is both time consuming and labor intensive", "In contrast unsupervised methods do not need any manually labeled instances", "Nevertheless it is difficult to assess their performance due to the lack of evaluation criteria", "As something between them semi supervised learning has received more and more attention recently", "With the plenitude of unlabeled natural language text at hand semisupervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance", "For example Abney 2002 proposes a bootstrapping algorithm which chooses the unlabeled instances with the highest probability of being correctly labeled and add them in turn into the labeled training data iteratively", "This paper focuses on bootstrappingbased semi supervised learning in relation extraction", "Since the performance of bootstrapping depends much on the quality and quantity of the seed set and researchers tend to employ as few seeds as possible eg 100 instances to save time and labor the quality of the seed set plays a critical role in bootstrapping", "Furthermore the imbalance of different classes and 346 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 346355 MIT Massachusetts USA 911 October 2010", "Qc 2010 Association for Computational Linguistics the inherent structural complexity of instances will severely weaken the strength of bootstrapping and semisupervised learning as well", "Therefore it is critical for a bootstrapping procedure to select an appropriate seed set which should be representative and diverse", "However most of current semi supervised relation extraction systems Zhang 2004 Chen et al 2006 use a random seed sampling strategy which fails to fully exploit the affinity nature in the training data to derive the seed set", "Alternatively Zhou et al", "2009 bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM and feed these instances into semisupervised relation extraction", "However their seed set is sequentially generated only to ensure that there are at least 5 instances for each relation class", "Our previous work Qian et al 2009 attempts to solve this problem via a simple stratified sampling strategy for selecting the seed set", "Experimentation on the ACE RDC 2004 corpus shows that the stratified sampling strategy achieves promising results for semisupervised learning", "Nevertheless the success of the strategy relies on the assumption that the true distribution of all relation types is already known which is impractical for real NLP applications", "This paper presents a clusteringbased stratified seed sampling approach for semisupervised relation extraction without the assumption on the true distribution of different relation types", "The motivations behind our approach are that the unlabeled data can be partitioned into a number of strata using a clustering algorithm and that representative and diverse seeds can be derived from such strata in the framework of stratified sampling Neyman 1934 for an oracle to annotate", "Particularly we employ a diversitymotivated intrastratum sampling scheme to pick a center and additional instances as seeds from each stratum", "Experimental results show the effectiveness of the clustering based stratified seed sampling for semisupervised relation classification", "The rest of this paper is organized as follows", "First an overview of the related work is given in Section 2", "Then Section 3 introduces the stratifiedbootstrapping framework including an intra stratum sampling scheme while Section 4 describes various clustering algorithms", "The experimental results on the ACE RDC 2004 corpus are reported in Section 5", "Finally we conclude our work and indicate some future directions in Section 6"]}, "D10-1042": {"title": ["Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 430 439 MIT Massachusetts USA 911 October 2010 c  2010 Association for Computational Linguistics Mining Name Translations from Entity Graph Mapping Gaewon You Seungwon Hwang YoungIn Song Long Jiang Zaiqing Nie Pohang University of Science and Technology Pohang Republic of Korea gwyouswhwangpostechackr Microsoft Research Asia Beijing China yosonglongjzniemicrosoftcom Abstract This paper studies the problem of mining entity translation specifically mining English and Chinese name pairs Existing efforts can be categorized into a a transliterationbased approach leveraging phonetic similarity and b a corpusbased approach exploiting bilingual co occurrences each of which suffers from inaccuracy and scarcity respectively In clear contrast we use unleveraged resources of monolingual entity cooccurrences crawled from entity search engines represented as two entityrelationship graphs extracted from two language corpora respectively Our problem is then abstracted as finding correct mappings across two graphs To achieve this goal we propose a holistic ap proach of exploiting both transliteration similarity and monolingual cooccurrences This approach building upon monolingual corpora complements existing corpusbased work requiring scarce resources of parallel or comparable corpus while significantly boosting the accuracy of transliterationbased work We validate our proposed system using reallife datasets 1 Introduction Entity translation aims at mapping the entity names eg people locations and organizations in source language into their corresponding names in target language While high quality entity translation is essential in crosslingual information access and transThis work was done when the first two authors visited Microsoft Research Asia lation it is nontrivial to achieve due to the challenge that entity translation though typically bearing pronunciation similarity can also be arbitrary eg Jackie Chan and  pronounced Cheng Long Existing efforts to address these challenges can be categorized into transliteration and corpusbased approaches Transliterationbased approaches Wan and Verspoor 1998 Knight and Graehl 1998 identify translations based on pronunciation similarity while corpusbased approaches mine bilingual cooccurrences of translation pairs obtained from parallel Kupiec 1993 Feng et al 2004 or comparable Fung and Yee 1998 corpora or alternatively mined from bilingual sentences Lin et al 2008 Jiang et al 2009 These two approaches have complementary strength transliterationbased similarity can be computed for any name pair but cannot mine translations of little or none phonetic similarity Corpusbased similarity can support arbitrary translations but require highly scarce resources of bilingual co occurrences obtained from parallel or comparable bilingual corpora In this paper we propose a holistic approach leveraging both transliteration and corpusbased similarity Our key contribution is to replace the use of scarce resources of bilingual cooccurrences with the use of untapped and significantly larger resources of monolingual cooccurrences for trans lation In particular we extract monolingual cooccurrences of entities from English and Chinese Web corpora which are readily available from en tity search engines such as PeopleEntityCube1 deployed by Microsoft Research Asia Such engine 1httppeopleentitycubecom 430 automatically extracts people names from text and their co occurrences to retrieve related entities based on cooccurrences To illustrate Figure 1a demonstrates the query result for Bill Gates retrieving and visualizing the  entityrelationship graph of related people names that frequently co occur with Bill in English corpus Similarly entityrelationship graphs can be built over other language corpora as Figure 1b demonstrates the corresponding results for the same query from Renlifang2 on ChineseWeb corpus From this point on for the sake of simplicity we refer to English and Chinese graphs simply asGe andGc respectively Though we illustrate with EnglishChinese pairs in the paper our method can be easily adapted to other language pairs In particular we propose a novel approach of ab stracting entity translation as a graph matching problem of two graphsGe andGc in Figures 1a and b Specifically the similarity between two nodes ve and vc in Ge and Gc is initialized as their transliteration similarity which is iteratively refined based on relational similarity obtained from monolingual cooccurrences To illustrate this an English news article mentioning  Bill Gates and  Melinda Gates evidences a relationship between the two entities which can be quantified from their co occurrences in the entire English Web corpus Similarly we can mine Chinese news articles to obtain the relationships between  and H  Once these two bilingual graphs of people and their relationships are harvested entity translation can leverage these parallel relationships to further evidence the mapping between translation pairs as Figure 1c illustrates To highlight the advantage of our proposed approach we compare our results with commercial machine translators 1 Engkoo3 developed in Microsoft Research Asia and 2 Google Translator4 In particular Figure 2 reports the precision for two groups heads that belong to top100 popular peo ple determined by the number of hits among randomly sampled 304 people names5 from six graph pairs of size 1000 each and the remaining  tails Commercial translators such as Google leveraging 2httprenlifangmsracn 3httpwwwengkoocom 4httptranslategooglecom 5See Section 4 for the sampling process Ours Google Engkoo0"], "abstract": ["This paper studies the problem of mining entity translation specifically mining English and Chinese name pairs", "Existing efforts can be categorized into a a transliteration based approach leveraging phonetic similarity and b a corpusbased approach exploiting bilingual cooccurrences each of which suffers from inaccuracy and scarcity respectively", "In clear contrast we use unleveraged resources of monolingual entity cooccurrences crawled from entity search engines represented as two entityrelationship graphs extracted from two language corpora respectively", "Our problem is then abstracted as finding correct mappings across two graphs", "To achieve this goal we propose a holistic approach of exploiting both transliteration similarity and monolingual cooccurrences", "This approach building upon monolingual corpora complements existing corpusbased work requiring scarce resources of parallel or comparable corpus while significantly boosting the accuracy of transliterationbased work", "We validate our proposed system using reallife datasets"], "inroduction": ["Entity translation aims at mapping the entity names eg people locations and organizations in source language into their corresponding names in target language", "While high quality entity translation is essential in crosslingual information access and trans lation it is nontrivial to achieve due to the challenge that entity translation though typically bearing pronunciation similarity can also be arbitrary eg Jackie Chan and fiX it pronounced Cheng Long", "Existing efforts to address these challenges can be categorized into transliteration and corpus based approaches", "Transliterationbased approaches Wan and Verspoor 1998 Knight and Graehl 1998 identify translations based on pronunciation similarity while corpusbased approaches mine bilingual cooccurrences of translation pairs obtained from parallel Kupiec 1993 Feng et al 2004 or comparable Fung and Yee 1998 corpora or alternatively mined from bilingual sentences Lin et al 2008 Jiang et al 2009", "These two approaches have complementary strength transliterationbased similarity can be computed for any name pair but cannot mine translations of little or none phonetic similarity", "Corpusbased similarity can support arbitrary translations but require highly scarce resources of bilingual cooccurrences obtained from parallel or comparable bilingual corpora", "In this paper we propose a holistic approach leveraging both transliteration and corpusbased similarity", "Our key contribution is to replace the use of scarce resources of bilingual cooccurrences with the use of untapped and significantly larger resources of monolingual cooccurrences for translation", "In particular we extract monolingual co occurrences of entities from English and Chinese Web corpora which are readily available from entity search engines such as PeopleEntityCube1 deployed by Microsoft Research Asia", "Such engine This work was done when the first two authors visited Microsoft Research Asia", "1 httppeopleentitycube", "com 430 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 430439 MIT Massachusetts USA 911 October 2010", "Qc 2010 Association for Computational Linguistics automatically extracts people names from text and their cooccurrences to retrieve related entities based on cooccurrences", "To illustrate Figure 1a demonstrates the query result for Bill Gates retrieving and visualizing the entityrelationship graph of related people names that frequently cooccur with Bill in English corpus", "Similarly entityrelationship graphs can be built over other language corpora as Figure 1b demonstrates the corresponding results for the same query from Renlifang2 on Chinese Web corpus", "From this point on for the sake of simplicity we refer to English and Chinese graphs simply 08 07 06 05 04 03 02 01 0 Tail Head Ours Google Engkoo as Ge and Gc respectively", "Though we illustrate with EnglishChinese pairs in the paper our method can be easily adapted to other language pairs", "In particular we propose a novel approach of abstracting entity translation as a graph matching problem of two graphs Ge and Gc in Figures 1a and b", "Specifically the similarity between two nodes ve and vc in Ge and Gc is initialized as their transliteration similarity which is iteratively refined based on relational similarity obtained from monolingual co occurrences", "To illustrate this an English news article mentioning Bill Gates and Melinda Gates evidences a relationship between the two entities which can be quantified from their cooccurrences in the entire English Web corpus", "Similarly we can mine Chinese news articles to obtain the re lationships between t Jli Vi and 11 li itJli Vi", "Once these two bilingual graphs of people and their relationships are harvested entity translation can leverage these parallel relationships to further evidence the mapping between translation pairs as Figure 1c illustrates", "To highlight the advantage of our proposed approach we compare our results with commercial machine translators 1 Engkoo3 developed in Microsoft Research Asia and 2 Google Translator4", "In particular Figure 2 reports the precision for two groups heads that belong to top100 popular people determined by the number of hits among randomly sampled 304 people names5 from six graph pairs of size 1000 each and the remaining tails", "Commercial translators such as Google leveraging 2 httprenlifangmsracn 3 httpwwwengkoocom 4 httptranslategooglecom 5 See Section 4 for the sampling process", "Figure 2 Comparison for Head and Tail datasets bilingual cooccurrences that are scarce for tails show significantly lower precision for tails", "Meanwhile our work depending solely on monolingual cooccurrences shows high precision for both heads and tails", "Our focus is to boost translation accuracy for long tails with nontrivial Web occurrences in each monolingual corpus but not with much bilingual co occurrences eg researchers publishing actively in two languages but not famous enough to be featured in multilingual Wikipedia entries or news articles", "As existing translators are already highly accurate for popular heads this focus well addresses the remaining challenges for entity translation", "To summarize we believe that this paper has the following contributions  We abstract entity translation problem as a graph mapping between entityrelationship graphs in two languages", " We develop an effective matching algorithm leveraging both pronunciation and co occurrence similarity", "This holistic approach complements existing approaches and en hances the translation coverage and accuracy", " We validate the effectiveness of our approach using various reallife datasets", "The rest of this paper is organized as follows", "Section 2 reviews existing work", "Section 3 then develops our framework", "Section 4 reports experimental results and Section 5 concludes our work", "a English PeopleEntityCube Ge b Chinese Renlifang Gc c Abstracting translation as graph mapping Figure 1 Illustration of entityrelationship graphs"]}, "D10-1100": {"title": ["Automatic Detection and Classification of Social Events"], "abstract": ["In this paper we introduce the new task of social event extraction from text", "We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact", "We annotate part of Automatic Content Extraction ACE data and perform experiments using Support Vector Machines with Kernel methods", "We use a combination of structures derived from phrase structure trees and dependency trees", "A characteristic of our events which distinguishes them from ACE events is that the participating entities can be spread far across the parse trees", "We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks", "We also use three data sampling approaches to solve the problem of data skewness", "Sampling methods improve the F1measure for the task of relation detection by over 20 absolute over the baseline"], "inroduction": ["This paper introduces a novel natural language processing NLP task social event extraction", "We are interested in this task because it contributes to our overall research goal which is to extract a social network from written text", "The extracted social network can be used for various applications such as summarization questionanswering or the detection of main characters in a story", "For example we manually extracted the social network of characters in Alice in Wonderland and ran standard social network analysis algorithms on the network", "The most influential characters in the story were correctly detected", "Moreover characters occurring in a scene together were given same social roles and positions", "Social network extraction has recently been applied to literary theory Elson et al 2010 and has the potential to help organize novels that are becoming machine readable", "We take a social network to be a network consisting of individual human beings and groups of human beings who are connected to each other by the virtue of participating in social events", "We define social events to be events that occur between people where at least one person is aware of the other and of the event taking place", "For example in the sentence John talks to Mary entities John and Mary are aware of each other and the talking event", "In the sentence John thinks Mary is great only John is aware of Mary and the event is the thinking event", "In the sentence Rabbit ran by Alice there is no evidence about the cognitive states of Rabbit and Alice because the Rabbit could have run by Alice without any one of them noticing each other", "A text can describe a social network in two ways explicitly by stating the type of relationship between two individuals eg husbandwife or implicitly by describing an event which creates or perpetuates a social relationship eg John talked to Mary", "We will call these types of events social events", "We define two types of social events interaction in which both parties are aware of the social event eg a conversation and observation in which only one party is aware of the interaction eg thinking about or 1024 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing pages 10241034 MIT Massachusetts USA 911 October 2010", "Qc 2010 Association for Computational Linguistics spying on someone", "Note that the notion of cognitive state is crucial to our definition", "This paper is the first attempt to detect and classify social events present in text", "Our task is different from related tasks notably from the Automated Content Extraction ACE relation and event extraction tasks because the events are different they are a class of events defined through the effect on participants cognitive state and the linguistic realization is different", "Mentions of entities1 engaged in a social event are often quite distant from each other in the sentence unlike in ACE relations where about 70 of relations are local in our social event annotation only 25 of the events are local", "In fact the average number of words between entities participating in any social event is 9", "We use tree kernel methods on structures derived from phrase structure trees and dependency trees in conjunction with Support Vector Machines SVMs to solve our tasks", "For the design of structures and type of kernel we take motivation from a system proposed by Nguyen et al", "2009 which is a state oftheart system for relation extraction", "Data skew ness turns out to be a big challenge for the task of relation detection since there are many more pairs of entities without a relation as compared to pairs of entities that have a relation", "In this paper we discuss three data sampling techniques that deal with this skewness and allow us to gain over 20 in F1 measure over our baseline system", "Moreover we introduce a new sequence kernel that outperforms previously proposed sequence kernels for the task of social event detection and plays a role to achieve the best performing system for the task of social event detection and classification", "The paper is structured as follows", "In Section 2 we compare our work to existing work notably the ACE extraction literature", "In Section 3 we present our task in detail and explain how we annotated our corpus", "We also show why this is a novel task and how it is different from the ACE extraction tasks", "We then discuss kernel methods and the structures we use and introduce our new structure in Section 4", "In Section 5 we present the sampling methods used for experiments", "In Section 6 we present our exper 1 An entity mention is a reference of an entity in text", "Also we use entities and people interchangeably since the only entities we are interested in are people or groups of people", "iments and results for social event detection and social event classification tasks", "We conclude in Section 7 and mention our future direction of research"]}, "D11-1044": {"title": ["QuasiSynchronous Phrase Dependency Grammars"], "abstract": ["We present a quasisynchronous dependency grammar Smith and Eisner 2006 for machine translation in which the leaves of the tree are phrases rather than words as in previous work Gimpel and Smith 2009", "This formulation allows us to combine structural components of phrasebased and syntaxbased MT in a single model", "We describe a method of extracting phrase dependencies from parallel text using a targetside dependency parser", "For decoding we describe a coarsetofine approach based on lattice dependency parsing of phrase lattices", "We demonstrate performance improvements for ChineseEnglish and UrduEnglish translation over a phrasebased baseline", "We also investigate the use of unsupervised dependency parsers reporting encouraging preliminary results"], "inroduction": ["Two approaches currently dominate statistical machine translation MT research", "Phrasebased models Koehn et al 2003 excel at capturing local reordering phenomena and memorizing multiword translations", "Models that employ syntax or syntax like representations Chiang 2005 Galley et al 2006 Zollmann and Venugopal 2006 Huang et al 2006 handle longdistance reordering better than phrasebased systems Auli et al 2009 but often require constraints on the formalism or rule extraction method in order to achieve computational tractability", "As a result certain instances of syntactic divergence are more naturally handled by phrasebased systems DeNeefe et al 2007", "In this paper we present a new way of combining the advantages of phrasebased and syntaxbased MT We propose a model in which phrases are organized into a tree structure inspired by dependency syntax", "Instead of standard dependency trees in which words are vertices our trees have phrases as vertices", "We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side and use them to compute targetside tree features", "We define additional stringtotree features and if a sourceside dependency parser is available treetotree features to capture properties of how phrase dependencies interact with reordering", "To leverage standard phrasebased features alongside our novel features we require a formalism that supports flexible feature combination and efficient decoding", "Quasisynchronous grammar QG provides this backbone Smith and Eisner 2006 we describe a coarsetofine approach for decoding within this framework advancing substantially over earlier QG machine translation systems Gimpel and Smith 2009", "The decoder involves generating a phrase lattice Ueffing et al 2002 in a coarse pass using a phrasebased model followed by lattice dependency parsing of the phrase lattice", "This approach allows us to feasibly explore the combined search space of segmentations phrase alignments and target phrase dependency trees", "Our experiments demonstrate an average improvement of 065 BLEU in ChineseEnglish translation across three test sets and an improvement of 075 BLEU in UrduEnglish translation over a phrasebased baseline", "We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers reporting promising results using a supervised Chinese parser and a stateoftheart unsupervised English parser provides our best results giving an averaged gain of 079 BLEU over the baseline", "We also discuss how our model improves translation quality and discuss future possibilities for combining approaches to ma 474 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 474485 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics chine translation using our framework"]}, "D11-1056": {"title": ["Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 605 615 Edinburgh Scotland UK July 27 31 2011 c  2011 Association for Computational Linguistics Nonparametric Bayesian Segmentation of Japanese Noun Phrases Yugo Murawaki and Sadao Kurohashi Graduate School of Informatics Kyoto University murawaki kuroikyotouacjp Abstract A key factor of high quality word segmentation for Japanese is a highcoverage dictionary but it is costly to manually build such a lexical resource Although external lexical resources for human readers are potentially good knowledge sources they have not been utilized due to differences in segmentation criteria To supplement a morphological dictionary with these resources we propose a new task of Japanese noun phrase segmentation We apply nonparametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text For inference we propose a novel block sampling procedure named hybrid typebased sampling which has the ability to directly escape a local optimum that is not too distant from the global optimum Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer 1 Introduction Word segmentation is the first step of natural language processing for Japanese Chinese and Thai because they do not delimit words by whitespace Segmentation for Japanese is a successful field of research achieving the Fscore of nearly 99 Kudo et al 2004 This success rests on a highcoverage dictionary Unknown words or words not covered by the dictionary are often misidentified Historically researchers have devoted extensive human resources to build and maintain highcoverage dictionaries Yokoi 1995 Since the orthography of Japanese does not specify a standard for segmentation researchers define their own criteria before constructing lexical resources For this reason it is difficult to exploit existing external resources such as dictionaries and encyclopedias for human readers where entry words are not segmented according to the criteria Among them encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover Most of these terms are noun phrases and consist of more than one word morpheme For example an encyclopedia has an entry      tsuneyamajou  Tsuneyama Castle  According to our segmentation criteria it consists of two words     tsuneyama and    jou However the morphological analyzer wrongly segments it into  tsune and     yamashiro because     tsuneyama is an unknown word In this paper we present the first attempt to utilize encyclopedias for word segmentation We segment each entry noun phrase into words To do this we examine the main text of the entry on the assumption that if the noun phrase in question consists of more than one word its constituents appear in the main text either freely or as part of other noun phrases For      tsuneyamajou its constituent     tsune appears by itself and as constituents of other nouns phrases such as      peak of Tsuneyama and      Tsuneyama Station while     yamashiro does not To segment each noun phrase we use nonparametric Bayesian language models Goldwater et al 2009 Mochihashi et al 2009 Our approach 605 is based on two key factors the bigram model and typebased block sampling The bigram model alleviates a problem of the unigram model that is a tendency to misidentify a sequence of words in common collocations as a single word Typebased sampling Liang et al 2010 has the ability to directly escape a local optimum making inference very efficient However typebased sampling is not easily applicable to the bigrammodel owing to sparsity and its dependence on latent assignments We propose a hybrid typebased sampling procedure which combines the MetropolisHastings algorithm with Gibbs sampling We circumvent the sparsity problem by joint sampling of unigramlevel type Also instead of calculating the probability of every possible state of the jointly sampled random variables we only compare the current state with a proposed state This greatly eases the sampling procedure while retaining the efficiency of typebased sampling Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer 2 Related Work Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and partofspeech tagging which is called morphological analysis Kurohashi et al 1994 Asahara and Matsumoto 2000 Kudo et al 2004 The standard approach in Japanese morphological analysis is lattice based path selection instead of characterbased IOB tagging Given a sentence an analyzer first builds a lattice of words with dictionary lookup and then selects an optimal path using predefined parameters This approach enables fast decoding and achieves accuracy high enough for practical use This success however depends on a highcoverage dictionary and unknown words are often misidentified Although a line of research attempts to identify unknown words on the fly Uchimoto et al 2001 Asahara and Matsumoto 2004 it by no means provides a definitive solution because it suffers from locality of contextual information avail able for identification Nakagawa and Matsumoto 2006 Therefore we like to perform separate lexical acquisition processes in which wider context can be examined Our approach in this paper has a complementary relationship with unknown word acquisition from text which we previously proposed Murawaki and Kurohashi 2008 Since unlike Chinese and Thai Japanese is rich in morphology morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired In general this method works pretty well but one exception is noun phrases Noun phrases can hardly be distinguished from single nouns because in Japanese no morphological marker is attached to join nouns to form a noun phrase We previously resort to a heuristic measure to segment noun phrases The new statistical method provides a straightforward solution to this problem Meanwhile our language models have their own problem The assumption that language is a sequence of invariant words fails to capture rich morphology as our segmentation criteria specify that each verb or adjective consists of an invariant stem and an ending that changes its form according to its grammatical roles For this reason we limit our scope to noun phrases in this paper Use of Noun Phrases Named entity recognition NER is a field where encyclopedic knowledge plays an important role Kazama and Torisawa 2008 encode information extracted from a gazetteer eg Wikipedia as features of a CRFbased Japanese NE tagger They formalize the NER task as the characterbased labeling of IOB tags Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags How ever this does not fully solve the knowledge bottleneck problem They also used the output of a morphological analyzer which does not utilize encyclopedic knowledge NER performance may be affected by segmentation errors in morphological analysis involving unknown words"], "abstract": ["A key factor of high quality word segmentation for Japanese is a highcoverage dictionary but it is costly to manually build such a lexical resource", "Although external lexical resources for human readers are potentially good knowledge sources they have not been utilized due to differences in segmentation criteria", "To supplement a morphological dictionary with these resources we propose a new task of Japanese noun phrase segmentation", "We apply nonparametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text", "For inference we propose a novel block sampling procedure named hybrid typebased sampling which has the ability to directly escape a local optimum that is not too distant from the global optimum", "Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer"], "inroduction": ["Word segmentation is the first step of natural language processing for Japanese Chinese and Thai because they do not delimit words by whitespace", "Segmentation for Japanese is a successful field of research achieving the Fscore of nearly 99 Kudo et al 2004", "This success rests on a highcoverage dictionary", "Unknown words or words not covered by the dictionary are often misidentified", "Historically researchers have devoted extensive human resources to build and maintain high coverage dictionaries Yokoi 1995", "Since the orthography of Japanese does not specify a standard for segmentation researchers define their own criteria before constructing lexical resources", "For this reason it is difficult to exploit existing external resources such as dictionaries and encyclopedias for human readers where entry words are not segmented according to the criteria", "Among them encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover", "Most of these terms are noun phrases and consist of more than one word morpheme", "For example an encyclopedia has an en try  tsuneyamajou Tsuneyama Castle", "According to our segmentation criteria it consists of two words  tsuneyama and  jou", "However the morphological analyzer wrongly segments it into  tsune and  yamashiro because  tsuneyama is an unknown word", "In this paper we present the first attempt to utilize encyclopedias for word segmentation", "We segment each entry noun phrase into words", "To do this we examine the main text of the entry on the assumption that if the noun phrase in question consists of more than one word its constituents appear in the main text either freely or as part of other noun phrases", "For  tsuneyamajou its constituent  tsune appears by itself and as constituents of other nouns phrases such as   peak of Tsuneyama and  Tsuneyama Station while  yamashiro does not", "To segment each noun phrase we use non parametric Bayesian language models Goldwater et al 2009 Mochihashi et al 2009", "Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 605615 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics is based on two key factors the bigram model and typebased block sampling", "The bigram model alleviates a problem of the unigram model that is a tendency to misidentify a sequence of words in common collocations as a single word", "Typebased sampling Liang et al 2010 has the ability to directly escape a local optimum making inference very efficient", "However typebased sampling is not easily applicable to the bigram model owing to sparsity and its dependence on latent assignments", "We propose a hybrid typebased sampling procedure which combines the MetropolisHastings algorithm with Gibbs sampling", "We circumvent the sparsity problem by joint sampling of unigramlevel type", "Also instead of calculating the probability of every possible state of the jointly sampled random variables we only compare the current state with a proposed state", "This greatly eases the sampling procedure while retaining the efficiency of type based sampling", "Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer"]}, "D11-1059": {"title": ["Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 638 647 Edinburgh Scotland UK July 27 31 2011 c  2011 Association for Computational Linguistics A Bayesian Mixture Model for PartofSpeech Induction Using Multiple Features Christos Christodoulopoulos School of Informatics University of Edinburgh christoscedacuk Sharon Goldwater School of Informatics University of Edinburgh sgwaterinfedacuk Mark Steedman School of Informatics University of Edinburgh steedmaninfedacuk Abstract In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model where each word type is constrained to belong to a single class By using a mixture model rather than a sequence model eg HMM we are able to easily add multiple kinds of features including those at both the type level morphology features and token level context and alignment features the latter from parallel corpora Using only context features our system yields results comparable to stateofthe art far better than a similar model without the oneclass pertype constraint Using the additional features provides added benefit and our final system outperforms the best published results on most of the 25 corpora tested 1 Introduction Research on unsupervised learning for NLP has become widespread recently with partofspeech induction or syntactic class induction being a particularly popular task1 However despite a recent proliferation of syntactic class induction systems Biemann 2006 Goldwater and Griffiths 2007 Johnson 2007 Ravi and Knight 2009 BergKirkpatrick et al 2010 Lee et al 2010 careful comparison indicates that very few systems perform better than some much simpler and quicker methods dating back ten or even twenty years Christodoulopoulos 1The task is more commonly referred to as partofspeech induction but we prefer the term syntactic class induction since the induced classes may not coincide with partofspeech tags et al 2010 This fact suggests that we should consider which features of the older systems led to their success and attempt to combine these features with some of the machine learning methods introduced by the more recent systems We pursue this strategy here developing a system based on Bayesian methods where the probabilistic model incorporates several insights from previous work Perhaps the most important property of our model is that it is typebased meaning that all tokens of a given word type are assigned to the same cluster This property is not strictly true of linguistic data but is a good approximation as Lee et al 2010 note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93 or more for most languages Since this is much better than the performance of current unsupervised syntactic class induction systems constraining the model in this way seems likely to improve performance by reducing the number of parameters in the model and incorporating useful linguistic knowledge Both of the older systems discussed by Christodoulopoulos et al 2010 ie Clark 2003 and Brown et al 1992 included this constraint and achieved very good performance relative to tokenbased systems More recently Lee et al 2010 presented a new typebased model and also reported very good results A second property of our model which distinguishes it from the typebased Bayesian model of Lee et al 2010 is that the underlying probabilistic model is a clustering model specifically a multinomial mixture model rather than a sequence model HMM In this sense our model is more closely re638 lated to several nonprobabilistic systems that cluster context vectors or lowerdimensional representations of them Redington et al 1998 Schu tze 1995 Lamar et al 2010 Sequence models are by far the most common method of supervised partofspeech tagging and have also been widely used for unsupervised partofspeech tagging both with and without a dictionary Smith and Eisner 2005 Haghighi and Klein 2006 Goldwater and Griffiths 2007 Johnson 2007 Ravi and Knight 2009 Lee et al 2010 However systems based on context vectors have also performed well in these latter scenarios Schu tze 1995 Lamar et al 2010 Toutanova and Johnson 2007 and present a viable alternative to sequence models One advantage of using a clustering model rather than a sequence model is that the features used for clustering need not be restricted to context words Additional types of features can easily be incorporated into the model and inference procedure using the same general framework as in the basic model that uses only context word features In particular we present two extensions to the basic model"], "abstract": ["In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model where each word type is constrained to belong to a single class", "By using a mixture model rather than a sequence model eg HMM we are able to easily add multiple kinds of features including those at both the type level morphology features and token level context and alignment features the latter from parallel corpora", "Using only context features our system yields results comparable to stateofthe art far better than a similar model without the oneclasspertype constraint", "Using the additional features provides added benefit and our final system outperforms the best published results on most of the 25 corpora tested"], "inroduction": ["Research on unsupervised learning for NLP has become widespread recently with partofspeech induction or syntactic class induction being a particularly popular task1 However despite a recent proliferation of syntactic class induction systems Biemann 2006 Goldwater and Griffiths 2007 Johnson 2007 Ravi and Knight 2009 BergKirkpatrick et al 2010 Lee et al 2010 careful comparison indicates that very few systems perform better than some much simpler and quicker methods dating back ten or even twenty years Christodoulopoulos 1 The task is more commonly referred to as partofspeech induction but we prefer the term syntactic class induction since the induced classes may not coincide with partofspeech tags", "et al 2010", "This fact suggests that we should consider which features of the older systems led to their success and attempt to combine these features with some of the machine learning methods introduced by the more recent systems", "We pursue this strategy here developing a system based on Bayesian methods where the probabilistic model incorporates several insights from previous work", "Perhaps the most important property of our model is that it is typebased meaning that all tokens of a given word type are assigned to the same cluster", "This property is not strictly true of linguistic data but is a good approximation as Lee et al", "2010 note assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93 or more for most languages", "Since this is much better than the performance of current unsupervised syntactic class induction systems constraining the model in this way seems likely to improve performance by reducing the number of parameters in the model and incorporating useful linguistic knowledge", "Both of the older systems discussed by Christodoulopoulos et al", "2010 ie Clark 2003 and Brown et al", "1992 included this constraint and achieved very good performance relative to tokenbased systems", "More recently Lee et al", "2010 presented a new typebased model and also reported very good results", "A second property of our model which distinguishes it from the typebased Bayesian model of Lee et al", "2010 is that the underlying probabilistic model is a clustering model specifically a multinomial mixture model rather than a sequence model HMM", "In this sense our model is more closely re 638 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 638647 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics lated to several nonprobabilistic systems that cluster context vectors or lowerdimensional representations of them Redington et al 1998 Schu tze 1995 Lamar et al 2010", "Sequence models are by far the most common method of supervised part ofspeech tagging and have also been widely used for unsupervised partofspeech tagging both with and without a dictionary Smith and Eisner 2005 Haghighi and Klein 2006 Goldwater and Griffiths 2007 Johnson 2007 Ravi and Knight 2009 Lee et al 2010", "However systems based on context vectors have also performed well in these latter scenarios Schu tze 1995 Lamar et al 2010 Toutanova and Johnson 2007 and present a viable alternative to sequence models", "One advantage of using a clustering model rather than a sequence model is that the features used for clustering need not be restricted to context words", "Additional types of features can easily be incorporated into the model and inference procedure using the same general framework as in the basic model that uses only context word features", "In particular we present two extensions to the basic model", "The first uses morphological features which serve as cues to syntactic class and seemed to partly explain the success of two bestperforming systems analysed by Christodoulopoulos et al", "2010", "The second extension to our model uses alignment features gathered from parallel corpora", "Previous work suggests that using parallel text can improve performance on various unsupervised NLP tasks Naseem et al 2009 Snyder and Barzilay 2008", "We evaluate our model on 25 corpora in 20 languages that vary substantially in both syntax and morphology", "As in previous work Lee et al 2010 we find that the oneclasspertype restriction boosts performance considerably over a comparable token based model and yields results that are comparable to stateoftheart even without the use of morphology or alignment features", "Including morphology features yields the best published results on 14 or 15 of our 25 corpora depending on the measure and alignment features can improve results further"]}, "D11-1076": {"title": ["Relation Acquisition using Word Classes and Partial Patterns"], "abstract": ["This paper proposes a semisupervised relation acquisition method that does not rely on extraction patterns eg X causes Y for causal relations but instead learns a combination of indirect evidence for the target relation  semantic word classes and partial patterns", "This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus  in extreme cases patterns that occur only once in the entire corpus", "Such patterns are beyond the reach of current pattern based methods", "We show that our method performs on par with stateoftheart pattern based methods and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns", "This ability to acquire long tail instances is crucial for risk management and innovation where an exhaustive database of highlevel semantic relations like causation is of vital importance"], "inroduction": ["Pattern based relation acquisition methods rely on lexicosyntactic patterns Hearst 1992 for extracting relation instances", "These are templates of natural language expressions such as X causes Y  that signal an instance of some semantic relation ie causality", "Pattern based methods Agichtein and Gravano 2000 Pantel and Pennacchiotti 2006b Pasca et al 2006 De Saeger et al 2009 learn many  This work was done when all authors were at the National Institute of Information and Communications Technology", "such patterns to extract new instances word pairs from the corpus", "However since extraction patterns are learned using statistical methods that require a certain frequency of observations pattern based methods fail to capture relations from complex expressions in which the pattern connecting the two words is rarely observed", "Consider the following sentence Curing hypertension alleviates the deterioration speed of the renal function thereby lowering the risk of causing intracranial bleeding Humans can infer that this sentence expresses a causal relation between the underlined noun phrases", "But the actual pattern connecting them ie Curing X alleviates the deterioration speed of the renal function thereby lowering the risk of causing Y  is rarely observed more than once even in a 108 page Web corpus", "In the sense that the term pattern implies a recurring event this expression contains no pattern for detecting the causal relation between hypertension and intracranial bleeding", "This is what we mean by long tail instances  words that cooccur infrequently and only in sparse extraction contexts", "Yet an important application of relation extraction is mining the Web for socalled unknown unknowns  in the words of D Rumsfeld things we dont know we dont know Torisawa et al 2010", "In knowledge discovery applications like risk management and innovation the usefulness of relation extraction lies in its ability to find many unexpected remedies for diseases causes of social problems and so on", "To give an example our relation extrac 825 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 825835 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics tion system found a blog post mentioning Japanese automaker Toyota as a hidden cause of Japans deflation", "Several months later the same connection was made in an article published in an authoritative economic magazine", "We propose a semisupervised relation extraction method that does not rely on direct pattern evidence connecting the two words in a sentence", "We argue that the role of binary patterns can be replaced by a combination of two types of indirect evidence semantic class information about the target relation and partial patterns which are fragments or sub patterns of binary patterns", "The intuition is this if a sentence like the example sentence above contains some word X belonging to the class of medical conditions and another word Y from the class of traumas and X matches the partial pattern ", "causing X there is a decent chance that this sentence expresses a causal relation between X and Y  We show that just using this combination of indirect evidence we can pick up semantic relations with roughly 50 precision regardless of the complexity or frequency of the expression in which the words cooccur", "Furthermore by combining this idea with a straightforward machine learning approach the overall performance of our method is on par with stateoftheart pattern based methods", "However our method manages to extract a large number of instances from sentences that contain no pattern that can be learned by pattern induction methods", "Our method is a twostage system", "Figure 1 presents an overview", "In Stage 1 we apply a state oftheart pattern based relation extractor to a Web corpus to obtain an initial batch of relation instances", "In Stage 2 a supervised classifier is built from various components obtained from the output of Stage 1", "Given the output of Stage 1 and access to a Web corpus the Stage 2 extractor is completely selfsufficient and the whole method requires no supervision other than a handful of seed patterns to start the first stage extractor", "The whole procedure is therefore minimally supervised", "Semantic word classes and partial patterns play a crucial role throughout all steps of the process", "We evaluate our method on three relation acquisition tasks causation prevention and material relations using a 600 million Japanese Web page cor Figure 1 Proposed method data flow", "pus Shinzato et al 2008 and show that our system can successfully acquire relations from both frequent and infrequent patterns", "Our system extracted 100000 causal relations with 846 precision 50000 prevention relations with 584 precision and 25000 material relations with 761 precision", "In the extreme case we acquired several thousand word pairs cooccurring only in patterns that appear once in the entire corpus", "We call such patterns single occurrence SO patterns", "Word pairs that cooccur only with SO patterns represent the theoretical limiting case of relations that cannot be acquired using existing pattern based methods", "In this sense our method can be seen as complementary with pattern based approaches and merging our methods output with that of a pattern based method may be beneficial"]}, "D11-1081": {"title": ["Fast Generation of Translation Forest"], "abstract": ["Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features it is hard to scale up to large data due to decoding complexity", "We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment", "Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation", "With millions of features trained on 519K sentences in 003 second per sentence our system achieves significant improvement by 084 BLEU over the baseline system on the NIST ChineseEnglish test sets"], "inroduction": ["Discriminative model Och and Ney 2002 can easily incorporate nonindependent and overlapping features and has been dominating the research field of statistical machine translation SMT in the last decade", "Recent work have shown that SMT benefits a lot from exploiting large amount of features Liang et al 2006 Tillmann and Zhang 2006 Watanabe et al 2007 Blunsom et al 2008 Chiang et al 2009", "However the training of the large number of features was always restricted in fairly small data sets", "Some systems limit the number of training examples while others use short sentences to maintain efficiency", "Overfitting problem often comes when training many features on a small data Watanabe et al 880 2007 Chiang et al 2009", "Obviously using much more data can alleviate such problem", "Furthermore large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT", "Despite these advantages to the best of our knowledge no previous discriminative training paradigms scale up to use a large amount of training data", "The main obstacle comes from the complexity of packed forests or nbest lists generation which requires to search through all possible translations of each training example which is computationally prohibitive in practice for SMT", "To make normalization efficient contrastive estimation Smith and Eisner 2005 Poon et al 2009 introduce neighborhood for unsupervised loglinear model and has presented positive results in various tasks", "Motivated by these work we use a translation forest Section 3 which contains both reference derivations that potentially yield the reference translation and also neighboring nonreference derivations that fail to produce the reference translation1However the complexity of generating this translation forest is up to On6 because we still need bi parsing to create the reference derivations", "Consequently we propose a method to fast generate a subset of the forest", "The key idea Section 4 is to initialize a reference derivation tree with maximum score by the help of word alignment and then traverse the tree to generate the subset forest in linear time", "Besides the efficiency improvement such a forest allows us to train the model without resort 1 Exactly there are no reference derivations since derivation is a latent variable in SMT", "We call them reference derivation just for convenience", "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 880888 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics 04 1 2 hyper edge rule 01 5 0 1 24 3 2 3 4 34 6 4 e1 r1 X  X1 bei X2 X1 was X2 e2 r2 X  qiangshou bei X1 t h e g u n m a n w a s X 1  e3 r3 X  jingfang X1 X1 by the police e4 r4 X  jingfang X1 police X1  e5 r5 X  qiangshou the gunman e6 r6 X  jibi shot dead Figure 1 A translation forest which is the running example throughout this paper", "The reference translation is the gunman was killed by the police", "1 Solid hyperedges denote a reference derivation tree t1 which exactly yields the reference translation", "2 Replacing e3 in t1 with e4 results a competing nonreference derivation t2  which fails to swap the order of X34 ", "3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3  Generally this is done by deleting a node X01  ing to constructing the oracle reference Liang et al 2006 Watanabe et al 2007 Chiang et al 2009 which is nontrivial for SMT and needs to be determined experimentally", "Given such forests we globally learn a loglinear model using stochastic gradient descend Section 5", "Overall both the generation of forests and the training algorithm are scalable enabling us to train millions of features on largescale data", "To show the effect of our framework we globally quence of SCFG rules ri", "Translation forest Miet al 2008 Li and Eisner 2009 is a compact repre sentation of all the derivations for a given sentence under an SCFG see Figure 1", "A tree t in the forest corresponds to a derivation", "In our paper tree means the same as derivation", "More formally a forest is a pair V E where V is the set of nodes E is the set of hyperedge", "For a given source sentence f  f n Each node v  V is in the form  which denotes the recognitiontrain millions of word level context features moti vated by word sense disambiguation Chan et al 2007 together with the features used in traditional SMT system Section 6", "Training on 519K sentence pairs in 003 seconds per sentence we achieve significantly improvement over the traditional pipeline by 084 BLEU"]}, "D11-1084": {"title": ["Cachebased Documentlevel Statistical Machine Translation"], "abstract": ["Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time ignoring documentlevel information", "In this paper we propose a cachebased approach to documentlevel translation", "Since caches mainly depend on relevant data to supervise subsequent decisions it is critical to fill the caches with highlyrelevant data of a reasonable size", "In this paper we present three kinds of caches to store relevant documentlevel information 1 a dynamic cache which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document 2 a static cache which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs ie source documents similar to the test document and their corresponding target documents in the training parallel corpus 3 a topic cache which stores the targetside topic words related with the test document in the sourceside", "In particular three new features are designed to explore various kinds of documentlevel information in above three kinds of caches", "Evaluation shows the effectiveness of our cachebased approach to documentlevel translation with the performance improvement of 081 in BLUE score over Moses", "Especially detailed analysis and discussion are presented to give new insights to documentlevel translation"], "inroduction": ["During last decade tremendous work has been done to improve the quality of statistical machine  Corresponding author", "translation SMT systems", "However there is still a huge performance gap between the stateofthe art SMT systems and human translators", "Bond 2002 suggested nine ways to improve machine translation by imitating the best practices of human translators Nida 1964 with parsing the entire document before translation as the first priority", "However most SMT systems still treat parallel corpora as a list of independent sentencepairs and ignore documentlevel information", "Documentlevel information can and should be used to help documentlevel machine translation", "At least the topic of a document can help choose specific translation candidates since when taken out of the context from their document some words phrases and even sentences may be rather ambiguous and thus difficult to understand", "Another advantage of documentlevel machine translation is its ability in keeping a consistent translation", "However documentlevel translation has drawn little attention from the SMT research community", "The reasons are manifold", "First of all most of parallel corpora lack the annotation of document boundaries Tam 2007", "Secondly although it is easy to incorporate a new feature into the classical loglinear model Och 2003 it is difficult to capture documentlevel information and model it via some simple features", "Thirdly reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts", "This makes the evaluation of documentlevel SMT systems extremely difficult", "Tiedemann 2010 showed that the repetition and consistency are very important when modeling natural language and translation", "He proposed to employ cachebased language and translation models in a phrasebased SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 909919 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics adaptation", "Especially the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences", "One problem with the dynamic cache is that those initial sentences in a test document may not benefit from the dynamic cache", "Another problem is that the dynamic cache may be prone to noise and cause error propagation", "This explains why the dynamic cache fails to much improve the performance", "This paper proposes a cachebased approach for documentlevel SMT using a static cache and a dynamic cache", "While such a approach applies to both phrasebased and syntaxbased SMT this paper focuses on phrasebased SMT", "In particular the static cache is employed to store relevant bilingual phrase pairs extracted from similar bilingual document pairs ie source documents similar to the test document and their target counterparts in the training parallel corpus while the dynamic cache is employed to store bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document", "In this way our cachebased approach can provide useful data at the beginning of the translation process via the static cache", "As the translation process continues the dynamic cache grows and contributes more and more to the translation of subsequent sentences", "Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple a human translator often collects similar bilingual document pairs to help translation", "If there are translation pairs of sentencesphraseswords in similar bilingual document pairs this makes the translation much easier", "Given a test document our approach imitates this procedure by first retrieving similar bilingual document pairs from the training parallel corpus which has often been applied in IRbased adaptation of SMT systems Zhao et al2004 Hildebrand et al2005 Lu et al2007 and then extracting bilingual phrase pairs from similar bilingual document pairs to store them in a static cacheHowever such a cachebased approach may in troduce many noisyunnecessary bilingual phrase pairs in both the static and dynamic caches", "In order to resolve this problem this paper employs a topic model to weaken those noisyunnecessary bilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the targetside text of similar bilingual document pairs", "Just like a human translator even with a big bilingual dictionary is often confused when he meets a source phrase which corresponds to several possible translations", "In this case some topic words can help reduce the perplexity", "In this paper the topic words are stored in a topic cache", "In some sense it has the similar effect of employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model", "The rest of this paper is organized as follows", "Section 2 reviews the related work", "Section 3 presents our cachebased approach to document level SMT", "Section 4 presents the experimental results", "Session 5 gives new insights on cache based documentlevel translation", "Finally we conclude this paper in Section 6"]}, "D11-1095": {"title": ["Hierarchical Verb Clustering Using Graph Factorization"], "abstract": ["Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data although many manually built classifications are taxonomic in nature", "Also Natural Language Processing NLP applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification", "We introduce a new clustering method called Hierarchical Graph Factorization Clustering HGFC and extend it so that it is optimal for the task", "Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet and that it yields stateoftheart performance also on a flat test set", "We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification"], "inroduction": ["A variety of verb classifications have been built to support NLP tasks", "These include syntactic and semantic classifications as well as ones which integrate aspects of both Grishman et al 1994 Miller 1995 Baker et al 1998 Palmer et al 2005 Kipper 2005 Hovy et al 2006", "Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness", "One such classification is the taxonomy of English verbs proposed by Levin 1993 which is based on shared morphosyntactic 1023 and semantic properties of verbs", "Levins taxonomy or its extended version in VerbNet Kipper 2005 has proved helpful for various NLP application tasks including eg parsing word sense disambiguation semantic role labeling information extraction questionanswering and machine translation Swier and Stevenson 2004 Dang 2004 Shi and Mihalcea 2005 Zapirain et al 2008", "Because verbs change their meaning and be haviour across domains it is important to be able to tune existing classifications as well to build novel ones in a costeffective manner when required", "In recent years a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose Schulte im Walde 2006 Joanis et al 2008 Sun et al 2008 Li and Brew 2008 Korhonen et al 2008 O Seaghdha and Copestake 2008 Vlachos et al 2009", "The best of such approaches have yielded promising results", "However they have mostly focussed on acquiring and evaluating flat classifications", "Levins classification is not flat but taxonomic in nature which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification", "In this paper we experiment with hierarchical Levinstyle clustering", "We adopt as our baseline method a wellknown hierarchical method  agglomerative clustering AGG  which has been previously used to acquire flat Levinstyle classifications Stevenson and Joanis 2003 as well as hierarchical verb classifications not based on Levin Fer rer 2004 Schulte im Walde 2008", "The method has also been popular in the related task of noun clus Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 10231033 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics tering Ushioda 1996 Matsuo et al 2006 Bassiou and Kotropoulos 2011", "We introduce then a new method called Hierarchical Graph Factorization Clustering HGFC Yu et al 2006", "This graphbased probabilistic clustering algorithm has some clear advantages over AGG eg it delays the decision on a verbs cluster membership at any level until a full graph is available minimising the problem of error propagation and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons Yu et al 2006", "The method has been applied to the identification of social network communities Lin et al 2008 but has not been used to the best of our knowledge in NLP before", "We modify HGFC with a new tree extraction algorithm which ensures a more consistent result and we propose two novel extensions to it", "The first is a method for automatically determining the tree structure ie number of clusters to be produced for each level of the hierarchy", "This avoids the need to predetermine the number of clusters manually", "The second is addition of soft constraints to guide the clustering performance Vlachos et al 2009", "This is useful for situations where a partial eg a flat verb classification is available and the goal is to extend it", "Adopting a set of lexical and syntactic features which have performed well in previous works we compare the performance of the two methods on test sets extracted from Levin and VerbNet", "When evaluated on a flat clustering task HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set Sun and Korhonen 2009", "When evaluated on a hierarchical task HGFC performs considerably better than AGG at all levels of gold standard classification", "The constrained version of HGFC performs the best as expected demonstrating the usefulness of soft constraints for extending partial classifications", "Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards", "The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members classes and levels of hierarchy"]}, "D11-1119": {"title": ["Exploiting Syntactic and Distributional  Information  for"], "abstract": ["We propose a novel way of incorporating dependency parse and word cooccurrence information into a stateoftheart webscale n gram model for spelling correction", "The syntactic and distributional information provides extra evidence in addition to that provided by a webscale ngram corpus and especially helps with data sparsity problems", "Experimental results show that introducing syntactic features into ngram based models significantly reduces errors by up to 124 over the current stateoftheart", "The word cooccurrence information shows potential but only improves overall accuracy slightly"], "inroduction": ["The function of contextsensitive text correction is to identify wordchoice errors in text Bergsma et al 2009", "It can be viewed as a lexical disambiguation task Lapata and Keller 2005 where a system selects from a predefined confusion word set such as affect effect or complement compliment and provides the most appropriate word choice given the context", "Typically one determines if a word has been used correctly based on lexical syntactic and semantic information from the context of the word", "One of the top performing models of spelling correction Bergsma et al 2010 is based on webscale ngram counts which reflect both syntax and meaning", "However even with a largescale ngram corpus data sparsity can hurt performance in two ways", " This work was done when the first author was an intern for Educational Testing Service", "First ngram based methods require exact word and order matches", "If there is a low frequency word in the context such as a persons name there will be little if any evidence in the ngram data to support the usage", "Second if the target confusable word is rare there will not be enough ngram support or training data to render a confident decision", "Because of the data sparsity problem language modeling is not always sufficient to capture the meaning of the sentence and the correct usage of the word", "Take a sentence from The New York Times NYT for example This fellows won a war the dean of the capitals press corps David Broder announced on Meet the Press after complimenting the president on the great sense of authority and command he exhibited in a flight suit Unfortunately neither the phrase complementing the president nor complimenting the president exists in the webscale Google Ngram corpus Brants and Franz 2006", "The ngram models decide solely based on the frequency of the bigrams after compleimenting and compleimenting the which are common usages for both words", "The real question is whether we are more likely to compliment or complement a person the president", "Several clues could help us answer that question", "A dependency parser can identify the word president as the subject of compliment or complement which also may be the case in some of the training data", "Lexical cooccurrence Edmonds 1997 and semantic word relatedness measurements such as Random Indexing Sahlgren 2006 could provide evidence that compliment is more likely to cooccur with president than complement", "Fur 1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing pages 12911300 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics thermore some important clues can be quite distant from the target word eg outside the 9word context window Bergsma et al", "2010 and Carlson 2007 used", "Consider another sentence in the NYT corpus GM says the addition of OnStar which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision complements the Vues top fivestar safety rating for the driver and front passenger in both front and side impact crash tests The dependency parser finds the object of complement is rating which is outside the 9word window", "We propose enhancing stateoftheart webscale ngram models for spelling correction with syntactic structures and distributional information", "For our work we build on a baseline system that combines ngram and lexical features Bergsma et al 2010", "Specifically this paper makes the following contributions 1", "We show that the baseline system can be", "improved by augmenting it with dependency parse features"]}, "D11-1125": {"title": ["Tuning as Ranking"], "abstract": ["We offer a simple effective and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking Herbrich et al 1999", "Unlike the popular MERT algorithm Och 2003 our pairwise ranking optimization PRO method is not limited to a handful of parameters and can easily handle systems with thousands of features", "Moreover unlike recent approaches built upon the MIRA algorithm of Crammer and Singer 2003 Watanabe et al 2007 Chi ang et al 2008b PRO is easy to implement", "It uses offtheshelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours", "We establish PROs scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrasebased and syntaxbased systems in a variety of language pairs using large scale data scenarios"], "inroduction": ["The MERT algorithm Och 2003 is currently the most popular way to tune the parameters of a statistical machine translation MT system", "MERT is wellunderstood easy to implement and runs quickly but can behave erratically and does not scale beyond a handful of features", "This lack of scalability is a significant weakness as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation", "Several researchers have attempted to address this weakness", "Recently Watanabe et al", "2007 and Chiang et al", "2008b have developed tuning methods using the MIRA algorithm Crammer and Singer 2003 as a nucleus", "The MIRA technique of Chiang et al has been shown to perform well on largescale tasks with hundreds or thousands of features 2009", "However the technique is complex and architecturally quite different from MERT", "Tellingly in the entire proceedings of ACL 2010 Hajic et al 2010 only one paper describing a statistical MT system cited the use of MIRA for tuning Chiang 2010 while 15 used MERT1 Here we propose a simpler approach to tuning that scales similarly to highdimensional feature spaces", "We cast tuning as a ranking problem Chen et al 2009 where the explicit goal is to learn to correctly rank candidate translations", "Specifically we follow the pairwise approach to ranking Herbrich et al 1999 Freund et al 2003 Burges et al 2005 Cao et al 2007 in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs", "Of primary concern to us is the ease of adoption of our proposed technique", "Because of this we adhere as closely as possible to the established MERT architecture and use freely available machine learning software", "The end result is a technique that scales and performs just as well as MIRAbased tuning but which can be implemented in a couple of hours by anyone with an existing MERT implementation", "Mindful that many wouldbe enhancements to the 1 The remainder either did not specify their tuning method", "though a number of these used the Moses toolkit Koehn et al 2007 which uses MERT for tuning or in one case set weights by hand", "stateoftheart are false positives that only show improvement in a narrowly defined setting or with limited data we validate our claims on both syntax and phrasebased systems using multiple language pairs and large data sets", "We describe tuning in abstract and somewhat formal terms in Section 2 describe the MERT algorithm in the context of those terms and illustrate its scalability issues via a synthetic experiment in Section 3 introduce our pairwise ranking optimization method in Section 4 present numerous largescale MT experiments to validate our claims in Section 5 discuss some related work in Section 6 and conclude in Section 7"]}, "D12-1003": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 24 36 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation Akihiro Tamura and Taro Watanabe and Eiichiro Sumita Multilingual Translation Laboratory MASTAR Project National Institute of Information and Communications Technology 35 Hikaridai Keihanna Science City Kyoto 6190289 JAPAN akihirotamuratarowatanabeeiichirosumitanictgojp Abstract This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation In previous work it was established that performance drastically decreases when the coverage of a seed lexicon is small We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations in which each word is represented by a distribution of translated seeds The seed distributions are propagated over a graph representing relations among words and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions We propose two types of the graphs a cooccurrence graph representing cooccurrence relations between words and a similarity graph representing context similarities between words Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods Further the similarity graph achieved improved performance by clustering synonyms into the same translation 1 Introduction Bilingual lexicons are important resources for bilingual tasks such as machine translation MT and crosslanguage information retrieval CLIR Therefore the automatic building of bilingual lexicons from corpora is one of the issues that have attracted many researchers As a solution a number of previous works proposed extracting bilingual lexicons from comparable corpora in which documents were not direct translations but shared a topic or domain1 The use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains Most of the previous methods are based on assumption I that a word and its translation tend to appear in similar contexts across languages Rapp 1999 Based on this assumption many methods calculate word similarity using context and then ex tract word translation pairs with a highcontext sim ilarity We call these methods context similaritybased methods The context similarities are usually computed using a seed bilingual lexicon eg a general bilingual dictionary by mapping contexts expressed in two different languages into the same space In the mapping information not represented by the seed lexicon is discarded Therefore the contextsimilaritybased methods could not find accurate translation pairs if using a small seed lexicon Some of the previous methods tried to alleviate the problem of the limited seed lexicon size Koehn and Knight 2002 Morin and Prochasson 2011 Hazem et al2011 while others did not require any seed lexicon Rapp 1995 Fung 1995 Haghighi et al 2008 Ismail and Manandhar 2010 Daume III and Jagarlamudi 2011 However they suffer the problems of high computational cost Rapp 1995 sensitivity to parameters Hazem et al2011 low accuracy Fung 1995 Ismail and Manandhar 2010 and ineffectiveness for language pairs with 1Although Vulic et al2011 regarded document aligned texts such as texts on Wikipedia as comparable corpora we do not limit comparable corpora to these kinds of texts 24 different types of characters Koehn and Knight 2002 Haghighi et al2008 Daume III and Jagarlamudi 2011 In face of the above problems we propose a novel method that uses a graphbased label propagation technique Zhu and Ghahramani 2002 The proposed method is based on assumption II which is derived by recursively applying assumption I to the contexts  a word and its translation tend to have similar co occurrence direct and indirect relations with all bilingual seeds across languages Based on assumption II we propose a threestep approach 1 constructing a graph for each language with each edge indicating a direct cooccurrence relation 2 representing every word as a seed translation distribution by iteratively propagating translated seeds in each graph 3 finding two words in different languages with a high similarity with respect to the seed distributions By propagating all the seeds on the graph indirect co occurrence relations are also considered when computing bilingual relations which have been neglected in previous methods In addition to the co occurrencebased graph construction we propose a similarity graph which also takes into account context similarities be tween words"], "abstract": ["This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graph based label propagation", "In previous work it was established that performance drastically decreases when the coverage of a seed lexicon is small", "We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations in which each word is represented by a distribution of translated seeds", "The seed distributions are propagated over a graph representing relations among words and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions", "We propose two types of the graphs a cooccurrence graph representing cooccurrence relations between words and a similarity graph representing context similarities between words", "Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods", "Further the similarity graph achieved improved performance by clustering synonyms into the same translation"], "inroduction": ["Bilingual lexicons are important resources for bilingual tasks such as machine translation MT and crosslanguage information retrieval CLIR", "Therefore the automatic building of bilingual lexicons from corpora is one of the issues that have attracted many researchers", "As a solution a number of previous works proposed extracting bilingual lexicons from comparable corpora in which documents were not direct translations but shared a topic or domain1", "The use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains", "Most of the previous methods are based on assumption I that a word and its translation tend to appear in similar contexts across languages Rapp 1999", "Based on this assumption many methods calculate word similarity using context and then extract word translation pairs with a highcontext similarity", "We call these methods contextsimilarity based methods", "The context similarities are usually computed using a seed bilingual lexicon eg a general bilingual dictionary by mapping contexts expressed in two different languages into the same space", "In the mapping information not represented by the seed lexicon is discarded", "Therefore the contextsimilaritybased methods could not find accurate translation pairs if using a small seed lexicon", "Some of the previous methods tried to alleviate the problem of the limited seed lexicon size Koehn and Knight 2002 Morin and Prochasson 2011 Hazem et al 2011 while others did not require any seed lexicon Rapp 1995 Fung 1995 Haghighi et al 2008 Ismail and Manandhar 2010 Daume III and Jagarlamudi 2011", "However they suffer the problems of high computational cost Rapp 1995 sensitivity to parameters Hazem et al 2011 low accuracy Fung 1995 Ismail and Manandhar 2010 and ineffectiveness for language pairs with 1 Although Vulic et al", "2011 regarded documentaligned texts such as texts on Wikipedia as comparable corpora we do not limit comparable corpora to these kinds of texts", "24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 2436 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics different types of characters Koehn and Knight 2002 Haghighi et al 2008 Daume III and Jagarlamudi 2011", "In face of the above problems we propose a novel method that uses a graphbased label propagation technique Zhu and Ghahramani 2002", "The proposed method is based on assumption II which is derived by recursively applying assumption I to the contexts a word and its translation tend to have similar cooccurrence direct and indirect relations with all bilingual seeds across languages", "Based on assumption II we propose a three step approach 1 constructing a graph for each language with each edge indicating a direct co occurrence relation 2 representing every word as a seed translation distribution by iteratively propagating translated seeds in each graph 3 finding two words in different languages with a high similarity with respect to the seed distributions", "By propagating all the seeds on the graph indirect cooccurrence relations are also considered when computing bilingual relations which have been neglected in previous methods", "In addition to the cooccurrencebased graph construction we propose a similarity graph which also takes into account context similarities between words", "The main contributions of this paper are as follows  We propose a bilingual lexicon extraction method that captures cooccurrence relations with all the seeds including indirect relations using graphbased label propagation", "In our experiments we confirm that the proposed method outperforms conventional contextsimilaritybased methods Rapp 1999 Andrade et al 2010 and works well even if the coverage of a seed lexicon is low", " We propose a similarity graph which represents context similarities between words", "In our experiments we confirm that a similarity graph is more effective than a cooccurrencebased graph"]}, "D12-1016": {"title": ["Aligning  Predicates across Monolingual Comparable Texts"], "abstract": ["Generating coherent discourse is an important aspect in natural language generation", "Our aim is to learn factors that constitute coherent discourse from data with a focus on how to realize predicateargument structures in a model that exceeds the sentence level", "We present an important subtask for this overall goal in which we align predicates across comparable texts admitting partial argument structure correspondence", "The contribution of this work is twofold We first construct a large corpus resource of comparable texts including an evaluation set with manual predicate alignments", "Secondly we present a novel approach for aligning predicates across comparable texts using graphbased clustering with Mincuts", "Our method significantly outperforms other alignment techniques when applied to this novel alignment task by a margin of at least 65 percentage points in F1 score"], "inroduction": ["Discourse coherence is an important aspect in natural language generation NLG applications", "A number of theories have investigated coherence inducing factors", "A prominent example is Centering Theory Grosz et al 1995 which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse", "A datadriven model based on this theory is the entitybased approach by Barzilay and Lap ata 2008 which models coherence phenomena by observing sentencetosentence transitions of entity occurrences", "Barzilay and Lapata show that their approach can discriminate between a coherent and a noncoherent set of ordered sentences", "However their model is not able to generate alternative entity realizations by itself", "Furthermore the entitybased approach only investigates realization patterns for individual entities in discourse in terms of core grammatical functions", "It does not investigate the interplay between entity transitions and realization patterns for full fledged semantic structures", "This interplay however is an important factor for a semanticsbased generative model of discourse coherence", "The main hypothesis of our work is that we can automatically learn contextspecific realization patterns for predicate argument structures PAS from a semantically parsed corpus of comparable text pairs", "Our assumption builds on the success of previous research where comparable and parallel texts have been exploited for a range of related learning tasks eg unsupervised discourse segmentation Barzilay and Lee 2004 and bootstrapping semantic analyzers Titov and Kozhevnikov 2010", "For our purposes we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events and hence involve the same set of underlying event participants", "By aligning predicates in such texts we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments", "These include the specific forms of argument realization as a pronoun or a specific type of referential expression as studied in prior work in NLG Belz et al 2009 inter alia", "The specific setup we examine however allows us to further investi 171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 171182 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics gate the factors that govern the nonrealization of an argument position as a special form of coherence inducing element in discourse", "Example 1 extracted from our corpus of aligned textsillustrates this point Both texts report on the same event of locating victims in an avalanche", "While 1a explicitly talks about the location of this event the role remains implicit in the second sentence of 1b given that it can be recovered from the preceding sentence", "In fact realization of this argument role would impede the fluency of discourse by being overly repetitive", "1 a   ", "The official said that no bodiesArg1 had been recovered from the avalanchesArg2 which occurred late Friday in the Central Asian country near the Afghan border some 300 kilometers 185 miles southeast of the capital Dushanbe", "b Three other victims were trapped in an avalanche in the village of Khichikh", "None of the victims bodiesArg1 have been found  Argmloc  This phenomenon clearly relates to the problem of discourselinking of implicit roles a very challenging task in discourse processing1 In our work we consider this problem from a contentbased generation perspective concentrating on the discourse factors that allow for the omission of a role", "Thus our aim is to identify comparable predications across aligned texts and to study the discourse coherence factors that determine the realization patterns of arguments in the respective discourses", "This can be achieved by considering the full set of arguments that can be recovered from the aligned predications", "This paper focuses on the first of these tasks henceforth called predicate alignment2 In line with datadriven approaches in NLP we automatically align predicates in a suitable corpus of paired texts", "The induced alignments will i serve to identify events described in both comparable texts and ii provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse", "We investigate a graphbased clustering method for induc 1 See the recent SemEval 2010 task Linking Events and their Participants in Discourse Ruppenhofer et al 2010"]}, "D12-1046": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 501 511 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Joint Chinese Word Segmentation POS Tagging and Parsing Xian Qian Yang Liu Computer Science Department The University of Texas at Dallas qxyanglhltutdallasedu Abstract In this paper we propose a novel decoding algorithm for discriminative joint Chinese word segmentation partofspeech POS tagging and parsing Previous work often used a pipeline method  Chinese word segmentation followed by POS tagging and parsing which suffers from error propagation and is unable to leverage information in later modules for earlier components In our approach we train the three individual models separately during training and incorporate them together in a unified framework during decoding We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features As far as we know this is the first work on joint Chinese word segmentation POS tagging and parsing Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the stateoftheart pipeline system 1 Introduction For Asian languages such as Japanese and Chi nese that do not contain explicitly marked word boundaries word segmentation is an important first step for many subsequent language processing tasks such as POS tagging parsing semantic role labeling and various applications Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided which is not the real scenario In a fully automatic system a pipeline approach is often adopted where raw sentences are first segmented into word sequences then POS tagging and parsing are performed This kind of approach suffers from error propagation For example word segmentation errors will result in tagging and parsing errors Additionally early modules cannot use information from subsequent modules Intuitively a joint model that performs the three tasks together should help the system make the best decisions In this paper we propose a unified model for joint Chinese word segmentation POS tagging and parsing Three submodels are independently trained using the stateoftheart methods We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters We use linear chain Conditional Random Fields CRFs Lafferty et al2001 to train the word segmentation model and POS tagging model and averaged perceptron Collins 2002 to learn the parsing model During decoding parameters of each submodel are scaled to represent its importance in the joint model Our decoding algorithm is an extension of CYK parsing Initially weights of all possible words together with their POS tags are calculated When searching the parse tree the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation Experiments are conducted on Chinese Tree Bank CTB 5 dataset which is widely used for Chinese word segmentation POS tagging and parsing We compare our proposed joint model with the pipeline system both built using the stateoftheart submodels We also propose an evaluation metric to 501 calculate the bracket scores for parsing in the face of word segmentation errors Our experimental results show that the joint model significantly outperforms the pipeline method based on the stateoftheart submodels 2 Related Work There is very limited previous work on joint Chinese word segmentation POS tagging and parsing Previous joint models mainly focus on word segmentation and POS tagging task such as the virtual nodes method Qian et al2010 cascaded linear model Jiang et al2008a perceptron Zhang and Clark 2008 subword based stacked learning Sun 2011 reranking Jiang et al2008b These joint models showed about 02  1 Fscore improvement over the pipeline method Recently joint tagging and de pendency parsing has been studied as well Li et al 2011 Lee et al2011 Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method Harper and Huang 2009 In Jiang et al2009 additional data was used to improve Chinese word segmentation which resulted in significant improvement on the parsing task using the pipeline framework Joint segmentation and parsing was also investigated for Arabic Green and Manning 2010 A study that is closely related to ours is Goldberg and Tsarfaty 2008 where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew Different from that work we use a discriminative model which benefits from large amounts of features and is easier to deal with unknown words Another main difference is that besides segmentation and parsing we also incorporate the POS tagging model into the CYK parsing framework 3 Methods For a given Chinese sentence our task is to generate the word sequence its POS tag sequence and the parse tree constituent parsing A joint model is expected to make more optimal decisions than a pipeline approach however such a model will be too complex and it is difficult to estimate model parameters Therefore we do not perform joint inference for training Instead we develop three individual models independently during training and perform joint decoding using them In this section we first describe the three submodels and then the joint decoding algorithm"], "abstract": ["In this paper we propose a novel decoding algorithm for discriminative joint Chinese word segmentation partofspeech POS tagging and parsing", "Previous work often used a pipeline method  Chinese word segmentation followed by POS tagging and parsing which suffers from error propagation and is unable to leverage information in later modules for earlier components", "In our approach we train the three individual models separately during training and incorporate them together in a unified framework during decoding", "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features", "As far as we know this is the first work on joint Chinese word segmentation POS tagging and parsing", "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the stateoftheart pipeline system"], "inroduction": ["For Asian languages such as Japanese and Chinese that do not contain explicitly marked word boundaries word segmentation is an important first step for many subsequent language processing tasks such as POS tagging parsing semantic role labeling and various applications", "Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided which is not the real scenario", "In a fully automatic system a pipeline approach is often adopted where raw sentences are first segmented into word sequences then POS tagging and parsing are performed", "This kind of approach suffers from error propagation", "For example word segmentation errors will result in tagging and parsing errors", "Additionally early modules cannot use information from subsequent modules", "Intuitively a joint model that performs the three tasks together should help the system make the best decisions", "In this paper we propose a unified model for joint Chinese word segmentation POS tagging and parsing", "Three submodels are independently trained using the stateoftheart methods", "We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters", "We use linear chain Conditional Random Fields CRFs Lafferty et al 2001 to train the word segmentation model and POS tagging model and averaged perceptron Collins 2002 to learn the parsing model", "During decoding parameters of each submodel are scaled to represent its importance in the joint model", "Our decoding algorithm is an extension of CYK parsing", "Initially weights of all possible words together with their POS tags are calculated", "When searching the parse tree the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation", "Experiments are conducted on Chinese Tree Bank CTB 5 dataset which is widely used for Chinese word segmentation POS tagging and parsing", "We compare our proposed joint model with the pipeline system both built using the stateoftheart sub models", "We also propose an evaluation metric to 501 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 501511 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics word segmentation errors", "Our experimental results show that the joint model significantly outperforms the pipeline method based on the stateoftheart submodels"]}, "D12-1074": {"title": ["Improving NLP through Marginalization of Hidden Syntactic Structure"], "abstract": ["Many NLP tasks make predictions that are inherently coupled to syntactic relations but for many languages the resources required to provide such syntactic annotations are unavailable", "For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models and what structures in the syntactic trees are most relevant to the current task", "We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task", "Our method couples latent syntactic representations constrained to form valid dependency graphs or constituency parses with the prediction task via specialized factors in a Markov random field", "At both training and test time we marginalize over this hidden structure learning the optimal latent representations for the problem", "Results show that this approach provides significant gains over a syntactically uninformed baseline outperforming models that observe syntax on an English relation extraction task and performing comparably to them in semantic role labeling"], "inroduction": ["Many NLP tasks are inherently tied to syntax and stateoftheart solutions to these tasks often rely on syntactic annotations as either a source for useful features Zhang et al 2006 path features in relation extraction or as a scaffolding upon which a more narrow specialized classification can occur as often done in semantic role labeling", "This decou pling of the end task from its intermediate representation is sometimes known as the twostage approach Chang et al 2010 and comes with several drawbacks", "Most notably this decomposition prohibits the learning method from utilizing the labels from the end task when predicting the intermediate representation a structure which must have some correlation to the end task to provide any benefit", "Relying on intermediate representations that are specifically syntactic in nature introduces its own unique set of problems", "Large amounts of syntactically annotated data is difficult to obtain costly to produce and often tied to a particular domain that may vary greatly from that of the desired end task", "Additionally current systems often utilize only a small amount of the annotation for any particular task", "For instance performing named entity recognition NER jointly with constituent parsing has been shown to improve performance on both tasks but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases Finkel and Manning 2009", "By instead discovering a latent representation jointly with the end task we address all of these concerns alleviating the need for any syntactic annotations while simultaneously attempting to learn a latent syntax relevant to both the particular domain and structure of the end task", "We phrase the joint model as factor graph and marginalize over the hidden structure of the intermediate representation at both training and test time to optimize performance on the end task", "Inference is done via loopy belief propagation making this framework trivially extensible to most graph structures", "Computation over latent syntactic rep 810 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 810820 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics resentations is made tractable with the use of special combinatorial factors which implement unlabeled variants of common dynamicprogramming parsing algorithms constraining the hidden representation to realize valid dependency graphs or constituency trees", "We apply this strategy to two common NLP tasks coupling a model for the end task prediction with latent and general syntactic representations via specialized logical factors which learn associations between latent and observed structure", "In comparisons with identical models which observe gold syntactic annotations derived from offtheshelf parsers or provided with the corpora we find that our hidden marginalization method is comparable in both tasks and almost every language tested sometimes significantly outperforming models which observe the true syntax", "The following sections serves as a preliminary introducing an inventory of factors and variables for constructing factor graph representations of syntacticallycoupled NLP tasks", "Section 3 explores the benefits of this method on relation extraction RE where we compare the use dependency and constituency structure as latent representations", "We then turn to a more established semantic role label ing SRL task 4 where we evaluate across a wide range of languages"]}, "D12-1086": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 940 951 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Learning Syntactic Categories Using Paradigmatic Representations of Word Context Mehmet Ali Yatbaz Enis Sert Deniz Yuret Artificial Intelligence Laboratory Koc University I stanbul Turkey myatbazesertdyuretkuedutr Abstract We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy Our best model based on Euclidean cooccurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80 manytoone accuracy on a 45tag 1M word corpus 1 Introduction Grammar rules apply not to individual words eg dog eat but to syntactic categories of words eg noun verb Thus constructing syntactic categories also known as lexical or partofspeech categories is one of the fundamental problems in language acquisition Syntactic categories represent groups of words that can be substituted for one another without altering the grammaticality of a sentence Linguists identify syntactic categories based on semantic syntactic and morphological properties of words There is also evidence that children use prosodic and phonological features to bootstrap syntactic category acquisition Ambridge and Lieven 2011 However there is as yet no satisfactory computational model that can match human performance Thus identifying the best set of features and best learning algorithms for syntactic category acquisition is still an open problem Relationships between linguistic units can be classified into two types syntagmatic concerning positioning and paradigmatic concerning substitution Syntagmatic relations determine which units can combine to create larger groups and paradigmatic relations determine which units can be substituted for one another Figure 1 illustrates the paradigmatic vs syntagmatic axes for words in a simple sentence and their possible substitutes In this study we represent the paradigmatic axis directly by building substitute vectors for each word position in the text The dimensions of a substitute vector represent words in the vocabulary and the magnitudes represent the probability of occurrence in the given position Note that the substitute vector for a word position eg the second word in Fig 1 is a function of the context only ie the cried and does not depend on the word that does actually appear there ie man Thus substiFigure 1 Syntagmatic vs paradigmatic axes for words in a simple sentence Chandler 2007 940 tute vectors represent individual word contexts not word types We refer to the use of features based on substitute vectors as paradigmatic representations of word context Our preliminary experiments indicated that using context information alone without the identity or the features of the target word eg using dimensionality reduction and clustering on substitute vectors has limited success and modeling the co occurrence of word and context types is essential for inducing syntactic categories In the models presented in this paper we combine paradigmatic representations of word context with features of co occurring words within the co occurrence data embedding CODE framework Globerson et al 2007 Maron et al 2010 The resulting embeddings for word types are split into 45 clusters using kmeans and the clusters are compared to the 45 gold tags in the 1M word Penn Treebank Wall Street Journal corpus Marcus et al 1999 We obtain manytoone accuracies up to 7680 using only distributional information the identity of the word and a representation of its context and 8023 using morphological and orthographic features of words improving the stateoftheart in unsupervised partofspeech tagging performance The high probability substitutes reflect both semantic and syntactic properties of the context as seen in the example below the numbers in parentheses give substitute probabilities Pierre Vinken 61 years old will join the board as a nonexecutive director Nov 29 the its 9011 the 0981 a 0006    board board 4288 company 2584 firm 2024 bank 0731    Top substitutes for the word  the consist of words that can act as determiners Top substitutes for  board are not only nouns but specifically nouns compatible with the semantic context"], "abstract": ["We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition", "Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words", "We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy", "Our best model based on Euclidean cooccurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80 manytoone accuracy on a 45tag 1M word corpus"], "inroduction": ["Grammar rules apply not to individual words eg dog eat but to syntactic categories of words eg noun verb", "Thus constructing syntactic categories also known as lexical or partofspeech categories is one of the fundamental problems in language acquisition", "Syntactic categories represent groups of words that can be substituted for one another without altering the grammaticality of a sentence", "Linguists identify syntactic categories based on semantic syntactic and morphological properties of words", "There is also evidence that children use prosodic and phonological features to bootstrap syntactic category acquisition Ambridge and Lieven 2011", "However there is as yet no satisfactory computational model that can match human performance", "Thus identify ing the best set of features and best learning algorithms for syntactic category acquisition is still an open problem", "Relationships between linguistic units can be classified into two types syntagmatic concerning positioning and paradigmatic concerning substitution", "Syntagmatic relations determine which units can combine to create larger groups and paradigmatic relations determine which units can be substituted for one another", "Figure 1 illustrates the paradigmatic vs syntagmatic axes for words in a simple sentence and their possible substitutes", "In this study we represent the paradigmatic axis directly by building substitute vectors for each word position in the text", "The dimensions of a substitute vector represent words in the vocabulary and the magnitudes represent the probability of occurrence in the given position", "Note that the substitute vector for a word position eg the second word in Fig", "1 is a function of the context only ie the cried and does not depend on the word that does actually appear there ie man", "Thus substi Figure 1 Syntagmatic vs paradigmatic axes for words in a simple sentence Chandler 2007", "940 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 940951 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics tute vectors represent individual word contexts not word types", "We refer to the use of features based on substitute vectors as paradigmatic representations of word context", "Our preliminary experiments indicated that using context information alone without the identity or the features of the target word eg using dimension ality reduction and clustering on substitute vectors has limited success and modeling the cooccurrence of word and context types is essential for inducing syntactic categories", "In the models presented in this paper we combine paradigmatic representations of word context with features of cooccurring words within the cooccurrence data embedding CODE framework Globerson et al 2007 Maron et al 2010", "The resulting embeddings for word types are split into 45 clusters using kmeans and the clusters are compared to the 45 gold tags in the 1M word Penn Treebank Wall Street Journal corpus Marcus et al 1999", "We obtain manytoone accuracies up to 7680 using only distributional information the identity of the word and a representation of its context and 8023 using morphological and orthographic features of words improving the stateof theart in unsupervised partofspeech tagging performance", "The high probability substitutes reflect both semantic and syntactic properties of the context as seen in the example below the numbers in parentheses give substitute probabilities Pierre Vinken 61 years old will join the board as a nonexecutive director Nov 29 the its 9011 the 0981 a 0006   ", "board board 4288 company 2584 firm 2024 bank 0731   ", "Top substitutes for the word the consist of words that can act as determiners", "Top substitutes for board are not only nouns but specifically nouns compatible with the semantic context", "This example illustrates two concerns inherent in all distributional methods i words that are generally substitutable like the and its are placed in separate categories DT and PRP by the gold standard ii words that are generally not substitutable like do and put are placed in the same category VB", "Freudenthal et al", "2005 point out that categories with unsubstitutable words fail the standard linguistic definition of a syntactic category and children do not seem to make errors of substituting such words in utterances eg What do you want vs What put you want", "Whether gold standard partofspeech tags or distributional categories are better suited to applications like parsing or machine translation can be best decided using extrinsic evaluation", "However in this study we follow previous work and evaluate our results by comparing them to gold standard partofspeech tags", "Section 2 gives a detailed review of related work", "Section 3 describes the dataset and the construction of the substitute vectors", "Section 4 describes co occurrence data embedding the learning algorithm used in our experiments", "Section 5 describes our experiments and compares our results with previous work", "Section 6 gives a brief error analysis and Section 7 summarizes our contributions", "All the data and the code to replicate the results given in this paper is available from the authors website at httpgooglRoqEh"]}, "D12-1094": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 1027 1037 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Ensemble Semantics for Largescale Unsupervised Relation Extraction"], "abstract": ["Discovering significant types of relations from the web is challenging because of its open nature", "Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance but most of them rely on tagging arguments of predefined types", "Recently a new algorithm was proposed to jointly extract relations and their argument semantic classes taking a set of relation instances extracted by an open IE algorithm as input", "However it cannot handle polysemy of relation phrases and fails to group many similar synonymous relation instances because of the sparseness of features", "In this paper we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems", "The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction", "Moreover it explicitly disambiguates polysemous relation phrases and groups synonymous ones", "While maintaining approximately the same precision the algorithm achieves significant improvement on recall compared to the previous method", "It is also very efficient", "Experiments on a real world dataset show that it can handle 147 million relation instances and extract a very large set of relations from the web"], "inroduction": ["Relation extraction aims at discovering semantic relations between entities", "It is an important task that has many applications in answering factoid questions building knowledge bases and improving search engine relevance", "The web has become a massive potential source of such relations", "However its open nature brings an openended set of relation types", "To extract these relations a system should not assume a fixed set of relation types nor rely on a fixed set of relation argument types", "The past decade has seen some promising solutions unsupervised relation extraction URE algorithms that extract relations from a corpus without knowing the relations in advance", "However most algorithms Hasegawa et al 2004 Shinyama and Sekine 2006 Chen et", "al 2005 rely on tagging predefined types of entities as relation arguments and thus are not wellsuited for the open domain", "Recently Kok and Domingos 2008 proposed Semantic Network Extractor SNE which generates argument semantic classes and sets of synonymous relation phrases at the same time thus avoiding the requirement of tagging relation argu ments of predefined types", "However SNE has 2 limitations 1 Following previous URE algorithms it only uses features from the set of input relation instances for clustering", "Empirically we found that it fails to group many relevant relation instances", "These features such as the surface forms of arguments and lexical sequences in between are very sparse in practice", "In contrast there exist several wellknown corpuslevel semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation its 2 argument semantic classes and a set of synonymous phrases", "For example semantic classes can be derived from a source corpus with contextual distributional similarity and web table cooccurrences", "The synony my 1 problem for clustering relation instances  Work done during an internship at Microsoft Research Asia 1027 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 10271037 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics could potentially be better solved by adding these resources", "2 SNE assumes that each entity or relation phrase belongs to exactly one cluster thus is not able to effectively handle polysemy of relation phrases2", "An example of a polysemous phrase is be the currency of as in 2 triples Euro be the currency of Germany and authorship be the currency of science", "As the target corpus expands from mostly news to the open web polysemy becomes more important as input covers a wider range of domains", "In practice around 22 section 3 of relation phrases are polysemous", "Failure to handle these cases significantly limits its effectiveness", "To move towards a more general treatment of the polysemy and synonymy problems we present a novel algorithm WEBRE for opendomain large scale unsupervised relation extraction without predefined relation or argument types", "The contribu tions are  WEBRE incorporates a wide range of corpuslevel semantic resources for improving relation extraction", "The effectiveness of each knowledge source and their combination are studied and compared", "To the best of our knowledge it is the first to combine and compare them for unsupervised relation extraction", " WEBRE explicitly disambiguates polysemous relation phrases and groups synonymous phrases thus fundamentally it avoids the limitation of previous methods", " Experiments on the Clueweb09 dataset lemurprojectorgclueweb09php show that WEBRE is effective and efficient", "We present a largescale evaluation and show that WEBRE can extract a very large set of highquality relations", "Compared to the closest prior work WEBRE significantly improves recall while maintaining the same level of precision", "WEBRE is efficient", "To the best of our knowledge it handles the largest triple set to date 7fold larger than largest previous effort", "Taking 147 million triples as input a complete run with one CPU core takes about a day", "1 We use the term synonymy broadly as defined in Section 3", "the phrase cluster for 2 different relations in SNE", "However this only accounts for 48 of the polysemous cases"]}, "D12-1125": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 1368 1378 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Learning to Map into a Universal POS Tagset Yuan Zhang Roi Reichart Regina Barzilay Massachusetts Institute of Technology yuanzh roiri reginacsailmitedu Amir Globerson The Hebrew University gamircshujiacil Abstract We present an automatic method for mapping languagespecific partofspeech tags to a set of universal tags This unified representation plays a crucial role in crosslingual syntactic transfer of multilingual dependency parsers Until now however such conversion schemes have been created manually Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping Given the ex ponential size of the mapping space we propose a novel method for optimizing over soft mappings and use entropy regularization to drive those towards hard mappings Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing1 1 Introduction In this paper we explore an automatic method for mapping languagespecific partofspeech tags to a universal tagset In multilingual parsing this unified input representation is required for crosslingual syntactic transfer Specifically the universal tagset annotations enable an unlexicalized parser to capitalize on annotations from one language when learning a model for another 1The source code and data for the work presented in this paper is available at httpgroupscsailmitedu rbgcodeunitagemnlp2012 While the notion of a universal POS tagset is widely accepted in practice it is hardly ever used for annotation of monolingual resources In fact available POS annotations are designed to capture languagespecific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset To reconcile these crosslingual annotation differences a number of mapping schemes have been proposed in the parsing community Zeman and Resnik 2008 Petrov et al 2011 Naseem et al 2010 In all of these cases the conversion is performed manually and has to be repeated for each language and annotation scheme anew Despite the apparent simplicity deriving a mapping is by no means easy even for humans In fact the universal tagsets manually induced by Petrov et al2011 and by Naseem et al2010 disagree on 10 of the tags An example of such discrepancy is the mapping of the Japanese tag PVfin to the universal tag  particle according to one scheme and to  verb according to another Moreover the quality of this conversion has a direct implication on the parsing performance In the Japanese example above this difference in mapping yields a 67 difference in parsing accuracy The goal of our work is to induce the mapping for a new language utilizing existing manuallyconstructed mappings as training data The existing mappings developed in the parsing community rely on gold POS tags for the target language A more realistic scenario is to employ the mapping technique to resourcepoor languages where gold POS annotations are lacking In such cases a mapping algorithm has to operate over automatically in1368 duced clusters on the target language eg using the Brown algorithm and convert them to universal tags We are interested in a mapping approach that can effectively handle both gold tags and induced clusters Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across languages Since universal tags play the same linguistic role in source and target languages we expect similarity in their global distributional statistics Figure 1a shows statistics for two close languages English and German We can see that their unigram frequencies on the five most common tags are very close Other properties concern POS tag per sentence statistics  eg every sentence has to have at least one verb Finally the mappings can be further constrained by typological properties of the target language that specify likely tag sequences This information is readily available even for resource poor language Haspelmath et al 2005 For instance since English and German are prepositional languages we expect to observe adpositionnoun sequences but not the reverse see Figure 1b for sample sentences We encode these heterogeneous properties into an objective function that guides the search for the optimal mapping Having defined a quality measure for mappings our goal is to find the optimal mapping However such partition optimization problems2 are NP hard Garey and Johnson 1979 A naive approach to the problem is to greedily improve the map but it turns out that this approach yields poor quality mappings We therefore develop a method for optimizing over soft mappings and use entropy regularization to drive those towards hard mappings We con struct the objective in a way that facilitates simple monotonically improving updates corresponding to solving convex optimization problems We evaluate our mapping approach on 19 languages that include representatives of IndoEuropean Semitic Basque Japonic and Turkic families We measure mapping quality based on the target language parsing accuracy In addition to considering gold POS tags for the target language 2Instances of related hard problems are 3partition and subsetsum we also evaluate the mapping algorithm on automatically induced POS tags In all evaluation scenarios our model consistently rivals the quality of manually induced mappings We also demonstrate that the proposed inference procedure outperforms greedy methods by a large margin highlighting the importance of good optimization techniques We further show that while all characteristics of the mapping contribute to the objective our largest gain comes from distributional features that capture global statistics Finally we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer which motivates further study of this topic 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic trans fer In this setup finding the mapping between various POS annotation schemes was not essential instead the transfer algorithm could induce it directly from the parallel data Hwa et al 2005 Xi and Hwa 2005 Burkett and Klein 2008 However more recent transfer approaches relinquish this data requirement learning to transfer from nonparallel data Zeman and Resnik 2008 McDonald et al 2011 Cohen et al 2011 Naseem et al 2010 These approaches assume access to a common input representation in the form of universal tags which enables the model to connect patterns observed in the source language to their counterparts in the target language Despite ongoing efforts to standardize POS tags across languages eg EAGLES initiative Eynde 2004 many corpora are still annotated with languagespecific tags In previous work their mapping to universal tags was performed manually Yet even though some of these mappings have been de veloped for the same CoNLL dataset Buchholz and Marsi 2006 Nivre et al 2007 they are not identical and yield different parsing performance Zeman and Resnik 2008 Petrov et al 2011 Naseem et al 2010 The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks here we focus on parsing As our results show we achieve this goal 1369 Noun Verb Det Prep Adj0"], "abstract": ["We present an automatic method for mapping languagespecific partofspeech tags to a set of universal tags", "This unified representation plays a crucial role in crosslingual syntactic transfer of multilingual dependency parsers", "Until now however such conversion schemes have been created manually", "Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages", "We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping", "Given the exponential size of the mapping space we propose a novel method for optimizing over soft mappings and use entropy regularization to drive those towards hard mappings", "Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing1"], "inroduction": ["In this paper we explore an automatic method for mapping languagespecific partofspeech tags to a universal tagset", "In multilingual parsing this unified input representation is required for crosslingual syntactic transfer", "Specifically the universal tagset annotations enable an unlexicalized parser to capitalize on annotations from one language when learning a model for another", "1 The source code and data for the work presented in this paper is available at httpgroupscsailmitedu rbgcodeunitagemnlp2012 While the notion of a universal POS tagset is widely accepted in practice it is hardly ever used for annotation of monolingual resources", "In fact available POS annotations are designed to capture languagespecific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset", "To reconcile these crosslingual annotation differences a number of mapping schemes have been proposed in the parsing community Zeman and Resnik 2008 Petrov et al 2011 Naseem et al 2010", "In all of these cases the conversion is performed manually and has to be repeated for each language and annotation scheme anew", "Despite the apparent simplicity deriving a mapping is by no means easy even for humans", "In fact the universal tagsets manually induced by Petrov et al", "2011 and by Naseem et al", "2010 disagree on 10 of the tags", "An example of such discrepancy is the mapping of the Japanese tag PVfin to the universal tag particle according to one scheme and to verb according to another", "Moreover the quality of this conversion has a direct implication on the parsing performance", "In the Japanese example above this difference in mapping yields a 67 difference in parsing accuracy", "The goal of our work is to induce the mapping for a new language utilizing existing manually constructed mappings as training data", "The existing mappings developed in the parsing community rely on gold POS tags for the target language", "A more realistic scenario is to employ the mapping technique to resourcepoor languages where gold POS annotations are lacking", "In such cases a mapping algorithm has to operate over automatically in 1368 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 13681378 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics duced clusters on the target language eg using the Brown algorithm and convert them to universal tags", "We are interested in a mapping approach that can effectively handle both gold tags and induced clusters", "Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across languages", "Since universal tags play the same linguistic role in source and target languages we expect similarity in their global distributional statistics", "Figure 1a shows statistics for two close languages English and German", "We can see that their unigram frequencies on the five most common tags are very close", "Other properties concern POS tag per sentence statistics  eg every sentence has to have at least one verb", "Finally the mappings can be further constrained by typological properties of the target language that specify likely tag sequences", "This information is readily available even for resource poor language Haspel math et al 2005", "For instance since English and German are prepositional languages we expect to observe adpositionnoun sequences but not the reverse see Figure 1b for sample sentences", "We encode these heterogeneous properties into an objective function that guides the search for the optimal mapping", "Having defined a quality measure for mappings our goal is to find the optimal mapping", "However such partition optimization problems2 are NP hard Garey and Johnson 1979", "A naive approach to the problem is to greedily improve the map but it turns out that this approach yields poor quality mappings", "We therefore develop a method for optimizing over soft mappings and use entropy regularization to drive those towards hard mappings", "We construct the objective in a way that facilitates simple monotonically improving updates corresponding to solving convex optimization problems", "We evaluate our mapping approach on 19 languages that include representatives of IndoEuropean Semitic Basque Japonic and Turkic families", "We measure mapping quality based on the target language parsing accuracy", "In addition to considering gold POS tags for the target language 2 Instances of related hard problems are 3partition and subsetsum", "we also evaluate the mapping algorithm on automatically induced POS tags", "In all evaluation scenarios our model consistently rivals the quality of manually induced mappings", "We also demonstrate that the proposed inference procedure outperforms greedy methods by a large margin highlighting the importance of good optimization techniques", "We further show that while all characteristics of the mapping contribute to the objective our largest gain comes from distributional features that capture global statistics", "Finally we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer which motivates further study of this topic"]}, "D12-1127": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 1389 1398 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics Wikily Supervised PartofSpeech Tagging Shen Li Computer  Information Science University of Pennsylvania shenliseasupennedu Joa o V Grac  a L2F INESCID Lisboa Portugal javgl2finescidpt Ben Taskar Computer  Information Science University of Pennsylvania taskarcisupennedu Abstract Despite significant recent work purely unsupervised techniques for partofspeech POS tagging have not achieved useful accuracies required by many language processing tasks Use of parallel text between resourcerich and resourcepoor languages is one source of weak supervision that significantly improves accuracy However parallel text is not always available and techniques for using it require multiple complex algorithmic steps In this paper we show that we can build POStaggers exceeding stateoftheart bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource the Wiktionary Across eight languages for which we have labeled data to evaluate results we achieve accuracy that significantly exceeds best unsupervised and parallel text methods We achieve highest accuracy reported for several languages and show that our approach yields better outofdomain taggers than those trained using fully supervised Penn Treebank 1 Introduction Partofspeech categories are elementary building blocks that play an important role in many natural language processing tasks from machine trans lation to information extraction Supervised learning of taggers from POSannotated training text is a wellstudied task with several methods achieving nearhuman tagging accuracy Ratnaparkhi 1996 Toutanova et al 2003 Shen et al 2007 However while English and a handful of other languages are fortunate enough to have comprehensive POSannotated corpora such as the Penn Treebank Marcus et al 1993 most of the worlds languages have no labeled corpora The annotated corpora that do exist were costly to build Abeille 2003 and are often not freely available or restricted to researchonly use Furthermore much of the annotated text is of limited genre normally focusing on newswire or literary text Performance of treebank trained systems degrades significantly when applied to new domains Blitzer et al 2006 Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications BergKirkpatrick et al 2010 Grac  a et al 2011 Lee et al 2010 Using additional information in the form of tag dictionaries or parallel text seems unavoidable at present Early work on using tag dictionaries used a labeled corpus to extract all allowed wordtag pairs Merialdo 1994 which is quite an unrealistic scenario More recent work has used a subset of the observed wordtag pairs and focused on generalizing dictionary entries Smith and Eisner 2005 Haghighi and Klein 2006 Toutanova and Johnson 2007 Goldwater and Griffiths 2007 Using corpusbased dictionaries greatly biases the test results and gives little information about the capacity to generalize to different domains Recent work by Das and Petrov 2011 builds a dictionary for a particular language by transferring annotated data from a resourcerich language through the use of word alignments in parallel text 1389 The main idea is to rely on existing dictionaries for some languages eg English and use parallel data to build a dictionary in the desired language and extend the dictionary coverage using label propagation However parallel text does not exist for many pairs of languages and the proposed bilingual projection algorithms are fairly complex In this work we use the Wiktionary a freely avail able high coverage and constantly growing dictionary for a large number of languages We ex periment with a very simple secondorder Hidden Markov Model with featurebased emissions BergKirkpatrick et al 2010 Grac  a et al 2011 We outperform best current results using parallel text supervision across 8 different languages even when the word type coverage is as low as 20 Furthermore using the Brown corpus as outofdomain data we show that using the Wiktionary produces better taggers than using the Penn Treebank dictionary 885 vs 859 Our empirical analysis and the natural growth rate of the Wiktionary suggest that free highquality and multidomain POStaggers for a large number of languages can be obtained by standard and efficient models The source code the dictionary mappings and the trained models described in this work are available at httpcodegooglecomp wikilysupervisedpostagger 2 Related Work The scarcity of labeled corpora for resource poor languages and the challenges of domain adaptation have led to several efforts to build systems for unsupervised POStagging Several lines of research have addressed the fully unsupervised POStagging task mutual information clustering Brown et al 1992 Clark 2003 has been used to group words according to their distributional context Using dimensionality reduction on word contexts followed by clustering has led to accuracy gains Schutze 1995 Lamar et al 2010 Sequence models HMMs in particular have been used to represent the probabilistic dependencies between consecutive tags In these approaches each observation corresponds to a particular word and each hidden state corresponds to a cluster However using maximum likelihood training for such models does not achieve good results Clark 2003 maximum likelihood training tends to result in very ambiguous distributions for common words in contradiction with the rather sparse wordtag distribution Several approaches have been proposed to mitigate this problem including Bayesian approaches using an improper Dirichlet prior to favor sparse model parameters Johnson 2007 Gao and Johnson 2008 Goldwater and Griffiths 2007 or using the Posterior Regularization to penalize ambiguous posteriors distributions of tags given tokens Graca et al 2009 Berg Kirkpatrick et al2010 and Grac  a et al 2011 proposed replacing the multinomial emission distributions of standard HMMs by maximum entropy ME featurebased distributions This allows the use of features to capture morphological information and achieves very promising results Despite these improvements fully unsupervised systems require an oracle to map clusters to true tags and the performance still fails to be of practical use In this paper we follow a different line of work where we rely on a prior tag dictionary indicating for each word type what POS tags it can take on Merialdo 1994 The task is then for each word token in the corpus to disambiguate between the possible POS tags Even when using a tag dictionary disambiguating from all possible tags is still a hard problem and the accuracy of these methods is still fall far behind their supervised counterparts The scarcity of large manuallyconstructed tag dictionaries led to the development of methods that try to generalize from a small dictionary with only a handful of entries Smith and Eisner 2005 Haghighi and Klein 2006 Toutanova and Johnson 2007 Goldwater and Griffiths 2007 however most previous works build the dictionary from the labeled corpus they learn on which does not represent a realistic dictionary In this paper we argue that the Wiktionary can serve as an effective and much less biased tag dictionary We note that most of the previous dictionary based approaches can be applied using the Wiktionary and would likely lead to similar accuracy increases that we show in this paper For example the work if Ravi and Knight 2009 minimizes the number of possible tagtag transitions in the HMM via a integer program hence discarding unlikely transitions that would confuse the model Models can also be trained jointly using parallel corpora in sev1390 eral languages exploiting the fact that different languages present different ambiguities Snyder et al 2008 The Wiktionary has been used extensively for other tasks such as domain specific information retrieval Mu ller and Gurevych 2009 ontology matching Krizhanovsky and Lin 2009 synonymy detection Navarro et al 2009 sentiment classification Chesley et al 2006 Recently Ding 2011 used the Wiktionary to initialize an HMM for Chinese POS tagging combined with label propagation 3 The Wiktionary and tagged corpora The Wiktionary1 is a collaborative project that aims to produce a free largescale multilingual dictionary Its goal is to describe all words from all languages currently more than 400 using definitions and descriptions in English The coverage of the Wiktionary varies greatly between languages currently there are around 75 languages for which there exists more than 1000 word types and 27 for which there exists more than 10000 word types Nevertheless the Wiktionary has been growing at a considerable rate see Figure 1 and the number of avail able words has almost doubled in the last three years As more people use the Wiktionary it is likely to grow Unlike tagged corpora the Wiktionary provides natural incentives for users to contribute missing entries and expand this communal resource akin to Wikipedia As with Wikipedia the questions of accuracy bias consistency across languages and selective coverage are paramount In this section we explore these concerns by comparing Wiktionary to dictionaries derived from tagged corpora"], "abstract": ["Despite significant recent work purely unsupervised techniques for partofspeech POS tagging have not achieved useful accuracies required by many language processing tasks", "Use of parallel text between resourcerich and resourcepoor languages is one source of weak supervision that significantly improves accuracy", "However parallel text is not always available and techniques for using it require multiple complex algorithmic steps", "In this paper we show that we can build POStaggers exceeding stateoftheart bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource the Wiktionary", "Across eight languages for which we have labeled data to evaluate results we achieve accuracy that significantly exceeds best unsupervised and parallel text methods", "We achieve highest accuracy reported for several languages and show that our approach yields better outofdomain taggers than those trained using fully supervised Penn Treebank"], "inroduction": ["Partofspeech categories are elementary building blocks that play an important role in many natural language processing tasks from machine translation to information extraction", "Supervised learning of taggers from POSannotated training text is a wellstudied task with several methods achieving nearhuman tagging accuracy Ratnaparkhi 1996 Toutanova et al 2003 Shen et al 2007", "However while English and a handful of other languages are fortunate enough to have comprehensive POS annotated corpora such as the Penn Treebank Marcus et al 1993 most of the worlds languages have no labeled corpora", "The annotated corpora that do exist were costly to build Abeille 2003 and are often not freely available or restricted to research only use", "Furthermore much of the annotated text is of limited genre normally focusing on newswire or literary text", "Performance of treebanktrained systems degrades significantly when applied to new domains Blitzer et al 2006", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation but despite recent progress the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications Berg Kirkpatrick et al 2010 Graca et al 2011 Lee et al 2010", "Using additional information in the form of tag dictionaries or parallel text seems unavoidable at present", "Early work on using tag dictionaries used a labeled corpus to extract all allowed wordtag pairs Merialdo 1994 which is quite an unrealistic scenario", "More recent work has used a subset of the observed wordtag pairs and focused on generalizing dictionary entries Smith and Eisner 2005 Haghighi and Klein 2006 Toutanova and Johnson 2007 Goldwater and Griffiths 2007", "Using corpus based dictionaries greatly biases the test results and gives little information about the capacity to generalize to different domains", "Recent work by Das and Petrov 2011 builds a dictionary for a particular language by transferring annotated data from a resourcerich language through the use of word alignments in parallel text", "1389 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 13891398 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics The main idea is to rely on existing dictionaries for some languages eg English and use parallel data to build a dictionary in the desired language and extend the dictionary coverage using label propagation", "However parallel text does not exist for many pairs of languages and the proposed bilingual projection algorithms are fairly complex", "In this work we use the Wiktionary a freely available high coverage and constantly growing dictionary for a large number of languages", "We experiment with a very simple secondorder Hidden Markov Model with featurebased emissions Berg Kirkpatrick et al 2010 Graca et al 2011", "We outperform best current results using parallel text supervision across 8 different languages even when the word type coverage is as low as 20", "Furthermore using the Brown corpus as outofdomain data we show that using the Wiktionary produces better taggers than using the Penn Treebank dictionary 885 vs 859", "Our empirical analysis and the natural growth rate of the Wiktionary suggest that free highquality and multidomain POStaggers for a large number of languages can be obtained by standard and efficient models", "The source code the dictionary mappings and the trained models described in this work are available at httpcodegooglecomp wikilysupervisedpostagger"]}, "D12-1133": {"title": ["Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 1455 1465 Jeju Island Korea 12 14 July 2012 c2012 Association for Computational Linguistics A TransitionBased System for Joint PartofSpeech Tagging and Labeled NonProjective Dependency Parsing Bernd Bohnet Institute for Natural Language Processing University Stuttgart bohnetimsunistuttgartde Joakim Nivre Department of Linguistics and Philology Uppsala University joakimnivrelingfiluuse Abstract Most current dependency parsers presuppose that input words have been morphologically disambiguated using a partofspeech tagger before parsing begins We present a transitionbased system for joint partofspeech tagging and labeled dependency parsing with nonprojective trees Experimental evaluation on Chinese Czech English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system which lead to improved stateoftheart results for all languages 1 Introduction Dependencybased syntactic parsing has been the focus of intense research efforts during the last decade and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference McDonald et al 2005 McDonald and Pereira 2006 Carreras 2007 Koo and Collins 2010 Bohnet 2010 but other inference methods have been explored especially for nonprojective parsing Riedel and Clarke 2006 Smith and Eisner 2008 Martins et al 2009 Martins et al 2010 Koo et al 2010 Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search Titov and Henderson 2007 Zhang and Clark 2008 Zhang and Clark 2011 Despite notable differences in model structure graphbased and transitionbased parsers both give stateoftheart accuracy with proper feature selection and optimization Koo and Collins 2010 Zhang and Nivre 2011 Bohnet 2011 It is noteworthy however that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using at least a partofspeech tagger This is in stark contrast to the best parsers based on PCFG models such as the Brown parser Charniak and Johnson 2005 and the Berkeley parser Petrov et al 2006 Petrov and Klein 2007 which not only can perform their own partofspeech tagging but normally give better parsing accuracy when they are allowed to do so This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other Thus Lee et al2011 show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin Ancient Greek Czech and Hungarian However Li et al2011 and Hatori et al2011 report improvements with a joint model also for Chinese which is not a richly inflected language but is nevertheless rich in partofspeech ambiguities In this paper we present a transitionbased model for joint partofspeech tagging and labeled de pendency parsing with nonprojective trees Exper1455 iments show that joint modeling improves both tagging and parsing accuracy leading to stateoftheart accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English To our knowledge this is the first joint system that performs labeled dependency parsing It is also the first joint system that achieves stateoftheart accuracy for nonprojective dependency parsing 2 TransitionBased Tagging and Parsing Transitionbased dependency parsing was pioneered by Yamada and Matsumoto 2003 and Nivre et al 2004 who used classifiers trained to predict individual actions of a deterministic shiftreduce parser Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions Zhang and Clark 2008 Huang and Sagae 2010 Zhang and Clark 2011 Zhang and Nivre 2011 Bohnet 2011 In addition a number of different transition systems have been proposed in particular for dealing with nonprojective dependencies which were beyond the scope of early systems Attardi 2006 Nivre 2007 Nivre 2009 Titov et al 2009"], "abstract": ["Most current dependency parsers presuppose that input words have been morphologically disambiguated using a partofspeech tagger before parsing begins", "We present a transition based system for joint partofspeech tagging and labeled dependency parsing with non projective trees", "Experimental evaluation on Chinese Czech English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system which lead to improved stateofthe art results for all languages"], "inroduction": ["Dependencybased syntactic parsing has been the focus of intense research efforts during the last decade and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning", "Graph based models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference McDonald et al 2005 McDonald and Pereira 2006 Carreras 2007 Koo and Collins 2010 Bohnet 2010 but other inference methods have been explored especially for nonprojective parsing Riedel and Clarke 2006 Smith and Eisner 2008 Martins et al 2009 Martins et al 2010 Koo et al 2010", "Transition based models parameterize the problem by elementary parsing actions and typically use incremental beam search Titov and Henderson 2007 Zhang and Clark 2008 Zhang and Clark 2011", "Despite notable differences in model structure graphbased and transitionbased parsers both give stateofthe art accuracy with proper feature selection and optimization Koo and Collins 2010 Zhang and Nivre 2011 Bohnet 2011", "It is noteworthy however that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using at least a partofspeech tagger", "This is in stark contrast to the best parsers based on PCFG models such as the Brown parser Charniak and Johnson 2005 and the Berkeley parser Petrov et al 2006 Petrov and Klein 2007 which not only can perform their own partofspeech tagging but normally give better parsing accuracy when they are allowed to do so", "This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing", "It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other", "Thus Lee et al", "2011 show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin Ancient Greek Czech and Hungarian", "However Li et al", "2011 and Hatori et al", "2011 report improvements with a joint model also for Chinese which is not a richly inflected language but is nevertheless rich in partofspeech ambiguities", "In this paper we present a transitionbased model for joint partofspeech tagging and labeled dependency parsing with nonprojective trees", "Exper 1455 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning pages 14551465 Jeju Island Korea 1214 July 2012", "Qc 2012 Association for Computational Linguistics iments show that joint modeling improves both tagging and parsing accuracy leading to stateoftheart accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English", "To our knowledge this is the first joint system that performs labeled dependency parsing", "It is also the first joint system that achieves stateoftheart accuracy for nonprojective dependency parsing"]}, "D13-1004": {"title": ["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 30 41 Seattle Washington USA 1821 October 2013 c  2013 Association for Computational Linguistics Exploring the utility of joint morphological and syntactic learning from childdirected speech Stella Frank sfrankinfedacuk Frank Keller kellerinfedacuk ILCC School of Informatics University of Edinburgh Edinburgh EH8 9AB UK Sharon Goldwater sgwaterinfedacuk Abstract Children learn various levels of linguistic structure concurrently yet most existing models of language acquisition deal with only a single level of structure implicitly assuming a sequential learning process Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning Here we present a model that jointly induces syntactic categories and morphological segmentations by combining two wellknown models for the individual tasks We test on childdirected utterances in English and Spanish and compare to singletask baselines In the morphologically poorer language English the model improves morphological segmentation while in the morphologically richer language Spanish it leads to better syntactic categorization These results provide further evidence that joint learning is useful but also suggest that the benefits may be different for typologically different languages 1 Introduction Models of language acquisition seek to infer linguistic structure from data with minimal amounts of prior knowledge in order to discover which characteristics of the input data are useful for learning and thus potentially utilised by human learners Most previous work has focused on learning individual aspects of linguistic structure However children clearly learn multiple aspects in parallel rather than sequentially implying that models of language acquisition should also incorporate joint learning Joint models investigate the interaction between different levels of linguistic structure during learning These interactions are often but not necessarily synergistic enabling better more robust learning by making use of cues from multiple sources Recent models using joint learning to model language acquisition have spanned various domains including phonology word segmentation syntax and semantics Feldman et al 2009 Elsner et al 2012 Doyle and Levy 2013 Johnson 2008 Kwiatkowski et al 2012 In this paper we examine the joint learning of syntactic categories and morphology which are acquired by children at roughly the same age Clark 2003b implying possible interactions in the learning process Both morphology and word order de pend on categorising words based on their morphosyntactic function However previous models of syntactic category learning have relied principally on surrounding context ie word order constraints whereas models of morphology use wordinternal cues Our joint model integrates both sources of information allowing the model to flexibly weigh them according to their utility Languages differ in the richness of their morphology and strictness of word order These characteristics appear to be anticorrelated with rich morphology co occurring with free word order and vice versa Blake 2001 McFadden 2003 The timecourse of acquisition is also influenced by language typology learners of morphologically rich languages become productive in morphology earlier Xanthos et al 2011 suggesting that richer morphology may be more salient for learners than impoverished morphology Sentence comprehension in children also shows crosslinguistic differences in the cues used to make sense of noncanonical sentence structure learners of a morphologically rich language Turkish disregard word order in 30 favour of morphology whereas learners of English favour word order Slobin 1982 MacWhinney et al 1984 These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength rich morphology versus strict word order and thus be more languagegeneral than singletask models Both syntactic category and morphology induction have been the focus of much recent work See Hammarstrom and Borin 2011 for an overview of unsupervised morphology learning likewise Christodoulopoulos et al 2010 for a comparison of part of speechsyntactic category induction systems However given the tightly coupled nature of these two tasks there has been surprisingly little work in joint learning of morphology and syntactic categories Systems for inducing syntactic categories often make use of morphemelike features such as wordfinal characters Smith and Eisner 2005 Haghighi and Klein 2006 Berg Kirkpatrick et al 2010 Lee et al 2010 or model words at the characterlevel Clark 2003a Blunsom and Cohn 2011 but do not include morphemes ex plicitly Other systems Dasgupta and Ng 2007 Christodoulopoulos et al 2011 use morphological segmentations learned by a separate morphology model as features in a pipeline approach Models of morphology induction generally operate over a lexicon ie a list of word types rather than token corpora Goldsmith 2006 Creutz and Lagus 2007 Kurimo et al 2010 These models find morphological categories on the basis of wordinternal features without taking syntactic context into account which is of course not available in a lexicon Lee et al 2011 and Sirts and Aluma e 2012 present models that infer morphological segmentations and syntactic categories jointly although Lee et al 2011 do not evaluate the inferred syntactic categories Both make use of a wordtype constraint which limits each word form to a single analysis ie all instances of ducks are assigned to a single category and will have the same morpheme analysis ignoring the gold standard distinction between a plural noun and third person singular verb This can make inference more tractable and often increases performance but does not respect the ambiguity inherent in natural language both over syntactic categories and morphological analyses The degree of ambiguity is language dependent so that even if a typeconstraint is perhaps relatively unproblematic in English it will pose problems in morphologically richer languages Furthermore these two models make use of an array of heuristics that may not allow them to be easily generalisable across languages and datasets eg likelihood scaling Sirts and Aluma e 2012 sequential suffix matching Lee et al 2011 In this paper we present a joint model composed of two wellknown individual models This allows us to cleanly investigate the effects of joint learning and its potential benefits over the single task models The simplicity of our models also allows us to avoid modelling and inference heuristics Previous models have used adultdirected written texts which differs significantly from the type of language available to child learners We test our joint model on childdirected utterances in English a morphologically poor language and Spanish with richer morphology1 Our results indicate that our joint model is able to flexibly accommodate languages with differing levels of morphological richness The joint model matches the performance of single task models on both tasks demonstrating that the additional complexity is not a problem ie it does not add noise Moreover the joint model improves performance significantly on the task corresponding to the language s weaker cue indicating a transfer of information from the stronger cue The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity 2 Model The task is to assign word tokens to part of speech categories and simultaneously segment the tokens into morphemes We assume a relatively simple yet commonly used concatenative morphology which models a word as a stem plus possibly null suffix2 1There are languages with much richer morphology than Spanish but none with a childdirected corpus suitably annotated for evaluation 2Fullwood and O Donnell 2013 recently presented a model of nonconcatenative morphology that could be integrated into this model however it does not perform well on English and presumably other mostly concatenative languages 31 Since this is an unsupervised model the inferred categories and morphemes lack meaningful labels but ideally will correspond to gold standard categories and morphemes"], "abstract": ["Children learn various levels of linguistic structure concurrently yet most existing models of language acquisition deal with only a single level of structure implicitly assuming a sequential learning process", "Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning", "Here we present a model that jointly induces syntactic categories and morphological segmentations by combining two wellknown models for the individual tasks", "We test on childdirected utterances in English and Spanish and compare to singletask baselines", "In the morphologically poorer language English the model improves morphological segmentation while in the morphologically richer language Spanish it leads to better syntactic categorization", "These results provide further evidence that joint learning is useful but also suggest that the benefits may be different for typologically different languages"], "inroduction": ["Models of language acquisition seek to infer linguistic structure from data with minimal amounts of prior knowledge in order to discover which characteristics of the input data are useful for learning and thus potentially utilised by human learners", "Most previous work has focused on learning individual aspects of linguistic structure", "However children clearly learn multiple aspects in parallel rather than sequentially implying that models of language acquisition should also incorporate joint learning", "Joint models investigate the interaction between different levels of linguistic structure during learning", "These interactions are often but not necessarily synergistic enabling better more robust learning by making use of cues from multiple sources", "Recent models using joint learning to model language acquisition have spanned various domains including phonology word segmentation syntax and semantics Feldman et al 2009 Elsner et al 2012 Doyle and Levy 2013 Johnson 2008 Kwiatkowski et al 2012", "In this paper we examine the joint learning of syntactic categories and morphology which are acquired by children at roughly the same age Clark 2003b implying possible interactions in the learning process", "Both morphology and word order depend on categorising words based on their morpho syntactic function", "However previous models of syntactic category learning have relied principally on surrounding context ie word order constraints whereas models of morphology use wordinternal cues", "Our joint model integrates both sources of information allowing the model to flexibly weigh them according to their utility", "Languages differ in the richness of their morphology and strictness of word order", "These characteristics appear to be anticorrelated with rich morphology cooccurring with free word order and vice versa Blake 2001 McFadden 2003", "The timecourse of acquisition is also influenced by language typology learners of morphologically rich languages become productive in morphology earlier Xanthos et al 2011 suggesting that richer morphology may be more salient for learners than impoverished morphology", "Sentence comprehension in children also shows crosslinguistic differences in the cues used to make sense of noncanonical sentence structure learners of a morphologically rich language Turkish disregard word order in 30 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 3041 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics favour of morphology whereas learners of English favour word order Slobin 1982 MacWhinney et al 1984", "These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength rich morphology versus strict word order and thus be more languagegeneral than singletask models", "Both syntactic category and morphology induction have been the focus of much recent work", "See Hammarstro m and Borin 2011 for an overview of unsupervised morphology learning likewise Christodoulopoulos et al", "2010 for a comparison of part of speechsyntactic category induction systems", "However given the tightly coupled nature of these two tasks there has been surprisingly little work in joint learning of morphology and syntactic categories", "Systems for inducing syntactic categories often make use of morphemelike features such as wordfinal characters Smith and Eisner 2005 Haghighi and Klein 2006 BergKirkpatrick et al 2010 Lee et al 2010 or model words at the characterlevel Clark 2003a Blunsom and Cohn 2011 but do not include morphemes explicitly", "Other systems Dasgupta and Ng 2007 Christodoulopoulos et al 2011 use morphological segmentations learned by a separate morphology model as features in a pipeline approach", "Models of morphology induction generally operate over a lexicon ie a list of word types rather than token corpora Goldsmith 2006 Creutz and Lagus 2007 Kurimo et al 2010", "These models find morphological categories on the basis of word internal features without taking syntactic context into account which is of course not available in a lexicon", "Lee et al", "2011 and Sirts and Alumae 2012 present models that infer morphological segmentations and syntactic categories jointly although Lee et al", "2011 do not evaluate the inferred syntactic categories", "Both make use of a wordtype constraint which limits each word form to a single analysis ie all instances of ducks are assigned to a single category and will have the same morpheme analysis ignoring the gold standard distinction between a plural noun and third person singular verb", "This can make inference more tractable and often increases performance but does not respect the ambiguity in herent in natural language both over syntactic categories and morphological analyses", "The degree of ambiguity is language dependent so that even if a typeconstraint is perhaps relatively unproblematic in English it will pose problems in morphologically richer languages", "Furthermore these two models make use of an array of heuristics that may not allow them to be easily generalisable across languages and datasets eg likelihood scaling Sirts and Alumae 2012 sequential suffix matching Lee et al 2011", "In this paper we present a joint model composed of two wellknown individual models", "This allows us to cleanly investigate the effects of joint learning and its potential benefits over the single task models", "The simplicity of our models also allows us to avoid modelling and inference heuristics", "Previous models have used adultdirected written texts which differs significantly from the type of language available to child learners", "We test our joint model on childdirected utterances in English a morphologically poor language and Spanish with richer morphology1", "Our results indicate that our joint model is able to flexibly accommodate languages with differing levels of morphological richness", "The joint model matches the performance of single task models on both tasks demonstrating that the additional complexity is not a problem ie it does not add noise", "Moreover the joint model improves performance significantly on the task corresponding to the languages weaker cue indicating a transfer of information from the stronger cue", "The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity"]}, "D13-1031": {"title": ["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 311 321 Seattle Washington USA 1821 October 2013 c  2013 Association for Computational Linguistics Exploring Representations from Unlabeled Data with Cotraining for Chinese Word Segmentation Longkai Zhang Houfeng Wang Xu Sun Mairgup Mansur Key Laboratory of Computational Linguistics Peking University Ministry of Education China zhlongkqqcom wanghfpkueducn xusunpkueducn mairgupgmailcom Abstract Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation However the ability of these models is restricted by the availability of an notated data and the design of features We propose a scalable semisupervised feature engineering approach In contrast to previous works using predefined taskspecific features with fixed values we dynamically extract representations of label distributions from both an indomain corpus and an outofdomain corpus We update the representation values with a semisupervised approach Experiments on the benchmark datasets show that our approach achieve good results and reach an fscore of 0961 The feature engineer ing approach proposed here is a general iterative semisupervised method and not limited to the word segmentation task 1 Introduction Chinese is a language without natural word delimiters Therefore Chinese Word Segmentation CWS is an essential task required by further language processing Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task and supervised models are more accurate than unsupervised models Xue 2003 Low et al 2005 However the resource of manually labeled training corpora is limited Therefore semisupervised learning has become one Corresponding author of the most natural forms of training for CWS Traditional semisupervised methods focus on adding new unlabeled instances to the training set by a given criterion The possible mislabeled instances which are introduced from the automatically labeled raw data can hurt the performance and not easy to exclude by setting a sound selecting criterion In this paper we propose a simple and scalable semisupervised strategy that works by providing semisupervision at the level of representation Previous works mainly assume that con text features are helpful to decide the potential label of a character However when some of the context features do not appear in the training corpus this assumption may fail An example is shown in table 1 Although the context of    and  is totally different they share a homogeneous structure as  verbnoun  Therefore A much better way is to map the context information to a kind of representation More precisely the mapping should let the similar contexts map to similar representations while let the distinct contexts map to distinct representations   Label B B Character       Context C1  C1  Features C0  C0  C1  C1  Table 1 Example of the context of    in   Eat fruits  and the context of    in     Play basketball We use the label distribution information that 311 is extracted from the unlabeled corpus as this representation to enhance the supervised model We add  pseudolabels by tagging the unlabeled data with the trained model on the training corpus These  pseudolabels are not accurate enough Therefore we use the label distribution which is much more accurate To accurately calculate the precise label distribution we use a framework similar to the co training algorithm to adjust the feature values iteratively Generally speaking unlabeled data can be classified as indomain data and outofdomain data In previous works these two kinds of unlabeled data are used separately for different purposes Indomain data is mainly used to solve the problem of data sparseness Sun and Xu 2011 On the other hand outof domain data is used for domain adaptation Chang and Han 2010 In our work we use indomain and outofdomain data together to adjust the labels of the unlabeled corpus We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoff Experiment results show that our approach yields improvements compared with the stateofart systems Even when the labeled data is insufficient our methods can still work better than traditional methods Com pared to the baseline CWS model which has already achieved an fscore above 095 we further reduce the error rate by 15 Our method is not limited to word segmentation It is also applicable to other problems which can be solved by sequence labeling models We also applied our method to the Chi nese Named Entity Recognition task and also achieved better results compared to traditional methods The main contributions of our work are as follows  We proposed a general method to utilize the label distribution given text contexts as representations in a semisupervised framework We let the cotraining process adjust the representation values from label distribution instead of using manually predefined feature templates  Compared with previous work our method achieved a new stateofart accuracy on the CWS task as well as on the NER task The remaining part of this paper is organized as follows Section 2 describes the details of the problem and our algorithm Section 3 describes the experiment and presents the results Section 4 reviews the related work Section 5 concludes this paper 2 System Architecture"], "abstract": ["Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation", "However the ability of these models is restricted by the availability of annotated data and the design of features", "We propose a scalable semisupervised feature engineering approach", "In contrast to previous works using predened task specic features with xed values we dynamically extract representations of label distributions from both an indomain corpus and an outofdomain corpus", "We update the representation values with a semisupervised approach", "Experiments on the benchmark datasets show that our approach achieve good results and reach an fscore of 0961", "The feature engineering approach proposed here is a general iterative semisupervised method and not limited to the word segmentation task"], "inroduction": ["Chinese is a language without natural word delimiters", "Therefore Chinese Word Segmentation CWS is an essential task required by further language processing", "Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task and supervised models are more accurate than unsupervised models Xue 2003 Low et al 2005", "However the resource of manually labeled training corpora is limited", "Therefore semisupervised learning has become one Corresponding author of the most natural forms of training for CWS", "Traditional semisupervised methods focus on adding new unlabeled instances to the training set by a given criterion", "The possible mislabeled instances which are introduced from the automatically labeled raw data can hurt the performance and not easy to exclude by setting a sound selecting criterion", "In this paper we propose a simple and scal able semisupervised strategy that works by providing semisupervision at the level of representation", "Previous works mainly assume that context features are helpful to decide the potential label of a character", "However when some of the context features do not appear in the training corpus this assumption may fail", "An example is shown in table 1", "Although the context of and  is totally dierent they share a homo geneous structure as verbnoun", "Therefore", "Amuch better way is to map the context informa tion to a kind of representation", "More precisely the mapping should let the similar contexts map to similar representations while let the distinct contexts map to distinct representations", "  Label B B Character       Context Features C1  C0  C1  C1  C0  C1  Table 1 Example of the context of  in   Eat fruits and the context of  in  Play basketball We use the label distribution information that 311 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 311321 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics is extracted from the unlabeled corpus as this representation to enhance the supervised model", "We add pseudolabels by tagging the unlabeled data with the trained model on the training corpus", "These pseudolabels are not accurate enough", "Therefore we use the label distribution which is much more accurate", "To accurately calculate the precise label distribution we use a framework similar to the co training algorithm to adjust the feature values iteratively", "Generally speaking unlabeled data can be classied as indomain data and outof domain data", "In previous works these two kinds of unlabeled data are used separately for dierent purposes", "Indomain data is mainly used to solve the problem of data sparseness Sun and Xu 2011", "On the other hand outof domain data is used for domain adaptation Chang and Han 2010", "In our work we use indomain and outofdomain data together to adjust the labels of the unlabeled corpus", "We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeo", "Experiment results show that our approach yields improvements compared with the stateofart systems", "Even when the labeled data is insucient our methods can still work better than traditional methods", "Compared to the baseline CWS model which has already achieved an fscore above 095 we further reduce the error rate by 15", "Our method is not limited to word segmentation", "It is also applicable to other problems which can be solved by sequence labeling models", "We also applied our method to the Chinese Named Entity Recognition task and also achieved better results compared to traditional methods", "The main contributions of our work are as follows  We proposed a general method to utilize the label distribution given text contexts as representations in a semisupervised framework", "We let the cotraining process adjust the representation values from label distribution instead of using manually pre dened feature templates", " Compared with previous work our method achieved a new stateofart accuracy on the CWS task as well as on the NER task", "The remaining part of this paper is organized as follows", "Section 2 describes the details of the problem and our algorithm", "Section 3 describes the experiment and presents the results", "Section 4 reviews the related work", "Section 5 concludes this paper"]}, "D13-1032": {"title": ["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 322 332 Seattle Washington USA 1821 October 2013 c  2013 Association for Computational Linguistics Efficient HigherOrder CRFs for Morphological Tagging Thomas Mu ller   Helmut Schmid   and Hinrich Schu tze Center for Information and Language Processing University of Munich Germany Institute for Natural Language Processing  University of Stuttgart Germany muelletscislmude Abstract Training higherorder conditional random fields is prohibitive for huge tag sets We present an approximated conditional random field using coarsetofine decoding and early updating We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higherorder models give significant improvements over 1storder models 1 Introduction Conditional Random Fields CRFs Lafferty et al 2001 are arguably one of the best performing sequence prediction models for many Natural Language Processing NLP tasks During CRF training forwardbackward computations a form of dynamic programming dominate the asymptotic runtime The training and also decoding times thus depend polynomially on the size of the tagset and exponentially on the order of the CRF This probably explains why CRFs despite their outstanding accuracy normally only are applied to tasks with small tagsets such as Named Entity Recognition and Chunking if they are applied to tasks with bigger tagsets  eg to partofspeech POS tagging for English  then they generally are used as 1storder models In this paper we demonstrate that fast and accurate CRF training and tagging is possible for large tagsets of even thousands of tags by approximating the CRF objective function using coarsetofine decoding Charniak and Johnson 2005 Rush and Petrov 2012 Our pruned CRF PCRF model has much smaller runtime than higherorder CRF models and may thus lead to an even broader application of CRFs across NLP tagging tasks We use POS tagging and combined POS and morphological POSMORPH tagging to demonstrate the properties and benefits of our approach POSMORPH disambiguation is an important preprocessing step for syntactic parsing It is usually tackled by applying sequence prediction POSMORPH tagging is also a good example of a task where CRFs are rarely applied as the tagsets are often so big that even 1storder dynamic programming is too expensive A workaround is to restrict the possible tag candidates per position by using either morphological analyzers MAs dictionaries or heuristics Hajic  2000 In this paper however we show that when using pruning ie PCRFs CRFs can be trained in reasonable time which makes hard constraints unnecessary In this paper we run successful experiments on six languages with different morphological properties we interpret this as evidence that our ap proach is a general solution to the problem of POSMORPH tagging The tagsets in our experiments range from small sizes of 12 to large sizes of up to 1811 We will see that even for the smallest tagset PCRFs need only 40 of the training time of standard CRFs For the bigger tagset sizes we can reduce training times from several days to several hours We will also show that training higherorder PCRF models takes only several minutes longer than training 1storder models and  depending on the language  may lead to substantial accuracy im322 Language Sentences Tokens POS MORPH POSMORPH OOV Tags Tags Tags Rate ar Arabic 15760 614050 38 516 516 458 cs Czech 38727 652544 12 1811 1811 858 en English 38219 912344 45 45 334 es Spanish 14329 427442 12 264 303 647 de German 40472 719530 54 255 681 764 hu Hungarian 61034 1116722 57 1028 1071 1071 Table 1 Training set statistics OutOfVocabulary OOV rate is regarding the development sets provements For example in German POSMORPH tagging a 1storder model trained in 32 minutes achieves an accuracy of 8896 while a 3rdorder model trained in 35 minutes achieves an accuracy of 9060 The remainder of the paper is structured as follows Section 2 describes our CRF implementation1 and the feature set used Section 3 summarizes related work on tagging with CRFs efficient CRF tagging and coarsetofine decoding Section 4 describes experiments on POS tagging and POSMORPH tagging and Section 5 summarizes the main contributions of the paper 2 Methodology"], "abstract": ["Training higherorder conditional random fields is prohibitive for huge tag sets", "We present an approximated conditional random field using coarsetofine decoding and early updating", "We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higherorder models give significant improvements over 1st order models"], "inroduction": ["Conditional Random Fields CRFs Lafferty et al 2001 are arguably one of the best performing sequence prediction models for many Natural Language Processing NLP tasks", "During CRF training forwardbackward computations a form of dynamic programming dominate the asymptotic run time", "The training and also decoding times thus depend polynomially on the size of the tagset and exponentially on the order of the CRF", "This probably explains why CRFs despite their outstanding accuracy normally only are applied to tasks with small tagsets such as Named Entity Recognition and Chunking if they are applied to tasks with bigger tagsets  eg to partofspeech POS tagging for English  then they generally are used as 1storder models", "In this paper we demonstrate that fast and accurate CRF training and tagging is possible for large tagsets of even thousands of tags by approximating the CRF objective function using coarsetofine decoding Charniak and Johnson 2005 Rush and Petrov 2012", "Our pruned CRF PCRF model has much smaller runtime than higherorder CRF models and may thus lead to an even broader application of CRFs across NLP tagging tasks", "We use POS tagging and combined POS and morphological POSMORPH tagging to demonstrate the properties and benefits of our approach", "POSMORPH disambiguation is an important pre processing step for syntactic parsing", "It is usually tackled by applying sequence prediction", "POSMORPH tagging is also a good example of a task where CRFs are rarely applied as the tagsets are often so big that even 1storder dynamic programming is too expensive", "A workaround is to restrict the possible tag candidates per position by using either morphological analyzers MAs dictionaries or heuristics Hajic 2000", "In this paper however we show that when using pruning ie PCRFs CRFs can be trained in reasonable time which makes hard constraints unnecessary", "In this paper we run successful experiments on six languages with different morphological properties we interpret this as evidence that our approach is a general solution to the problem of POSMORPH tagging", "The tagsets in our experiments range from small sizes of 12 to large sizes of up to 1811", "We will see that even for the smallest tagset PCRFs need only 40 of the training time of standard CRFs", "For the bigger tagset sizes we can reduce training times from several days to several hours", "We will also show that training higherorder PCRF models takes only several minutes longer than training 1storder models and  depending on the language  may lead to substantial accuracy im 322 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 322332 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics Language Sentences Tokens POS Tags MORPH Tags POSMORPH Tags OOV Rate ar Arabic cs Czech en English es Spanish de German hu Hungarian 15760 38727 38219 14329 40472 61034 614050 652544 912344 427442 719530 1116722 38 12 45 12 54 57 516 1811 264 255 1028 516 1811 45 303 681 1071 458 858 334 647 764 1071 Table 1 Training set statistics", "OutOfVocabulary OOV rate is regarding the development sets", "provements", "For example in German POSMORPH tagging a 1storder model trained in 32 minutes achieves an accuracy of 8896 while a 3rdorder model trained in 35 minutes achieves an accuracy llD    xyD log p y x  of 9060", "The remainder of the paper is structured as follows Section 2 describes our CRF implementation1 and the feature set used", "Section 3 summarizes related work on tagging with CRFs efficient CRF tagging and coarsetofine decoding", "Section 4 describes experiments on POS tagging and POSMORPH tagging and Section 5 summarizes the main contributions of the paper"]}, "D13-1033": {"title": ["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 333 344 Seattle Washington USA 1821 October 2013 c  2013 Association for Computational Linguistics The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn Institute for Natural Language Processing University of Stuttgart seekerjonasimsunistuttgartde Abstract Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in stateoftheart models 1 Introduction In this paper we investigate the interplay between syntax and morphology with respect to the task of assigning morphological descriptions or tags to each token of a sentence Specifically we examine the effect of syntactic information when it is integrated into the feature model of a morphological tagger We test the effect of syntactic features on four languages  Czech German Hungarian and Spanish  and find that syntactic features improve our tagger considerably for Czech and German but not for Hungarian and Spanish Our analysis of constructions that show morphosyntactic agreement suggests that syntactic features are important if the language shows frequent word form syncretisms1 that can be disambiguated by the syntactic context The meaning of a sentence is structurally encoded 1Syncretism describes the situation where a word form is ambiguous between several different morphological descriptions within its inflection paradigm by morphological and syntactic means2 Different languages however use them to a different extent Languages like English encode grammatical infor mation like the subject vs object status of an argument via word order whereas languages like Czech or Hungarian use different word forms Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models Linguistic theory e g Bresnan 2001 Melcuk 2009 suggests many interactions between morphology and syntax For example languages with a case system use different forms of the same word to mark different syntactic or semantic relations Blake 2001 In many languages two words that participate in a syntactic relation show covariance in some or all of their morphological features socalled agreement Corbett 20063 Automatic annotation of morphology assigns morphological descriptions e g nominativesingularmasculine to word forms It is usually modeled as a sequence model often in combination with partofspeech tagging and lemmatization Collins 2002 Hajic 2004 Smith et al 2005 Chrupaa et al 2008 and others Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships As an example for these hierarchical relationships 2And also by prosodic means which we will not discuss since textbased tools rarely have access to this information 3For example in English the subject of a sentence and the finite verb agree with respect to their number and person feature 333 die wirtschaftlich am weitesten entwickelten  modernen und zum Teil katholisch gepra gten Regionen nomaccplfem nomaccplfem the economic  most developed  modern and to part catholic influenced regions NK MO PM MO NK CJ CD MO NK MO CJ the regions that are economically most developed modern and partly catholic Figure 1 Example of a German noun phrase First and last word agree in number gender and case value Figure 1 shows a German noun phrase taken from the German TiGer corpus Brants et al 2002 The two boldfaced words are the determiner and the head noun of the phrase and they agree in their gender number and case values The word Regionen regions is fourway ambiguous for its case value which is reduced to a twoway ambiguity between nominative and accusative by the determiner Further disambiguation would require information about the syntactic role of the noun phrase in a sentence There are 11 tokens between these two words which would require a context window of at least 13 to capture the agreement relation within a sequence model Syntactically however as indicated by the dependency tree the determiner and the head are linked directly The interdependency between morphology and syntax in the example thus manifests itself in the morphological disambiguation of a highly syncretic word form because of its government or agreement relation to its respective syntactic headdependents Of course the sequence model is most of the time a reasonable approximation because the majority of noun phrases in the TiGer corpus are not as long as the example in Figure 14 Furthermore not all languages show this kind of relationship be tween morphological forms and syntactic relation as demonstrated for German But taking advantage of the morphosyntactic dependencies in a language can give us better models that may even be capable of handling the more difficult or rare cases We therefore advocate that models for predicting morphology should be designed with the typological characteristics of a language and its morphosyntactic properties in mind and should where appropriate integrate 4We find 57551 noun phrases with less than three tokens between determiner and noun and 4670 with three or more syntactic information in order to better model the morphosyntactic interdependencies of the language In the remainder of the paper we show empirically that taking syntactic information into account produces stateoftheart models for languages with a high interdependency between morphology and syntax We use a simple setup where we combine a morphological tagger and a dependency parser in a bootstrapping architecture in order to analyze the effect of syntactic information on the performance of the morphological tagger Section 2 Using syntactic features in morphology prediction requires a syntactically annotated corpus for training a statistical parser which may not be available for languages with few resources We show in Section 3 that only very little syntactically annotated data is required to achieve the improvements We furthermore expect that the improved morphological information also improves parsing performance and present a preliminary experiment in Section 4 2 Experiments In this section we present a series of experiments that investigate the effect of syntactic information on the prediction of morphological features We start by describing our data sets and the system that we used for the experiments"], "abstract": ["Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies", "We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages", "We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in stateof theart models"], "inroduction": ["In this paper we investigate the interplay between syntax and morphology with respect to the task of assigning morphological descriptions or tags to each token of a sentence", "Specifically we examine the effect of syntactic information when it is integrated into the feature model of a morphological tag ger", "We test the effect of syntactic features on four languages  Czech German Hungarian and Spanish  and find that syntactic features improve our tag ger considerably for Czech and German but not for Hungarian and Spanish", "Our analysis of constructions that show morphosyntactic agreement suggests that syntactic features are important if the language shows frequent word form syncretisms1 that by morphological and syntactic means2 Different languages however use them to a different extent", "Languages like English encode grammatical information like the subject vs object status of an argument via word order whereas languages like Czech or Hungarian use different word forms", "Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models", "Linguistic theory e g Bresnan 2001 Melcuk 2009 suggests many interactions between morphology and syntax", "For example languages with a case system use different forms of the same word to mark different syntactic or semantic relations Blake 2001", "In many languages two words that participate in a syntactic relation show covariance in some or all of their morphological features socalled agreement Corbett 20063 Automatic annotation of morphology assigns morphological descriptions e g nominative singularmasculine to word forms", "It is usually modeled as a sequence model often in combination with partofspeech tagging and lemmatization Collins 2002 Hajic 2004 Smith et al 2005 Chrupaa et al 2008 and others", "Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships", "As an example for these hierarchical relationships can be disambiguated by the syntactic context", "The meaning of a sentence is structurally encoded 1 Syncretism describes the situation where a word form is ambiguous between several different morphological descriptions within its inflection paradigm", "since textbased tools rarely have access to this information", "3 For example in English the subject of a sentence and the", "finite verb agree with respect to their number and person feature", "NK NK CJ MO CJ MO PM MO CD NK MO die wirtschaftlich am weitesten entwickelten  modernen und zum Teil katholisch gepragten Regionen nomaccplfem nomaccplfem the economic  most developed  modern and to part catholic influenced regions the regions that are economically most developed modern and partly catholic Figure 1 Example of a German noun phrase", "First and last word agree in number gender and case value", "Figure 1 shows a German noun phrase taken from the German TiGer corpus Brants et al 2002", "The two boldfaced words are the determiner and the head noun of the phrase and they agree in their gender number and case values", "The word Regionen regions is fourway ambiguous for its case value which is reduced to a twoway ambiguity between nominative and accusative by the determiner", "Further disambiguation would require information about the syntactic role of the noun phrase in a sentence", "There are 11 tokens between these two words which would require a context window of at least 13 to capture the agreement relation within a sequence model", "Syntactically however as indicated by the dependency tree the determiner and the head are linked directly", "The interdependency between morphology and syntax in the example thus manifests itself in the morphological disambiguation of a highly syncretic word form because of its government or agreement relation to its respective syntactic headdependents", "Of course the sequence model is most of the time a reasonable approximation because the majority of noun phrases in the TiGer corpus are not as long as the example in Figure 14 Furthermore not all languages show this kind of relationship between morphological forms and syntactic relation as demonstrated for German", "But taking advantage of the morphosyntactic dependencies in a language can give us better models that may even be capable of handling the more difficult or rare cases", "We therefore advocate that models for predicting morphology should be designed with the typological characteristics of a language and its morphosyntactic properties in mind and should where appropriate integrate 4 We find 57551 noun phrases with less than three tokens between determiner and noun and 4670 with three or more", "syntactic information in order to better model the morphosyntactic interdependencies of the language", "In the remainder of the paper we show empirically that taking syntactic information into account produces stateoftheart models for languages with a high interdependency between morphology and syntax", "We use a simple setup where we combine a morphological tagger and a dependency parser in a bootstrapping architecture in order to analyze the effect of syntactic information on the performance of the morphological tagger Section 2", "Using syntactic features in morphology prediction requires a syntactically annotated corpus for training a statistical parser which may not be available for languages with few resources", "We show in Section 3 that only very little syntactically annotated data is required to achieve the improvements", "We furthermore expect that the improved morphological information also improves parsing performance and present a preliminary experiment in Section 4"]}, "D13-1060": {"title": ["Identifying Phrasal Verbs Using Many Bilingual Corpora"], "abstract": ["We address the problem of identifying multiword expressions in a language focusing on English phrasal verbs", "Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages", "Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel rankingoriented boosting algorithm produces a comprehensive set of English phrasal verbs achieving performance comparable to a humancurated set"], "inroduction": ["A multiword expression MWE or noncompositional compound is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words", "These idiosyncratic phrases are prevalent in the lexicon of a language Jackendoff 1993 estimates that their number is on the same order of magnitude as that of single words and Sag et al", "2002 suggest that they are much more common though quantifying them is challenging Church 2011", "The task of identifying MWEs is relevant not only to lexical semantics applications but also machine translation Koehn et al 2003 Ren et al 2009 Pal et al 2010 information retrieval Xu et al 2010 Acosta et al 2011 and syntactic parsing Sag et al 2002", "Awareness of MWEs has empirically proven useful in a number of domains Finlayson and Kulkarni 2011 for example use MWEs to attain a significant perfor We focus on a particular subset of MWEs English phrasal verbs", "A phrasal verb consists of a head verb followed by one or more particles such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words Baldwin and Villavicencio 2002 Dixon 1982 Bannard et al 20031 Examples of phrasal verbs include count on rely look after tend or take off remove the meanings of which do not involve counting looking or taking", "In contrast there are verbs followed by particles that are not phrasal verbs because their meaning is compositional such as walk towards sit behind or paint on", "We identify phrasal verbs by using frequency statistics calculated from parallel corpora consisting of bilingual pairs of documents such that one is a translation of the other with one document in English", "We leverage the observation that a verb will translate in an atypical way when occurring as the head of a phrasal verb", "For example the word look in the context of look after will tend to translate differently from how look translates generally", "In order to characterize this difference we calculate a frequency distribution over translations of look then compare it to the distribution of translations of look when followed by the word after", "We expect that idiomatic phrasal verbs will tend to have unexpected translation of their head verbs measured by the KullbackLeibler divergence between those distributions", "Our polyglot ranking approach is motivated by the hypothesis that using many parallel corpora of different languages will help determine the degree of semantic idiomaticity of a phrase", "In order to com mance improvement in word sense disambiguation Venkatapathy and Joshi 2006 use features associated with MWEs to improve word alignment", "Research conducted during an internship at Google", "1 Nomenclature varies the term verbparticle construction", "is also used to denote what we call phrasal verbs further the term phrasal verb is sometimes used to denote a broader class of constructions", "bine evidence from multiple languages we develop a novel boosting algorithm tailored to the task of ranking multiword expressions by their degree of id iomaticity", "We train and evaluate on disjoint subsets of the phrasal verbs in English Wiktionary2", "In our 3 1experiments the set of phrasal verbs identified au tomatically by our method achieves heldout recall that nears the performance of the phrasal verbs in WordNet 30 a humancurated set", "Our approach strongly outperforms a monolingual system and continues to improve when incrementally adding translation statistics for 50 different languages"]}, "D13-1141": {"title": ["Bilingual Word Embeddings for PhraseBased Machine Translation"], "abstract": ["We introduce bilingual word embeddings semantic embeddings associated across two languages in the context of neural language models", "We propose a method to learn bilingual embeddings from a large unlabeled corpus while utilizing MT word alignments to constrain translational equivalence", "The new em beddings significantly outperform baselines in word semantic similarity", "A single semantic similarity feature induced with bilingual em beddings adds near half a BLEU point to the results of NIST08 ChineseEnglish machine translation task"], "inroduction": ["It is difficult to recognize and quantify semantic similarities across languages", "The FrEn phrasepair un cas de force majeure case of absolute necessity ZhEn phrase pair persist in a stubborn manner are similar in semantics", "If co occurrences of exact word combinations are rare in the training parallel text it can be difficult for classical statistical MT methods to identify this similarity or produce a reasonable translation given the source phrase", "We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages", "As an extension to their monolingual counterpart Turian et al 2010 Huang et al 2012 Bengio et al 2003 bilingual embeddings capture not only semantic information of monolingual words but also semantic relationships across different languages", "This prop erty allows them to define semantic similarity metrics across phrasepairs making them perfect features for machine translation", "To learn bilingual embeddings we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence", "The latter utilizes word alignments a natural subtask in the machine translation pipeline", "Through large scale curriculum training Bengio et al 2009 we obtain bilingual distributed representations which lie in the same feature space", "Embeddings of direct translations overlap and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus", "Consequently we produce for the research community a first set of Mandarin Chinese word embed dings with 100000 words trained on the Chinese Gigaword corpus", "We evaluate these embedding on Chinese word semantic similarity from SemEval 2012 Jin and Wu 2012", "The embeddings significantly outperform prior work and pruned tfidf baselines", "In addition the learned embeddings give rise to 011 F1 improvement in Named Entity Recognition on the OntoNotes dataset Hovy et al 2006 with a neural network model", "We apply the bilingual embeddings in an endto end phrasebased MT system by computing semantic similarities between phrase pairs", "On NIST08 ChineseEnglish translation task we obtain an improvement of 048 BLEU from a competitive baseline 3001 BLEU to 3049 BLEU with the Stanford Phrasal MT system", "1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 13931398 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics"]}, "D13-1161": {"title": ["Scaling Semantic Parsers with Onthefly Ontology Matching"], "abstract": ["We consider the challenge of learning semantic parsers that scale to large opendomain problems such as question answering with Freebase", "In such settings the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology", "For example even simple phrases such as daughter and number of people living in cannot be directly represented in Freebase whose ontology instead encodes facts about gender parenthood and population", "In this paper we introduce a new semantic parsing approach that learns to resolve such ontological mismatches", "The parser is learned from questionanswer pairs uses a probabilistic CCG to build linguistically motivated logical form meaning representations and includes an ontology matching model that adapts the output logical forms for each target ontology", "Experiments demonstrate stateoftheart performance on two benchmark semantic parsing datasets including a nine point accuracy improvement on a recent Freebase QA corpus"], "inroduction": ["Semantic parsers map sentences to formal representations of their underlying meaning", "Recently algorithms have been developed to learn such parsers for many applications including question answering QA Kwiatkowski et al 2011 Liang et al 2011 relation extraction Krishnamurthy and Mitchell 2012 robot control Matuszek et al 2012 Krishnamurthy and Kollar 2013 interpreting instruc tions Chen and Mooney 2011 Artzi and Zettlemoyer 2013 and generating programs Kushman and Barzilay 2013", "In each case the parser uses a predefined set of logical constants or an ontology to construct meaning representations", "In practice the choice of ontology significantly impacts learning", "For example consider the following questions Q and candidate meaning representations MR Q1 What is the population of Seattle", "Q2 How many people live in Seattle", "MR1 xpopulationSeattle x MR2 countxpersonx  livex Seattle A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2 these pairings align constants count person etc directly to phrases How many people etc", "Unfortunately few ontologies have sufficient coverage to support both meaning representations for example many QA databases would only include the population relation required for MR1", "Most existing approaches would given this deficiency simply aim to produce MR1 for Q2 thereby introducing significant lexical ambiguity that complicates learning", "Such ontological mismatches become increasingly common as domain and language complexity increases", "In this paper we introduce a semantic parsing approach that supports scalable opendomain ontological reasoning", "The parser first constructs a linguistically motivated domainindependent meaning representation", "For example possibly producing MR1 for Q1 and MR2 for Q2 above", "It then uses a learned ontology matching model to transform this represen 1545 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pages 15451556 Seattle Washington USA 1821 October 2013", "Qc 2013 Association for Computational Linguistics x  How many people visit the public library of New York annually l0  xeqx countypeopley  evisity zpublicz  libraryz  of z new york e  annuallye y  xlibrarypublic library systemannual visitsx new york public library a  13554002 x  What works did Mozart dedicate to Joseph Haydn l0  xworksx  ededicatemozart x e  tohaydn e y  xdedicated workx  ededicated bymozart e  dedicationx e  dedicated tohaydn e a   String Quartet No 19 Haydn Quartets String Quartet No 16 String Quartet No 18 String Quartet No 17  Figure 1 Examples of sentences x domainindependent underspecified logical forms l0 fully specified logical forms y and answers a drawn from the Freebase domain", "tation for the target domain", "In our example producing either MR1 MR2 or another more appropriate option depending on the QA database schema", "This two stage approach enables parsing without any domaindependent lexicon that pairs words with logical constants", "Instead word meaning is filled in onthefly through ontology matching enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains", "Figure 1 shows the desired outputs for two example Freebase sentences", "The first parsing stage uses a probabilistic combinatory categorial grammar CCG Steedman 2000 Clark and Curran 2007 to map sentences to new underspecified logicalform meaning representations containing generic logical constants that are not tied to any specific ontology", "This approach enables us to share grammar structure across domains instead of repeatedly relearning different grammars for each target ontology", "The ontologymatching step considers a large number of typeequivalent domainspecific meanings", "It enables us to incorporate a number of cues including the target ontology structure and lexical similarity between the names of the domainindependent and dependent constants to construct the final logical forms", "During learning we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching", "Following a number of recent approaches Clarke et al 2010 Liang et al 2011 we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs", "This approach aligns naturally with our twostage parsing setup where the final logical expression can be directly used to provide answers", "We report performance on two benchmark datasets GeoQuery Zelle and Mooney 1996 and Freebase QA FQ Cai and Yates 2013a", "Geo Query includes a geography database with a small ontology and questions with relatively complex compositional structure", "FQ includes questions to Freebase a large communityauthored database that spans many subdomains", "Experiments demonstrate stateoftheart performance in both cases including a nine point improvement in recall for the FQ test"]}, "E06-1030": {"title": ["Web Text Corpus for Natural Language Processing"], "abstract": ["Web text has been successfully used as training data for many NLP applications", "While most previous work accesses web text through search engine hit counts we created a Web Corpus by downloading web pages to create a topicdiverse collection of 10 billion words of English", "We show that for contextsensitive spelling correction the Web Corpus results are better than using a search engine", "For thesaurus extraction it achieved similar overall results to a corpus of newspaper text", "With many more words available on the web better results can be obtained by collecting much larger web corpora"], "inroduction": ["Traditional written corpora for linguistics research are created primarily from printed text such as newspaper articles and books", "With the growth of the World Wide Web as an information resource it is increasingly being used as training data in Natural Language Processing NLP tasks", "There are many advantages to creating a corpus from web data rather than printed text", "All web data is already in electronic form and therefore readable by computers whereas not all printed data is available electronically", "The vast amount of text available on the web is a major advantage with Keller and Lapata 2003 estimating that over 98 billion words were indexed by Google in 2003", "The performance of NLP systems tends to improve with increasing amount of training data", "Banko and Brill 2001 showed that for context sensitive spelling correction increasing the training data size increases the accuracy for up to 1 billion words in their experiments", "To date most NLP tasks that have utilised web data have accessed it through search engines using only the hit counts or examining a limited number of results pages", "The tasks are reduced to determining ngram probabilities which are then estimated by hit counts from search engine queries", "This method only gathers information from the hit counts but does not require the computationally expensive downloading of actual text for analysis", "Unfortunately search engines were not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations Nakov and Hearst 2005", "Volk 2002 proposed a linguistic search engine to extract word relationships more accurately", "We created a 10 billion word topicdiverse Web Corpus by spidering websites from a set of seed URLs", "The seed set is selected from the Open Directory to ensure that a diverse range of topics is included in the corpus", "A process of text cleaning transforms the HTML text into a form useable by most NLP systems  tokenised words one sentence per line", "Text filtering removes unwanted text from the corpus such as nonEnglish sentences and most lines of text that are not grammatical sentences", "We compare the vocabulary of the Web Corpus with newswire", "Our Web Corpus is evaluated on two NLP tasks", "Contextsensitive spelling correction is a disambiguation problem where the correction word in aconfusion set eg their theyre needs to be se lected for a given context", "Thesaurus extraction is a similarity task where synonyms of a target word are extracted from a corpus of unlabelled text", "Our evaluation demonstrates that web text can be used for the same tasks as search engine hit counts and newspaper text", "However there is a much larger quantity of freely available web text to exploit"]}, "E06-2012": {"title": ["Maytag A multistaged approach to identifying"], "abstract": ["We present a novel application of NLP and text mining to the analysis of financial documents", "In particular we describe an implemented prototype Maytag which combines information extraction and subject classification tools in an interactive exploratory framework", "We present experimental results on their performance as tailored to the financial domain and some forwardlooking extensions to the approach that enables users to specify classifications on the fly"], "inroduction": ["Our goal is to support the discovery of complex events in text", "By complex events we mean events that might be structured out of multiple occurrences of other events or that might occur over a span of time", "In financial analysis the domain that concerns us here an example of what we mean is the problem of understanding corporate acquisition practices", "To gauge a companys modus operandi in acquiring other companies it isnt enough to know just that an acquisition occurred but it may also be important to understand the degree to which it was debtleveraged or whether it was performed through reciprocal stock exchanges", "In other words complex events are often composed of multiple facets beyond the basic event itself", "One of our concerns is therefore to enable end users to access complex events through a combination of their possible facets", "Another key characteristic of rich domains like financial analysis is that facts and events are subject to interpretation in context", "To a financial analyst it makes a difference whether a multimilliondollar loss occurs in the context of recurring operations a potentially chronic problem or in the context of a onetime event such as a merger or layoff", "A second concern is thus to enable end users to interpret facts and events through automated context assessment", "The route we have taken towards this end is to model the domain of corporate finance through an interactive suite of language processing tools", "Maytag our prototype makes the following novel contribution", "Rather than trying to model complex events monolithically we provide a range of multipurpose information extraction and text classification methods and allow the end user to combine these interactively", "Think of it as Boolean queries where the query terms are not keywords but extracted facts events entities and contextual text classifications"]}, "E09-1025": {"title": ["Proceedings of the 12th Conference of the European Chapter of the ACL pages 211 219 Athens Greece 30 March  3 April 2009 c2009 Association for Computational Linguistics Inference Rules and their Application to Recognizing Textual Entailment Georgiana Dinu Saarland University Campus D66123 Saarbru cken dinucoliunisbde Rui Wang Saarland University Campus D66123 Saarbru cken rwangcoliunisbde Abstract In this paper we explore ways of improving an inference rule collection and its ap plication to the task of recognizing textual entailment For this purpose we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a handcrafted lexical resource Following this we derive a dependencybased structure representation from texts which aims to provide a proper base for the inference rule application The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements 1 Introduction Textual inference plays an important role in many natural language processing NLP tasks In recent years the recognizing textual entailment RTE Dagan et al 2006 challenge which focuses on detecting semantic inference has attracted a lot of attention Given a text T several sentences and a hypothesis H one sentence the goal is to detect if H can be inferred from T Studies such as Clark et al 2007 attest that lexical substitution eg synonyms antonyms or simple syntactic variation account for the entailment only in a small number of pairs Thus one essential issue is to identify more complex expressions which in appropriate contexts convey the same or similar meaning However more generally we are also interested in pairs of expressions in which only a unidirectional inference relation holds1 1We will use the term inference rule to stand for such concept the two expressions can be actual paraphrases if the relation is bidirectional A typical example is the following RTE pair in which accelerate to in H is used as an alternative formulation for reach speed of in T"], "abstract": ["In this paper we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment", "For this purpose we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a handcrafted lexical resource", "Following this we derive a dependencybased structure representation from texts which aims to provide a proper base for the inference rule application", "The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements"], "inroduction": ["Textual inference plays an important role in many natural language processing NLP tasks", "In recent years the recognizing textual entailment RTE Dagan et al 2006 challenge which focuses on detecting semantic inference has attracted a lot of attention", "Given a text T several sentences and a hypothesis H one sentence the goal is to detect if H can be inferred from T Studies such as Clark et al 2007 attest that lexical substitution eg synonyms antonyms or simple syntactic variation account for the entail ment only in a small number of pairs", "Thus one essential issue is to identify more complex expressions which in appropriate contexts convey the same or similar meaning", "However more generally we are also interested in pairs of expressions in which only a unidirectional inference relation holds1", "1 We will use the term inference rule to stand for such concept the two expressions can be actual paraphrases if the relation is bidirectional A typical example is the following RTE pair in which accelerate to in H is used as an alternative formulation for reach speed of in T T The highspeed train scheduled for a trial run on Tuesday is able to reach a maximum speed of up to 430 kilometers per hour or 119 meters per second", "H The train accelerates to 430 kilometers per hour", "One way to deal with textual inference is through rule representation for example X wrote Y  X is author of Y However manually building collections of inference rules is timeconsuming and it is unlikely that humans can exhaustively enumerate all the rules encoding the knowledge needed in reasoning with natural language", "Instead an alternative is to acquire these rules automatically from large corpora", "Given such a rule collection the next step to focus on is how to successfully use it in NLP applications", "This paper tackles both aspects acquiring inference rules and using them for the task of recognizing textual entailment", "For the first aspect we extend and refine an existing collection of inference rules acquired based on the Distributional Hypothesis DH", "One of the main advantages of using the DH is that the only input needed is a large corpus of parsed text2", "For the extension and refinement a handcrafted lexical resource is used for augmenting the original inference rule collection and exclude some of the incorrect rules", "For the second aspect we focus on applying these rules to the RTE task", "In particular we use a structure representation derived from the dependency parse trees of T and H which aims to capture the essential information they convey", "The rest of the paper is organized as follows Section 2 introduces the inference rule collection 2 Another line of work on acquiring paraphrases uses comparable corpora for instance Barzilay and McKeown 2001 Pang et al 2003 Proceedings of the 12th Conference of the European Chapter of the ACL pages 211219 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics we use based on the Discovery of Inference Rules from Text henceforth DIRT algorithm and discusses previous work on applying it to the RTE task", "Section 3 focuses on the rule collection itself and on the methods in which we use an external lexical resource to extend and refine it", "Section 4 discusses the application of the rules for the RTE data describing the structure representation we use to identify the appropriate context for the rule application", "The experimental results will be presented in Section 5 followed by an error analysis and discussions in Section 6", "Finally Section 7 will conclude the paper and point out future work directions"]}, "E09-1045": {"title": ["An Empirical Study on Classbased Word Sense Disambiguation"], "abstract": ["As empirically demonstrated by the last SensEval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed", "One possible reason could be the use of inappropriate set of meanings", "In fact WordNet has been used as a defacto standard repository of meanings", "However to our knowledge the meanings represented by WordNet have been only used for WSD at a very finegrained sense level or at a very coarsegrained class level", "We suspect that selecting the appropriate level of abstraction could be on between both levels", "We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet", "We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform classbased Word Sense Disambiguation allowing accuracy figures over 80"], "inroduction": ["Word Sense Disambiguation WSD is an intermediate Natural Language Processing NLP task which consists in assigning the correct semantic interpretation to ambiguous words in context", "One of the most successful approaches in the last years is the supervised learning from examples in which statistical or Machine Learning classification models are induced from semantically annotated corpora Marquez et al 2006", "Generally supervised systems have obtained better results than the unsupervised ones as shown by experimental work and international evaluation exercises such This paper has been supported by the European Union under the projects QALLME FP6 IST033860 and KYOTO FP7 ICT211423 and the Spanish Government under the project TextMess TIN200615265C0601 and KNOW TIN200615049C0301 as Senseval1", "These annotated corpora are usually manually tagged by lexicographers with word senses taken from a particular lexical semantic resource most commonly WordNet2 WN Fell baum 1998", "WN has been widely criticized for being a sense repository that often provides too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broad coverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Possibly building classbased classifiers would allow to avoid the data sparseness problem of the wordbased approach", "Recently using WN as a sense repository the organizers of the English allwords task at SensEval3 reported an interannotation agreement of 725 Snyder and Palmer 2004", "Interestingly this result is difficult to outperform by stateoftheart sensebased WSD systems", "Thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldovan 2001 Agirre and LopezDeLaCalle 2003 Navigli 2006 and Snow et al 2007", "That is they provide methods for grouping senses of the same word thus producing coarser word sense groupings for better disambiguation", "Wikipedia3 has been also recently used to overcome some problems of automatic learning methods excessively finegrained definition of meanings lack of annotated data and strong domain dependence of existing annotated corpora", "In this way Wikipedia provides a new very large source of annotated data constantly expanded Mihalcea 2007", "1 httpwwwsensevalorg 2 httpwordnetprincetonedu 3 httpwwwwikipediaorg Proceedings of the 12th Conference of the European Chapter of the ACL pages 389397 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics In contrast some research have been focused on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 and Ciaramita and Altun 2006", "That is grouping senses of different words into the same explicit and comprehensive semantic class", "Most of the later approaches used the original Lexicographical Files of WN more recently called SuperSenses as very coarsegrained sense distinctions", "However not so much attention has been paid on learning classbased classifiers from other available sensegroupings such as WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 EuroWordNet Base Concepts Vossen et al 1998 Top Concept Ontology labels Alvez et al 2008 or Basic Level Concepts Izquierdo et al 2007", "Obviously these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD", "Possibly their combination could improve the overall results since they offer different semantic perspectives of the data", "Furthermore to our knowledge to date no comparative evaluation has been performed on SensEval data exploring different levels of abstraction", "In fact Villarejo et al 2005 studied the performance of classbased WSD comparing only SuperSenses and SUMO by 10fold crossvalidation on SemCor but they did not provide results for SensEval2 nor SensEval3", "This paper empirically explores on the supervised WSD task the performance of different levels of abstraction provided by WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 and Basic Level Concepts Izquierdo et al 2007", "We refer to this approach as classbased WSD since the classifiers are created at a class level instead of at a sense level", "Classbased WSD clusters senses of different words into the same explicit and comprehensive grouping", "Only those cases belonging to the same semantic class are grouped to train the classifier", "For example the coarser word grouping obtained in Snow et al 2007 only has one remaining sense for church", "Using a set of Base Level Concepts Izquierdo et al 2007 the three senses of church are still represented by faithn3 buildingn1 and religious ceremonyn1", "The contribution of this work is threefold", "We empirically demonstrate that a Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class based WSD b that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c that the classbased approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers", "After this introduction section 2 presents the sensegroupings used in this study", "In section 3 the approach followed to build the classbased system is explained", "Experiments and results are shown in section 4", "Finally some conclusions are drawn in section 5"]}, "E09-1072": {"title": ["Empirical evaluations of animacy annotation"], "abstract": ["This article presents empirical evaluations of aspects of annotation for the linguistic property of animacy in Swedish ranging from manual human annotation automatic classification and finally an external evaluation in the task of syntactic parsing", "We show that a treatment of animacy as a lexical semantic property of noun types enables generalization over distributional properties of these nouns which proves beneficial in automatic classification and furthermore gives significant improvements in terms of parsing accuracy for Swedish compared to a stateofthe art baseline parser with gold standard ani macy information"], "inroduction": ["The property of animacy influences linguistic phenomena in a range of different languages such as case marking Aissen 2003 and argument realization Bresnan et al 2005 de Swart et al 2008 and has been shown to constitute an important factor in the production and comprehension of syntactic structure Branigan et al 2008 Weckerly and Kutas 19991 In computational linguistic work animacy has been shown to provide important information in anaphora resolution Orasan and Evans 2007 argument disambiguation DellOrletta et al 2005 and syntactic parsing in general vrelid and Nivre 2007The dimension of animacy roughly distin guishes between entities which are alive and entities which are not however other distinctions 1 Parts of the research reported in this paper has been supported by the Deutsche Forschungsgemeinschaft DFG Son derforschungsbereich 632 project D4", "are also relevant and the animacy dimension is often viewed as a continuum ranging from humans to inanimate objects", "Following Silverstein 1976 several animacy hierarchies have been proposed in typological studies focusing on the linguistic category of animacy ie the distinctions which are relevant for linguistic phenomena", "An example of an animacy hierarchy taken from Aissen 2003 is provided in 1 1 Human  Animate  Inanimate Clearly nonhuman animates like animals are not less animate than humans in a biological sense however humans and animals show differing linguistic behaviourEmpirical studies of animacy require human an notation efforts and in particular a welldefined annotation task", "However annotation studies of animacy differ distinctly in their treatment of ani macy as a type or tokenlevel phenomenon as well as in terms of granularity of categories", "The use of the annotated data as a computational resource furthermore poses requirements on the annotation which do not necessarily agree with more theoretical considerations", "Methods for the induction of animacy information for use in practical applications require the resolution of issues of level of representation as well as granularityThis article addresses these issues through em pirical and experimental evaluation", "We present an indepth study of a manually annotated data set which indicates that animacy may be treated as a lexical semantic property at the type level", "We then evaluate this proposal through supervised machine learning of animacy information and focus on an indepth error analysis of the resulting classifier addressing issues of granularity of the animacy dimension", "Finally the automatically an Proceedings of the 12th Conference of the European Chapter of the ACL pages 630638 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics notated data set is employed in order to train a syntactic parser and we investigate the effect of the an imacy information and contrast the automatically acquired features with gold standard ones", "The rest of the article is structured as follows", "In section 2 we briefly discuss annotation schemes for animacy the annotation strategies and categories proposed there", "We go on to describe annotation for the binary distinction of human reference found in a Swedish dependency treebank in section 3 and we perform an evaluation of the consistency of the human annotation in terms of linguistic level", "In section 4 we present experiments in lexical acquisition of animacy based on morphosyntactic features extracted from a considerably larger corpus", "Section 5 presents experiments with the acquired animacy information applied in the datadriven dependency parsing of Swedish", "Finally section 6 concludes the article and provides some suggestions for future research"]}, "E09-1079": {"title": ["Proceedings of the 12th Conference of the European Chapter of the ACL pages 692 700 Athens Greece 30 March  3 April 2009 c2009 Association for Computational Linguistics"], "abstract": [""], "inroduction": ["There are various questions that need to be answered during the design of a tagset", "The granularity of the tagset is the first problem in this regard", "A tagset may consist either of general parts of speech only or it may consist of additional morphosyntactic categories such as number gender and case", "In order to facilitate the tagger training and to reduce the lexical and syntactic ambiguity we decided to concentrate on the syntactic categories of the language", "Purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging2 Marcus et al 1993", "Urdu is influenced from Arabic and can be considered as having three main parts of speech namely noun verb and particle Platts 1909 Javed 1981 Haq 1987", "However some grammarians proposed ten main parts of speech for Urdu Schmidt 1999", "The work of Urdu grammar writers provides a full overview of all the features of the language", "However in the perspective of the tagset their analysis is lackingthe computational grounds", "The semantic mor phological and syntactic categories are mixed in their distribution of parts of speech", "For exampleHaq 1987 divides the common nouns into sit uational smile sadness darkness locative park office morning evening instrumental knife sword and collective nouns army dataIn 2003 Hardie proposed the first com putational part of speech tagset for Urdu Hardie 2 A part of speech tagger for Indian languages available at httpshivaiiitacinSPSAL2007 iiittagsetguidelinespdf Proceedings of the 12th Conference of the European Chapter of the ACL pages 692700 Athens Greece 30 March  3 April 2009", "Qc 2009 Association for Computational Linguistics 2003a", "It is a morphosyntactic tagset based on the EAGLES guidelines", "The tagset contains 350 different tags with information about number gender case etc", "van Halteren 2005", "The EAGLES guidelines are based on three levels major word classes recommended attributes and optional attributes", "Major word classes include thirteen tags noun verb adjective pronoundeterminer article adverb adposition conjunction numeral interjection unassigned residual and punctuation", "The recommended attributes include number gender case finite ness voice etc3 In this paper we will focus on purely syntactic distributions thus will not go into the details of the recommended attributes of the EAGLES guidelines", "Considering the EAGLES guidelines and the tagset of Hardie in comparison with the general parts of speech of Urdu there are no articles in Urdu", "Due to the phrase level and semantic differences pronoun and demonstrative are separate parts of speech in Urdu", "In the Hardie tagset the possessive pronouns like Y", "mera my  t tumhara your   humara our are assigned to the category of possessive adjective", "Most of the Urdu grammarians consider them as pronouns Platts 1909 Javed 1981 Haq 1987", "However all these possessive pronouns require a noun in their noun phrase thus show a similar behavior as demonstratives", "The locative and temporal adverbs u t yahan here u J wahan there y ab now etc and the locative and temporal nouns   subah morning  ", "sham evening Y gher home appear in a very similar syntactic context", "In order to keep the structure of pronoun and noun consistent locative and temporal adverbs are treated as pronouns", "The tense and aspect of a verb in Urdu is represented by a sequence of auxiliaries", "Consider the example4 c  Y  Hai raha Ja kerta kam Jan Is Doing Kept Work John John is kept on doing work Table 1 The aspect of the verb Y kerta doing is represented by two separate words ja and  raha and the last word of the sentence c hai is shows the tense of the verb"]}, "E12-1020": {"title": ["Compensating for Annotation Errors in Training a Relation Extractor"], "abstract": ["The wellstudied supervised Relation Extraction algorithms require training data that is accurate and has good coverage", "To obtain such a gold standard the common practice is to do independent double annotation followed by adjudication", "This takes significantly more human effort than annotation done by a single annotator", "We do a detailed analysis on a snapshot of the ACE 2005 annotation files to understand the differences between singlepass annotation and the more expensive nearly threepass process and then propose an algorithm that learns from the much cheaper singlepass annotation and achieves a performance on a par with the extractor trained on multipass annotated data", "Furthermore we show that given the same amount of human labor the better way to do relation annotation is not to annotate with highcost quality assurance but to annotate more"], "inroduction": ["Relation Extraction aims at detecting and categorizing semantic relations between pairs of entities in text", "It is an important NLP task that has many practical applications such as answering factoid questions building knowledge bases and improving web search", "Supervised methods for relation extraction have been studied extensively since rich annotated linguistic resources eg the Automatic Content Extraction1 ACE training corpus were released", "We will give a summary of related methods in section 2", "Those methods rely on accurate and complete annotation", "To obtain high quality annotation the common wisdom is to let 1 httpwwwitlnistgoviadmigtestsace two annotators independently annotate a corpus and then asking a senior annotator to adjudicate the disagreements 2", "This annotation procedure roughly requires 3 passes3 over the same corpus", "Therefore it is very expensive", "The ACE 2005 annotation on relations is conducted in this way", "In this paper we analyzed a snapshot of ACE training data and found that each annotator missed a significant fraction of relation mentions and annotated some spurious ones", "We found that it is possible to separate most missing examples from the vast majority of truenegative unlabeled examples and in contrast most of the relation mentions that are adjudicated as incorrect contain useful expressions for learning a relation extractor", "Based on this observation we propose an algorithm that purifies negative examples and applies transductive inference to utilize missing examples during the training process on the singlepass annotation", "Results show that the extractor trained on singlepass annotation with the proposed algorithm has a performance that is close to an extractor trained on the 3pass annotation", "We further show that the proposed algorithm trained on a singlepass annotation on the complete set of documents has a higher performance than an extractor trained on 3pass annotation on 90 of the documents in the same corpus although the effort of doing a singlepass annotation over the entire set costs less than half that of doing 3 passes over 90 of the documents", "From the perspective of learning a highperformance relation extractor it suggests that a better way to do relation annotation is not to annotate with a highcost quality assurance but to annotate more"]}, "E12-1054": {"title": ["Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics pages 529538"], "abstract": ["We evaluate measures of contextual fitness on the task of detecting realword spelling errors", "For that purpose we extract naturally occurring errors and their contexts from the Wikipedia revision history", "We show that such natural errors are better suited for evaluation than the previously used artificially created errors", "In particular the precision of statistical methods has been largely overestimated while the precision of knowledgebased approaches has been underestimated", "Additionally we show that knowledgebased approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations", "Finally we show that statistical and knowledge based methods can be combined for increased performance"], "inroduction": ["Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition Inkpen and Desilets 2005 optical character recognition Wick et al 2007 coreference resolution Bean and Riloff 2004 or malapropism detection Bolshakov and Gelbukh 2003", "The main idea is always to test what fits better into the current context the actual term or a possible replacement that is phonetically structurally or semantically similar", "We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components eg the quality of the optical character recognition module Walker et al 2010", "A malapropism or realword spelling error occurs when a word is replaced with another correctly spelled word which does not suit the context eg People with lots of honey usually live in big houses where money was replaced with honey", "Besides typing mistakes a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word Hirst and Budanitsky 2005", "A realword spelling error is hard to detect as the erroneous word is not misspelled and fits syntactically into the sentence", "Thus measures of contextual fitness are required to detect words that do not fit their contexts", "Existing measures of contextual fitness can be categorized into knowledgebased Hirst and Budanitsky 2005 and statistical methods Mays et al 1991 WilcoxOHearn et al 2008", "Both test the lexical cohesion of a word with its context", "For that purpose knowledgebased approaches employ the structural knowledge encoded in lexicalsemantic networks like WordNet Fellbaum 1998 while statistical approaches rely on cooccurrence counts collected from large corpora eg the Google Web1T corpus Brants and Franz 2006", "So far evaluation of contextual fitness measures relied on artificial datasets Mays et al 1991 Hirst and Budanitsky 2005 which are created by taking a sentence that is known to be correct and replacing a word with a similar word from the vocabulary", "This has a couple of disadvantages i the replacement might be a synonym of the original word and perfectly valid in the given context ii the generated error might 529 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics pages 529538 Avignon France April 2327 2012", "Qc 2012 Association for Computational Linguistics be very unlikely to be made by a human and iii inserting artificial errors often leads to unnatural sentences that are quite easy to correct eg if the word class has changed", "However even if the word class is unchanged the original word and its replacement might still be variants of the same lemma eg a noun in singular and plural or a verb in present and past form", "This usually leads to a sentence where the error can be easily detected using syntactical or statistical methods but is almost impossible to detect for knowledgebased measures of contextual fitness as the meaning of the word stays more or less unchanged", "To estimate the impact of this issue we randomly sampled 1000 artificially created realword spelling errors1 and found 387 singularplural pairs and 57 pairs which were in another direct relation eg adjectiveadverb", "This means that almost half of the artificially created errors are not suited for an evaluation targeted at finding optimal measures of contextual fitness as they overestimate the performance of statistical measures while underestimating the potential of semantic measures", "In order to investigate this issue we present a framework for mining naturally occurring errors and their contexts from the Wikipedia revision history", "We use the resulting English and German datasets to evaluate statistical and knowledgebased measures", "We make the full experimental framework publicly available2 which will allow reproducing our experiments as well as conducting followup experiments", "The framework contains i methods to extract natural errors from Wikipedia ii reference implementations of the knowledgebased and the statistical methods and iii the evaluation datasets described in this paper"]}, "E95-1024": {"title": ["Offline Optimization for Earleystyle HPSG Processing"], "abstract": ["A novel approach to HPSG based natural language processing is described that uses an offline compiler to automatically prime a declarative grammar for generation or parsing and inputs the primed grammar to an advanced Earleystyle processor", "This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation", "Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar"], "inroduction": ["Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention Strzalkowski 1993a", "This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use see among others Appelt 1987", "We address this topic in describing a novel approach to HPSG Pollard and Sag 1994 based language processing that uses an offline compiler to automatically prime a declarative grammar for generation or parsing and hands the primed grammar to an advanced Earley processor", "The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars", "In this paper we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation", "Shieber 1988 gave the first use of Earleys algorithm for generation but this algorithm does not The presented research was sponsored by leilprojekt B4 Constraints on Grammar for Efficient Generation of the Sonderforschungsbereich 340 Sprachtheoretische Grundlagen fiir die Computerllnguistik of the Deutsche Forschungsgemeinschaft", "The authors wish to thank Paul King Detmar Meurers and Shuly Wintner for valuable comments and discussion", "Of course the authors are responsible for all remaining errors", "use the prediction step to restrict feature instantiations on the predicted phrases and thus lacks goal directedness", "Though Gerdemann 1991 showed how to modify the restriction function to make top down information available for the bottomup completion step Earley generation with topdown prediction still has a problem in that generating the sub parts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination", "Gerdemann 1991 partly overcame this problem by incorpQrating a headdriven strategy into Earleys algorithm", "However evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing displaced or underspecified", "Furthermore Martinovid and Strzalkowski 1992 and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules", "Strzalkowskis Essential Arguments Approach Ehh 1993b is a topdown approach to generation and parsing with logic grammars that uses offline compilation to automatically invert parseroriented logic grammars", "The inversion process consists of both the automatic static reordering of nodes in the grammar and the interchanging of arguments in rules with recursively defined heads", "It is based on the notion of essential arguments arguments which must be instantiated to ensure the efficient and terminating execution of a node", "Minnen et al", "1995 observe that the EAA is computationally infeasible because it demands the investigation of almost all possible permutations of a grammar", "Moreover the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent", "The Direct Inversion Approach DI of Minnen et al", "1995 overcomes these problems by making the reordering process more goaldirected and developing a reformulation technique that allows the successful treatment of rules which exhibit headrecursion", "Both the EAA and the DIA were presented as approaches to the inversion of parser 21 Optimizations oriented grammars into grammars suitable for generation", "However both approaches can just as well take a declarative grammar specification as input to produce generator andor parseroriented grammars as in Dymetman et al", "1990", "In this paper we adopt the latter theoretically more interesting perspective", "We developed a compiler for offline optimization of phrase structure rulebased typed feature structure grammars which generalizes the techniques developed in the context of the DIA and we advanced a typed extension of the Earleystyle generator of Gerdemann 1991", "Offline compilation section 3 is used to produce grammars for the Earleystyle generator section 2", "We show that our use of off line grammar optimization overcomes problems with empty or displaced heads", "The developed techniques are extensively tested with a large HPSG grammar for partial vP topicallzation in German iiinrichs et al 1994", "This uncovered some important constraints on the form of the phrase structure rules phrase structure rules in a grammar imposed by the compiler section 4", "Advanced Earley Generation As Shieber 1988 noted the main shortcoming of Earley generation is a lack of goaldirectedness that results in a proliferation of edges", "Gerdemann 1991 tackled this shortcoming by modifying the restriction function to make topdown information available for the bottomup completion step", "Gerdemanns generator follows a headdriven strategy in order to avoid inefficient evaluation orders", "More specifically the head of the righthand side of each grammar rule is distinguished and distinguished categories are scanned or predicted upon first", "The resulting evaluation strategy is similar to that of the headcorner approach Shieber et al 1990 Gerdemann and IIinrichs in press prediction follows the main flow of semantic information until a lexical pivot is reached and only then are the head dependent subparts of the construction built up in a bottomup fashion", "This mixture of topdown and bottomup information flow is crucial since the top down semantic information from the goal category must be integrated with the bottomup subcategorization information from the lexicon", "A strict top down evaluation strategy suffers from what may be called headrecursionie the generation analog of left recursion in parsing", "Shieber et al", "1990 show that a topdown evaluation strategy will fail for rules such as vP  vp x irrespective of the order of evaluation of the righthand side categories in the rule", "By combining the offline optimization process with a mixed bottomuptopdown evaluation strategy we can refrain from a complete reformulation of the grammar as for example in Minnen et al", "1995", "We further improved a typed extension of Gerdemanns Earley generator with a number of techniques that reduce the number of edges created during generation", "Three optimizations were especially helpful", "The first supplies each edge in the chart with two indices a backward index pointing to the state in the chart that the edge is predicted from and a forward index poinfing to the states that are predicted from the edge", "By matching forward and backward indices the edges that must be combined for completion can be located faster", "This indexing technique as illustrated below improves upon the more complex indices in Gerdemann 1991 and is closely related to OLDTresolution Tamaki and Sato 1986", "1 activeXoXlX212 2 activeXY1Y23 3 activeX2Y1Y2 iy 4 passiveX2 Y1 I2 o 2 Active edge 2 resulted from active edge 1 through prediction", "The backward index of edge 2 is therefore identified with the forward index of edge 1", "Completion of an active edge results in an edge with identical backward index", "In the case of our example this would be the steps from edge 2 to edge 3 and edge 3 to edge 4", "As nothing gets predicted from a passive edge 4 it does not have a forward index", "In order to use passive edge 4 for completion of an active edge we only need to consider those edges which have a forward index identical to the backward index of 4", "The second optimization creates a table of the categories which have been used to make predictions from", "As discussed in Gerdemann 1991 such a table can be used to avoid redundant predictions without a full and expensive subsumption test", "The third indexes lexical entries which is necessary to obtain constanttime lexical access", "The optimizations of our Earleygenerator lead to significant gains in efficiency", "However despite these heuristic improvements the problem of goal directedness is not solved", "22 Empty Heads", "Empty or displaced heads present the principal goal directedness problem for any headdriven generation approach Shieber et al 1990 K6nig 1994 Gerdemann and IIinrichs in press where empty head refers not just to a construction in which the head has an empty phonology but to any construction in which the head is partially unspecified", "Since 174 phonology does not guide generation the phonological realization of the head of a construction plays no part in the generation of that construction", "To better illustrate the problem that underspecified heads pose consider the sentence Hal Karl Marie gekiflt f Has Karl Marie kissed", "Did Karl kiss Mary for which we adopt the argument composition analysis presented in Hinrichs and Nakazawa 1989 the subeat list of the auxiliary verb is partially instantiated in the lexicon and only becomes fully instantiated upon its combination with its verbal complement the main verb", "The phrase structure rule that describes this construction is 1 cat subcat 0 cont cat v cat v lJ fin  aux  subcat  31l r1  L ubt EI  I I Lcont  Though a headdriven generator must generate first the head of the rule nothing prescribes the order of generation of the complements of the head", "If the generator generates second the main verb then the subcat list of the main verb instantiates the subcat list of the head and generation becomes a deterministic procedure in which complements are generated in sequence", "However if the generator generates second some complement other than the main verb then the subcat list of the head contains no restricting information to guide deterministic generation and generation becomes a generateandtest procedure in which complements are generated at random only to be eliminated by further unifications", "Clearly then the order of evaluation of the complements in a rule can profoundly influence the efficiency of generation and an efficient headdriven generator must order the evaluation of the complements in a rule accordingly", "23 Offline versus Online", "Dynamic online reordering can solve the ordering problem discussed in the previous subsection but is rather unattractive interpreting grammar rules at 1For expository reasons we refrain from a division between the subject and the other complements of a verb as in chapter 9 of Pollard and Sag 1994", "The testgrammar does make this division and always guarantees the correct order of the complements on the comps list with respect to the obliqueness hierarchy", "Furthermore we use abbreviations of paths such as coat for syasemlloccoat  and assume that the semantics principle is encoded in the phrase structure rule", "run time creates much overhead and locally determining the optimal evaluation order is often impossible", "Goalfreezing can also overcome the ordering problem but is equally unappealing goalfreezing is computationally expensive it demands the procedural annotation of an otherwise declarative grammar specification and it presupposes that a grammar writer possesses substantial computational processing expertise", "We chose instead to deal with the ordering problem by using offline compilation to automatically optimize a grammar such that it can be used for generation without additional provision for dealing with the evaluation order by our Earley generator", "3 Offline Grammar Optimization", "Our offline grammar optimization is based on a generalization of the dataflow analysis employed in the DIA to a dataflow analysis for typed feature structure grammars", "This dataflow analysis takes as input a specification of the paths of the start category that are considered fully instantiated", "In case of generation this means that the user annotates the path specifying the logical form ie the path cont or some of its subpaths as bound", "We use the type hierarchy and an extension of the unification and generalization operations such that path annotations are preserved to determine the flow of semantic information between the rules and the lexical entries in a grammar", "Structure sharing determines the dataflow within the rules of the grammar", "The dataflow analysis is used to determine the relative efficiency of a particular evaluation order of the righthand side categories in a phrase structure rule by computing the maximal degree of nondeterminacy introduced by the evaluation of each of these categories", "The maximal degree of nondeterminacy introduced by a righthand side category equals the maximal number of rules andor lexical entries with which this category unifies given its binding annotations", "The optimal evaluation order of the right hand side categories is found by comparing the maximal degree of nondeterminacy introduced by the evaluation of the individual categories with the degree of nondeterminacy the grammar is allowed to introduce if the degree of nondeterminacy introduced by the evaluation of one of the righthand side categories in a rule exceeds the admissible degree of nondeterminacy the ordering at hand is rejected", "The degree of nondeterminacy the grammar is allowed to introduce is originally set to one and consecutively incremented until the optimal evaluation order for all rules in the grammar is found", "31 Example", "The compilation process is illustrated on the basis of the phrase structure rule for argument composition discussed in 22", "Space limitations force us to abstract over the recursive optimization of the rules defining the righthand side categories through considering only the defining lexical entries", "Unifying the user annotated start category with the lefthand side of this phrase structure rule leads to the annotation of the path specifying the logical form of the construction as bound see below", "As a result of the structuresharing between the lefthand side of the rule and the auxiliary verb category the contvalue of the auxiliary verb can be treated as bound as well", "In addition the paths with a value of a maximal specific type for which there are no appropriate features specified for example the path cat can be considered bound subcatboa cont b o ", "a llbnd   flnbound8UXbound   53  2  lUXbond  ubt I El L ubt 3157 LcOntboa  On the basis of this annotated rule we investigate the lexical entries defining its righthand side categories", "The auxiliary verb category is unified with its defining lexical entries under preservation of the binding annotations", "The following is an example of such a lexical entry", "Note that subpaths of a path marked as bound are considered bound too", "cttbound V finbou4  UXbond I subcat contbou4  contboaInucleusboaI argboa The binding annotations of the lexical entries defining the auxiliary verb are used to determine with how many lexical entries the righthand side category of the rule maximally unifies ie its maximal degree of nondeterminacy", "In this case the maximal degree of nondeterminacy that the evaluation of the auxiliary verb introduces is very low as the logical form of the auxiliary verb is considered fully instantiated", "Now we mark the paths of the defining lexical entries whose instantiation can be deduced from the type hierarchy", "To mimic the evaluation of the auxiliary verb we determine the information common to all defining lexical entries by taking their generalization ie the most specific feature structure subsuming all and unify the result with the original righthand side category in the phrase structure rule", "Because both the generalization and the unification operations preserve binding annotations this leads via structuresharing to the annotation that the logical form of the verbal complement can be considered instantiated", "Note that the nonverbal complements do not become further instantiated", "By subsequent investigation of the maximal degree of nondeterminacy introduced by the evaluation of the complements in various permutations we find that the logical form of a sentence only restricts the evaluation of the nonverbal complements after the evaluation of the verbal complement", "This can be verified on the basis of a sample lexical entry for a main verb", "phon lieben cat v fin aux subcat  coat 6 cont rT  ro o cnt  numeus loved r j The relative efficiency of this evaluation leads our compiler to choose cat  subcat   cont cat vfin 4 UX L   aUX i Lubt 531  Lcont  as the optimal evaluation order of our phrase structure rule for argument composition", "32 Processing Head", "The optimal evaluation order for a phrase structure rule need not necessarily be headfirst", "Our dataflow anMysis treats heads and complements alike and includes the head in the calculation of the optimal evaluation order of a rule", "If the evaluation of the head of a rule introduces much nondeterminacy or provides insufficient restricting information for the evaluation of its complements our dataflow analysis might not select the head as the first category to be evaluated and choose instead subcat  cont pat v  It at v4 fin  aux ux   N  Lsubeat Fill  L nt   as the optimal evaluation order", "This clearly demonstrates an extremely important consequence of using our dataflow analysis to compile a declarative grammar into a grammar optimized for generation", "Empty or displaced heads pose us no problem since the optimal evaluation order of the righthand side of a rule is determined regardless of the head", "Our dataflow analysis ignores the grammatical head but identifies instead the processing head and no less 176 importantly the first processing complement the second processing complement and so on", "4 Constraints on Grammar", "Our Earley generator and the described compiler for offline grammar optimization have been extensively tested with a large HPSG grammar", "This test grammar is based on the implementation of an analysis of partial vP topicalization in German Hinrichs et al 1994 in the Troll system Gerdemann and King 1994", "Testing the developed techniques uncovered important constraints on the form of the phrase structure rules in a grammar imposed by the compiler", "41 Complement Displacement", "The compiler is not able to find an evaluation order such that the Earley generator has sufficient restricting information to generate all subparts of the construction efficiently in particular cases of complement displacement", "More specifically this problem arises when a complement receives essential restricting information from the head of the construction from which it has been extracted while at the same time it provides essential restricting information for the complements that stayed behind", "Such a case is represented schematically in figure 1 see next page", "The first processing complement el of the head H has been displaced", "This is problematic in case cl provides essential bindings for the successful evaluation of the complement c2", "cl can not be evaluated prior to the head and once H is evaluated it is no longer possible to evaluate cl prior to c2", "An example of problematic complement displacement taken from our testgrammar is given in figure 2 see next page", "The topicalized partial vP Anna lichen receives its restricting semantic information from the auxiliary verb and upon its evaluation provides essential bindings not only for the direct object but also for the subject that stayed behind in the Mittelfeld together with the auxiliary verb", "These mutual dependencies between the sub constituents of two different local trees lead either to the unrestricted generation of the partial vP or to the unrestricted generation of the subject in the Mittelfeld", "We handled this problem by partial execution Pereira and Shieber 1987 of the fillerhead rule", "This allows the evaluation of the filler right after the evaluation of the auxiliary verb but prior to the subject", "A headdriven generator has to rely on a similar solution as it will not be able to find a successful ordering for the local trees either simply because it does not exist", "42 Generalization", "A potential problem for our approach constitutes the requirement that the phrase structure rules in the grammar need to have a particular degree of specificity for the generalization operation to be used successfully to mimic its evaluation", "This is best illustrated on the basis of the following more schematic phrase structure rule cat l i at fin 1 v subcat ", " ffNN ubcat    Lcont cont Underspecification of the head of the rule allows it to unify with both finite auxiliaries and finite ditransitive main verbs", "In combination with the underspecification of the complements this allows the rule not only to be used for argument composition constructions as discussed above but also for constructions in which a finite main verb becomes saturated", "This means that the logical form of the nonverbal complements if and  becomes available either upon the evaluation of the complement tagged  in case of argument composition or upon the evaluation of the finite verb in case the head of the rule is a ditransitive main verb", "As a result the use of generalization does not suffice to mimic the evaluation of the respective righthand side categories", "Because both verbal categories have defining lexical entries which do not instantiate the logical form of the nonverbal arguments the dataflow analysis leads to the conclusion that the logical form of the nonverbal complements never becomes instantiated", "This causes the rejection of all possible evaluation orders for this rule as the evaluation of an unrestricted nonverbal complement clearly exceeds the allowed maximal degree of nondeterminacy of the grammar", "We are therefore forced to split this schematic phrase structure rule into two more specific rules at least during the optimization process", "It is important to note that this is a consequence of a general limitation of dataflow analysis see also Mellish 1981", "5 Concluding Remarks", "An innovative approach to HPSG processing is described that uses an offline compiler to automatically prime a declarative grammar for generation or parsing and inputs the primed grammar to an advanced Earley processor", "Our offline compiler extends the techniques developed in the context of the DIA in that it compiles typed feature structure grammars rather than simple logic grammars", "The approach allows efficient bidirectional processing with similar generation and parsing times", "It is shown that combining offline techniques with an advanced Earleystyle generator provides an elegant solution to the general problem that empty or displaced heads pose for conventional headdriven generation", "The developed offline compilation techniques make crucial use of the fundamental properties of the HPSG formalism", "The monostratal uniform treatment of syntax semantics and phonology supports H  C2 l Figure 1 Complement displacement", "c at v subcat  lover karl", " cont nucleus I arg nucleus loved anna  Fcat  ll isL  Fsubcat Lcont slash cont  cat v  fin fin  aux  Fat n Fat n subcat g lease m  case subcat  cont cont slh D slash  eont Illnueleuslarg  lover cont  nucleus loved NI Anna lieben wird Karl", "Anna love will Karl", "Karl will love Anna Figure 2 Example of problematic complement displacement", "178", "dataflow analysis which is used extensively to provide the information upon which offline compilation is based", "Our compiler uses the type hierarchy to determine paths with a value of a minimal type without appropriate features as bound", "However the equivalent of this kind of minimal types in untyped feature structure grammars are constants which can be used in a similar fashion for offline optimization"]}, "E99-1007": {"title": ["Automatic  Verb Classification Using"], "abstract": ["We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes based on distributional approximations of diathe ses extracted from a very large anno tated corpus", "Distributions of four gram matical features are sufficient to reduce error rate by 50 over chance", "We con clude that corpus data is a usable repos itory of verb class information and that corpusdriven extraction of grammatical features is a promising methodology for automatic lexical acquisition"], "inroduction": ["Recent years have witnessed a shift in grammar development methodology from crafting large grammars to annotation of corpora", "Correspond ingly there has been a change from developing rulebased parsers to developing statistical meth ods for inducing grammatical knowledge from an notated corpus data", "The shift has mostly oc curred because building widecoverage grammars is timeconsuming error prone and difficult", "The same can be said for crafting the rich lexical rep resentations that are a central component of lin guistic knowledge and research in automatic lex ical acquisition has sought to address this Dorr and Jones 1996 Dorr 1997 among others", "Yet there have been few attempts to learn fine grained lexical classifications from the statisti cal analysis of distributional data analogously to the induction of syntactic knowledge though see eg Brent 1993 Klavans and Chodorow 1992 Resnik 1992", "In this paper we propose such an approach for the automatic classification of verbs into lexical semantic classes1 We can express the issues raised by this ap proach as follows", "1", "Which linguistic distinctions among lexical", "classes can we expect to find in a corpus"]}, "E99-1024": {"title": [""], "abstract": ["In this paper we propose a practical method to detect Japanese homophone errors in Japanese texts", "It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently", "In order to detect ho mophone errors we have only to solve the homophone problem", "We can use the decision list to do it because the homo phone problem is equivalent to the word sense disambiguation problem", "However the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot", "In this pa per we incorporate the written word into the original decision list by obtaining the identifying strength of the written word", "The improved decision list can raise the Fmeasure of error detection"], "inroduction": ["In this paper we propose a method of detect ing Japanese homophone errors in Japanese texts", "Our method is based on a decision list proposed by Yarowsky Yarowsky 1994 Yarowsky 1995", "We improve the original decision list by using writ ten words in the default evidence", "The imprqved decision list can raise the Fmeasure of error de tection", "Most Japanese texts are written using Japanese word processors", "To input a word composed of kanji characters we first input the phonetic hira gana sequence for the word and then convert it to the desired kanji sequence", "However multiple converted kanji sequences are generally produced and we must then choose the correct kanji se quence", "Therefore Japanese texts suffer from ho mophone errors caused by incorrect choices", "Care lessness of choice alone is not the cause of homo phone errors Ignorance of the difference among homophone words is serious", "For example many Japanese are not aware of the difference between i and i V or between fil and filll 1 In this paper we define the term homophone set as a set of words consisting of kanji charac ters that have the same phone 2  Then we define the term homophone word as a word in a ho mophone set", "For example the set  proba bility JL establishment is a homophone set because words in the set are composed of kanji characters that have the same phone kakuritu", "Thus  and  JL are homophone words", "In this paper we name the problem of choosing the correct word from the homophone set the homo phone problem", "In order to detect homophone errors we make a list of homophone sets in ad vance find a homophone word in the text and then solve the homophone problem for the homo phone word", "Many methods of solving the homophone prob lem have been proposed Tochinai et al 1986 lbuki et al 1997 Oku and Matsuoka 1997 Oku 1994 Wakita and Kaneko 1996", "However they are restricted to the homophone problem that is they are heuristic methods", "On the other hand the homophone problem is equivalent to the word sense disambiguation problem if the phone of the homophone word is regarded as the word and the homophone word as the sense", "Therefore we can solve the homophone problem by using various 1ift and  ", " have a same phone ishi", "The meaning of ifi is a general will and the meaning of ifE is a strong positive will", " and lil have a same phone chokkan", "The meaning of is an intuition through a feeling and the meaning of lil is an intuition through a latent knowledge", "2 We ignore the difference of accents stresses and parts of speech", "That is the homophone set is the set of words having the same expression in hiragana characters", "statistical methods proposed for the word sense disambiguation problemFujii 1998", "Take the case of contextsensitive spelling error detection 3 which is equivalent to the homophone problem", "For that problem some statistical methods have been applied and succeededGolding 1995 Gold ing and Schabes 1996", "Hence statistical meth ods are certainly valid for the homophone prob lem", "In particular the decision list is valid for the homophone problemShinnou 1998", "The de cision list arranges evidences to identify the word sense in the order of strength of identifying the sense", "The word sense is judged by the evidence with the highest identifying strength in the con text", "Although the homophone problem is equivalent to the word sense disambiguation problem the former has a distinct difference from the latter", "In the homophone problem almost all of the an swers are given correctly because almost all of the expressions written in the given text are correct", "It is difficult to decide which is the meaning of crane crane of animal or crane of tool", "How ever it is almost right that the correct expression of JiilL in a text is not lit but Jii3t", "In the homophone problem the choice of the writ ten word results in high precision", "We should use this information", "However the method to always choose the written word is useless for error detec tion because it doesnt detect errors at all", "The method used for the homophone problem should be evaluated from the precision and the recall of the error detection", "In this paper we evaluate it by the Fmeasure to combine the precision and the recall and use the written word to raise the Fmeasure of the original decision list", "We use the written word as an evidence of the decision list", "The problem is how much strength to give to that evidence", "If the strength is high the precision rises but the recall drops", "On the other hand if the strength is low the decision list is not improved", "In this paper we calculate the strength that gives the maximum Fmeasure in a training corpus", "As a result our decision list can raise the Fmeasure of error detection"]}, "E99-1031": {"title": ["A Flexible Architecture for Reference Resolution"], "abstract": ["This paper describes an architecture for performing anaphora resolution in a flexible way", "Systems which con form to these guidelines are well encapsulated and portable and can be used to compare anaphora resolu tion techniques for new language un derstanding applications", "Our im plementation of the architecture in a pronoun resolution testing platform demonstrates the flexibility of the ap proach"], "inroduction": ["When building natural language understand ing systems choosing the best technique for anaphora resolution is a challenging task", "The system builder must decide whether to adopt an existing technique or design a new approach", "A huge variety of techniques are described in the literature many of them achieving high suc cess rates on their own evaluation texts cf", "Hobbs 1986 Strube 1998 Mitkov 1998", "Each technique makes different assumptions about the data available to reference resolution for ex ample some assume perfect parses others as sume only POStagged input some assume se mantic information is available etc The chances are high that no published technique will ex actly match the data available to a particular sys tems reference resolution component so it may The authors thank James Allen for help on this project as well as the anonymous reviewers for helpful comments on the paper", "This material is based on work supported by USAFRome Labs contract F3060295l0025 ONR grant N0001495l1088 and Columbia Univ grant OPGI307", "not be apparent which method will work best", "Choosing a technique is especially problematic for designers of dialogue systems trying to pre dict how anaphora resolution techniques devel oped for written monologue will perform when adapted for spoken dialogue", "In an ideal world the system designer would implement and com pare many techniques on the input data available in his system", "As a good software engineer he would also ensure that any pronoun resolution code he implements can be ported to future ap plications or different language domains without modification", "The architecture described in this paper was designed to provide just that functionality", "Anaphora resolution code developed within the architecture is encapsulated to ensure portabil ity across parsers language genres and domains", "Using these architectural guidelines a testbed system for comparing pronoun resolution tech niques has been developed at the University of Rochester", "The testbed provides a highly config urable environment which uses the same pronoun resolution code regardless of the parser frontend and language type under analysis", "It can be used inter alia to compare anaphora resolution tech niques for a given application to compare new techniques to published baselines or to compare a particular techniques performance across lan guage types"]}, "H01-1052": {"title": ["Mitigating the PaucityofData Problem Exploring the"], "abstract": ["In this paper we discuss experiments applying machine learning techniques to the task of confusion set disambiguation using three orders of magnitude more training data than has previously been used for any disambiguationinstringcontext problem", "In an attempt to determine when current learning methods will cease to benefit from additional training data we analyze residual errors made by learners when issues of sparse data have been significantly mitigated", "Finally in the context of our results we discuss possible directions for the empirical natural language research community", "Keywords Learning curves data scaling very large corpora natural language disambiguation"], "inroduction": ["A significant amount of work in empirical natural language processing involves developing and refining machine learning techniques to automatically extract linguistic knowledge from online text corpora", "While the number of learning variants for various problems has been increasing the size of training sets such learning algorithms use has remained essentially unchanged", "For instance for the muchstudied problems of part of speech tagging base noun phrase labeling and parsing the Penn Treebank first released in 1992 remains the de facto training corpus", "The average training corpus size reported in papers published in the ACLsponsored Workshop on Very Large Corpora was essentially unchanged from the 1995 proceedings to the 2000 proceedings", "While the amount of available online text has been growing at an amazing rate over the last five years by some estimations there are currently over 500 billion readily accessible words on the web the size of training corpora used by our field has remained static", "Confusable word set disambiguation the problem of choosing the correct use of a word given a set of words with which it is commonly confused eg to too two your youre is a prototypical problem in NLP", "At some level this task is identical to many other natural language problems including word sense disambiguation determining lexical features such as pronoun case and determiner number for machine translation part of speech tagging named entity labeling spelling correction and some formulations of skeletal parsing", "All of these problems involve disambiguating from a relatively small set of tokens based upon a string context", "Of these disambiguation problems lexical confusables possess the fortunate property that supervised training data is free since the differences between members of a confusion set are surfaceapparent within a set of wellwritten text", "To date all of the papers published on the topic of confusion set disambiguation have used training sets for supervised learning of less than one million words", "The same is true for most if not all of the other disambiguationinstringcontext problems", "In this paper we explore what happens when significantly larger training corpora are used", "Our results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues rather than continuing to explore different learning methods applied to the relatively small extant training corpora"]}, "H05-1001": {"title": ["Improving LSAbased Summarization with Anaphora Resolution"], "abstract": ["We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver and using Singular Value Decomposition S V D to identify the main terms", "We demonstrate that adding anaphoric information results in significant performance improvements over a previously developed system in which only lexical terms are used as the input to S V D However we also show that how anaphoric information is used is crucial whereas using this information to add new terms does result in improved performance simple substitution makes the performance worse"], "inroduction": ["Many approaches to summarization can be very broadly characterized as T E R M BA S E D they attempt to identify the main topics which generally are T E R M S and then to extract from the document the most important information about these terms Hovy and Lin 1997", "These approaches can be divided again very broadly in lex Kennedy 1999 Azzam et al 1999 Bergler et al 2003 Stuckardt 2003 identify these terms by running a coreference or anaphoric resolver over the text1 We are not aware however of any attempt to use both lexical and anaphoric information to identify the main terms", "In addition to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure a reference corpus and baseline and widely accepted evaluation measures", "In this paper we compare two sentence extraction based summarizers", "Both use Latent Semantic Analysis L S A Landauer 1997 to identify the main terms of a text for summarization however the first system Steinberger and Jezek 2004 discussed in Section 2 only uses lexical information to identify the main topics whereas the second system exploits both lexical and anaphoric information", "This second system uses an existing anaphora resolution system to resolve anaphoric expressions G U I  TA R Poesio and Kabadjov 2004 but crucially two different ways of using this information for summarization were tested", "Section 3", "Both sum marizers were tested over the C A S T corpus Orasan et al 2003 as discussed in Section 4 and sig ical approaches among which we would include L S Abased approaches and coreferencebased approaches  Lexical approaches to termbased sum marization use lexical relations to identify central terms Barzilay and Elhadad 1997 Gong and Liu 2002 coreference or anaphora based approaches Baldwin and Morton 1998 Boguraev and 1 The terms anaphora resolution and coreference resolu", "tion have been variously defined Stuckardt 2003 but the latter term is generally used to refer to the coreference task as defined in M U C and AC E We use the term anaphora resolution to refer to the task of identifying successive mentions of the same discourse entity realized via any type of noun phrase proper noun definite description or pronoun and whether such discourse entities refer to objects in the world or not", "1 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing HLTEMNLP pages 18 Vancouver October 2005", "Qc 2005 Association for Computational Linguistics nificant improvements were observed over both the baseline C A S T system and our previous L S Abased summarizer"]}, "H05-1003": {"title": ["Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language"], "abstract": ["We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses", "Experiments show that this new framework can improve performance on two quite different languages  English and Chinese"], "inroduction": ["Reference resolution has proven to be a major obstacle in building robust systems for information extraction question answering text summarization and a number of other natural language processing tasks", "Most reference resolution systems use representations built out of the lexical and syntactic attrib utes of the noun phrases or mentions for which reference is to be established", "These attributes may involve string matching agreement syntactic dis tance and positional information and they tend to rely primarily on the immediate context of the noun phrases with the possible exception of sen tencespanning distance measures such as Hobbs distance", "Though gains have been made with such methods Tetreault 2001 Mitkov 2000 Soon et al 2001 Ng and Cardie 2002 there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly", "Coreference is by definition a semantic relationship two noun phrases corefer if they both forms of semantic information in order to resolve hard cases", "If for example two nouns refer to people who work for two different organizations we want our system to infer that these noun phrases cannot corefer", "Further progress will likely be aided by flexible frameworks for representing and using the information provided by this kind of semantic relation between noun phrases", "This paper tries to make a small step in that direction", "It describes a robust reference resolver that incorporates a broad range of semantic information in a general news domain", "Using an ontology that describes relations between entities the Automated Content Extraction program1 relation ontology along with a training corpus annotated for relations under this ontology we first train a classifier for identifying relations", "We then apply the output of this relation tagger to the task of reference resolution", "The rest of this paper is structured as follows", "Section 2 briefly describes the efforts made by previous researchers to use semantic information in reference resolution", "Section 3 describes our own method for incorporating documentlevel semantic context into coreference decisions", "We propose a representation of semantic context that isolates a particularly informative structure of interactionbetween semantic relations and coreference", "Sec tion 4 explains in detail our strategies for using relation information to modify coreference deci sions and the linguistic intuitions behind these strategies", "Section 5 then presents the system architectures and algorithms we use to incorporate rela tional information into reference resolution", "refer to the same realworld entity", "We should therefore expect a successful coreference system to exploit world knowledge inference and other 1 The ACE task description can be found at", "httpwwwitlnistgoviad89401testsace and the ACE guidelines at httpwwwldcupenneduProjectsACE 17 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing HLTEMNLP pages 1724 Vancouver October 2005", "Qc 2005 Association for Computational Linguistics Section 6 presents the results of experiments on both English and Chinese test data", "Section 7 presents our conclusions and directions for future work"]}, "H05-1083": {"title": ["MultiLingual Coreference Resolution With Syntactic  Features"], "abstract": ["In this paper we study the impact of a group of features extracted automatically from machinegenerated parse trees on coreference resolution", "One focus is on designing syntactic features using the binding theory as the guideline to improve pronoun resolution although linguistic phenomenon such as apposition is also modeled", "These features are applied to the Arabic Chinese and English coreference resolution systems and their effectiveness is evaluated on data from the Automatic Content Extraction ACE task", "The syntactic features improve the Arabic and English systems significantly but play a limited role in the Chinese one", "Detailed analyses are done to understand the syntactic features impact on the three coreference systems"], "inroduction": ["A coreference resolution system aims to group together mentions referring to the same entity where a mention is an instance of reference to an object and the collection of mentions referring to the same object in a document form an entity", "In the following example I John believes himself to be the best student mentions are underlined", "The three mentions John himself  the best student are of type name pronoun 1  and nominal respectively", "They form an entity since they all refer to the same person", "Syntactic information plays an important role in coreference resolution", "For example the binding theory Haege man 1994 Beatrice and Kroch 2000 provides a good account of the constraints on the antecedent of English pronouns", "The theory relies on syntactic parse trees to determine the governing category which defines the scope 1 Pronoun in this paper refers to both anaphor and normal pronoun", "of binding constraints", "We will use the theory as a guideline to help us design features in a machine learning framework", "Previous pronoun resolution work Hobbs 1976 Lappin and Leass 1994 Ge et al 1998 Stuckardt 2001 explicitly utilized syntactic information before", "But there are unique challenges in this study 1 Syntactic information is extracted from parse trees automatically generated", "This is possible because of the availability of statistical parsers which can be trained on humanannotated tree banks Marcus et al 1993 Xia et al 2000 Maamouri and Bies 2004 for multiple languages 2 The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system 3 The syntactic features are evaluated on three languages Arabic Chinese and English one goal is to see if features motivated by the English language can help coreference resolution in other languages", "All con trastive experiments are done on publiclyavailable data 4 Our coreference system resolves coreferential relationships among all the annotated mentions not just for pronouns", "Using machinegenerated parse trees eliminates the need of handlabeled trees in a coreference system", "However it is a major challenge to extract useful information from these noisy parse trees", "Our approach is encoding the structures contained in a parse tree into a set of computable features each of which is associated with a weight automatically determined by a machine learning algorithm", "This contrasts with the approach of extracting rules and assigning weights to these rules by hand Lap pin and Leass 1994 Stuckardt 2001", "The advantage of our approach is robustness if a particular structure is helpful it will be assigned a high weight if a feature is extracted from a highly noisy parse tree and is not informative in coreference resolution it will be assigned a small weight", "By avoiding writing rules we automatically incorporate useful information into our model and at the same time limit the potentially negative impact from noisy parsing output", "660 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing HLTEMNLP pages 660667 Vancouver October 2005", "Qc 2005 Association for Computational Linguistics"]}, "H89-2014": {"title": ["Augmenting a Hidden Markov Model"], "abstract": [], "inroduction": ["The paper describes refinements that are currently being investigated in a model for partofspeech assignment to words in unrestricted text", "The model has the advantage that a pretagged training corpus is not required", "Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabularyindependent model", "State chains are used to model selective higherorder conditioning in the model which obviates the proliferation of parameters attendant in uniformly higherorder models", "The structure of the state chains is based on both an analysis of errors and linguistic knowledge", "Examples show how word dependency across phrases can be modeled"]}, "I05-1065": {"title": ["Anaphora Resolution for Biomedical Literature by"], "abstract": ["In this paper a resolution system is presented to tackle nominal and pronominal anaphora in biomedical literature by using rich set of syntactic and semantic features", "Unlike previous researches the verification of semantic association between anaphors and their antecedents is facilitated by exploiting more outer resources including UMLS WordNet GENIA Corpus 302p and PubMed", "Moreover the resolution is implemented with a genetic algorithm on its feature selection", "Experimental results on different biomedical corpora showed that such approach could achieve promising results on resolving the two common types of anaphora"], "inroduction": ["Correct identification of antecedents for an anaphor is essential in message understanding systems as well as knowledge acquisition systems", "For example efficient anaphora resolution is needed to enhance protein interaction extraction from biomedical literature by mining more protein entity instances which are represented with pronouns or general concepts", "In biomedical literature pronominal and nominal anaphora are the two common types of anaphora", "In past literature different strategies to identify antecedents of an anaphor have been presented by using syntactic semantic and pragmatic clues", "For example grammatical roles of noun phrases were used in 9 10", "In addition to the syntactic information statistical information like cooccurring patterns obtained from a corpus is employed during antecedent finding in 3", "However a large corpus is needed for acquiring sufficient cooccurring patterns and for dealing with data sparseness", "On the other hand outer resources like WordNet1 are applied in 41215 and proved to be helpful to improve the system like the one described in 12 where ani macy information is exploited by analyzing the hierarchical relation of nouns and verbs in the surrounding context learned from WordNet", "Nevertheless using Word Net alone for acquiring semantic information is not sufficient for solving unknown words", "To tackle this problem a richer resource the Web was exploited in 16 1 httpwordnetprincetonedu where anaphoric information is mined from Google search results at the expense of less precision", "The domainspecific ontologies like UMLS2 Unified Medical Language System has been employed in 2 in such a way that frequent semantic types associated to agent subject and patient object role of subjectaction or actionobject patterns can be extracted", "The result showed such kind of patterns could gain increase in both precision 76 to 80 and recall 67 to 71", "On the other hand Kim and Park 11 built their BioAR to relate protein names to SWISSProt entries by using the centering theory presented by 7 and salience measures by 2", "In this paper a resolution system is presented for tackling both nominal anaphora and pronominal anaphora in biomedical literature by using various kinds of syntactic and semantic features", "Unlike previous approaches our verification of the semantic association between anaphors and their antecedents is facilitated with the help of both general domain and domainspecific resources", "For example the semantic type checking for resolving nominal anaphora can be done by the domain ontology UMLS and PubMed3 the search engine for MEDLINE databases", "Here UMLS is used not only for tagging the semantic type for the noun phrase chunks if they are in UMLS but also for generating the key lexicons for each type so that we can use them to tag those chunks if they are not in UMLS", "If no type information can be obtained from an chunk then its type finding will be implemented through the web mining of PubMed", "On the other hand the domain corpus GENIA 302p corpus 20 is exploited while we solve the semantic type checking for pronominal anaphora", "With simple weight calculation the key SAAO subjectaction or actionobject patterns for each type can be mined from the corpus and they turn out to be helpful in resolution", "Beside the semantic type agreement the implicit resemblance between an anaphor and its antecedents is another evidence useful for verifying the semantic association", "Hence the general domain thesaurus WordNet which supporting more relationship between concepts and subconcepts is also employed to enhance the resemblance extraction", "The presented resolution system is constructed on a basis of a salience grading", "In order to boost the system we implemented a simple genetic algorithm on its selection of the rich feature set", "The system was developed on the small evaluation corpus MedStract 4  Nevertheless we constructed a larger test corpus denoted as 100MEDLINE so that more instances of anaphors can be resolved", "Experimental results show that our resolution on MedStract can yield 92 and 78 FScores on resolving pronominal and nominal anaphora respectively", "Promising results were also obtained on the larger corpus in terms of 8743 and 8061 Fscores on resloving pronominal and nominal anaphora respectively"]}, "I05-2040": {"title": ["Transformation Based Chinese Entity Detection and Tracking"], "abstract": ["This paper proposes a unified Transformation Based Learning TBL Brill 1995 framework for Chinese Entity Detection and Tracking EDT", "It consists of two sub models a mention detection model and an entity trackingcoreference model", "The first submodel is used to adapt existing Chinese word segmentation and Named Entity NE recognition results to a specific EDT standard to find all the mentions", "The second submodel is used to find the coreference relation between the mentions", "In addition a feedback technique is proposed to further improve the performance of the system", "We evaluated our methods on the Automatic Content Extraction ACE NIST 2003 Chinese EDT corpus", "Results show that it outperforms the baseline and achieves comparable performance with the state oftheart methods"], "inroduction": ["The task of Entity Detection and Tracking EDT is suggested by the Automatic Content Extraction ACE project NIST 2003", "The goal is to detect all entities in a given text and track all mentions that refer to the same entity", "The task is a fundamental to many Natural LanguageProcessing NLP applications such as informa tion retrieval and extraction text classification summarization question answering and machine translation", "EDT is an extension of the task of coreference resolution in that in EDT we not only resolve the coreference between mentions but also detect the entities", "Each of those entities may have one or more mentions", "In the ACE project there are five types of entities defined in EDT person PER geography political Entity GPE organization ORG location LOC and facility FAC", "Many traditional coreference techniques can be extended to EDT for entity tracking", "Early work on pronoun anaphora resolution usually uses rulebased methods eg Hobbs 1976 Ge et al 1998 Mitkov 1998 which try to mine the cues of the relation between the pronouns and its antecedents", "Recent research Soon et al 2001 Yang et al 2003 Ng and Cardie 2002 Ittycherah et al 2003 Luo et al 2004 focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases including namenominal and pronoun phrase", "One common ap proach applied by them is to first train a binary statistical model to measure how likely a pair of  This work is done while the first author is visiting Microsoft Research Asia", "mentions corefer and then followed by a greedy procedure to group the mentions into entities", "Mention detection is to find all the named entity noun or noun phrase pronoun or pronoun phrase", "Therefore it needs Named Entity Rec ognition but not only", "Though the detection of entity mentions is an essential problem for EDTcoreference there has been relatively less previous research", "Ng and Cardie 2002 shows that improving the recall of noun phrase identification can improve the performance of a coreference system", "Florian et al", "2004 formulate the mention detection problem as a characterbased classification problem", "They assign for each character in the text a label indicating whether it is the start of a specific mention inside a specific mention or outside of any mention", "In this paper we propose a unified EDT model based on the Transformation Based Learning TBL Brill 1995 framework for Chinese", "The model consists of two sub models a mention detection model and a coreference model", "The first submodel is used to adapt existing Chinese word segmentation and Named Entity NE recognition system to a specific EDT standard", "TBL is a widely used machine learning method but it is the first time it is applied to coreference resolution", "In addition a feedback technique is proposed to further improve the performance of the system", "The rest of the paper is organized as follows", "Raw Document MSRSegPOS Tagging SegPOSNE Document Coreference Model Mentions Entities Mention Detection Model In section 2 we propose the unified TBL Chinese EDT model framework", "We describe the four key techniques of our Chinese EDT the word segmentation adaptation model the mention detection model the coreference model and the feedback technique in section 3 4 5 and 6 accordingly", "The experimental results on the ACE Chinese EDT corpus are shown in section 7"]}, "I05-3013": {"title": [""], "abstract": ["Informal language is actively used in networkmediated communication eg chat room BBS email and text message", "We refer the anomalous terms used in such context as network informal language NIL expressions", "For example ou3 is used to replace  wo3 in Chinese ICQ", "Without unconventional resource knowledge and techniques the existing natural language processing approaches exhibit less effectiveness in dealing with NIL text", "We propose to study NIL expressions with a NIL corpus and investigate techniques in processing NIL expressions", "Two methods for Chinese NIL expression recognition are designed in NILER system", "The experimental results show that pattern matching method produces higher precision and support vector machines method higher F1 measure", "These results are encouraging and justify our future research effort in NIL processing"], "inroduction": ["The rapid global proliferation of Internet applications has been showing no deceleration since the new millennium", "For example in commerce more and more physical customer servicescall centers are replaced by Internet solutions eg via MSN ICQ etc Network informal language NIL is actively used in these applications", "Following this trend we forecast that NIL would become a key language for human communication via network", "Today NIL expressions are ubiquitous", "They appear for example in chat rooms BBS email text message etc There is growing importance in understanding NIL expressions from both technology and humanity research points of view", "For instance comprehension of customeroperator dialogues in the aforesaid commercial application would facilitate effective Customer Relationship Management CRM", "Recently sociologists showed many interests in studying impact of networkmediated communication on language evolution from psychological and cognitive perspectives Danet 2002 McElhearn 2000 Nishimura 2003", "Researchers claim that languages have never been changing as fast as today since inception of the Internet and the language for Internet communication ie NIL gets more concise and effective than formal language", "Processing NIL text requires unconventional linguistic knowledge and techniques", "Unfortunately developed to handle formal language text the existing natural language processing NLP approaches exhibit less effectiveness in dealing with NIL text", "For example we use ICTCLAS Zhang et al 2003 tool to process sentence  Is he going to attend a meeting", "The word segmentation result is ", "In this sentence     xi4 ba1 xi4 is a NIL expression which means is he  in this case", "It can be concluded that without identifying the expression further Chinese text processing techniques are not able to produce reasonable result", "This problem leads to our recent research in NIL is Not Nothing project which aims to produce techniques for NIL processing thus avails understanding of change patterns and behaviors in language particularly in Internet language evolution", "The latter could make us more adaptive to the dynamic language environment in the cyber worldRecently some linguistic works have been car ried out on NIL for English", "A shared dictionary has been compiled and made available online", "It contains 308 English NIL expressions including English abbreviations acronyms and emoticons", "Similar efforts for Chinese are rare", "This is because Chinese language has not been widely used on the Internet until ten years ago", "Moreover Chinese NIL expression involves processing of Chinese Pinyin and dialects which results in higher complexity in Chinese NIL processing", "In NIL is Not Nothing project we develop a comprehensive Chinese NIL dictionary", "This is a difficult task because resource of NIL text is rather restricted", "We download a collection of BBS text from an Internet BBS system and construct a NIL corpus by annotating NIL expressions in this collection by hand", "An empirical study is conducted on the NIL expressions with the NIL corpus and a knowledge mining tool is designed to construct the NIL dictionary and generate statistical NIL features automatically", "With these knowledge and resources the NIL processing system ie NILER is developed to extract NIL expressions from NIL text by employing stateoftheart information extraction techniques", "The remaining sections of this paper are organized as follow", "In Section 2 we observe formation of NIL expressions", "In Section 3 we present the related works", "In Section 4 we describe NIL corpus and the knowledge engineering component in NIL dictionary construction and NIL features generation", "In Section 5 we present the methods for NIL expression recognition", "We outline the experiments discussions and error analysis in Section 6 and finally Section 7 concludes the paper"]}, "I05-3030": {"title": ["Chinese Word Segmentation  based on Mixing Model"], "abstract": ["This paper presents our recent work for participation in the Second International Chinese Word Segmentation Bakeoff", "According to difficulties we divide word segmentation into several subtasks which are solved by mixed language models so as to take advan2 ELUS Segmenter All the words are categorized into five types Lexicon words LW Factoid words FT Morphologically derived words MDW Named entities NE and New words NW", "Accordingly four main modules are included to identify each kind of words as shown in Figure 1", "tage of each approach in addressing special problems", "The experiment indicated that this system achieved 967 and 972 in Fmeasure in PKU and MSR open test respectively"], "inroduction": ["Word is a logical semantic and syntactic unit in natural language", "So word segmentation is the foundation of most Chinese NLP tasks", "Though much progress has been made in the last two decades there is no existing model that can solve all the problems perfectly at present", "So we try to apply different language models to Sentence Basic Segmentation NE Recognization NW Detection Disambiguation String Factoid Detect Lexicon words Morphology Word Result solve each special subtask due to No Free Lunch Theorem and Ugly Duckling Theorem", "Our system participated in the Second International Chinese Word Segmentation Bakeoff henceforce the bakeoff held in 2005", "Recently we have done more work in dealing with three main subtasks 1 Segmentation disambigua Figure 1 ELUS Segmenter Classbased trigram model Gao 2004 is adopted in the Basic Segmentation to convert the sentence into a word sequence", "Let w  w1 w2 wn be a word class sequence then the most likely word class sequence w in trigram is n w arg max Pw  w w   tion 2 Named entities recognition 3 New w w w  i i 2 i 1 words1 detection", "We apply different approachs to solve above three problems and all the mod ules are integrated into a pragmatic system ELUS", "Due to the limitation of available resource some kinds of features eg POS have been erased in our participation system", "This segmenter will be briefly describled in this paper", "1 New words refer to this kind of outof vocabulary words", "that are neither recognized named entities or factoid words nor morphological words", "1 2 n i 1 where let Pw0w2 w1 be Pw0 and let Pw1w1 w0 be Pw1w0", "And wi represents LW or a type of FT or MDW", "Viterbi algorithm is used to search the best candidate", "Absolute smoothing algorithm is applied to overcome the data sparseness", "Here LW FT and MDW are idendified Zhao Yan 2005", "All the Factoid words can be represented as regular expressions", "As a result the detection of factoid words can be archieved by Finite State Machines", "Four kinds of Named entities are detected ie Chinese person name foreign person name location name and orgnization name", "This is the most complicated module in ELUS", "Three kinds of models are applied here", "HMM model one order is described as n ELUS the most segmentation errors are one segmentation errors about 95", "ie the two words on both sides of current segmentation errors are right", "These include LW ambiguities and FT ambiguities etc Here we adopt Maximum Entropy model", "The same as other modules it is defined over HhT in segmentation T  arg max PW  T PT  T   i i T1T2 Tn i 1 i i 1 disambiguation where H is the set of possible contexts around target word that will be tagged where Ti represents the tag of current word and T is the set of allowable tags", "Then the Viterbi algorithm is used to search the best path", "models conditional probability is defined as Another model is Maximum Entropy Zhao Jian 2005 Hai Leong Chieu 2002", "Take Chinese person name as example", "Firstly we combine pt  h ph t  t T ph t    where HMM and Maximum Entropy ME model to lable the person name tag eg CPB CPI CPI Tongmei Yao Secondly the tagged ph t k SP D j 1 f j  ht  j name is merged by combining ME Model and Support Vector Machine SVM and some aided rules eg merged into  in PKU test", "Some complex features are added into ME model described in Zhao Jian 2005 in addition we also collect more than 110000 person names and acquire the statistic about common name characters these kinds of features are also fused into the ME model to detect NE", "The other kinds of NE recognition adopt similar method except for individual features", "New Words is another important kind of OOV words especially in closed test", "Take PKU test as example we collect NW suffixes such as citylamp", "Those usually construct new words eg sighting lamp", "A variancebased method is used to collect suffixes", "And three points need to be considered1 It is tail of many words2 It has large variance in constructing word3 It is seldom used alone", "We acquire about 25 common suffixes in PKU training corpus by above method", "We use Local Maximum Entropy model eg  1 1Huanggang city ie only the nearer characters are judged before the suffix  city", "By our approach the training corpus can be generated via given PKU corpus in thebakeoff", "The features come from the nearer con text besides common single words and punctuations are not regarded as a part of New Word", "The last module is Word Disambiugation", "Word segmentation ambiguities are usually classified into two classes overlapping ambiguity and combination ambiguity", "By evaluating where h is current context and t is one of the possible tags", "The ambiguous words are mainly collected by evaluating our system", "In NE module and Word Disambiguation module we introduce rough rule features which are extracted by Rough Set Wang Xiaolong 2004 eg display ability  only can just person  the reporterpersonreport", "Previous ex periment had indicated word disambiguation could achieve better performance by applying Rough Set", "3 Performance and", "analysis The performance of ELUS in the bakeoff is presented in Table 1 and Table 2 respectively in terms of recallR precisionP and F score in percentages", "Table 1 Closed test in percentages  Table 2 Open test in percentages  Our system has good performance in terms of Fmeasure in simplified Chinese open test including PKU and MSR open test", "In addition its IV word identification performance is remarkable ranging from 977 to 991 stands at the top or nearer the top in all the tests in which we have participated", "This good performance owes to classbased trigram absolute smoothing and word disambiguation module and rough rules", "There is almost the same IV performance between open test and closed test in MSR CITYU and AS respectively because we adopt the same Lexicon between open test and closed test respectively", "While in open test of PKU we adopt another Lexicon that comes from sixmonth corpora of Peoples Daily China in 1998 which were also annotated by Peking University", "The OOV word identification performance seems uneven compared with PKU the other tests seem lower due to the following reasons 1 Because of our resource limitation NE training resource is sixmonth corpora of Peoples Daily China in 1998 which came from Peking University and some newspapers and web pages annotated by our laboratory 2 We have no traditional Chinese corpus so the NE training resource for CITYU and AS is acquired via converting above corpora", "Since these corpora are converted from simplified Chinese they are not well suitable to traditional Chinese corpora 3 The different corpora have different criterions in NE detection especially in location name and organization name eg    Cuicun Town Xiangtang Hogpen in PKU and  in MSR criterion", "Even if our system recognizes the   as a orgnization name we are not easily to regard     as one word in PKU since  isnt a lexical word", "However in MSR that is easy because its criterion regard the whole Orgnization as a word 4 We need do more to comply with the segmentation criterion eg outlier in CITYU come from    while this kind of false segment is due to our bad understanding to CITYU criterion", "Though there are above problems our system does well in regonization precision since we adopt two steps in recognizing NE especial in recognizing Chinese person name", "And from the result of evalution in the bakeoff we need to improve the NE recall in the future", "In order to make our New words comply with the criterion we conservatively use New Word Detection module in order to avoid having bad recognition result since each corpus has its own New Word criterion", "4 Conclusion and Future work", "We have briefly describled our system based on mixed models", "Different approachs are adopted to solve each special subtask since there is No Free Lunch Theorem", "And mixed models are used in NE detection", "This sytem has a good performance in the simplified Chinese in the bakeoff", "The future work is mainly concentrating on two directions finding effective features and delicately adjusting internal relations among different modules in order to improve segmentation performance"]}, "I08-1004": {"title": ["ContextSensitive Convolution Tree Kernel"], "abstract": ["This paper proposes a contextsensitive convolution tree kernel for pronoun resolution", "It resolves two critical problems in previous researches in two ways", "First given a parse tree and a pair of an anaphor and an antecedent candidate it implements a dynamicexpansion scheme to automatically determine a proper tree span for pronoun resolution by taking predicate and antecedent competitorrelated information into consideration", "Second it applies a contextsensitive convolution tree kernel which enumerates both contextfree and contextsensitive subtrees by considering their ancestor node paths as their contexts", "Evaluation on the ACE 2003 corpus shows that our dynamicexpansion tree span scheme can well cover necessary structured information in the parse tree for pronoun resolution and the contextsensitive tree kernel much outperforms previous tree kernels"], "inroduction": ["It is well known that syntactic structured information plays a critical role in many critical NLP applications such as parsing semantic role labeling semantic relation extraction and coreference resolution", "However it is still an open question on what kinds of syntactic structured information are effective and how to well incorporate such structured information in these applications", "Much research work has been done in this direction", "Prior researches apply featurebased methods to select and define a set of flat features which can be mined from the parse trees to represent particular structured information in the parse tree such as the grammatical role eg subject or object according to the particular application", "Indeed such featurebased methods have been widely applied in parsing Collins 1999 Charniak 2001 semantic role labeling Pradhan et al 2005 semantic relation extraction Zhou et al 2005 and coreference resolution Lapin and Leass 1994 Aone and Bennett 1995 Mitkov 1998 Yang et al 2004 Luo and Zitouni 2005 Bergsma and Lin 2006", "The major problem with featurebased methods on exploring structured information is that they may fail to well capture complex structured information which is critical for further performance improvement", "The current trend is to explore kernelbased methods Haussler 1999 which can implicitly explore features in a high dimensional space by employing a kernel to calculate the similarity between two objects directly", "In particular the kernelbased methods could be very effective at reducing the burden of feature engineering for structured objects in NLP eg the parse tree structure in coreference resolution", "During recent years various tree kernels such as the convolution tree kernel Collins and Duffy 2001 the shallow parse tree kernel Zelenko et al 2003 and the dependency tree kernel Culota and Sorensen 2004 have been proposed in the literature", "Among previous tree kernels the convolution tree kernel represents the stateoftheart and have been successfully applied by Collins and Duffy 2002 on parsing Moschitti 2004 on semantic role labeling Zhang et al 2006 on semantic relation extraction and Yang et al 2006 on pronoun resolution", "However there exist two problems in Collins and Duffys kernel", "The first is that the subtrees enumerated in the tree kernel are contextfree", "That is each subtree enumerated in the tree kernel does not consider the context information outside the subtree", "The second is how to decide a proper tree span in the tree kernel computation according to the particular application", "To resolve above two problems this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic structured information in pronoun resolution whose task is to find the corresponding antecedent for a given pronominal anaphor in text", "The rest of this paper is organized as follows", "In Section 2 we review related work on exploring syntactic structured information in pronoun resolution and their comparison with our method", "Section 3 first presents a dynamicexpansion tree span scheme by automatically expanding the shortest path to include necessary structured information such as predicate and antecedent competitor related information", "Then it presents a context sensitive convolution tree kernel which not only enumerates contextfree subtrees but also context sensitive subtrees by considering their ancestor node paths as their contexts", "Section 4 shows the experimental results", "Finally we conclude our work in Section 5"]}, "I08-1070": {"title": ["Computing Paraphrasability of Syntactic Variants Using Web Snippets Atsushi Fujita Satoshi Sato Graduate School of Engineering Nagoya University fujitassatonueenagoyauacjp Abstract In a broad range of natural language processing tasks largescale knowledgebase of paraphrases is anticipated to improve their performance The key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability ie paraphrasability between given pair of expressions This paper addresses the issues of computing paraphrasability focusing on syntactic variants of predicate phrases Our model estimates paraphrasability based on traditional distributional similarity measures where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases Several feature sets are evaluated through empirical experiments 1 Introduction One of the common characteristics of human languages is that the same concept can be expressed by various linguistic expressions Such linguistic variations are called paraphrases Handling paraphrases is one of the key issues in a broad range of natural language processing NLP tasks In information retrieval information extraction and question answering technology of recognizing if or not the given pair of expressions are paraphrases is desired to gain a higher coverage On the other hand a system which generates paraphrases for given expressions is useful for texttranscoding tasks such as machine translation and summarization as well as beneficial to human for instance in texttospeech text simplification and writing assistance Paraphrase phenomena can roughly be divided into two groups according to their compositionality Examples in 1 exhibit a degree of compositionality while each example in 2 is composed of totally different lexical items"], "abstract": ["In a broad range of natural language processing tasks largescale knowledgebase of paraphrases is anticipated to improve their performance", "The key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability ie paraphrasability between given pair of expressions", "This paper addresses the issues of computing paraphrasability focusing on syntactic variants of predicate phrases", "Our model estimates paraphrasability based on traditional distributional similarity measures where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases", "Several feature sets are evaluated through empirical experiments"], "inroduction": ["One of the common characteristics of human languages is that the same concept can be expressed by various linguistic expressions", "Such linguistic variations are called paraphrases", "Handling paraphrases is one of the key issues in a broad range of natural language processing NLP tasks", "In information retrieval information extraction and question answering technology of recognizing if or not the given pair of expressions are paraphrases is desired to gain a higher coverage", "On the other hand a system which generates paraphrases for given expressions is useful for texttranscoding tasks such as machine translation and summarization as well as beneficial to human for instance in texttospeech text simplification and writing assistance", "Paraphrase phenomena can roughly be divided into two groups according to their compositionality", "Examples in 1 exhibit a degree of compositionality while each example in 2 is composed of totally different lexical items", "1 a be in our favor  be favorable for us b show a sharp decrease  decrease sharply Fujita et al 2007 2 a burst into tears  cried b comfort  console Barzilay and McKeown 2001 A number of studies have been carried out on both compositional morphosyntactic and non compositional lexical and idiomatic paraphrases see Section 2", "In most research paraphrases have been represented with the similar templates such as shown in 3 and 4", "3 a N1 V N2  N1 s V ing of N2 b N1 V N2  N2 be V en by N1 Harris 1957 4 a X wrote Y  X is the author of Y b X solves Y  X deals with Y Lin and Pantel 2001 The weakness of these templates is that they should be applied only in some contexts", "In other words the lack of applicability conditions for slot fillers may lead incorrect paraphrases", "One way to specify the applicability condition is to enumerate correct slot fillers", "For example Pantel et al", "2007 have harvested instances for the given paraphrase templates based on the cooccurrence statistics of slot fillers and lexicalized part of templates eg deal with in 4b", "Yet there is no method which assesses semantic equivalence and syntactic substitutability of resultant pairs of expressions", "In this paper we propose a method of directly computing semantic equivalence and syntactic sub stitutability ie paraphrasability particularly focusing on automatically generated compositional paraphrases henceforth syntactic variants of predicate phrases", "While previous studies have mainly targeted at words or canned phrases we treat predicate phrases having a bit more complex structures", "This paper addresses two issues in handling phrases", "The first is feature engineering", "Generally speaking phrases appear less frequently than single words", "This implies that we can obtain only a small amount of information about phrases", "To overcome the data sparseness problem we investigate if the Web snippet can be used as a dense corpus for given phrases", "The second is the measurement of paraphrasability", "We assess how well the traditional distributional similarity measures approximate the paraphrasability of predicate phrases"]}, "I08-2080": {"title": ["Japanese Named Entity Recognition"], "abstract": ["This paper presents an approach that uses structural information for Japanese named entity recognition NER", "Our NER system is based on Support Vector Machine SVM and utilizes four types of structural information cache features coreference relations syntactic features and caseframe features which are obtained from structural analyses", "We evaluated our approach on CRL NE data and obtained a higher Fmeasure than existing approaches that do not use structural information", "We also conducted experiments on IREX NE data and an NEannotated web corpus and conrmed that structural information improves the performance of NER"], "inroduction": ["Named entity recognition NER is the task of identifying and classifying phrases into certain classes of named entities NEs such as names of persons organizations and locations", "Japanese texts which we focus on are written without using blank spaces", "Therefore Japanese NER has tight relation with morphological analysis and thus it is often performed immediately after morphological analysis Masayuki and Matsumoto 2003 Yamada 2007", "However such approaches rely only on local context", "The Japanese NER system proposed in Nakano and Hirai 2004 which achieved the highest Fmeasure among conventional systems introduced the bunsetsu1 feature in order to consider wider context but considers only adjacent bunsetsus", "Research Fellow of the Japan Society for the Promotion of Science JSPS 1 Bunsetsu is a commonly used linguistic unit in Japanese consisting of one or more adjacent content words and zero or more following functional words", "On the other hand as for English or Chinese various NER systems have explored global information and reported their effectiveness", "In Malouf 2002 Chieu and Ng 2002 information about features assigned to other instances of the same token is utilized", "Ji and Grishman 2005 uses the information obtained from coreference analysis for NER", "Mohit and Hwa 2005 uses syntactic features in building a semisupervised NE tagger", "In this paper we present a Japanese NER system that uses global information obtained from several structural analyses", "To be more specic our system is based on SVM recognizes NEs after syntactic case and coreference analyses and uses information obtained from these analyses and the NER results for the previous context integrally", "At this point it is true that NER results are useful for syntactic case and coreference analyses and thus these analyses and NER should be performed in a complementary way", "However since we focus on NER we recognize NE after these structural analyses"]}, "I08-4009": {"title": ["Which Performs Better on InVocabulary Word Segmentation Based on Word or Character Zhenxing Wang12 Changning Huang2 and Jingbo Zhu1 1 Institute of Computer Software and Theory Northeastern University Shenyang China 110004 2 Microsoft Research Asia 49 Zhichun Road Haidian District Beijing China 100080 zxwangicsneueducn vcnhmicrosoftcom zhujingbomailneueducn"], "abstract": ["Since the first Chinese Word Segmentation CWS Bakeoff on 2003 CWS has experienced a prominent flourish because Bakeoff provides a platform for the participants which helps them recognize the merits and drawbacks of their segmenters", "However the evaluation metric of bakeoff is not sufficient enough to measure the performance thoroughly sometimes even misleading", "One typical example caused by this insufficiency is that there is a popular belief existing in the research field that segmentation based on word can yield a better result than characterbased tagging CT on invocabulary IV word segmentation even within closed tests of Bakeoff", "Many efforts were paid to balance the performance on IV and outof vocabulary OOV words by combining these two methods according to this belief", "In this paper we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method which is a typical way to seek such balance", "Our evaluation metric shows that CT outperforms dictionarybased or so called wordbased in general segmentation on both IV and OOV words within Bakeoff  The work is done when the first author is working in MSRA as an intern", "closed tests", "Furthermore our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation"], "inroduction": ["Chinese Word Segmentation CWS has been witnessed a prominent progress in the last three Bakeoffs Sproat and Emerson 2003 Emerson 2005 Levow 2006", "One of the reasons for this progress is that Bakeoff provides standard corpora and objective metric which makes the result of each system comparable", "Through those evaluations researchers can recognize the advantage and disadvantage of their methods and improve their systems accordingly", "However in the evaluation metric of Bakeoff only the overall F measure precision recall IV in vocabulary recall and OOV outofvocabulary recall are included and such a metric is not sufficient to give a completely measure on the performance especially when the performance on IV and OOV word segmentation need to be evaluated", "An important issue is that segmentation based on which word or character can yield the better performance on IV words", "We give a detailed explanation about this issue as following", "Since CWS was firstly treated as a character based tagging task we call it CT for short hereafter in Xue and Converse 2002 this method has been widely accepted and further developed by researchers Peng et al 2004 Tseng et al 2005 Low et al 2005 Zhao et al 2006", "Relatively to dictionarybased segmentation we call it DS for short hereafter CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole", "Thus CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006", "Although CT has shown its merits in word segmentation task some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test", "Consequently many strategies are proposed to balance the IV and OOV performance Goh et al 2005 Zhang et al 2006a", "Among these strategies the confidence measure used to combine the results of CT and DS is a straightforward one which is introduced in Zhang et al 2006a", "The basic assumption of such combination is that DS method performs better on IV words and Zhang derives this belief from the fact that DS achieves higher IV recall rate as Table 1 shows", "In which AS CityU MSRA and PKU are four corpora used in Bakeoff 2005 also see Table 2 for detail", "We provide a more detailed evaluation metric to analyze these two methods including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test", "The precision and F measure are existing metrics and the definitions of them are clear", "Here we just employ them to evaluate segmentation results", "Furthermore our error analysis on the results of combination reveals that confidence measure in Zhang et al 2006a has a representation flaw and we propose an EIV tag method to revise it", "Finally we give an empirical comparison between existing pure CT method and combination which shows that pure CT method can produce stateoftheart results on both IV word and overall segmentation", "Corpus RIV ROOV DS CT DS CT AS 0982 0967 0038 0647 CityU 0989 0967 0164 0736 MSRA 0993 0972 0048 0716 PKU 0981 0955 0408 0754 Table 1 IV and OOV recall in Zhang et al 2006a The rest of this paper is organized as follows", "In Section 2 we give a brief introduction to Zhangs DS method and subwordbased tagging which is a special CT method", "And by comparing the results of this special CT method and DS according our detailed metric we show that CT performs better on both IV and OOV", "We review in Section 3 how confidence measure works and indicate its representation flaw", "Furthermore an EIV tag method is proposed to revise the confidence measure", "In Section 4 the experimental results of existing pure CT method are demonstrated to compare with combination result based on which we discuss the related work", "In Section 5 we conclude the contributions of this paper and discuss the future work"]}, "I08-4015": {"title": ["The Characterbased CRF Segmenter of MSRANEU"], "abstract": ["This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff", "Base on Conditional Random Field CRF model a basic segmenter is designed as a problem of characterbased tagging", "To further improve the performance of our segmenter we employ a wordbased approach to increase the invocabulary IV word recall and a postprocessing to increase the outofvocabulary OOV word recall", "We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora"], "inroduction": ["Since Chinese Word Segmentation was firstly treated as a characterbased tagging task in Xue and Converse 2002 this method has been widely accepted and further developed by researchers Peng et al 2004 Tseng et al 2005 Low et al 2005 Zhao et al 2006", "Thus as a powerful sequence tagging model CRF became the dominant method in the Bakeoff 2006 Levow 2006In this paper we improve basic segmenter un der the CRF work frame in two aspects namely IV and OOV identification respectively", "We use the result from wordbased segmentation to revise the CRF output so that we gain a higher IV word recall", "For the OOV part a postprocessing rule is proposed to find those OOV words which are wrongly segmented into several fractions", "Our system performs well in the Fourth Bakeoff achieving four second best and on the fifth in all the five corpora", "In the following of this paper we describe our method in more detail", "The rest of this paper is organized as follows", "In Section 2 we first give a brief review to the basic CRF tagging approach and then we propose our methods to improve IV and OOV performance respectively", "In Section 3 we give the experiment results on the fourth Bakeoff corpora to show that our method is effective to improve the performance of the segmenter", "In Section 4 we conclude our work"]}, "I08-4030": {"title": ["CRFbased Hybrid Model for Word Segmentation NER and even POS Tagging Zhiting Xu Xian Qian Yuejie Zhang  Yaqian Zhou Department of Computer Science  Engineering Shanghai Key Laboratory of Intelligent Information Processing Fudan University Shanghai 200433 P R China"], "abstract": ["This paper presents systems submitted to the close track of Fourth SIGHAN Bakeoff", "We built up three systems based on Conditional Random Field for Chinese Word Segmentation Named Entity Recognition and PartOfSpeech Tagging respectively", "Our systems employed basic features as well as a large number of linguistic features", "For segmentation task we adjusted the BIO tags according to confidence of each character", "Our final system achieve a Fscore of 9418 at CTB 9286 at NCC 9459 at SXU on Segmentation 8526 at MSRA on Named Entity Recognition and 9065 at PKU on PartOfSpeech Tagging"], "inroduction": ["Fourth SIGHAN Bakeoff includes three tasks that is Word Segmentation Named Entity Recognition NER and PartOfSpeech POS Tagging", "In the POS Tagging task the testing corpora are pre segmented", "Word Segmentation NER and POSTagging could be viewed as classification prob We attended the close track of CTB NCC SXU on Segmentation MSRA on NER and PKU on POS Tagging", "In the close track we cannot use any external resource and thus we extracted several word lists from training corpora to form multiple features beside basic features", "Then we trained CRF models based on these feature sets", "In CRF models a margin of each character can be gotten and the margin could be considered as the confidence of that character", "For the Segmentation task we performed the Maximum Probability Segmentation first through which each character is assigned a BIO tag B represents the Beginning of a word I represents In a word and O represents Out of a word", "If the confidence of a character is lower than the threshold the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation R Zhang et al 2006"]}, "J00-4003": {"title": ["An Empirically Based System for Processing Definite Descriptions"], "abstract": ["We present an implemented system for processing definite descriptions in arbitrary domains", "The design of the system is based on the results of a corpus analysis previousl y reported which highlighted the prevalence of discoursenew descriptions in newspa per corpora", "The annotated corpus was used to extensivel y evaluate the pro posed techniques for matching definite descriptions with their antecedents discourse segmentation recognizing discoursenew descriptions and suggesting anchors for bridging descriptions"], "inroduction": ["Most models of definite description processing proposed in the literature tend to emphasise the anaphoric role of these elements1 Heim 1982 is perhaps the best for malization of this type of theory", "This approach is challenged by the results of exper iments we reported previously Poesio and Vieira 1998 in which subjects were asked to classify the uses of definite descriptions in Wall Street Journal articles according to schemes derived from proposals by Hawkins 1978 and Prince 1981", "The results of these experiments indicated that definite descriptions are not primarily anaphoric about half of the time they are used to introduce a new entity in the discourse", "In this paper we present an implemented system for processing definite descriptions based on the results of that earlier study", "In our system techniques for recognizing discoursenew descriptions play a role as important as techniques for identifying the antecedent of anaphoric ones", "A central characteristic of the work described here is that we intended from the start to develop a system whose performance could be evaluated using the texts an notated in the experiments mentioned above", "Assessing the performance of an NLP system on a large number of examples is increasingly seen as a much more thorough evaluation of its performance than trying to come up with counterexamples it is con sidered essential for language engineering applications", "These advantages are thought by many to offset some of the obvious disadvantages of this way of developing NLP theoriesin particular the fact that given the current state of language processing technology many hypotheses of interest cannot be tested yet see below", "As a result quantitative evaluation is now commonplace in areas of language engineering such as parsing and quantitative evaluation techniques are being proposed for semantic  Univcrsidade do Vale do Rio dos SinosUNISINOS Av", "Unisinos 950Cx", "Postal 275 93022000 Sao Leopoldo RS Brazil", "Email renataexatasunisinosbr t University of Edinburgh ICCS and Informatics 2 Buccleuch Place EHS 9LW Edinburgh UK", "Email MassimoPoesioedacuk 1 We use the term definite description Russell 1905 to indicate definite noun phrases with the definite", "article the such as the car", "We are not concerned with other types of definite noun phrases such as pronouns demonstratives or possessive deiptions", "Anaphoric expressions are those linguistic expressions used to signat evoke or refer to previously mentioned entities", " 2001 Association for Computational Linguistics interpretation as well for example at the Sixth and Seventh Message Understa nd ing Conferences MUC6 and MUC7 Sundheim 1995 Chinchor 1997 which also included evaluations of systems on the socalled coreference task a subtask of which is the resolution of definite descriptions", "The system we present was developed to be evaluated in a quantitative fashion as well but because of the problems concerning agreement between annotators observed in our previous study we evaluated the sys tem both by measuring precisionrecall against a gold standard as done in MUC and by measuring agreement between the annotations produced by the system and those proposed by the annotators", "The decision to develop a system that could be quantitatively evaluated on a large number of examples resulted in an important constraint we could not make use of inference mechanisms such as those assumed by traditional computational theories of definite description resolution eg Sidner 1979 Carter 1987 Alshawi 1990 Poesio 1993", "Too many facts and axioms would have to be encoded by hand for theories of this type to be tested even on a mediumsized corpus", "Our system therefore is based on a shallowprocessing approach more radical even than that attempted by the first advocate of this approach Carter 1987 or by the systems that participated in the MUC evaluations Appelt et al 1995 Gaizaukas et al 1995 Humphreys et al 1988 since we made no attempt to finetune the system to maximize performance on a particular domain", "The system relies only on structural information on the in formation provided by preexisting lexical sources such as WordNet Fellbaum 1998 on minimal amounts of general handcoded information or on information that could be acquired automatically from a corpus", "As a result the system does not really have the resources to correctly resolve those definite descriptions whose interpretation does require complex reasoning we grouped these in what we call the bridging class", "We nevertheless developed heuristic techniques for processing these types of definites as well the idea being that these heuristics may provide a baseline against which the gains in performance due to the use of commonsense knowledge can be assessed more clearly2 The paper is organized as follows We first summarize the results of our previous corpus study Poesio and Vieira 1998 Section 2 and then discuss the model of defi nite description processing that we adopted as a result of that work and the general architecture of the system Section 3", "In Section 4 we discuss the heuristics that we developed for resolving anaphoric definite descriptions recognizing discoursenew descriptions and processing bridging descriptions and in Section 5 how the per formance of these heuristics was evaluated using the annotated corpus", "Finally we present the final configuration of the two versions of the system that we developed Section 6 review other systems that perform similar tasks Section 7 and present our conclusions and indicate future work Section 8"]}, "J01-2002": {"title": ["Improving Accuracy in Word Class Tagging through the Combination of Machine Learning Systems"], "abstract": ["We examine how differences in language models learned by different datadriven systems per forming the same NLP task can be exploited to yield a higher accuracy than the best individual system", "We do this by means of experiments involving the task of morphos yntactic word class tagging on the basis of three different tagged corpora", "Four wellknown tagger generators hidden Markov model memorybased transformation rules and maximum entropy are trained on the same corpus data", "After comparison their outputs are combined using several voting strategies and secondstage classifiers", "All combination taggers outperform their best component", "The re duction in error rate varies with the material in question but can be as high as 243 with the LOB corpus"], "inroduction": ["In all natural language processing NLP systems we find one or more language models that are used to predict classify or interpret languagerelated observations", "Because most realworld NLP tasks require something that approaches full language understanding in order to be perfect but automatic systems only have access to limited and often superficial information as well as limited resources for reasoning with that information such language models tend to make errors when the system is tested on new material", "The engineering task in NLP is to design systems that make as few errors as possible with as little effort as possible", "Common ways to reduce the error rate are to devise better representations of the problem to spend more time on encoding language knowledge in the case of handcrafted systems or to find more training data in the case of datadriven systems", "However given limited resources these options are not always available", "Rather than devising a new representation for our task in this paper we combine different systems employing known representations", "The observation that suggests this approach is that systems that are designed differently either because they use a different formalism or because they contain different knowledge will typically produce different errors", "We hope to make use of this fact and reduce the number of errors with  PO Box 9103 6500 HD Nijmegen The Netherlands", "Email hvhletkunnl", "t Universiteitsplein 1 2610 Wilrijk Belgium", "Email zavreltextkernelnl", "t Universiteitsplein 1 2610 Wilrijk Belgium", "Email daelemuiauaacbe", " 2001 Association for Computational Linguistics very little additional effort by exploiting the disagreement between different language models", "Although the approach is applicable to any type of language model we focus on the case of statistical disambiguators that are trained on annotated corpora", "The examples of the task that are present in the corpus and its annotation are fed into a learning algorithm which induces a model of the desired inputoutput mapping in the form of a classifier", "We use a number of different learning algorithms simultaneously on the same training corpus", "Each type of learning method brings its own inductive bias to the task and will produce a classifier with slightly different characteristics so that different methods will tend to produce different errors", "We investigate two ways of exploiting these differences", "First we make use of the gang effect", "Simply by using more than one classifier and voting between their outputs we expect to eliminate the quirks and hence errors that are due to the bias of one particular learner", "However there is also a way to make better use of the differences we can create an arbiter effect", "We can train a secondlevel classifier to select its output on the basis of the patterns of cooccurrence of the outputs of the various classifiers", "In this way we not only counter the bias of each component but actually exploit it in the identification of the correct output", "This method even admits the possibility of correcting collective errors", "The hypothesis is that both types of approaches can yield a more accurate model from the same training data than the most accurate component of the combination and that given enough training data the arbiter type of method will be able to outperform the gang type1 Inthe machine learning literature there has been much interest recently in the the oretical aspects of classifier combination both of the gang effect type and of the arbiter type see Section 2", "In general it has been shown that when the errors are uncorre lated to a sufficient degree the resulting combined classifier will often perform better than any of the individual systems", "In this paper we wish to take a more empirical approach and examine whether these methods result in substantial accuracy improve ments in a situation typical for statistical NLP namely learning morphosyntactic word class tagging also known as partofspeech or POS tagging from an annotated corpus of several hundred thousand words", "Morphosyntactic word class tagging entails the classification tagging of each token of a natural language text in terms of an element of a finite palette tagset of word class descriptors tags", "The reasons for this choice of task are several", "First of all tagging is a widely researched and wellunderstood task see van Halteren 1999", "Second current performance levels on this task still leave room for improvement stateoftheart performance for datadriven automatic word class taggers on the usual type of material eg tagging English text with single tags from a lowdetail tagset is at 9697 correctly tagged words but accuracy levels for specific classes of ambiguous words are much lower", "Finally a number of rather different methods that automatically generate a fully functional tagging system from annotated text are available offtheshelf", "First experiments van Halteren Zavrel and Daelemans 1998 Brill and Wu 1998 demonstrated the basic validity of the approach for tagging with the error rate of the best combiner being 191 lower than that of the best individual tagger van Halteren Zavrel and Daelemans 1998", "However these experiments were restricted to a single language a single tagset and more importantly a limited amount of training data for the combiners", "This led us to perform further more extensive 1In previous work van Halteren Zavrel and Daelemans 1998 we were unable to confirm the latter half of the hypothesis unequivocally", "As we judged this to be due to insufficient training data for proper training of the secondlevel classifiers we greatly increase the amount of training data in the present work through the use of crossvalidation", "tagging experiments before moving on to other tasks", "Since then the method has also been applied to other NLP tasks with good results see Section 6", "In the remaining sections we first introduce classifier combination on the basis of previous work in the machine learning literature and present the combination meth ods we use in our experiments Section 2", "Then we explain our experimental setup Section 3 also describing the corpora 31 and tagger generators 32 used in the experiments", "In Section 4 we go on to report the overall results of the experiments starting with a comparison between the component taggers and hence between the underlying tagger generators and continuing with a comparison of the combination methods", "The results are examined in more detail in Section 5 where we discuss such aspects as accuracy on specific words or tags the influence of inconsistent training data training set size the contribution of individual component taggers and tagset granularity", "In Section 6 we discuss the results in the light of related work after which we conclude Section 7 with a summary of the most important observations and interesting directions for future research"]}, "J01-2004": {"title": ["Probabilistic TopDown Parsing and Language Modeling"], "abstract": ["This pa per describes thefunctioning of a broadcoverage probabilistic topdown parser and its application to the problem of language modelingfor speech recognition", "The pa per first introduces key notions in language modeling and probabilistic parsing and briefly reviews some previous approaches to using syntactic structure for language modeling", "A lexicalized probabilistic top down parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they arefound relative to the best broadcoverage statistical parsers", "A new language model that utilizes probabilistic topdown parsing is then outlined and empirical results show that it improves upon previous work in test corpus per plexit y Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model", "A small recognition experiment also demonstrates the utility of the model"], "inroduction": ["With certain exceptions computational linguists have in the past generally formed a separate research community from speech recognition researchers despite some obvious overlap of interest", "Perhaps one reason for this is that until relatively re cently few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems", "In the past few years however some improve ments have been made over these language models through the use of statistical meth ods of natural language processing and the development of innovative linguistically wellmotivated techniques for improving language models for speech recognition is generating more interest among computational linguists", "While language models built around shallow local dependencies are still the standard in stateoftheart speech recognition systems there is reason to hope that better language models can and will be developed by computational linguists for this task", "This paper will examine language modeling for speech recognition from a nat ural language processing point of view", "Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture longdistance depen dencies for language modeling will be reviewed", "A new language model based on probabilistic topdown parsing will be outlined and compared with the previous liter ature and extensive empirical results will be presented which demonstrate its utility", "Two features of our topdown parsing approach will emerge as key to its success", "First the topdown parsing algorithm builds a set of rooted candidate parse trees from left to right over the string which allows it to calculate a generative probability for  Department of Cognitive and Linguistic Sciences Box 1978 Brown University Providence RI 02912  2001 Association for Computational Linguistics each prefix string from the probabilistic grammar and hence a conditional probability for each word given the previous words and the probabilistic grammar", "A leftto right parser whose derivations are not rooted ie with derivations that can consist of disconnected tree fragments such as an LR or shiftreduce parser cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar because their derivations include probability mass from unrooted structures", "Only at the point when their derivations become rooted at the end of the string can generative string probabilities be calculated from the grammar", "These parsers can calculate word probabilities based upon the parser stateas in Chelba and Jelinek 1998abut such a distribution is not generative from the probabilistic grammar", "A parser that is not left to right but which has rooted derivations eg a head first parser will be able to calculate generative joint probabilities for entire strings however it will not be able to calculate probabilities for each word conditioned on previously generated words unless each derivation generates the words in the string in exactly the same order", "For example suppose that there are two possible verbs that could be the head of a sentence", "For a headfirst parser some derivations will have the first verb as the head of the sentence and the second verb will be generated after the first hence the second verbs probability will be conditioned on the first verb", "Other derivations will have the second verb as the head of the sentence and the first verbs probability will be conditioned on the second verb", "In such a scenario there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule", "Of course the joint probability can be used as a language model but it cannot be interpolated on a wordbyword basis with say a trigram model which we will demonstrate is a useful thing to do", "Thus our topdown parser allows for the incremental calculation of generative conditional word probabilities a property it shares with other lefttoright parsers with rooted derivations such as Earley parsers Earley 1970 or leftcorner parsers Rosenkrantz and Lewis II 1970", "A second key feature of our approach is that topdown guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model", "Because the rooted partial derivation is fully connected all of the conditioning information that might be extracted from the topdown left context has already been specified and a conditional probability model built on this information will not impose any additional burden on the search", "In contrast an Earley or leftcorner parser will underspecify certain connections between constituents in the left context and if some of the underspecified information is used in the conditional probability model it will have to become specified", "Of course this can be done but at the expense of search efficiency the more that this is done the less benefit there is from the underspecification", "A topdown parser will in contrast derive an efficiency benefit from precisely the information that is underspecified in these other approaches", "Thus our topdown parser makes it very easy to condition the probabilistic gram mar on an arbitrary number of values extracted from the rooted fully specified deriva tion", "This has lead us to a formulation of the conditional probability model in terms of values returned from treewalking functions that themselves are contextually sen sitive", "The topdown guidance that is provided makes this approach quite efficient in practice", "The following section will provide some background in probabilistic contextfree grammars and language modeling for speech recognition", "There will also be a brief review of previous work using syntactic information for language modeling before we introduce our model in Section 4", "a st I s b st S STOP  c  s t S STOP NP VP I NP V P I s NP VP I Spot VBD NP I chased DT NN I I the ball Spot VBD JP I chased DT NN I I the ball Spot Figure 1 Three parse trees a a complete parse tree b a complete parse tree with an explicit stop symbol and c a partial parse tree"]}, "J01-3003": {"title": ["Automatic  Verb Classification"], "abstract": ["Automatic acquisition of lexical knowledge is critical to a wide range of natural language pro cessing tasks", "Especially important is knowledge about verbs which are the primary source of relational information in a sentencethe predicateargument structure that relates an action or state to its participants ie who did what to whom", "In this work we report on super vised learning experiments to automatically classify three major types of English verbs based on their argument structurespecifically the thematic roles they assign to participants", "We use linguisticallymotivated statistical indicators extracted from large annotated corpora to train the classifier achieving 698 accuracy for a task whose baseline is 34 and whose expertbased upper bound we calculate at 865", "A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument struc ture of the verbs", "Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification and that it can be gleaned from a corpus by automatic means", "We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques"], "inroduction": ["Automatic acquisition of lexical knowledge is critical to a wide range of natural lan guage processing NLP tasks Boguraev and Pustejovsky 1996", "Especially important is knowledge about verbs which are the primary source of relational information in a sentencethe predicateargument structure that relates an action or state to its par ticipants ie who did what to whom", "In facing the task of automatic acquisition of knowledge about verbs two basic questions must be addressed  What information about verbs and their relational properties needs to be learned", " What information can in practice be learned through automatic means", "In answering these questions some approaches to lexical acquisition have focused on learning syntactic information about verbs by automatically extracting subcategoriza tion frames from a corpus or machinereadable dictionary Brent 1993 Briscoe and Carroll 1997 Dorr 1997 Lapata 1999 Manning 1993 McCarthy and Korhonen 1998", " Linguistics Department University of Geneva 2 rue de Candolle 1211 Geneva 4 Switzerland merlo lettresunigech t Department of Computer Science University of Toronto 6 Kings College Road Toronto ON MSS 3H5 Canada suzannecstorontoed u  2001 Association for Computational Linguistics Table 1 Examples of verbs from the three optionally intransitive classes", "Unergative The horse raced past the barn", "The jockey raced the horse past the barn", "Unaccusative The butter melted in the pan", "The cook melted the butter in the pan", "ObjectDrop The boy played", "The boy played soccer", "Other work has attempted to learn deeper semantic properties such as selectional re strictions Resnik 1996 Riloff and Schmelzenbach 1998 verbal aspect Klavans and Chodorow 1992 Siegel 1999 or lexicalsemantic verb classes such as those proposed by Levin 1993 Aone and McKee 1996 McCarthy 2000 Lapata and Brew 1999 Schulte im Walde 2000", "In this paper we focus on argument structurethe thematic roles as signed by a verb to its argumentsas the way in which the relational semantics of the verb is represented at the syntactic level", "Specifically our proposal is to automatically classify verbs based on argument structure properties using statistical corpusbased methods", "We address the prob lem of classification because it provides a means for lexical organization which can effectively capture generalizations over verbs Palmer 2000", "Within the context of classification the use of argument structure provides a finer discrimination among verbs than that induced by subcategorization frames as we see below in our example classes which allow the same subcategorizations but differ in thematic assignment but a coarser classification than that proposed by Levin in which classes such as ours are further subdivided according to more detailed semantic properties", "This level of classification granularity appears to be appropriate for numerous language engineering tasks", "Because knowledge of argument structure captures fundamental participant event relations it is crucial in parsing and generation eg Srinivas and Joshi 1999 Stede 1998 in machine translation Dorr 1997 and in information re trieval Klavans and Kan 1998 and extraction Riloff and Schmelzenbach 1998", "Our use of statistical corpusbased methods to achieve this level of classification is moti vated by our hypothesis that classbased differences in argument structure are reflected in statistics over the usages of the component verbs and that those statistics can be automatically extracted from a large annotated corpus", "The particular classification problem within which we investigate this hypothesis is the task of learning the three major classes of optionally intransitive verbs in English unergative unaccusative and objectdrop verbs", "For the unergativeunaccusative dis tinction see Perlmutter 1978 Burzio 1986 Levin and Rappaport Hovav 1995", "Table 1 shows an example of a verb from each class in its transitive and intransitive usages", "These three classes are motivated by theoretical linguistic properties see dis cussion and references below and in Stevenson and Merlo 1997b Merlo and Steven son 2000b", "Furthermore it appears that the classes capture typological distinctions that are useful for machine translation for example causative unergatives are un grammatical in many languages as well as processing distinctions that are useful for generating naturally occurring language for example reduced relatives with unerga tive verbs are awkward but they are acceptable and in fact often preferred to full relatives for unaccusative and objectdrop verbs Stevenson and Merlo 1997b Merlo and Stevenson 1998", "Table 2 Summary of thematic role assignments by class", "Transitive Intransitive Classes Subject Object Unergative Agent of Causation Agent Unaccusative Agent of Causation Theme ObjectDrop Agent Theme Subject Agent Theme Agent The question then is what underlies these distinctions", "We identify the property that precisely distinguishes among these three classes as that of argument structure ie the thematic roles assigned by the verbs", "The thematic roles for each class and their mapping to subject and object positions are summarized in Table 2", "Note that verbs across these three classes allow the same subcategorization frames taking an NP object or occurring intransitively thus classification based on subcategorization alone would not distinguish them", "On the other hand each of the three classes is comprised of multiple Levin classes because the latter reflect more detailed semantic distinctions among the verbs Levin 1993 thus classification based on Levins labeling would miss generalizations across the three broader classes", "By contrast as shown in Table 2 each class has a unique pattern of thematic assignments which categorize the verbs precisely into the three classes of interest", "Although the granularity of our classification differs from Levins we draw on her hypothesis that semantic properties of verbs are reflected in their syntactic behavior", "The behavior that Levin focuses on is the notion of diathesis alternationan alter nation in the expression of the arguments of a verb such as the different mappings between transitive and intransitive that our verbs undergo", "Whether a verb partici pates in a particular diathesis alternation or not is a key factor in Levins approach to classification", "We like others in a computational framework have extended this idea by showing that statistics over the alternants of a verb effectively capture information about its class Lapata 1999 McCarthy 2000 Lapata and Brew 1999", "In our specific task we analyze the pattern of thematic assignments given in Table 2 to develop statistical indicators that are able to determine the class of an op tionally intransitive verb by capturing information across its transitive and intransitive alternants", "These indicators serve as input to a machine learning algorithm under a supervised training methodology which produces an automatic classification system for our three verb classes", "Since we rely on patterns of behavior across multiple occur rences of a verb we begin with the problem of assigning a single class to the entire set of usages of a verb within the corpus", "For example we measure properties across all occurrences of a word such as raced in order to assign a single classification to the lexical entry for the verb race", "This contrasts with work classifying individual oc currences of a verb in each local context which have typically relied on training that includes instances of the verbs to be classifiedessentially developing a bias that is used in conjunction with the local context to determine the best classification for new instances of previously seen verbs", "By contrast our method assigns a classification to verbs that have not previously been seen in the training data", "Thus while we do not as yet assign different classes to the instances of a verb we can assign a single predominant class to new verbs that have never been encountered", "To preview our results we demonstrate that combining just five numerical indi cators automatically extracted from large text corpora is sufficient to reduce the error rate in this classification task by more than 50 over chance", "Specifically we achieve almost 70 accuracy in a task whose baseline chance performance is 34 and whose expertbased upper bound is calculated at 865", "Beyond the interest for the particular classification task at hand this work ad dresses more general issues concerning verb class distinctions based in argument structure", "We evaluate our hypothesis that such distinctions are reflected in statis tics over corpora through a computational experimental methodology in which we investigate as indicated each of the subhypotheses below in the context of the three verb classes under study  Lexical features capture argument structure differences between verb classes1  The linguistically distinctive features exhibit distributional differences across the verb classes that are apparent within linguistic experience ie they can be collected from text", " The statistical distributions of some of the features contribute to learning the classifications of the verbs", "In the following sections we show that all three hypotheses above are borne out", "In Section 2 we describe the argument structure distinctions of our three verb classes in more detail", "In support of the first hypothesis above we discuss lexical correlates of the underlying differences in thematic assignments that distinguish the three verb classes under investigation", "In Section 3 we show how to approximate these features by simple syntactic counts and how to perform these counts on available corpora", "We confirm the second hypothesis above by showing that the differences in distribution predicted by the underlying argument structures are largely found in the data", "In Section 4 in a series of machine learning experiments and a detailed analysis of errors we confirm the third hypothesis by showing that the differences in the distribution of the extracted features are successfully used for verb classification", "Section 5 evaluates the significance of these results by comparing the programs accuracy to an expert based upper bound", "We conclude the paper with a discussion of its contributions comparison to related work and suggestions for future extensions"]}, "J03-1005": {"title": ["c 2003 Association for Computational Linguistics Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation Christoph Tillmann Hermann Ney IBM T J Watson Research Center RWTH Aachen In this article we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming DP The search algorithm uses the translation model presented in Brown et al 1993 Starting from a DPbased solution to the travelingsalesman problem we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm Word reordering restrictions especially useful for the translation direction German to English are presented The restrictions are generalized and a set of four parameters to control the word reordering is introduced which then can easily be adopted to new translation directions The beam search procedure has been successfully tested on the Verbmobil task German to English 8000word vocabulary and on the Canadian Hansards task French to English 100000word vocabulary For the mediumsized Verbmobil task a sentence can be translated in a few seconds only a small number of search errors occur and there is no performance degradation as measured by the word error criterion used in this article"], "abstract": ["In this article we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming DP", "The search algorithm uses the translation model presented in Brown et al", "1993", "Starting from a DPbased solution to the travelingsalesman problem we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm", "Word reordering restrictions especially useful for the translation direction German to English are presented", "The restrictions are generalized and a set of four parameters to control the word reordering is introduced which then can easily be adopted to new translation directions", "The beam search procedure has been successfully tested on the Verbmobil task German to English 8000word vocabulary and on the Canadian Hansards task French to English 100000word vocabulary", "For the mediumsized Verbmobil task a sentence can be translated in a few seconds only a small number of search errors occur and there is no performance degradation as measured by the word error criterion used in this article"], "inroduction": ["This article is about a search procedure for statistical machine translation MT", "The task of the search procedure is to find the most likely translation given a source sentence and a set of model parameters", "Here we will use a trigram language model and the translation model presented in Brown et al", "1993", "Since the number of possible translations of a given source sentence is enormous we must find the best output without actually generating the set of all possible translations instead we would like to focus on the most likely translation hypotheses during the search process", "For this purpose we present a datadriven beam search algorithm similar to the one used in speech recognition search algorithms Ney et al 1992", "The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language which does not enter into speech recognition", "Tillmann Vogel Ney and Zubiaga 1997 proposes a dynamic programming DPbased search algorithm for statistical MT that monotonically translates the input sentence from left to right", "The word order difference is dealt with using a suitable preprocessing step", "Although the resulting search procedure is very fast the preprocessing is language specific and requires a lot of manual  IBM T J Watson Research Center Yorktown Heights NY 10598", "Email ctillusibmcom", "The research reported here was carried out while the author was with Lehrstuhl fu r Informatik VI Computer Science Department RWTH Aachen", " Lehrstuhl fu r Informatik VI Computer Science Department RWTH Aachen D52056 Aachen Germany", "Email neyinformatikrwthaachende", "Oc 2003 Association for Computational Linguistics work", "Currently most search algorithms for statistical MT proposed in the literature are based on the A concept Nilsson 1971", "Here the word reordering can be easily included in the search procedure since the input sentence positions can be processed in any order", "The work presented in Berger et al", "1996 that is based on the A concept however introduces word reordering restrictions in order to reduce the overall search space", "The search procedure presented in this article is based on a DP algorithm to solve the travelingsalesman problem TSP", "A datadriven beam search approach is presented on the basis of this DPbased algorithm", "The cities in the TSP correspond to source positions of the input sentence", "By imposing constraints on the possible word reorderings similar to that described in Berger et al", "1996 the DPbased approach becomes more effective when the constraints are applied the number of word re orderings is greatly reduced", "The original reordering constraint in Berger et al", "1996 is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions1 A set of four parameters is given to control the word reordering", "Additionally a set of four states is introduced to deal with grammatical reordering restrictions eg for the translation direction German to English the word order difference between the two languages is mainly due to the German verb group", "In combination with the reordering restrictions a datadriven beam search organization for the search procedure is proposed", "A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria 1 The partial hypotheses cover the same set of source sentence positionsand 2 the partial hypotheses cover sets C of source sentence positions of equal car dinality", "A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process", "To verify the effectiveness of the proposed techniques we report and analyze results for two translation tasks the German to English Verbmobil task and French to English Canadian Hansards taskThe article is structured as follows", "Section 2 gives a short introduction to the trans lation model used and reports on other approaches to the search problem in statistical MT In Section 3 a DPbased search approach is presented along with appropriate pruning techniques that yield an efficient beam search algorithm", "Section 4 reports and analyzes translation results for the different translation directions", "In Section 5 we conclude with a discussion of the achieved results"]}, "J04-1003": {"title": [""], "abstract": ["Levins 1993 study of verb classes is a widely used resource for lexical semantics", "In her framework some verbs such as give exhibit no class ambiguity", "But other verbs such as write have several alternative classes", "We extend Levins inventory to a simple statistical model of verb class ambiguity", "Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus", "We additionally show that these preferences are useful as priors for a verb sense disambiguator"], "inroduction": ["Much research in lexical semantics has concentrated on the relation between verbs and their arguments", "Many scholars hypothesize that the behavior of a verb particularly with respect to the expression and interpretation of its arguments is to a large extent determined by its meaning Talmy 1985 Jackendoff 1983 Goldberg 1995 Levin 1993 Pinker 1989 Green 1974 Gropen et al 1989 Fillmore 1965", "The correspondence between verbal meaning and syntax has been extensively studied in Levin 1993 which argues that verbs which display the same diathesis alternationsalternations in the realization of their argument structurecan be assumed to share certain meaning components and to form a semantically coherent class", "The converse of this assumption is that verb behavior ie participation in diathe sis alternations can be used to provide clues about aspects of meaning which in turn can be exploited to characterize verb senses referred to as classes in Levins 1993 terminology", "A major advantage of this approach is that criteria for assigning senses can be more concrete than is traditionally assumed in lexicographic work eg WordNet or machinereadable dictionaries concerned with sense distinctions Palmer 2000", "As an example consider sentences 14 taken from Levin", "Examples 1 and 2 illustrate the dative and benefactive alternations respectively", "Dative verbs alternate between the prepositional frame NP1 V NP2 to NP3 see 1a and the doubleobject frame NP1 V NP2 NP3 see 1b whereas benefactive verbs alternate between the double object frame see 2a and the prepositional frame NP1 V NP2 for NP3 see 2b", "To decide whether a verb is benefactive or dative it suffices to test the acceptability of the for and to frames", "Verbs undergoing the conative alternation can be attested either as transitive or as intransitive with a prepositional phrase headed by the word at1 The role filled by the object of the transitive variant is shared by the noun phrase complement of at in the intransitive variant see 3", "This example makes explicit that class assignment depends not only on syntactic facts but also on judgments about  Department of Computer Science Regent Court 211 Portobello Street Sheffield S1 4DP UK", "Email mlapdcsshefacuk Department of Linguistics Oxley Hall1712 Neil Avenue Columbus OH", "Email cbrewlingohio stateedu", "1 At is the most likely choice but for some conative verbs the preposition is instead on or onto", "Oc 2004 Association for Computational Linguistics semantic roles", "Similarly the possessor object alternation involves a possessor and a possessed attribute that can be manifested either as the verbal object or as the object of a prepositional phrase headed by for see 4", "1 a Bill sold a car to Tom", "b Bill sold Tom a car", "2 a Martha carved the baby a toy", "b Martha carved a toy for the baby", "3 a Paula hit the fence", "b Paula hit at the fence", "4 a I admired his honesty", "b I admired him for his honesty", "Observation of the semantic and syntactic behavior of pay and give reveals that they pattern with sell in licensing the dative alternation", "These verbs are all members of the Give class", "Verbs like make and build behave similarly to carve in licensing the benefactive alternation and are members of the class of Build verbs", "The verbs beat kick and hit undergo the conative alternation they are all members of the Hit verb class", "By grouping together verbs that pattern together with respect to diathesis alternations Levin 1993 defines approximately 200 verb classes which she argues reflect important semantic regularities", "These analyses and many similar ones by Levin and her successors rely primarily on straightforward syntactic and syntacticosemantic criteria", "To adopt this approach is to accept some limitations on the reach of our analyses since not all semantically interesting differences will have the appropriate reflexes in syntax", "Nevertheless the emphasis on concretely available observables makes Levins methodology a good candidate for automation Palmer 2000", "Therefore Levins 1993 classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora", "These exploit syntactic cues or at least cues that are plausibly related to syntax Merlo and Stevenson 2001 Schulte im Walde 2000 Lapata 1999 McCarthy 2000", "Other work has used Levins classification in conjunction with other lexical resources to create dictionaries that express the systematic correspondence between syntax and meaning Dorr 1997 Dang Rosenzweig and Palmer 1997 Dorr and Jones 1996", "Levins inventory of verbs and classes has been also useful for applications such as machine translation Dorr 1997 Palmer and Wu 1995 generation Stede 1998 information retrieval Levow Dorr and Lin 2000 and document classification Klavans and Kan 1998", "Although the classification provides a general framework for describing verbal meaning it says only which verb meanings are possible staying silent on the relative likelihoods of the different meanings", "The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities", "Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic constraints", "More precisely Levin 1993 provides an index of 3024 verbs for which she lists the semantic classes and diathesis alternations", "The mapping between verbs and classes is not one to one", "Of the 3024 verbs which she covers 784 are listed as having more than one class", "Even though Levins monosemous verbs outnumber her polysemous verbs by a factor of nearly four to one the total frequency of the former 4252715 46 Table 1 Polysemous verbs according to Levin", "Cla sses Ve rbs BN C fre qu en cy 1 2 23 9 4  2 5 2  7 1 5 2 5 3 6 2  3 2 5  9 8 2 3 1 7 3 7 3 8  8 5 4 4 4 3 3 9 5  2 1 2 5 2 3 2 2 2  7 4 7 6 7 2 7 2  6 6 9 7 2 2 6  1 2 3 1 0 1 4  4 2 7 Figure 1 Relation between number of classes and alternations", "is comparable to the total frequency of the latter 3986014", "This means that close to half of the cases processed by a semantic tagger would manifest some degree of ambiguity", "The frequencies are detailed in Table 1 and were compiled from a lemma tized version of the British National Corpus BNC Burnard 1995", "Furthermore as shown in Figure 1 the level of ambiguity increases in tandem with the number of alternations licensed by a given verb", "Consider for example verbs participating in one alternation only Of these 904 have one semantic class 86 have two classes 07 have three classes and 03 have four classes", "In contrast of the verbs licensing six different alternations 14 have one class 17 have two classes 124 have three classes 536 have four classes 2 have six classes and 1 have seven classes", "As ambiguity increases so does the availability and potential utility of information about diathesis alternations", "Palmer 2000 and Dang et al", "1998 argue that syntactic frames and verb classes are useful for developing principled classifications of verbs", "We go beyond this showing that they can also be of assistance in disambiguation", "Consider for instance the verb serve which is a member of four Levin classes Give Fit Masquerade and Fulfilling", "Each of these classes can in turn license four distinct syntactic frames", "47 As shown in the examples2 below in 5a serve appears ditransitively and belongs to the semantic class of Give verbs in 5b it occurs transitively and is a member of the class of Fit verbs in 5c it takes the predicative complement as minister of the interior and is a member of the class of Masquerade verbs", "Finally in sentence 5d serve is a Fulfilling verb and takes two complements a noun phrase an apprenticeship and a prepositional phrase headed by to to a stilllife photographer", "In the case of verbs like serve we can guess their semantic class solely on the basis of the frame with which they appear", "5 a Im desperately trying to find a venue for the reception which can serve our guests an authentic Italian meal", "NP1 V NP2 NP3 b The airline serves 164 destinations in over 75 countries", "NP1 V NP2 c JeanAntoine Chaptal was a brilliant chemist and technocrat who served Napoleon as minister of the interior from 1800 to 1805", "NP1 V NP2 as NP3 d Before her brief exposure to pop stardom she served an apprenticeship to a stilllife photographer", "NP1 V NP2 to NP3 But sometimes we do not have the syntactic information that would provide cues for semantic disambiguation", "Consider example 6", "The verb write is a member of three Levin classes two of which Message Transfer Performance take the double object frame", "In this case we have the choice between the Message Transfer reading see 6a and the Performance reading see 6b", "The same situation arises with the verb toast which is listed as a Prepare verb and a Judgment verb both these classes license the prepositional frame NP1 V NP2 for NP3 In sentence 7a the preferred reading is that of Prepare rather than that of Judgment see sentence 7b", "The verb study is ambiguous among three classes when attested in the transitive frame Learn see example 8a Sight see example 8b and Assessment see example 8c", "The verb convey when attested in the prepositional frame NP1 V NP2 to NP3 can be ambiguous between the Say class see example 9a and the Send class see example 9b", "In order to correctly decide the semantic class for a given ambiguous verb we would need not only detailed semantic information about the verbs arguments but also a considerable amount of world knowledge", "Admittedly selectional restrictions are sufficient for distinguishing 7a from 7b one normally heats up inanimate entities and salutes animate ones but selectional restrictions alone are probably not enough to disambiguate 6a from 6b since both letter and screenplay are likely to be described as written material", "Rather we need finegrained world knowledge Both scripts and letters can be written for someone only letters can be written to someone", "6 a A solicitor wrote him a letter at the airport", "b I want you to write me a screenplay called The Trip 7 a He sat by the fire and toasted a piece of bread for himself", "b We all toasted Nigel for his recovery", "2 Unless otherwise stated our example sentences were taken possibly in simplified form from the BNC", "48 8 a Chapman studied medicine at Cambridge", "b Romanov studied the old man carefully looking for some sign that he knew exactly what had been awaiting him at the bank", "c The alliance will also study the possibility of providing service to other highvolume products such as IBM and multivendor workstations", "9 a By conveying the news to her sister she would convey by implication something of her own anxiety", "b The judge signed the committal warrant and the police conveyed Mr Butler to prison giving the warrant to the governor", "This need for world knowledge or at least a convenient way of approximating this knowledge is not an isolated phenomenon but manifests itself across a variety of classes and frames eg double object transitive prepositional frame see examples 69", "We have argued that the concreteness of Levinstyle verb classes is an advantage but this advantage would be compromised if we tried to fold too much world knowledge into the classification", "We do not do this", "Instead Section 5 of the current article describes disambiguation experiments in which our probabilistic Levin classes are used in tandem with proxies for appropriate world knowledge", "It is important to point out that Levins 1993 classification is not intended as an exhaustive description of English verbs their meanings and their likelihood", "Many other classifications could have been built using the same principles", "A different grouping might for example have occurred if finer or coarser semantic distinctions were taken into account see Merlo and Stevenson 2001 and Dang Rosenzweig and Palmer 1997 for alternative classifications or if the containment of ambiguity was one of the classification objectives", "As pointed out by Kipper Dang and Palmer 2000 Levin classes exhibit inconsistencies and verbs are listed in multiple classes some of which have conflicting sets of syntactic frames", "This means that some ambiguities may also arise as a result of accidental errors or inconsistencies", "The classification was created not with computational uses in mind but for human readers so it has not been necessary to remedy all the errors and omissions that might cause trouble for machines", "Similar issues arise in almost all efforts to make use of preexisting lexical resources for computational purposes Briscoe and Carroll 1997 so none of the above comments should be taken as criticisms of Levins achievement", "The objective of this article is to show how to train and use a probabilistic version of Levins classification in verb sense disambiguation", "We treat errors and inconsistencies in the classification as noise", "Although all our tests have used Levins classes and the British National Corpus the method itself depends neither on the details of Levins classification nor on parochial facts about the English language", "Our future work will include tests on other languages other classifications and other corpora", "The model developed in this article takes as input a partially parsed corpus and generates for each combination of a verb and its syntactic frame a probability distribution over the available verb classes", "The corpus itself does not have to be labeled with classes", "This makes it feasible to use large corpora", "Our model is not immediately useful for disambiguation because it cannot discriminate among different occurrences of the same verb and frame but it can as we show in Section 5 be used as a prior in a full disambiguation system that does take appropriate account of context", "The model relies on several gross simplifications it does not take selectional restrictions discourse or pragmatic information into account but is demonstrably superior to simpler priors that make no use of subcategorization", "49 The remainder of this article is organized as follows", "In Section 2 we describe the probabilistic model and the estimation of the various model parameters", "In Sections 3 and 4 we report on the results of two experiments that use the model to derive the dominant class for polysemous verbs", "Sections 5 and 6 discuss our verb class disambiguation experiments", "We base our results on the BNC a 100millionword collection of samples of written and spoken language from a wide range of sources designed to represent a wide crosssection of current British English both spoken and written Burnard 1995", "We discuss our results in Section 7 and review related work in Section 8"]}, "J04-2003": {"title": ["c 2004 Association for Computational Linguistics Statistical Machine Translation with Scarce Resources Using Morphosyntactic Information Sonja Nie  en  Hermann Ney RWTH Aachen RWTH Aachen In statistical machine translation correspondences between the words in the source and the target language are learned from parallel corpora and often little or no linguistic knowledge is used to structure the underlying models In particular existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words In addition we introduce sentence level restructuring transformations which aim at the assimilation of word order in related sentences We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested We were able to reduce the amount of bilingual training data to less than 10 of the original corpus while losing only 16 in translation quality The improvement of the translation results is demonstrated on two GermanEnglish corpora taken from the Verbmobil task and the Nespole task"], "abstract": ["In statistical machine translation correspondences between the words in the source and the target language are learned from parallel corpora and often little or no linguistic knowledge is used to structure the underlying models", "In particular existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another", "The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms", "We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words", "In addition we introduce sentencelevel restructuring transformations which aim at the assimilation of word order in related sentences", "We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation", "The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested We were able to reduce the amount of bilingual training data to less than 10 of the original corpus while losing only 16 in translation quality", "The improvement of the translation results is demonstrated on two GermanEnglish corpora taken from the Verbmobil task and the Nespole", "task"], "inroduction": ["The statistical approach to machine translation has proved successful in various comparative evaluations since its revival by the work of the IBM research group more than a decade ago", "The IBM group dispensed with linguistic analysis at least in its earliest publications", "Although the IBM group finally made use of morphological and syntactic information to enhance translation quality Brown et al 1992 Berger et al 1996 most of todays statistical machine translation systems still consider only surface forms and use no linguistic knowledge about the structure of the languages involved", "In many applications only small amounts of bilingual training data are available for the desired domain and language pair and it is highly desirable to avoid at least parts of the costly data collection process", "The main objective of the work reported in this article is to introduce morphological knowledge in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary expected in testing", "This is achieved by explicitly taking into account the interdependencies of related inflected forms", "In this work a hierarchy of equivalence classes at different levels of abstraction is proposed", "Features from those hierarchy levels are combined to form hierarchical lexicon models which can replace the standard probabilistic lexicon used  Lehrstuhl fu r Informatik VI Computer Science Department RWTH AachenUniversity of Technology D52056 Aachen Germany", "Email sonjaniessengmxde neycsrwthaachende", "Oc 2004 Association for Computational Linguistics in most statistical machine translation systems", "Apart from the improved coverage the proposed lexicon models enable the disambiguation of ambiguous word forms by means of annotation with morphosyntactic tags", "11 Overview", "The article is organized as follows", "After briefly reviewing the basic concepts of the statistical approach to machine translation we discuss the state of the art and related work as regards the incorporation of morphological and syntactic information into systems for natural language processing", "Section 2 describes the information provided by morphosyntactic analysis and introduces a suitable representation of the analyzed corpus", "Section 3 suggests solutions for two specific aspects of structural differencenamely question inversion and separated verb prefixes", "Section 4 is dedicated to hi erarchical lexicon models", "These models are able to infer translations of word forms from the translations of other word forms of the same lemma", "Furthermore they use morphosyntactic information to resolve categorial ambiguity", "In Section 5 we describe how disambiguation between different readings and their corresponding translationscan be performed when no context is available as is typically the case for conven tional electronic dictionaries", "Section 6 provides an overview of our procedure for training model parameters for statistical machine translation with scarce resources", "Experimental results are reported in Section 7", "Section 8 concludes the presentation with a discussion of the achievements of this work", "12 Statistical Machine Translation", "In statistical machine translation every target language string eI  e1  eI is assigned a probability PreI  of being a valid word sequence in the target language and a probability PreI f J  of being a translation for the given source language string f J  1 1 1 f1  fJ  According to Bayes decision rule the optimal translation for f J is the target string that maximizes the product of the target language model PreI  and the string translation model Prf J eI ", "Many existing systems for statistical machine translation 1 1 GarcaVarea and Casacuberta 2001 Germann et al 2001 Nieen et al 1998 Och Tillmann and Ney 1999 implement models presented by Brown Della Pietra Della Pietra and Mercer 1993 The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position", "The probability that a certain target language word will occur in the target string is assumed to depend basically only on the source words aligned with it", "13 Related Work", "131 Morphology", "Some publications have already dealt with the treatment of mor phology in the framework of language modeling and speech recognition Kanevsky Roukos and Sedivy 1997 propose a statistical language model for inflected languagesThey decompose word forms into stems and affixes", "Maltese and Mancini 1992 re port that a linear interpolation of word ngrams part of speech ngrams and lemma ngrams yields lower perplexity than pure wordbased models", "Larson et al", "2000 apply a datadriven algorithm for decomposing compound words in compounding languages as well as for recombining phrases to enhance the pronunciation lexicon and the language model for largevocabulary speech recognition systems", "As regards machine translation the treatment of morphology is part of the analysis and generation step in virtually every symbolic machine translation system", "For this purpose the lexicon should contain base forms of words and the grammatical category 182 subcategorization features and semantic information in order to enable the size of the lexicon to be reduced and in order to account for unknown word forms that is word forms not present explicitly in the dictionary", "Todays statistical machine translation systems build upon the work of P F Brown and his colleagues at IBM", "The translation models they presented in various papers between 1988 and 1993 Brown et al 1988 Brown et al 1990 Brown Della Pietra Della Pietra and Mercer 1993 are commonly referred to as IBM models 15 based on the numbering in Brown Della Pietra Della Pietra and Mercer 1993", "The underlying probabilistic lexicon contains only pairs of full forms", "On the other hand Brown et al", "1992 had already suggested word forms be annotated with morphosyntactic information but they did not perform any investigation on the effects", "132 Translation with Scarce Resources", "Some recent publications like AlOnaizan et al", "2000 have dealt with the problem of translation with scarce resources", "AlOnaizan et al report on an experiment involving TetuntoEnglish translation by different groups including one using statistical machine translation", "AlOnaizan et al assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries", "Nevertheless they found that the human mind is very well capable of deriving dependencies such as morphology cognates proper names and spelling variations and that this capability was finally at the basis of the better results produced by humans compared to corpusbased machine translation", "The additional information results from complex reasoning and it is not directly accessible from the fullword form representation in the data", "This article takes a different point of view Even if full bilingual training data are scarce monolingual knowledge sources like morphological analyzers and data for training the target language model as well as conventional dictionaries one word and its translations per entry may be available and of substantial usefulness for improving the performance of statistical translation systems", "This is especially the case for moreinflecting major languages like German", "The use of dictionaries to augment or replace parallel corpora has already been examined by Brown Della Pietra Della Pietra and Goldsmith 1993 and Koehn and Knight 2001 for instance"]}, "J04-4002": {"title": ["c 2004 Association for Computational Linguistics The Alignment Template Approach to Statistical Machine Translation Franz Josef Och Hermann Ney Google RWTH Aachen A phrasebased statistical machine translation approach  the alignment template approach  is described This translation approach allows for general manytomany relations between words Thereby the context of words is taken into account in the translation model and local changes in word order from source to target language can be learned explicitly The model is described using a loglinear modeling approach which is a generalization of the often used source channel approach Thereby the model is easier to extend than classical statistical machine translation systems We describe in detail the process for learning phrasal translations the feature functions used and the search algorithm The evaluation of this approach is performed on three different tasks For the German English speech Verbmobil task we analyze the effect of various system components On the French English Canadian Hansards task the alignment template system obtains significantly better results than a singlewordbased translation model In the ChineseEnglish 2002 National Institute of Standards and Technology NIST machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems"], "abstract": ["A phrasebased statistical machine translation approach  the alignment template approach  is described", "This translation approach allows for general manytomany relations between words", "Thereby the context of words is taken into account in the translation model and local changes in word order from source to target language can be learned explicitly", "The model is described using a loglinear modeling approach which is a generalization of the often used sourcechannel approach", "Thereby the model is easier to extend than classical statistical machine translation systems", "We describe in detail the process for learning phrasal translations the feature functions used and the search algorithm", "The evaluation of this approach is performed on three different tasks", "For the GermanEnglish speech Verbmobil task we analyze the effect of various system components", "On the FrenchEnglish Canadian Hansards task the alignment template system obtains significantly better results than a singlewordbased translation model", "In the ChineseEnglish 2002 National Institute of Standards and Technology NIST machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems"], "inroduction": ["Machine translation MT is a hard problem because natural languages are highly complex many words have various meanings and different possible translations sentences might have various readings and the relationships between linguistic entities are often vague", "In addition it is sometimes necessary to take world knowledge into account", "The number of relevant dependencies is much too large and those dependencies are too complex to take them all into account in a machine translation system", "Given these boundary conditions a machine translation system has to make decisions produce translations given incomplete knowledge", "In such a case a principled approach to solving that problem is to use the concepts of statistical decision theory to try to make optimal decisions given incomplete knowledge", "This is the goal of statistical machine translation", "The use of statistical techniques in machine translation has led to dramatic improvements in the quality of research systems in recent years", "For example the statistical approaches of the Verbmobil evaluations Wahlster 2000 or the US National  1600 Amphitheatre Parkway Mountain View CA 94043", "Email ochgooglecom", " Lehrstuhl fu r Informatik VI Computer Science Department RWTH AachenUniversity of Technology Ahornstr", "55 52056 Aachen Germany", "Email neycsrwthaachende", "Submission received 19 November 2002 Revised submission received 7 October 2003 Accepted for publication 1 June 2004 c 2004 Association for Computational Linguistics Institute of Standards and Technology NISTTIDES MT evaluations 2001 through 20031 obtain the best results", "In addition the field of statistical machine translation israpidly progressing and the quality of systems is getting better and better", "An im portant factor in these improvements is definitely the availability of large amounts of data for training statistical models", "Yet the modeling training and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s Brown et al 1990 Brown et al 1993 Berger etal", "1994", "This article focuses on an important improvement namely the use of gen eralized phrases instead of just single words as the core elements of the statistical translation model", "We describe in Section 2 the basics of our statistical translation model", "We suggest the use of a loglinear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters", "This approach can be seen as a generalization of the originally suggested sourcechannel modeling framework for statistical machine translation", "In Section 3 we describe the statistical alignment models used to obtain a word alignment and techniques for learning phrase translations from word alignments", "Here the term phrase just refers to a consecutive sequence of words occurring in text and has to be distinguished from the use of the term in a linguistic sense", "The learned bilingual phrases are not constrained by linguistic phrase boundaries", "Compared to the wordbased statistical translation models in Brown et al", "1993 this model is based on a statistical phrase lexicon instead of a singlewordbased lexicon", "Looking at the results of the recent machine translation evaluations this approach seems currently to give the best results and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes Marcu and Wong 2002 Venugopal Vogel and Waibel 2003 Tillmann 2003 Koehn Och and Marcu 2003", "Our approach to learning a phrase translation lexicon works in two stages In the first stage we compute an alignment between words and in the second stage we extract the aligned phrase pairs", "In our machine translation system we then use generalized versions of these phrases called alignment templates that also include the word alignment and use word classes instead of the words themselves", "In Section 4 we describe the various components of the statistical translationmodel", "The backbone of the translation model is the alignment template feature function which requires that a translation of a new sentence be composed of a set of align ment templates that covers the source sentence and the produced translation", "Other feature functions score the wellformedness of the produced target language sentenceie language model feature functions the number of produced words or the or der of the alignment templates", "Note that all components of our statistical machine translation model are purely datadriven and that there is no need for linguisticallyannotated corpora", "This is an important advantage compared to syntaxbased trans lation models Yamada and Knight 2001 Gildea 2003 Charniak Knight and Yamada 2003 that require a parser for source or target language", "In Section 5 we describe in detail our search algorithm and discuss an efficient implementation", "We use a dynamicprogrammingbased beam search algorithm that allows a tradeoff between efficiency and quality", "We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size", "In Section 6 we describe various results obtained on different tasks", "For theGermanEnglish Verbmobil task we analyze the effect of various system compo 1 httpwwwnistgovspeechtestsmt", "418 Figure 1 Architecture of the translation approach based on a loglinear modeling approach", "nents", "On the FrenchEnglish Canadian Hansards task the alignment template system obtains significantly better results than a singlewordbased translation model", "In the ChineseEnglish 2002 NIST machine translation evaluation it yields results that are significantly better statistically than all competing research and commercial translation systems"]}, "J05-1004": {"title": ["The  Proposition Bank  An Annotated"], "abstract": ["The Proposition Bank project takes a practical approach to semantic representation adding a layer of predicateargument information or semantic role labels to the syntactic structures of the Penn Treebank", "The resulting resource can be thought of as shallow in that it does not represent coreference quantification and many other higherorder phenomena but also broad in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated", "We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntacticsemantic alternations in the corpus", "We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information including a comparison of full syntactic parsing with a f lat representation and the contribution of the empty trace categories of the treebank"], "inroduction": ["Robust syntactic parsers made possible by new statistical techniques Ratnaparkhi 1997 Collins 1999 2000 Bangalore and Joshi 1999 Charniak 2000 and by the availability of large handannotated training corpora Marcus Santorini and Marcinkiewicz 1993 Abeille 2003 have had a major impact on the field of natural language processing in recent years", "However the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed", "As a simple example in the sentences 1 John broke the window", "2 The window broke", "a syntactic analysis will represent the window as the verbs direct object in the first sentence and its subject in the second but does not indicate that it plays the same underlying semantic role in both cases", "Note that both sentences are in the active voice  Department of Computer and Information Science University of Pennsylvania 3330 Walnut Street Philadelphia PA 19104", "Email mpalmercisupennedu", "Department of Computer Science University of Rochester PO Box 270226 Rochester NY 14627", "Email", "gildeacsrochesteredu", "Submission received 9th December 2003 Accepted for publication 11th July 2004  2005 Association for Computational Linguistics and that this alternation in subject between transitive and intransitive uses of the verb does not always occur for example in the sentences 3 The sergeant played taps", "4 The sergeant played", "the subject has the same semantic role in both uses", "The same verb can also undergo syntactic alternation as in 5 Taps played quietly in the background", "and even in transitive uses the role of the verbs direct object can differ 6 The sergeant played taps", "7 The sergeant played a beatup old bugle", "Alternation in the syntactic realization of semantic arguments is widespread affecting most English verbs in some way and the patterns exhibited by specific verbs vary widely Levin 1993", "The syntactic annotation of the Penn Treebank makes it possible to identify the subjects and objects of verbs in sentences such as the above examples", "While the treebank provides semantic function tags such as temporal and locative for certain constituents generally syntactic adjuncts it does not distinguish the different roles played by a verbs grammatical subject or object in the above examples", "Because the same verb used with the same syntactic subcategorization can assign different semantic roles roles cannot be deterministically added to the treebank by an automatic conversion process with 100 accuracy", "Our semanticrole annotation process begins with a rulebased automatic tagger the output of which is then hand corrected see section 4 for details", "The Proposition Bank aims to provide a broadcoverage handannotated corpus of such phenomena enabling the development of better domainindependent language understanding systems and the quantitative study of how and why these syntactic alternations take place", "We define a set of underlying semantic roles for each verb and annotate each occurrence in the text of the original Penn Treebank", "Each verbs roles are numbered as in the following occurrences of the verb offer from our data 8 ", " Arg0 the company to  offer  a 15 to 20 stake  Arg2 to the public wsj03451 9 ", " Arg0 Sothebys  offered  Arg2 the Dorrance heirs  Arg1a money back guarantee wsj1928 10 ", " an amendment offered  Arg0 by Rep Peter DeFazio ", "wsj0107 11 ", " Subcontractors will be offered  Arg1 a settlement ", "wsj0187 We believe that providing this level of semantic representation is important for applications including information extraction question answering and machine 1 Example sentences drawn from the treebank corpus are identified by the number of the file in which they", "occur", "Constructed examples usually feature John", "72 translation", "Over the past decade most work in the field of information extraction has shifted from complex rulebased systems designed to handle a wide variety of semantic phenomena including quantification anaphora aspect and modality eg Alshawi 1992 to more robust finitestate or statistical systems Hobbs et al 1997 Miller et al 1998", "These newer systems rely on a shallower level of semantic representation similar to the level we adopt for the Proposition Bank but have also tended to be very domain specific", "The systems are trained and evaluated on corpora annotated for semantic relations pertaining to for example corporate acquisitions or terrorist events", "The Proposition Bank PropBank takes a similar approach in that we annotate predicates semantic roles while steering clear of the issues involved in quantification and discourselevel structure", "By annotating semantic roles for every verb in our corpus we provide a more domainindependent resource which we hope will lead to more robust and broadcoverage natural language understanding systems", "The Proposition Bank focuses on the argument structure of verbs and provides a complete corpus annotated with semantic roles including roles traditionally viewed as arguments and as adjuncts", "It allows us for the first time to determine the frequency of syntactic variations in practice the problems they pose for natural language understanding and the strategies to which they may be susceptible", "We begin the article by giving examples of the variation in the syntactic realization of semantic arguments and drawing connections to previous research into verb alternation behavior", "In section 3 we describe our approach to semanticrole annotation including the types of roles chosen and the guidelines for the annotators", "Section 5 compares our PropBank methodology and choice of semanticrole labels to those of another semantic annotation project FrameNet", "We conclude the article with a discussion of several preliminary experiments we have performed using the PropBank annotations and discuss the implications for natural language research"]}, "J06-1004": {"title": ["FiniteState Registered  Automata"], "abstract": ["We introduce finitestate registered automata FSRAs a new computational device within the framework of finitestate technology specifically tailored for implementing nonconcatenative morphological processes", "This model extends and augments existing finitestate techniques which are presently not optimized for describing this kind of phenomena", "We first define the model and discuss its mathematical and computational properties", "Then we provide an extended regular language whose expressions denote FSRAs", "Finally we exemplify the utility of the model by providing several examples of complex morphological and phonological phenomena which are elegantly implemented with FSRAs"], "inroduction": ["Finitestate FS technology has been considered adequate for describing the morphological processes of the worlds languages since the pioneering works of Koskenniemi 1983 and Kaplan and Kay 1994", "Several toolboxes provide extended regular expression description languages and compilers of the expressions to finitestate automata FSAs and transducers FSTs Karttunen et al 1996 Mohri 1996 van Noord and Gerdemann 2001a", "While FS approaches to most natural languages have generally been very successful it is widely recognized that they are less suitable for nonconcatenative phenomena in particular FS techniques are assumed not to be able to efficiently account for the nonconcatenative word formation processes that Semitic languages exhibit Lavie et al 1988", "While much of the inflectional morphology of Semitic languages can be rather straightforwardly described using concatenation as the primary operation the main word formation process in such languages is inherently nonconcatenative", "The standard account describes words in Semitic languages as combinations of two morphemes a root and a pattern1 The root consists of consonants only by default three although longer roots are known", "The pattern is a combination of vowels and possibly consonants too with slots into which the root consonants can be inserted", "Words are created by interdigitating roots into patterns The first consonant of the root is inserted into the first consonantal slot of the pattern the second root consonant fills the second slot and the third fills the last slot", "After the root combines with the pattern some  Department of Computer Science University of Haifa 31905 Haifa Israel", "Email yaelccshaifaacil", " Department of Computer Science University of Haifa 31905 Haifa Israel", "Email shulycshaifaacil", "1 An additional morpheme vocalization is used to abstract the pattern further for the present purposes", "this distinction is irrelevant", "Submission received 17 August 2004 revised submission received 15 June 2005 accepted for publication 26 September 2005", " 2006 Association for Computational Linguistics Figure 1 Nave FSA with duplicated paths", "morphophonological alternations take place which may be nontrivial but are mostly concatenative", "The major problem that we tackle in this work is mediumdistance dependencies whereby some elements that are related to each other in some deeplevel representation eg the consonants of the root are separated on the surface", "While these phenomena do not lie outside the descriptive power of FS systems navely implementing them in existing finitestate calculi is either impossible or at best results in large networks that are inefficient to process as the following examples demonstrate", "Example 1 We begin with a simplified problem namely accounting for circumfixes", "Consider three Hebrew patterns haDDaDa hitDaDaDut and miDDaD where the empty boxes indicate the slots in the patterns into which the consonants of the roots are inserted", "Hebrew orthography2 dictates that these patterns be written hDDDa htDDDut and mDDD respectively ie the consonants are inserted into the D slots as one unit ie the patterns can be viewed as circumfixes", "An automaton that accepts all the possible combinations of threeconsonant stems and these three circumfixes is illustrated in Figure 13 Given r stems and p circumfixes the number of its states is 2r  2p  2 ie increases linearly with the number of stems and circumfixes", "The number of arcs in this automaton is 3rp  2p ie also Orp", "Evidently the three basic different paths that result from the three circumfixes have the same body which encodes the stems", "An attempt to avoid the duplication of paths is represented by the automaton of Figure 2 which accepts the language denoted by the regular expression ht  h  mrootut  a  E", "The number of states here is 2r  4 ie is independent of the number of circumfixes", "The number of arcs is 3r  2p that is Or  p and thus the complexity of the number of arcs is also reduced", "Obviously however such an automaton overgenerates by accepting also invalid words such as mDDDut", "In other words it ignores the dependencies which hold between prefixes and suffixes of the same circumfix", "Since finitestate devices have no 3 This is an oversimplified example in practice the process of combining roots with patterns is highly", "idiosyncratic like other derivational morphological processes", "50 Figure 2 Overgenerating FSA", "memory save for the states there is no simple and spaceefficient way to account for such dependencies", "Example 2 Consider now a representation of Hebrew where all vowels are explicit eg the pattern hitDaDeD", "Consider also the roots rgz bl and gbr The consonants of a given root are inserted into the D slots to obtain bases such as hitragez hitbael and hitgaber", "The finite state automaton of Figure 3 is the minimized automaton accepting the language it has fifteen states", "If the number of three letter roots is r then a general automaton accepting the combinations of the roots with this pattern will have 4r  3 states and 5r  1 arcs", "Notice the duplicated arcs which stem from copying the pattern in the different paths", "Example 3 Another nonconcatenative process is reduplication The process in which a morpheme or part of it is duplicated", "Full reduplication is used as a pluralization process in Malay and Indonesian partial reduplication is found in Chamorro to indicate intensivity", "It can also be found in Hebrew as a diminutive formation of nouns and adjectives kel eb kl ab la b a pa n p an pa n z a q a n zq an qa n  a x o r x ar xa r dog pu pp y ra bb it bu nn y b e a r d go at ee bl a c k da rk qa ta n litt le q ta n ta n ti n y Let  be a finite alphabet", "The language L  ww  w   is known to be transregular therefore no finitestate automaton accepts it", "However the language Ln  ww  w   w  n for some constant n is regular", "Recognizing Ln is a finite approximation of the general problem of recognizing L The length of the words in natural languages can in most cases be bounded by some n  N hence the amount of reduplication in natural languages is practically limited", "Therefore the descriptive power of Ln is sufficient for the amount of reduplication in natural languages by Figure 3 FSA for the pattern hitDaDeD", "51 constructing Ln for a small number of different ns", "An automaton that accepts Ln can be constructed by listing a path for each accepted string since  and n are finite the number of words in Ln is finite", "The main drawback of such an automaton is thegrowth in its size as  and n increase The number of strings in Ln is  n  Thus finite state techniques can account for limited reduplication but the resulting networks are spaceinefficient", "As a final nonlinguistic motivating example consider the problem of nbit incrementation introduced by Kornai 1996", "Example 4 The goal of this example is to construct a transducer over   0 1 whose input is a 32 bit binary number and whose output is the result of adding 1 to the input", "A transducer that performs addition by 1 on binary numbers has only 5 states and 12 arcs4 but this transducer is neither sequential nor sequentiable", "The problem is that since the input is scanned left to right but the carry moves right to left the output of the first bit has to be delayed possibly even until the last input bit is scanned", "Thus for an nbit binary incrementor 2n disjunctions have to be considered and therefore a minimized transducer has to assign a separate state to each combination of bits resulting in 2n states and a similar number of transitions", "In this work we propose a novel FS model which facilitates the expression of mediumdistance dependencies such as interdigitation and reduplication in an efficient way", "Our main motivation is theoretical ie reducing the complexity of the number of states and arcs in the networks we show that these theoretical contributions result in practical improvements", "In Section 3 we define the model formally show that it is equivalent to FSAs and define many closure properties directly5 We then Section 4 define a regular expression language for denoting FSRAs", "In Section 5 we provide dedicated regular expression operators for some nonconcatenative phenomena and exemplify the usefulness of the model by efficiently accounting for the motivating examples", "In Section 6 we extend FSRAs to transducers", "The model is evaluated through an actual implementation in Section 7", "We conclude with suggestions for future research"]}, "J07-4005": {"title": ["Unsupervised Acquisition of Predominant"], "abstract": ["There has been a great deal of recent research into word sense disambiguation particularly since the inception of the Senseval evaluation exercises", "Because a word often has more than one meaning resolving word sense ambiguity could benefit applications that need some level of semantic interpretation of language input", "A major problem is that the accuracy of word sense disambiguation systems is strongly dependent on the quantity of manually sensetagged data available and even the best systems when tagging every word token in a document perform little better than a simple heuristic that guesses the first or predominant sense of a word in all contexts", "The success of this heuristic is due to the skewed nature of word sense distributions", "Data for the heuristic can come from either dictionaries or a sample of sense tagged data", "However there is a limited supply of the latter and the sense distributions and predominant sense of a word can depend on the domain or source of a document", "The first sense of star for example would be different in the popular press and scientific journals", "In this article we expand on a previously proposed method for determining the predominant sense of a word automatically from raw text", "We look at a number of different data sources and parameterizations of the method using evaluation results and error analyses to identify where the method performs well and also where it does not", "In particular we find that the method does not work as well for verbs and adverbs as nouns and adjectives but produces more accurate predominant sense information than the widely used SemCor corpus for nouns with low coverage in that corpus", "We further show that the method is able to adapt successfully to domains when using domain specific corpora as input and where the input can either be handlabeled for domain or automatically classified", " Department of Informatics Brighton BN1 9QH UK", "Email dianamrobkjuliewejohncasussexacuk", "Submission received 16 November 2005 revised submission received 12 July 2006 accepted for publication 16 February 2007", " 2007 Association for Computational Linguistics"], "inroduction": ["In word sense disambiguation the first sense heuristic choosing the first or predominant sense of a word is used by most stateoftheart systems as a backoff method when information from the context is not sufficient to make a more informed choice", "In this article we present an indepth study of a method for automatically acquiring predominant senses for words from raw text McCarthy et al 2004a", "The method uses distributionally similar words listed as nearest neighbors in automatically acquired thesauruses eg Lin 1998a and takes advantage of the observation that the more prevalent a sense of a word the more neighbors will relate to that sense and the higher their distributional similarity scores will be", "The senses of a word are defined in a sense inventory", "We use WordNet Fellbaum 1998 because this is widely used is publicly available and has plenty of goldstandard evaluation data available Miller et al 1993 Cotton et al 2001 Preiss and Yarowsky 2001 Mihalcea and Edmonds 2004", "The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic similarity which relies on the relationships between word senses such as hyponyms available in an inventory such as WordNet or overlap in the definitions of word senses available in most dictionaries or both", "In this article we provide a detailed discussion and quantitative analysis of the motivation behind the first sense heuristic and a full description of our method", "We extend previously reported work in a number of different directions  We evaluate the method on all parts of speech PoS on SemCor Miller et al 1993", "Previous experiments McCarthy et al 2004c evaluated only nouns on SemCor or all PoS but only on the Senseval2 Cotton et al 2001 and Senseval3 Mihalcea and Edmonds 2004 data", "The evaluation on all PoS is much more extensive because the SemCor corpus is composed of 220000 words in contrast to the 6 documents in the Senseval2 and 3 English all words data 10000 words", " We compare two WordNet similarity measures in our evaluation on nouns and also contrast performance using two publicly available thesauruses both produced from the same NEWSWIRE corpus but one derived using a proximitybased approach and the other using dependency relations from a parser", "It turns out that the results from the proximitybased thesaurus are comparable to those from the dependency based thesaurus this is encouraging for applying the method to languages without sophisticated analysis tools", " We manually analyze a sample of errors from the SemCor evaluation", "A small number of errors can be traced back to inherent shortcomings of our method but the main source of error is due to noise from related senses", "This is a common problem for all WSD systems Ide and Wilks 2006 but one which is only recently starting to be addressed by the WSD community Navigli Litkowski and Hargraves 2007", " One motivation for an automatic method for acquiring predominant senses is that there will always be words for which there are insufficient data available in manually sensetagged resources", "We compare the performance of our automatic method with the first sense heuristic derived from SemCor on nouns in the Senseval2 data", "We find that the 554 automatic method outperforms the one obtained from manual annotations in SemCor for nouns with fewer than five occurrences in SemCor", " Aside from the lack of coverage of manually annotated data there is a need for first sense heuristics to be specific to domain", "We explore the potential for applying the method with domainspecific text for all PoS in an experiment using a goldstandard domainspecific resource Magnini and Cavaglia 2000 which we have used previously only with nouns", "We show that although there is a little mileage to be had from domainspecific first sense heuristics for verbs nouns benefit greatly from domainspecific training", " In previous work Koeling McCarthy and Carroll 2005 we produced manually senseannotated domainspecific test corpora for a lexical sample and demonstrated that predominant senses acquired from handclassified corpora in the same domain as the test data outperformed the SemCor first sense", "We further this exploration by contrasting with results from training on automatically categorized text from the English Gigaword Corpus and show that the results are comparable to those using handclassified domain data", "The article is organized as follows", "In the next section we motivate the use of predominant sense information in WSD systems and the need for acquiring this information automatically", "In Section 3 we give an overview of related work in WSD focusing on the acquisition of prior sense distributions and domainspecific sense information", "Section 4 describes our acquisition method", "Section 5 describes the experimental setup for the work reported in this article", "Section 6 describes four experiments", "The first evaluates the first sense heuristic using predominant sense information acquired for all PoS on SemCor for nouns we compare two semantic similarity methods and three different types of distributional thesaurus", "We also report an error analysis for all PoS of our method", "The second experiment compares the performance of the automatic method to the manually produced data in SemCor on nouns in the Senseval2 data looking particularly at nouns which have a low frequency in SemCor", "The third uses corpora in restricted domains and the subject field code gold standard of Magnini and Cavaglia 2000 to investigate the potential for domainspecific rankings for different PoS", "The fourth compares results when we train and test on domainspecific corpora where the training data is 1 manually categorized for domain and from the same corpus as the test data and 2 where the training data is harvested automatically from another corpus which is categorized automatically", "Finally we conclude Section 7 and discuss directions for future work Section 8"]}, "J09-3004": {"title": ["Bootstrapping Distributional Feature"], "abstract": ["This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity", "The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment", "Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufcient quality of the word feature vectors caused by decient feature weighting", "This observation led to the denition of a bootstrapping scheme which yields improved feature weights and hence higher quality feature vectors", "The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings and thus should be promoted", "This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space", "The superior performance of the bootstrapping method was assessed in two different experiments one based on direct human goldstandard annotation and the other based on an automatically created disambiguation dataset", "These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions", "Improved feature weighting also allows massive feature reduction which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector", "Finally experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied"], "inroduction": ["11 Motivation", "Distributional word similarity has long been an active research area Hindle 1990 Ruge 1992 Grefenstette 1994 Lee 1997 Lin 1998 Dagan Lee and Pereira 1999 Weeds and  Department of Information Science BarIlan University RamatGan Israel", "Email zhitomimmailbiuacil", " Department of Computer Science BarIlan University RamatGan Israel", "Email dagancsbiuacil", "Submission received 6 December 2006 revised submission received 9 July 2008 accepted for publication 21November 2008", " 2009 Association for Computational Linguistics Weir 2005", "This paradigm is inspired by Harriss distributional hypothesis Harris 1968 which states that semantically similar words tend to appear in similar contexts", "In a computational realization each word is characterized by a weighted feature vector where features typically correspond to other words that cooccur with the characterized word in the context", "Distributional similarity measures quantify the degree of similarity between a pair of such feature vectors", "It is then assumed that two words that occur within similar contexts as measured by similarity of their context vectors are indeed semantically similar", "The distributional word similarity measures were often applied for two types of inferences", "The rst type is making similaritybased generalizations for smoothing word cooccurrence probabilities in applications such as language modeling and disambiguation", "For example assume that we need to estimate the likelihood of the verb object cooccurrence pair visitcountry although it did not appear in our sample corpus", "Cooccurrences of the verb visit with words that are distributionally similar to country such as state city and region however do appear in the corpus", "Consequently we may infer that visitcountry is also a plausible expression using some mathematical scheme of similaritybased generalization Essen and Steinbiss 1992 Dagan Marcus and Markovitch 1995 Karov and Edelman 1996 Ng and Lee 1996 Ng 1997 Dagan Lee and Pereira 1999 Lee 1999 Weeds and Weir 2005", "The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts", "A second type of semantic inference which primarily motivated our own research is meaningpreserving lexical substitution", "Many NLP applications such as question answering information retrieval information extraction and multidocument sum marization need to recognize that one word can be substituted by another one in a given context while preserving or entailing the original meaning", "Naturally recognizing such substitutable lexical entailments is a prominent component within the textual entailment recognition paradigm which models semantic inference as an application independent task Dagan Glickman and Magnini 2006", "Accordingly several textual entailment systems did utilize the output of distributional similarity measures to model entailing lexical substitutions Jijkoun and de Rijke 2005 Adams 2006 Ferrandez et al 2006 Nicholson Stokes and Baldwin 2006 Vanderwende Menezes and Snow 2006", "In some of these papers the distributional information typically complements manual lexical resources in textual entailment systems most notably WordNet Fellbaum 1998", "Lexical substitution typically requires that the meaning of one word entails the meaning of the other", "For instance in question answering the word company in a question can be substituted in an answer text by rm automaker or subsidiary whose meanings entail the meaning of company", "However as it turns out traditional distributional similarity measures do not capture well such lexical substitution relationships but rather capture a somewhat broader and looser notion of semantic similarity", "For example quite distant cohyponyms such as party and company also come out as distributionally similar to country due to a partial overlap of their semantic properties", "Clearly the meanings of these words do not entail each other", "Motivated by these observations our longterm goal is to investigate whether the distributional similarity scheme may be improved to yield tighter semantic similarities and eventually better approximation of lexical entailments", "This article presents one component of this research plan which focuses on improving the underlying semantic 436 quality of distributional word feature vectors", "The article describes the methodology denitions and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance", "12 Main Contributions and Outline", "As a starting point for our investigation an operational denition was needed for evaluating the correctness of candidate pairs of similar words", "Following the lexical substitution motivation in Section 3 we formulate the substitutable lexical entailment relation or lexical entailment for brevity rening earlier denitions in Geffet and Dagan 2004 2005", "Generally speaking this relation holds for a pair of words if a possible meaning of one word entails a meaning of the other and the entailing word can substitute the entailed one in some typical contexts", "Lexical entailment overlaps partly with traditional lexical semantic relationships while capturing more generally the lexical substitution needs of applications", "Empirically high interannotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment", "Next we analyzed the typical behavior of existing word similarity measures relative to the lexical entailment criterion", "Choosing the commonly used measure of Lin 1998 as a representative case the analysis shows that quite noisy feature vectors are a major cause for generating rather loose semantic similarities", "On the other hand one may expect that features which seem to be most characteristic for a words meaning should receive the highest feature weights", "This does not seem to be the case however for common feature weighting functions such as Pointwise Mutual Information Church and Patrick 1990 Hindle 1990", "Following these observations we developed a bootstrapping formula that improves the original feature weights Section 4 leading to better feature vectors and better similarity predictions", "The general idea is to promote the weights of features that are common for semantically similar words since these features are likely to be most characteristic for the words meaning", "This idea is implemented by a bootstrapping scheme where the initial and cruder similarity measure provides an initial approximation for semantic word similarity", "The bootstrapping method yields a high concentration of semantically characteristic features among the topranked features of the vector which also allows aggressive feature reduction", "The bootstrapping scheme was evaluated in two experimental settings which correspond to the two types of applications for distributional similarity", "First it achieved signicant improvements in predicting lexical entailment as assessed by human judgments when applied over several base similarity measures Section 5", "Additional analysis relative to the lexical entailment dataset revealed cleaner and more characteristic feature vectors for the bootstrapping method", "To obtain a quantitative analysis of this behavior we dened a measure called average commonfeature rank ratio", "This measure captures the idea that a prominent feature for a word is expected to be prominent also for semantically similar words while being less prominent for unrelated words", "To the best of our knowledge this is the rst proposed measure for direct analysis of the quality of feature weighting functions without the need to employ them within some vector similarity measure", "As a second evaluation we applied the bootstrapping scheme for similaritybased prediction of cooccurrence likelihood within a typical pseudoword sense disambiguation experiment obtaining substantial error reductions Section 7", "Section 8 concludes 437 this article suggesting the relevance of our analysis and bootstrapping scheme for the general use of distributional feature vectors1"]}, "J10-3003": {"title": ["Generating Phrasal and Sentential Paraphrases A Survey of DataDriven Methods Nitin Madnani University of Maryland College Park Bonnie J Dorr University of Maryland College Park The task of paraphrasing is inherently familiar to speakers of all languages Moreover the task of automatically generating or extracting semantic equivalences for the various units of language words phrases and sentences is an important part of natural language processing NLP and is being increasingly employed to improve the performance of several NLP applications In this article we attempt to conduct a comprehensive and application independent survey of datadriven phrasal and sentential paraphrase generation methods while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research Recent work done in manual and automatic construction of paraphrase corpora is also examined We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation"], "abstract": ["The task of paraphrasing is inherently familiar to speakers of all languages", "Moreover the task of automatically generating or extracting semantic equivalences for the various units of language words phrases and sentencesis an important part of natural language processing NLP and is being increasingly employed to improve the performance of several NLP applications", "In this article we attempt to conduct a comprehensive and applicationindependent survey of datadriven phrasal and sentential paraphrase generation methods while also conveying an appreciation for the importance and potential use of paraphrases in the eld of NLP research", "Recent work done in manual and automatic construction of paraphrase corpora is also examined", "We also discuss the strategies used for evaluating paraphrase generation techniques and briey explore some future trends in paraphrase generation"], "inroduction": ["Although everyone may be familiar with the notion of paraphrase in its most fundamental sense there is still room for elaboration on how paraphrases may be automatically generated or elicited for use in language processing applications", "In this survey we make an attempt at such an elaboration", "An important outcome of this survey is the discovery that there are a large variety of paraphrase generation methods each with widely differing sets of characteristics in terms of performance as well as ease of deployment", "We also nd that although many paraphrase methods are developed with a particular application in mind all methods share the potential for more general applicability", "Finally we observe that the choice of the most appropriate method for an application depends on proper matching of the characteristics of the produced paraphrases with an appropriate method", "It could be argued that it is premature to survey an area of research that has shown promise but has not yet been tested for a long enough period and in enough systems", "However we believe this argument actually strengthens the motivation for a survey  Department of Computer Science and Institute for Advanced Computer Studies AV Williams Bldg University of Maryland College Park MD 20742 USA", "Email nmadnaniumiacsumdedu", " Department of Computer Science and Institute for Advanced Computer Studies AV Williams Bldg University of Maryland College Park MD 20742 USA", "Email bonnieumiacsumdedu", "Submission received 16 December 2008 revised submission received 30 November 2009 accepted for publication 7 March 2010", " 2010 Association for Computational Linguistics that can encourage the community to use paraphrases by providing an application independent cohesive and condensed discussion of datadriven paraphrase generation techniques", "We should also acknowledge related work that has been done on furthering the communitys understanding of paraphrases", "Hirst 2003 presents a comprehensive survey of paraphrasing focused on a deep analysis of the nature of a paraphrase", "The current survey focuses instead on delineating the salient characteristics of the various paraphrase generation methods with an emphasis on describing how they could be used in several different NLP applications", "Both these treatments provide different but valuable perspectives on paraphrasing", "The remainder of this section formalizes the concept of a paraphrase scopes out the coverage of this surveys discussion and provides broader context and motivation by discussing applications in which paraphrase generation has proven useful along with examples", "Section 2 briey describes the tasks of paraphrase recognition and textual entailment and their relationship to paraphrase generation and extraction", "Section 3 forms the major contribution of this survey by examining various corporabased techniques for paraphrase generation organized by corpus type", "Section 4 examines recent work done to construct various types of paraphrase corpora and to elicit human judgments for such corpora", "Section 5 considers the task of evaluating the performance of paraphrase generation and extraction techniques", "Finally Section 6 provides a brief glimpse of the future trends in paraphrase generation and Section 7 concludes the survey with a summary", "11 What is a Paraphrase", "The concept of paraphrasing is most generally dened on the basis of the principle of semantic equivalence A paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form", "Paraphrases may occur at several levels", "Individual lexical items having the same meaning are usually referred to as lexical paraphrases or more commonly synonyms for example hot warm and eat consume", "However lexical paraphrasing cannot be restricted strictly to the concept of synonymy", "There are several other forms such as hyperonymy where one of the words in the paraphrastic relationship is either more general or more specic than the other for example reply say and landlady hostess", "The term phrasal paraphrase refers to phrasal fragments sharing the same semantic content", "Although these fragments most commonly take the form of syntactic phrases work on soften up and take over assume control of  they may also be patterns with linked variables for example Y was built by X X is the creator of Y", "Two sentences that represent the same semantic content are termed sentential paraphrases for example I nished my work I completed my assignment", "Although it is possible to generate very simple sentential paraphrases by simply substituting words and phrases in the original sentence with their respective semantic equivalents it is signicantly more difcult to generate more interesting ones such as He needed to make a quick decision in that situation The scenario required him to make a splitsecond judgment", "Culicover 1968 describes some common forms of sentential paraphrases", "12 Scope of Discussion", "The idea of paraphrasing has been explored in conjunction with and employed in a large number of natural language processing applications", "Given the difculty inherent 342 in surveying such a diverse task an unfortunate but necessary remedy is to impose certain limits on the scope of our discussion", "In this survey we will be restricting our discussion to only automatic acquisition of phrasal paraphrases including paraphrastic patterns and on generation of sentential paraphrases", "More specically this entails the exclusion of certain categories of paraphrasing work", "However as a compromise for the interested reader we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey", "For one we do not discuss paraphrasing techniques that rely primarily on knowledgebased resources such as dictionaries Wallis 1993 Fujita et al 2004 handwritten rules Fujita et al 2007 and formal grammars McKeown 1979 Dras 1999 Gardent Amoia and Jacquey 2004 Gardent and Kow 2005", "We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts Inoue 1991 Crouch and Yang 1992 Pereira Tishby and Lee 1993 Grefenstette 1994 Lin 1998 Gasperin et al 2001 Glickman and Dagan 2003 Shimohata and Sumita 20051 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for specic applications are also excluded Bangalore and Rambow 2000 Duclaye Yvon and Collin 2003 Murakami and Nasukawa 2004 Kauchak and Barzilay 2006", "Methods at the other end of the spectrum that paraphrase suprasentential units such as paragraphs and entire documents are also omitted from discussion Hovy 1988 Inui and Nogami 2001 Hallett and Scott 2005 Power and Scott 2005", "Finally we also do not discuss the notion of nearsynonymy Hirst 1995 Edmonds and Hirst 2002", "13 Applications of Paraphrase Generation", "Before describing the techniques used for paraphrasing it is essential to examine the broader context of the application of paraphrases", "For some of the applications we discuss subsequently the use of paraphrases in the manner described may not yet be the norm", "However wherever applicable we cite recent research that promises gains in performance by using paraphrases for these applications", "Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey phrasal and sentential", "131 Query and Pattern Expansion", "One of the most common applications of paraphrasing is the automatic generation of query variants for submission to information retrieval systems or of patterns for submission to information extraction systems", "Culicover 1968 describes one of the earliest theoretical frameworks for query keyword expansion using paraphrases", "One of the earliest works to implement this approach Spa rckJones and Tait 1984 generates several simple variants for compound nouns in queries submitted to a technical information retrieval system", "For example Original  circuit details Variant 1  details about the circuit Variant 2  the details of circuits 1 Inferring words to be similar based on similar contexts can be thought of as the most common instance", "of employing distributional similarity", "The concept of distributional similarity also turns out to be quite important for phrasal paraphrase generation and is discussed in more detail in Section 31", "343 In fact in recent years the information retrieval community has extensively explored the task of query expansion by applying paraphrasing techniques to generate similar or related queries Beeferman and Berger 2000 Jones et al 2006 Sahami and Hellman 2006 Metzler Dumais and Meek 2007 Shi and Yang 2007", "The generation of paraphrases in these techniques is usually effected by utilizing the query log a log containing the record of all queries submitted to the system to determine semantic similarity", "Jacquemin 1999 generates morphological syntactic and semantic variants for phrases in the agricultural domain", "For example Original  simultaneous measurements Variant  concurrent measures Original  development area Variant  area of growth Ravichandran and Hovy 2002 use semisupervised learning to induce several paraphrastic patterns for each question type and use them in an opendomain question answering system", "For example for the INVENTOR question type they generate Original  X was invented by Y Variant 1  Ys invention of X Variant 2  Y inventor of X Riezler et al", "2007 expand a query by generating nbest paraphrases for the query via a pivotbased sentential paraphrasing model employing bilingual parallel corpora detailed in Section 3 and then using any new words introduced therein as additional query terms", "For example for the query how to live with cat allergies they may generate the following two paraphrases", "The novel words in the two paraphrases are highlighted in bold and are used to expand the original query P1  ways to live with feline allergy P2  how to deal with cat allergens Finally paraphrases have also been used to improve the task of relation extraction Romano et al 2006", "Most recently Bhagat and Ravichandran 2008 collect paraphras tic patterns for relation extraction by applying semisupervised paraphrase induction to a very large monolingual corpus", "For example for the relation of acquisition they collect Original  X agreed to buy Y Variant 1  X completed its acquisition of Y Variant 2  X purchased Y 132 Expanding Sparse Human Reference Data for Evaluation", "A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 task for a given set of data and using the output so created as a reference against which to measure the performance of the system", "The two applications where comparison against humanauthored reference output has become the norm are machine translation and document summarization", "In machine translation evaluation the translation hypotheses output by a machine translation system are evaluated against reference translations created by human translators by measuring the ngram overlap between the two Papineni et al 2002", "However it is impossible for a single reference translation to capture all possible verbalizations that can convey the same semantic content", "This may unfairly penalize translation hypotheses that have the same meaning but use ngrams that are not present in the reference", "For example the given system output S will not have a high score against the reference R even though it conveys precisely the same semantic content S We must consider the entire community", "R We must bear in mind the community as a whole", "One solution is to use multiple reference translations which is expensive", "An alternative solution tried in a number of recent approaches is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically even if not lexically correct Owczarzak et al 2006 Zhou Lin and Hovy 2006", "In evaluation of document summarization automatically generated summaries peers are also evaluated against reference summaries created by human authors models", "Zhou et al", "2006 propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of ngram overlap between peer summaries and multiple model summaries", "133 Machine Translation", "Besides being used in evaluation of machine translation systems paraphrasing has also been applied to directly improve the translation process", "CallisonBurch Koehn and Osborne 2006 use automatically induced paraphrases to improve a statistical phrasebased machine translation system", "Such a system works by dividing the given sentence into phrases and translating each phrase individually by looking up its translation in a table", "The coverage of the translation system is improved by allowing any source phrase that does not have a translation in the table to use the translation of one of its paraphrases", "For example if a given Spanish sentence contains the phrase presidente de Brazil but the system does not have a translation for it another Spanish phrase such as presidente brasilen o may be automatically detected as a paraphrase of presidente de Brazil then if the translation table contains a translation for the paraphrase the system can use the same translation for the given phrase", "Therefore paraphrasing allows the translation system to properly handle phrases that it does not otherwise know how to translate", "Another important issue for statistical machine translation systems is that of reference sparsity", "The fundamental problem that translation systems have to face is that there is no such thing as the correct translation for any sentence", "In fact any given source sentence can often be translated into the target language in many valid ways", "Because there can be many correct answers almost all models employed by SMT systems require in addition to a large bitext a heldout development set comprising multiple highquality humanauthored reference translations in the target language in order to tune their parameters relative to a translation quality metric", "However given 345 the time and cost implications of such a process most such data sets usually have only a single reference translation", "Madnani et al", "2007 2008b generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters"]}, "J11-1005": {"title": ["Syntactic Processing Using the Generalized Perceptron and Beam Search Yue Zhang University of Cambridge Stephen Clark University of Cambridge We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model trained by the generalized perceptron together with a generic beamsearch decoder We apply the framework to word segmentation joint segmentation and POStagging dependency parsing and phrasestructure parsing Both components of the framework are conceptually and computationally very simple The beamsearch decoder only requires the syntactic processing task to be broken into a sequence of decisions such that at each stage in the process the decoder is able to consider the topn candidates and generate all possibilities for the next stage Once the decoder has been defined it is applied to the training data using trivial updates according to the generalized perceptron to induce a model This simple framework performs surprisingly well giving accuracy results competitive with the stateoftheart on all the tasks we consider The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives including loglinear and largemargin training algorithms and dynamic programming for decoding Moreover the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow We discuss how the general framework is applied to each of the problems studied in this article making comparisons with alternative learning and decoding algorithms We also show how the comparability of candidates considered by the beam is an important factor in the performance We argue that the conceptual and computational sim plicity of the framework together with its languageindependent nature make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches"], "abstract": ["We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model trained by the generalized perceptron together with a generic beam search decoder", "We apply the framework to word segmentation joint segmentation and POS tagging dependency parsing and phrasestructure parsing", "Both components of the framework are conceptually and computationally very simple", "The beamsearch decoder only requires the syntactic processing task to be broken into a sequence of decisions such that at each stage in the process the decoder is able to consider the topn candidates and generate all possibilities for the next stage", "Once the decoder has been defined it is applied to the training data using trivial updates according to the generalized perceptron to induce a model", "This simple framework performs surprisingly well giving accuracy results competitive with the stateoftheart on all the tasks we consider", "The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives including loglinear and largemargin training algorithms and dynamicprogramming for decoding", "Moreover the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow", "We discuss how the general framework is applied to each of the problems studied in this article making comparisons with alternative learning and decoding algorithms", "We also show how the comparability of candidates considered by the beam is an important factor in the performance", "We argue that the conceptual and computational simplicity of the framework together with its languageindependent nature make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches"], "inroduction": ["In this article we study a range of syntactic processing tasks using a general framework for structural prediction that consists of the generalized perceptron Collins 2002 and  University of Cambridge Computer Laboratory William Gates Building 15 JJ Thomson Avenue Cambridge UK", "Email yuezhangclcamacuk", " University of Cambridge Computer Laboratory William Gates Building 15 JJ Thomson Avenue Cambridge UK", "Email stephenclarkclcamacuk", "Submission received 10 November 2009 revised submission received 12 August 2010 accepted for publication 20 September 2010", " 2011 Association for Computational Linguistics beamsearch", "We show that the framework which is conceptually and computationally simple is practically effective for structural prediction problems that can be turned into an incremental process allowing accuracies competitive with the stateoftheart to be achieved for all the problems we consider", "The framework is extremely flexible and easily adapted to each task", "One advantage of beamsearch is that it does not impose any requirements on the structure of the problem for example the optimal subproblem property required for dynamic programming and can easily accommodate nonlocal features", "The generalized per ceptron is equally flexible relying only on a decoder for each problem and using a trivial online update procedure for each training example", "An advantage of the linear perceptron models we use is that they are global models assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions", "Here we are following a recent line of work applying global discriminative models to tagging and widecoverage parsing problems Lafferty McCallum and Pereira 2001 Collins 2002 Collins and Roark 2004 McDonald Crammer and Pereira 2005 Clark and Curran 2007 Carreras Collins and Koo 2008 Finkel Kleeman and Manning 2008", "The flexibility of our framework leads to competitive accuracies for each of the tasks we consider", "For word segmentation we show how the framework can accommodate a wordbased approach rather than the standard and more restrictive characterbased tagging approaches", "For POStagging we consider joint segmentation and POStagging showing that a single beamsearch decoder can be used to achieve a significant accuracy boost over the pipeline baseline", "For Chinese and English dependency parsing we show how both graphbased and transitionbased algorithms can be implemented as beamsearch and then combine the two approaches into a single model which outperforms both in isolation", "Finally for Chinese phrasestructure parsing we describe a global model for a shiftreduce parsing algorithm in contrast to current deterministic approaches which use only local models at each step of the parsing process", "For all these tasks we present results competitive with the best results in the literature", "In Section 2 we describe our general framework of the generic beamsearch algorithm and the generalized perceptron", "Then in the subsequent sections we describe each task in turn based on conference papers including Zhang and Clark 2007 2008a 2008b 2009 2010 presented in our single coherent framework", "We give an updated set of results plus a number of additional experiments which probe further into the advantages and disadvantages of our framework", "For the segmentation task we also compare our beamsearch framework with alternative decoding algorithms including an exact dynamicprogramming method showing that the beamsearch method is significantly faster with comparable accuracy", "For the joint segmentation and POStagging task we present a novel solution using the framework in this article and show that it gives comparable accuracies to our previous work Zhang and Clark 2008a while being more than an order of magnitude faster", "In Section 7 we provide further discussion of the framework based on the studies of the individual tasks", "We present the main advantages of the framework and give an analysis of the main reasons for the high speeds and accuracies achieved", "We also discuss how this framework can be applied to a potential new task and show that the comparability of candidates in the incremental process is an important factor to consider", "In summary we study a general framework for incremental structural prediction showing how the framework can be tailored to a range of syntactic processing problems to produce results competitive with the stateoftheart", "The conceptual and computational simplicity of the framework together with its languageindependent nature 106 make it a competitive choice that should be considered for comparison by developers of alternative approaches"]}, "J12-1003": {"title": ["Learning Entailment Relations by Global Graph Structure Optimization Jonathan Berant Tel Aviv University Ido Dagan BarIlan University Jacob Goldberger BarIlan University Identifying entailment relations between predicates is an important part of applied semantic inference In this article we propose a global inference algorithm that learns such entailment rules First we define a graph structure over predicates that represents entailment relations as directed edges Then we use a global transitivity constraint on the graph to learn the optimal set of edges formulating the optimization problem as an Integer Linear Program The algorithm is applied in a setting where given a target concept the algorithm learns on the fly all entailment rules between predicates that co occur with this concept Results show that our global algorithm improves performance over baseline algorithms by more than 10"], "abstract": ["Identifying entailment relations between predicates is an important part of applied semantic inference", "In this article we propose a global inference algorithm that learns such entailment rules", "First we dene a graph structure over predicates that represents entailment relations as directed edges", "Then we use a global transitivity constraint on the graph to learn the optimal set of edges formulating the optimization problem as an Integer Linear Program", "The algorithm is applied in a setting where given a target concept the algorithm learns on the y all entailment rules between predicates that cooccur with this concept", "Results show that our global algorithm improves performance over baseline algorithms by more than 10"], "inroduction": ["The Textual Entailment TE paradigm is a generic framework for applied semantic inference", "The objective of TE is to recognize whether a target textual meaning can be inferred from another given text", "For example a question answering system has to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces blood pressure to answer the question What affects blood pressure", "In the TE framework entailment is dened as a directional relationship between pairs of text expressions denoted by T the entailing text and H the entailed hypothesis", "The text T is said to entail the hypothesis H if typically a human reading T would infer that H is most likely true Dagan et al 2009", "TE systems require extensive knowledge of entailment patterns often captured as entailment rulesrules that specify a directional inference relation between two text fragments when the rule is bidirectional this is known as paraphrasing", "A common type of text fragment is a proposition which is a simple natural language expression that contains a predicate and arguments such as alcohol affects blood pressure where the predicate denotes some semantic relation between the concepts that are expressed  TelAviv University PO Box 39040 TelAviv 69978 Israel", "Email jonatha6posttauacil", " BarIlan University RamatGan 52900 Israel", "Email dagancsbiuacil", " BarIlan University RamatGan 52900 Israel", "Email goldbejengbiuacil", "Submission received 28 September 2010 revised submission received 5 May 2011 accepted for publication 5 July 2011", " 2012 Association for Computational Linguistics by the arguments", "One important type of entailment rule species entailment between propositional templates that is propositions where the arguments are possibly replaced by variables", "A rule corresponding to the aforementioned example may be X reduce blood pressure  X affect blood pressure", "Because facts and knowledge are mostly expressed by propositions such entailment rules are central to the TE task", "This has led to active research on broadscale acquisition of entailment rules for predicates Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2008 Yates and Etzioni 2009 Schoenmackers et al 2010", "Previous work has focused on learning each entailment rule in isolation", "It is clear however that there are interactions between rules", "A prominent phenomenon is that entailment is inherently a transitive relation and thus the rules X  Y and Y  Z imply the rule X  Z1 In this article we take advantage of these global interactions to improve entailment rule learning", "After reviewing relevant background Section 2 we describe a structure termed an entailment graph that models entailment relations between propositional templates Section 3", "Next we motivate and discuss a specic type of entailment graph termed a focused entailment graph where a target concept instantiates one of the arguments of all propositional templates", "For example a focused entailment graph about the target concept nausea might specify the entailment relations between propositional templates like X induce nausea X prevent nausea and nausea is a symptom of X In the core section of the article we present an algorithm that uses a global approach to learn the entailment relations which comprise the edges of focused entailment graphs Section 4", "We dene a global objective function and look for the graph that maximizes that function given scores provided by a local entailment classier and a global transitivity constraint", "The optimization problem is formulated as an Integer Linear Program ILP and is solved with an ILP solver which leads to an optimal solution with respect to the global function", "In Section 5 we demonstrate that this algorithm outperforms by 1213 methods that utilize only local information as well as methods that employ a greedy optimization algorithm Snow Jurafsky and Ng 2006 rather than an ILP solver", "The article also includes a comprehensive investigation of the algorithm and its components", "First we perform manual comparison between our algorithm and the baselines and analyze the reasons for the improvement in performance Sections 531 and 532", "Then we analyze the errors made by the algorithm against manually prepared goldstandard graphs and compare them to the baselines Section 54", "Last we perform a series of experiments in which we investigate the local entailment classier and specically experiment with various sets of features Section 6", "We conclude and suggest future research directions in Section 7", "This article is based on previous work Berant Dagan and Goldberger 2010 while substantially expanding upon it", "From a theoretical point of view we reformulate the two ILPs previously introduced by incorporating a prior", "We show a theoretical relation between the two ILPs and prove that the optimization problem tackled is NPhard", "From an empirical point of view we conduct many new experiments that examine both the local entailment classier as well as the global algorithm", "Last a rigorous analysis of the algorithm is performed and an extensive survey of previous work is provided", "1 Assuming that Y has the same sense in both X  Y and Y  Z as we discuss later in Section 3", "74"]}, "J13-1007": {"title": ["Word Segmentation Unknown word Resolution and Morphological Agreement in a Hebrew Parsing System Yoav Goldberg Ben Gurion University of the Negev Michael Elhadad  Ben Gurion University of the Negev We present a constituency parsing system for Modern Hebrew The system is based on the PCFG LA parsing method of Petrov et al 2006 which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank We show that parsing performance can be enhanced by utilizing a language resource external to the treebank specifically a lexicon based morphological analyzer We present a computational model of interfacing the external lexicon and a treebankbased parser also in the common case where the lexicon and the treebank follow different annotation schemes We show that Hebrew wordsegmentation and constituencyparsing can be performed jointly using CKY lattice parsing Performing the tasks jointly is effective and substantially outperforms a pipelinebased model We suggest modeling grammatical agreement in a constituencybased parser as a filter mechanism that is orthogonal to the grammar and present a concrete implementation of the method Although the constituency parser does not make many agreement mistakes to begin with the filter mechanism is effective in fixing the agreement mistakes that the parser does make These contributions extend outside of the scope of Hebrew processing and are of general applicability to the NLP community Hebrew is a specific case of a morphologically rich language and ideas presented in this work are useful also for processing other languages including English The lattice based parsing methodology is useful in any case where the input is uncertain Extending the lexical coverage of a treebank derived parser using an external lexicon is relevant for any language with a small treebank"], "abstract": ["We present a constituency parsing system for Modern Hebrew", "The system is based on the PCFGLA parsing method of Petrov et al", "2006 which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank", "We show that parsing performance can be enhanced by utilizing a language resource external to the treebank specifically a lexiconbased morphological analyzer", "We present a computational model of interfacing the external lexicon and a treebankbased parser also in the common case where the lexicon and the treebank follow different annotation schemes", "We show that Hebrew wordsegmentation and constituencyparsing can be performed jointly using CKY lattice parsing", "Performing the tasks jointly is effective and substantially outperforms a pipeline based model", "We suggest modeling grammatical agreement in a constituencybased parser as a filter mechanism that is orthogonal to the grammar and present a concrete implementation of the method", "Although the constituency parser does not make many agreement mistakes to begin with the filter mechanism is effective in fixing the agreement mistakes that the parser does make", "These contributions extend outside of the scope of Hebrew processing and are of general applicability to the NLP community", "Hebrew is a specific case of a morphologically rich language and ideas presented in this work are useful also for processing other languages including English", "The latticebased parsing methodology is useful in any case where the input is uncertain", "Extending the lexical coverage of a treebankderived parser using an external lexicon is relevant for any language with a small treebank"], "inroduction": ["Different languages have different syntactic properties", "In English word order is relatively fixed whereas in other languages word order is much more flexible in Hebrew the subject may appear either before or after a verb", "In languages with a flexible word order the meaning of the sentence is realized using other structural elements like word  Computer Science Department Ben Gurion University of the Negev Israel", "Email yoavgoldberggmailcom", " Computer Science Department Ben Gurion University of the Negev Israel", "Email elhadadcsbguacil", "Submission received 30 September 2011 revised submission received 19 May 2012 accepted for publication 3 August 2012", " 2013 Association for Computational Linguistics inflections or markers which are referred to as morphology in Hebrew the marker  is used to mark definite objects distinguishing them from subjects in the same position", "In addition verbs and nouns are marked for gender and number and subject and verb must share the same gender and number", "A limited form of morphology also exists in English the s and ed suffixes are examples of English morphological markings", "In other languages morphological processes may be much more involved", "The lexical units words in English are always separated by white space", "In Chinese such separation is not available", "In Hebrew and Arabic most words are separated by white space but many of the function words determiners like the conjunctions such as and and prepositions like in or of  do not stand on their own but are instead attached to the following words", "A large part of the parsing literature is devoted to automatic parsing of English a language with a relatively simple morphology relatively fixed word order and a large treebank", "Datadriven English parsing is now at the state where naturally occurring text in the news domain can be automatically parsed with accuracies of around 90 according to standard parsing evaluation measures", "When moving from English to languages with richer morphologies and lessrigid word orders however the parsing algorithms developed for English exhibit a large drop in accuracy", "In addition whereas English has a large treebank containing over one million annotated words many other languages have much smaller treebanks which also contribute to the drop in the accuracies of the datadriven parsers", "A similar drop in parsing accuracy is also exhibited in English when moving from the news domain on which parsers have traditionally been trained to other genres such as prose blogs poetry product reviews or biomedical texts which use different vocabularies and to some extent different syntactic rules", "This work focuses on constituency parsing of Modern Hebrew a Semitic language with a rich and productive morphology relatively free word order1 and a small tree bank", "Several natural questions arise Can the small size of the treebank be compensated for using other available resources or sources of information", "How should the word segmentation issue that function words do not appear in isolation but attach to the next word forming ambiguous letter patterns be handled", "Can morphological information be used effectively in order to improve parsing accuracy", "We present a system which is based on a stateoftheart model for constituency parsing namely the probabilistic contextfree grammar PCFG with latent annotations PCFGLA model of Petrov et al", "2006 as implemented in the BerkeleyParser", "After evaluating the outofthebox performance of the BerkeleyParser on the Hebrew tree bank we discuss some of its limitations and then go on to extend the PCFGLA parsing model in several directions making it more suitable for parsing Hebrew and related languages", "Our extensions are based on the following themes", "Separation of lexical and syntactic knowledge", "There are two kinds of knowledge inherent in a parsing system", "One of them is syntactic knowledge governing the way in which words can be combined to form structures which in turn can be combined to form ever larger structures", "The other is lexical knowledge about the identities of individual words the word classes they belong to and the kinds of syntactic structures they can participate in", "We argue that the amount of syntactic knowledge needed for a parsing system is relatively limited and that sufficiently large parts of it can be captured also 1 To be more precise in Hebrew the order of constituents is relatively free whereas the order of the words", "within certain constituents is relatively fixed", "122 based on a relatively small treebank", "Lexical knowledge on the other hand is much more vast and we should not rely on a treebank small or large to provide adequate lexical coverage", "Instead we should aim to find ways of integrating lexical knowledge which is external to the treebank into the parsing process", "We extend the lexical coverage of a treebankbased parser using a dictionarybased morphological analyzer", "We present a way of integrating the two resources also for the common case where their annotations schemes diverge", "This method is very effective in improving parsing accuracy", "Encoding input uncertainty using a latticebased representation", "Sometimes the language signal the input to the parser may be uncertain", "This happens in Hebrew when a spacedelimited token such as  can represent either a single word an onion or a sequence of two words or three words in shadow and in the shadow respectively", "When computationally feasible it is best to let the uncertainty be resolved by the parser rather than in a separate preprocessing step", "We propose encoding the inputuncertainty in a word lattice and use lattice parsing Chappelier et al 1999 Hall 2005 to perform joint word segmentation and syntactic disambiguation Cohen and Smith 2007 Goldberg and Tsarfaty 2008", "Performing the tasks jointly is effective and substantially outperforms a pipelinebased model", "Using morphological information to improve parsing accuracy", "Morphology provides useful hints for resolving syntactic ambiguity and the parsing model should have a way of utilizing these hints", "There is a range of morphological hints than can be utilized from functional marking elements such as the  marker indicating a definite direct object to elements marking syntactic properties such as definiteness such as the Hebrew  marker to agreement patterns requiring a compatibility in properties such as gender number and person between syntactic constituents such as a verb and its subject or an adjective and the noun it modifies", "We suggest modeling agreement as a filtering process that is orthogonal to the grammar", "Although the constituency parser does not make many agreement mistakes to begin with the filter mechanism is effective in fixing the agreement mistakes that the parser does make without introducing new mistakes", "Aspects of the work presented in this article are discussed in earlier publications", "Goldberg and Tsarfaty 2008 suggest the latticeparsing mechanism Goldberg et al", "2009 discuss ways of interfacing a treebankderived PCFGparser with an external lexicon and Goldberg and Elhadad 2011 present experiments using the PCFGLA BerkeleyParser", "Here we provide a cohesive presentation of the entire system as well as a more detailed description and an expanded evaluation", "We also extend the previous work in several dimensions We introduce a new method of interfacing the parser and the external lexicon which contributes to an improved parsing accuracy and suggest incorporating agreement information as a filter", "The methodologies we suggest extend outside the scope of Hebrew processing and are of general applicability to the NLP community", "Hebrew is a specific case of a morphologically rich language and ideas presented in this work are useful also for processing other languages including English", "The latticebased parsing methodology is useful in any case where the input is uncertain", "Indeed we have used it to solve the problem of parsing while recovering null elements in both English and Chinese Cai Chiang and Goldberg 2011 and others have used it for the joint segmentation and parsing of Arabic Green and Manning 2010", "Extending the lexical coverage of a treebankderived parser using an external lexicon is relevant for any language with 123 a small treebank and also for domain adaptation scenarios for English", "Finally the agreementasfilter methodology is applicable to any morphologically rich language and although its contribution to the parsing task may be limited it is of wide applicability to syntactic generation tasks such as targetsidesyntax machine translation in a morphologically rich language"]}, "J13-1008": {"title": ["Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features Yuval Marton Nuance Communications Nizar Habash Center for Computational Learning Systems Columbia University Owen Rambow Center for Computational Learning Systems Columbia University We explore the contribution of lexical and inflectional morphology features to dependency parsing of Arabic a morphologically rich language with complex agreement patterns Using controlled experiments we contrast the contribution of different partofspeech POS tag sets and morphological features in two input conditions machinepredicted condition in which POS tags and morphological feature values are automatically assigned and gold condition in which their true values are known We find that more informative finegrained tag sets are useful in the gold condition but may be detrimental in the predicted condition where they are outperformed by simpler but more accurately predicted tag sets We identify a set of features definiteness person number gender and undiacritized lemma that improve parsing quality in the predicted condition whereas other features are more useful in gold We are the first to show that functional features for gender and number eg broken plurals and optionally the related rationality  humanness   feature are more helpful for parsing than form based gender and number We finally show that parsing quality in the predicted condition can dramatically improve by training in a combined goldpredicted condition We experimented with two transitionbased parsers MaltParser and Easy First Parser Our findings are robust across parsers models and input conditions This suggests that the contribution of the linguistic knowledge in the tag sets and features we identified goes beyond particular experimental settings and may be informative for other parsers and morphologically rich languages"], "abstract": ["We explore the contribution of lexical and inflectional morphology features to dependency parsing of Arabic a morphologically rich language with complex agreement patterns", "Using controlled experiments we contrast the contribution of different partofspeech POS tag sets and morphological features in two input conditions machinepredicted condition in which POS tags and morphological feature values are automatically assigned and gold condition in which their true values are known", "We find that more informative finegrained tag sets are useful in the gold condition but may be detrimental in the predicted condition where they are outperformed by simpler but more accurately predicted tag sets", "We identify a set of features definiteness person number gender and undiacritized lemma that improve parsing quality in the predicted condition whereas other features are more useful in gold", "We are the first to show that functional features for gender and number eg broken plurals and optionally the related rationality humanness feature are more helpful for parsing than formbased gender and number", "We finally show that parsing quality in the predicted condition can dramatically improve by training in a combined goldpredicted condition", "We experimented with two transitionbased parsers MaltParser and EasyFirst Parser", "Our findings are robust across parsers models and input conditions", "This suggests that the contribution of the linguistic knowledge in the tag sets and features we identified goes beyond particular experimental settings and may be informative for other parsers and morphologically rich languages"], "inroduction": ["For Arabicas for other morphologically rich languagesthe role of morphology is often expected to be essential in syntactic modeling and the role of word order is less important than in morphologically poorer languages such as English", "Morphology  Nuance Communications 505 First Ave S Suite 700 Seattle WA 98104", "Email yuvalmartongmailcom", " Center for Computational Learning Columbia University", "Email habashcclscolumbiaedu", " Center for Computational Learning Columbia University", "Email rambowcclscolumbiaedu", "Submission received October 1 2011 revised submission received June 16 2012 accepted for publication August 3 2012", " 2013 Association for Computational Linguistics interacts with syntax in two ways agreement and assignment", "In agreement there is coordination between the morphological features of two words in a sentence based on their syntactic configuration eg subjectverb or nounadjective agreement in GENDER andor NUMBER", "In assignment specific morphological feature values are assigned in certain syntactic configurations eg CASE assignment for the subject or direct object of a verb1 Parsing model design aims to come up with features that best help parsers learn the syntax and choose among different parses", "The choice of optimal linguistic features depends on three factors relevance redundancy and accuracy", "A feature has relevance if it is useful in making an attachment or labeling decision", "A particular feature may or may not be relevant to parsing", "For example the GENDER feature may help parse the Arabic phrase vJ 1  vJ 1 J LJ1 ", "v bAb AlsyArn AljdydAljdydn door thecar thenewmascsgfemsg lit2 using syntactic agreement if thenew is masculine  vJ 1 it should attach to the masculine door resulting in the meaning the car s new door  if thenew is feminine  vJ 1 it should attach to the feminine thecar resulting in the door of the new car Conversely the ASPECT feature does not constrain any syntactic decision", "Even if relevant a feature may not necessarily contribute to optimal performance because it may be redundant with other features that surpass it in relevance", "For example as we will see the DET and STAT E features alone both help parsing because they help identify the idafa construction but they are redundant with each other and the DET feature is more helpful because it also helps with adjectival modification of nouns", "Finally the accuracy of automatically predicting the feature values ratio of correct predictions out of all predictions of course affects the value of a feature on unseen text", "Even if relevant and nonredundent a feature may be hard to predict with sufficient accuracy by current technology in which case it will be of little or no help for parsing even if helpful when its gold values are provided", "As we will see the CASE feature is very relevant and not redundant but it cannot be predicted with high accuracy and overall it is not useful", "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors", "In the past it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough then they contribute to parsing accuracy", "For example modeling CASE in Czech improves Czech parsing Collins et al 1999 CASE is relevant not redundant and can be predicted with sufficient accuracy", "It has been more difficult showing that agreement morphology helps parsing however with negative results for dependency parsing in several languages Eryigit Nivre and Oflazer 2008 Nivre Boguslavsky and Iomdin 2008 Nivre 2009", "In this article we investigate morphological features for dependency parsing of Modern Standard Arabic MSA", "For MSA the space of possible morphological features is fairly large", "We determine which morphological features help and why", "We further determine the upper bound for their contribution to parsing quality", "Similar to previous 1 Other morphological features such as MOOD or ASPECT do not interact with syntax at all", "Note also that", "we do not commit to a specific linguistic theory with these terms hence other theoretical terms such as the Minimalist feature checking may be used here just as well", "2 All Arabic transliterations are presented in the HSB transliteration scheme Habash Soudi and", "Buckwalter 2007 alphabetically AbtjHxdrzsSDTD  fqklmnhwy and the additional symbols    A 1 A  w  y   n  a u i  1   u    1  162 results assignment features specifically CASE are very helpful in MSA though only under gold conditions Because CASE is rarely explicit in the typically undiacritized written MSA it has a dismal accuracy rate which makes it useless when used in a machinepredicted real nongold condition", "In contrast with previous results we show agreement features are quite helpful in both gold and predicted conditions", "This is likely a result of MSA having a rich agreement system covering both verbsubject and nounadjective relations", "The result holds for both the MaltParser Nivre 2008 and the EasyFirst Parser Goldberg and Elhadad 2010", "Additionally almost all work to date in MSA morphological analysis and partof speech POS tagging has concentrated on the morphemic form of the words", "Often however the functional morphology which is relevant to agreement and relates to the meaning of the word is at odds with the surface formbased morphology a wellknown example of this are the broken irregular plurals of nominals", "We show that by modeling the functional morphology rather than the formbased morphology we obtain a further increase in parsing performance again both when using gold and when using predicted POS and morphological features", "To our knowledge this work is the first to use functional morphology features in MSA processing", "As a further contribution of this article we show that for parsing with predicted POS and morphological features training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios", "The article is structured as follows", "We first present relevant Arabic linguistic facts their representation in the annotated corpus we use and variations of abstraction thereof in several POS tag sets Section 2", "We follow with a survey of related work Section 3 and describe our basic experiments in Section 4", "We first explore the contribution of various POS tag sets formbased morphological features and promising combinations thereof to Arabic dependency parsing qualityin straightforward feature engineering design and combination heuristics", "We also explore more sophisticated feature engineering for the determiner DET feature", "In Section 5 we proceed to an extended exploration of functional features", "This includes using functional NUMBER and GENDER feature values instead of formbased values using the nonformbased rationality RAT feature and combinations thereof", "We additionally consider the applicability of our results to a different parser Section 6 and consider combining gold and predicted data for training Section 7", "Section 8 presents a result validation on unseen test data as well as an analysis of parsing error types under different conditions", "We conclude and provide a download link to our model in Section 9", "Last we include an appendix with further explorations of PERSON feature engineering binning of Arabic number constructions according to their complex syntactic patterns and embedding useful morphological features in the POS tag set", "Much of Sections 25 was presented in two previous publications Marton Habash and Rambow 2010 2011", "This article extends that previous work by 1", "evaluating all our parsing models in both gold and nongold conditions where before this was true for only select models in Sections 45 2", "using a newer version of our Arabic functional morphology resource Section 5 3", "evaluating several of our most notable parsing models with an additional parser Section 6 4", "exploring two additional training methods as already mentioned above Section 7 and 163 5", "providing an extended discussion and comparison of several notable and best performing models including analyses of their performance per dependency tag Section 8"]}, "J13-1009": {"title": ["Parsing Models for Identifying Multiword Expressions Spence Green  Stanford University MarieCatherine de Marneffe Stanford University Christopher D Manning Stanford University Multiword expressions lie at the syntaxsemantics interface and have motivated alternative theories of syntax like Construction Grammar Until now however syntactic analysis and multiword expression identification have been modeled separately in natural language processing We develop two structured prediction models for joint parsing and multiword expression identification The first is based on contextfree grammars and the second uses tree substitution grammars a formalism that can store larger syntactic fragments Our experiments show that both models can identify multiword expressions with much higher accuracy than a stateoftheart system based on word cooccurrence statistics We experiment with Arabic and French which both have pervasive multiword expressions Relative to English they also have richer morphology which induces lexical sparsity in finite corpora To combat this sparsity we develop a simple factored lexical representation for the context free parsing model Morphological analyses are automatically transformed into rich feature tags that are scored jointly with lexical items This technique which we call a factored lexicon improves both standard parsing and multiword expression identification accuracy"], "abstract": ["Multiword expressions lie at the syntaxsemantics interface and have motivated alternative theories of syntax like Construction Grammar", "Until now however syntactic analysis and multiword expression identification have been modeled separately in natural language processing", "We develop two structured prediction models for joint parsing and multiword expression identification", "The first is based on contextfree grammars and the second uses tree substitution grammars a formalism that can store larger syntactic fragments", "Our experiments show that both models can identify multiword expressions with much higher accuracy than a stateofthe art system based on word cooccurrence statisticsWe experiment with Arabic and French which both have pervasive multiword expressions", "Relative to English they also have richer morphology which induces lexical sparsity in finite corpora", "To combat this sparsity we develop a simple factored lexical representation for the contextfree parsing model", "Morphological analyses are automatically transformed into rich feature tags that are scored jointly with lexical items", "This technique which we call a factored lexicon improves both standard parsing and multiword expression identification accuracy"], "inroduction": ["Multiword expressions are groups of words which taken together can have unpredictable semantics", "For example the expression part of speech refers not to some aspect of speaking but to the syntactic category of a word", "If the expression is altered in some wayspart of speeches part of speaking type of speechthen the idiomatic meaning is lost", "Other modifications however are permitted as in the plural parts of speech", "These characteristics make multiword expressions MWEs difficult to identify and classify", "But if they can be identified then the incorporation of MWE knowledge has been shown to improve task accuracy for a range of NLP applications  Department of Computer Science", "Email spencegstanfordedu", " Department of Linguistics", "Email mcdmstanfordedu", " Departments of Computer Science and Linguistics", "Email manningstanfordedu", "Submission received October 1 2011 revised submission received June 9 2012 accepted for publication August 3 2012", "No rights reserved", "This work was authored as part of the Contributor s official duties as an Employee of the United States Government and is therefore a work of the United States Government", "In accordance with 17 USC 105 no copyright protection is available for such works under US law", "including dependency parsing Nivre and Nilsson 2004 supertagging Blunsom and Baldwin 2006 sentence generation Hogan et al 2007 machine translation Carpuat and Diab 2010 and shallow parsing Korkontzelos and Manandhar 2010", "The standard approach to MWE identification is ngram classification", "This technique is simple", "Given a corpus all ngrams are extracted filtered using heuristics and assigned feature vectors", "Each coordinate in the feature vector is a realvalued quantity such as log likelihood or pointwise mutual information", "A binary classifier is then trained to render a MWEnonMWE decision", "All entries into the 2008 MWE Shared Task Evert 2008 utilized variants of this techniqueBroadly speaking ngram classification methods measure word cooccurrence", "Sup pose that a corpus contains more occurrences of part of speech than parts of speech", "Surface statistics may erroneously predict that only the former is an MWE and the latter is not", "More worrisome is that the statistics for the two ngrams are separate thus missing an obvious generalization", "In this article we show that statistical parsing models generalize more effectively over arbitrarylength multiword expressions", "This approach has not been previously demonstrated", "To show its effectiveness we build two parsing models for MWE identification", "The first model is based on a contextfree grammar CFG with manual rule refinements Klein and Manning 2003", "This parser also includes a novel lexical modelthe factored lexiconthat incorporates morphological features", "The second model is based on tree substitution grammar TSG a formalism with greater strong generative capacity that can store larger structural tree fragments some of which are lexicalized", "We apply the models to Modern Standard Arabic henceforth MSA or simply Arabic and French two morphologically rich languages MRLs", "The lexical sparsity in finite corpora induced by rich morphology poses a particular challenge for ngram classification", "Relative to English French has a richer array of morphological features such as grammatical gender and verbal conjugation for aspect and voice", "Arabic also has richer morphology including gender and dual number", "It has pervasive verb initial matrix clauses although preposed subjects are also possible", "For languages like these it is well known that constituency parsing models designed for English often do not generalize well", "Therefore we focus on the interplay among language annotation choices and parsing model design for each language Levy and Manning 2003 Kbler 2005 inter alia although our methods are ultimately very general", "Our modeling strategy for MWEs is simple We mark them with flat bracketings in phrase structure trees", "This representation implicitly assumes a locality constraint on idioms an assumption with a precedent in linguistics Marantz 1997 inter alia", "Of course it is easy to find nonlocal idioms that do not correspond to surface constituents or even contiguous strings OGrady 1998", "Utterances such as All hell seemed to break loose and The cat got Marys tongue are clearly idiomatic yet the idiomatic elements are discontiguous", "Our models cannot identify these MWEs but then again neither can ngram classification", "Nonetheless many common MWE types like nominal compounds are contiguous and often correspond to constituent boundaries", "Consider again the phrasal compound part of speech1 which is noncompositional The idiomatic meaning syntactic category does not derive from any of the component 1 It is common to hyphenate some nominal compounds eg partofspeech", "This practice invites a wordswithspaces treatment of idioms", "However hyphens are inconsistently used in English", "Hyphenation is more common in French but totally absent in Arabic", "words", "This noncompositionality affects the syntactic environment of the compound as shown by the addition of an attributive adjective 1 a Noun is a part of speech", "b Noun is a big part of speech", "2 c a Noun is a big part", "Liquidity is a part of growth", "b Liquidity is a big part of growth", "c Liquidity is a big part", "In Example 1a the copula predicate part of speech as a whole describes Noun", "In Examples 1b and 1c big clearly modifies only part and the idiomatic meaning is lost", "The attributive adjective cannot probe arbitrarily into the noncompositional compound", "In contrast Example 2 contains parallel data without idiomatic semantics", "The conventional syntactic analysis of Example 2a is identical to that of Example 1a except for the lexical items yet part of growth is not idiomatic", "Consequently many pre modifiers are appropriate for part which is semantically vacuous", "In Example 2b big clearly modifies part and of growth is just an optional PP complement as shown by Example 2c which is still grammatical", "This article proposes different phrase structures for examples such as 1a and 2a", "Figure 1a shows a Penn Treebank PTB Marcus Marcinkiewicz and Santorini 1993 parse of Example 1a and Figure 1b shows the parse of a paraphrase", "The phrasal compound part of speech functions syntactically like a singleword nominal like category and indeed Noun is a big category is grammatical", "Singleword paraphrasability is a common though not mandatory characteristic of MWEs Baldwin and Kim 2010", "Starting from the paraphrase parse we create a representation like Figure 1c", "The MWE is indicated by a label in the predicted structure which is flat", "This representation explicitly models the idiomatic semantics of the compound and is contextfree so we can build efficient parsers for it", "Crucially MWE identification becomes a byproduct of parsing as we can trivially extract MWE spans from full parses", "We convert existing Arabic and French syntactic treebanks to the new MWE representation", "With this representation the TSG model yields the best MWE identification results for Arabic 819 F1 and competitive results for French 713 even though its parsing results lag stateoftheart probabilistic CFG PCFGbased parsers", "The TSG model also learns humaninterpretable MWE rules", "The factored lexicon model with gold morphological annotations achieves the best MWE results for French 873 F1 and competitive results for Arabic 782 F1", "For both languages the factored lexicon model also approaches stateoftheart basic parsing accuracy", "The remainder of this article begins with linguistic background on common MWE types in Arabic and French Section 2", "We then describe two constituency parsing models that are tuned for MWE identification Sections 3 and 4", "These models are supervised and can be trained on existing linguistic resources Section 5", "We evaluate the models for both basic parsing and MWE identification Section 6", "Finally we compare our results with a stateoftheart ngram classification system Section 7 and to prior work Section 8"]}, "J93-2006": {"title": ["Coping with Ambiguity and Unknown Words through Probabilistic Models"], "abstract": ["From spring 1990 through fall 1991 we performed a battery of small experiments to test the effectiveness of supplementing knowledgebased techniques with probabilistic models", "This pa per reports our experiments in predicting parts of speech of highly ambiguous words predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints and learning caseframe information for verbsfrom example uses", "From these experiments we are convinced that probabilistic models based on annotated corpora can effectivel y reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus by supplementing knowledgebased techniques", "Based on the results of those experiments we have constructed a new natural language system PLUM  for extracting data from text eg newswire text"], "inroduction": ["Natural language processing and Al in general have focused mainly on building rule based systems with carefully handcrafted rules and domain knowledge", "Our own natural language database query systems JANUS Weischedel et al 1989 Parlance 1 and Delphi Stallard 1989 have used these techniques quite successfully", "However as we move from the application of understanding database queries in limited domains to applications of processing openended text we found challenges that questioned our previous assumptions and suggested probabilistic models instead", "1", "We could no longer assume a limited vocabulary", "Rather in the domain", "of terrorist incidents of the Third Message Understanding conference MUC 3 Sundheim 1991 roughly 20000 vocabulary items appear in a corpus 430000 words long", "Additional text from that domain would undoubtedly contain new words", "Probabilistic models offer a mathematicall y grounded  empirically based means of predicting the most likely interpretation", " BBN Systems and Technologies 70 Fawcett Street Cambridge MA 02138", "t Sage Lab Rensselaer Polytechnic Institute Troy NY 12180", " Computer Science Department Bowdoin College Brunswick ME 04011", "1 Parlance is a trademark of BBN Systems and Technologies", " 1993 Association for Computational Linguistics would violate the limited domain assumption since roughly 50 of the message stream mentions no terrorist incident and even those that do may be primarily about a different topic or topics", "Therefore the power of semantic constraints in limited domains would be diluted", "Probabilit y models could be employed where less knowledge was available", "3", "Given the vocabulary size we could not expect to give full syntactic or", "semantic features", "The labor for handcrafted definitions would not be warranted", "Statistical language models have a learning component that might supplement handcrafted knowledge", "4", "Purely rulebased techniques seemed too brittle for dealing with the", "variety of constructions the long sentences averaging 29 words per sentence and the degree of unexpected input", "Statistical models based on local information eg DeRose 1988 Church 1988 might operate effectivel y in spite of sentence length and unexpected input", "To see whether our four hypotheses in italics above effectively addressed the four concerns above we chose to test the hypotheses on two wellknown problems ambiguity both at the structural level and at the partofspeech level and inferring syntactic and semantic information about unknown words", "Guided by the past success of probabilistic models in speech processing we have integrated probabilistic models into our language processing systems", "Early speech research used purely knowledgebased approaches analogous to knowledgebased approaches in NLP systems today", "These required much detailed handcrafted knowl edge from several sources eg acoustic and phonetic", "However when it became clear that these techniques were too brittle and not scalable speech researchers turned to probabilistic models", "These provided a flexible control structure for combining mul tiple sources of knowledge providing improved accuracy and ability to deal with more complex domains and algorithms for training the system on large bodies of data providing reduced cost in moving the technology to a new application domain", "Since probability theory offers a general mathematical modeling tool for estimating how likely an event is probability theory may be applied at all levels in natural language processing because some set of events can be associated with each algorithm", "For example in morphological processing in English Section 2 the events are the use of a word with a particular part of speech in a string of words", "At the level of syntax Section 3 an event is the use of a particular structure the model predicts what the most likely rule is given a particular situation", "One can similarly use probabilities for assigning semantic structure Section 4", "We report in Section 2 on our experiments on the assignment of part of speech to words in text", "The effectiveness of such models is well known DeRose 1988 Church 1988 Kupiec 1989 Jelinek 1985 and they are currently in use in parsers eg de Mar cken 1990", "Our work is an incremental improvement on these models in three ways 1 Much less training data than theoretically required proved adequate 2 we inte grated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution and 3 we have applied the forwardbackward algorithm to accurately compute the most likely tag set", "In Section 3 we demonstrate that probability models can improve the performance of knowledgebased syntactic and semantic processing in dealing with structural am biguity and with unknown words", "Though the probability model employed is not new our empirical findings are novel", "When a choice among alternative interpretations pro duced by a unificationbased parser and semantic interpreter must be made a simple contextfree probability model reduced the error rate by a factor of two compared with using no model", "It is well known that a unification parser can process an unknown word by collecting the assumptions it makes while trying to find an interpretation for a sentence", "As a second result we found that adding a contextfree probability model improved the unification predictions of syntactic and semantic properties of an unknown word reducing the error rate by a factor of two compared with no model", "In Section 4 we report an experiment in learning case frame information of un known verbs from examples", "The probabilistic algorithm is critical to selecting the appropriate generalizations to make from a set of examples", "The effectiveness of the semantic case frames inferred is measured by testing how well those case frames pre dict the correct attachment point for prepositional phrases", "In this case a significant new model synthesizing both semantic and syntactic knowledge is employed"]}, "J98-1006": {"title": ["Using  Corpus Statistics  and WordNet"], "abstract": ["Corpusbased approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneckWe show how knowledgebased techniques can be used to open the bottleneck by automatically locating training corpora", "We describe a statistical classifier that combines topical context with local cues to identify a word sense", "The classifier is used to disambiguate a nouna verband an adjective", "A knowledge base in the form ofWordNets lexical relations is used to automatically locate training examples in a general text corpus", "Test results are compared with those from manually tagged training examples"], "inroduction": ["An impressive array of statistical methods have been developed for word sense identi fication", "They range from dictionarybased approaches that rely on definitions Veronis and Ide 1990 Wilks et al 1993 to corpusbased approaches that use only word co occurrence frequencies extracted from large textual corpora Schiitze 1995 Dagan and Itai 1994", "We have drawn on these two traditions using corpusbased cooccurrence and the lexical knowledge base that is embodied in the WordNet lexicon", "The two traditions complement each other", "Corpusbased approaches have the advantage of being generally applicable to new texts domains and corpora without needing costly and perhaps errorprone parsing or semantic analysis", "They require only training corpora in which the sense distinctions have been marked but therein lies their weakness", "Obtaining training materials for statistical methods is costly and time consumingit is a knowledge acquisition bottleneck Gale Church and Yarowsky 1992a", "To open this bottleneck we use WordNets lexical relations to locate unsuper vised training examples", "Section 2 describes a statistical classifier TLC TopicalLocal Classifier that uses topical context the openclass words that cooccur with a particular sense local con text the open and closedclass items that occur within a small window around a word or a combination of the two", "The results of combining the two types of context to disambiguate a noun line a verb serve and an adjective hard are presented", "The following questions are discussed When is topical context superior to local context and vice versa", "Is their combination superior to either type alone", "Do the answers to these questions depend on the size of the training", "Do they depend on the syntactic category of the target", " Division of Cognitive and Instructional Science Princeton NJ 08541 email cleacocketsorg", "The work reported here was done while the author was at Princeton University", "t Department of Psychology 695 Park Avenue New York NY 10021 email mschccunyvmcunyedu j Cognitive Science Laboratory 221 Nassau Street Princeton NJ 08542 email geoclarityprincetonedu  1998 Association for Computational Linguistics Manually tagged training materials were used in the development of TLC and the experiments in Section 2", "The Cognitive Science Laboratory at Princeton Univer sity with support from NSFARPA is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation", "Examples of the different meanings of one thousand common polysemous openclass English words are being manually tagged", "The results of this effort will be a useful resource for train ing statistical classifiers but what about the next thousand polysemous words and the next", "In order to identify senses of these words it will be necessary to learn how to harvest training examples automatically", "Section 3 describes WordNets lexical relations and the role that monosemous relatives of polysemous words can play in creating unsupervised training materials", "TLC is trained with automatically extracted examples its performance is compared with that obtained from manually tagged training materials"]}, "J99-1004": {"title": ["Statistical Properties of Probabilistic ContextFree Grammars"], "abstract": ["We prove a number of useful results about probabilistic contextfree grammars PCFGs and their Gibbs representations", "Wepresent a method called the relative weighted frequency method to assign production probabilities that impose pro per PCFG distributions on finite parses", "We demonstrate that these distributions have finite entropies", "In addition under the distributions sizes of parses havefinite moment of any order", "Weshow that Gibbs distributions on CFG parses which generalize PCFG distributions and are more powerful become PCFG distributions if their features only include frequencies of production rules in parses", "Under these circumstances we prove the equivalence of the maximumlikelihood ML estimation procedures for these two types of probabilit y distributions on parses", "We introduce the renormalization of improper PCFGs to pro per ones", "We also stud y PCFGs from the pers pective of stochastic branching processes", "We prove that with their production probabilities assigned by the relative weighted frequency method PCFGs are subcritical ie their branching rates are less than one", "We also show that by renormalization connected supercritical PCFGs become subcritical ones", "Finall y some minor issues including identifiabilit y and approximation of production probabilities of PCFGs are discussed"], "inroduction": ["This article proves a number of useful properties of probabilistic contextfree grammars PCFGs", "In this section we give an introduction to the results and related topics", "11 Assignment of Proper PCFG Distributions", "Finite parse trees or parses generated by a contextfree grammar CFG can be equipped with a variety of probability distributions", "The simplest way to do this is by production probabilities", "First for each nonterminal symbol in the CFG a probability distribution is placed on the set of all productions from that symbol", "Then each finite parse tree is allocated a probability equal to the product of the probabilities of all productions in the tree", "More specifically denote a finite parse tree by T  For any production rule A  a of the CFG letf A  aT be the number of times it occurs in T Let R be the set of all production rules", "Then pT   II p A  a f  Aar  ", "AaER A CFG with a probability distribution on its parses assigned in this way is called a probabilistic contextfree grammar PCFG Booth and Thompson 1973 Grenander  Department of Statistics University of Chicago Chicago IL 60637 USA", "Email chigaltonuchicagoedu", "This work was supported by the Army Research Office DAAL0392G0115 the National Science Foundation DMS9217655 and the Office of Naval Research N000149610647", " 1999 Association for Computational Linguistics 19761 and the probability distribution is called a PCFG distribution", "A PCFG may be improper ie the total probability of parses may be less than one", "For instance consider the CFG in Chomsky normal form 5  55 5  a 1 where 5 is the only nonterminal symbol and a is the only terminal symbol", "If p5  55  p then p5  a   1p", "Let xh be the total probability of all parses with height no larger than h Clearly xh is increasing", "It is not hard to see that xhl  1 p  px  Therefore the limit of xh which is the total probability of all parses is a solution for the equation x  1  p  px 2  The equation has two solutions 1 and 1p  1", "It can be shown that x is the smaller of the two x  minl 1p  1", "Therefore if p  12 x  1an improper probability", "How to assign proper production probabilities is quite a subtle problem", "A suffi cient condition for proper assignment is established by Chi and Geman 1998 who prove that production probabilities estimated by the maximumlikelihood ML esti mation procedure or relative frequency estimation procedure as it is called in com putational linguistics always impose proper PCFG distributions", "Without much diffi culty this result can be generalized to a simple procedure called the relative weighted frequency method which assigns proper production probabilities of PCFGs", "We will give more details of the generalization in Section 3 and summarize the method in Proposition 1", "12 Entropy and Moments of Parse Tree Sizes", "As a probabilistic model for languages the PCFG model has several important statis tical properties among which is the entropy of PCFG distribution on parses", "Entropy is a measure of the uncertainty of a probability distribution", "The larger its entropy the less one can learn about parses randomly sampled from the distribution", "As an example suppose we have a set 5 of N parsesor any objectsT1   ", " TN where N is very large", "We may ask how much one can learn from the sentence T is a random sample from 5", "At one extreme let the distribution on 5 be p T1  1 and p Ti  0 for i I 1", "Then because with probability one T  T1there is no uncertainty about the sample", "In other words we can get full information from the above sentence", "At the other extreme suppose the distribution is p T1    ", " p TN  1N In this case all the elements of 5 are statistically equivalent", "No specific information is given about T that would make it possible to know it from 5", "Greater effort is requiredfor example enumerating all the elements in 5to find what T is Since 5 is big the uncertainty about the sample is then much greater", "Correspondingly for the two cases the entropy is O and N log N 0 respectively", "Entropy plays a central role in the theory of information", "For an excellent exposi tion of this theory we refer the reader to Cover and Thomas 1991", "The theory has been applied in probabilistic language modeling Mark Miller and Grenander 1996 Mark et al 1996 Johnson 1998 natural language processing Berger Della Pietra and Della Pietra 1996 Della Pietra Della Pietra and Lafferty 1997 as well as computational vision Zhu Wu and Mumford 1997", "In addition all the models proposed in these articles are based on an important principle called the maximum entropy principle", "Chapter 11 of Cover and Thomas 1991 gives an introduction to this principle", "1A probabilistic contextfree grammar is also called a stochastic contextfree grammar SCFG Briefly the maximum entropy principle says that among all the distributions that satisfy the same given conditions the one that achieves the largest entropy should be the model of choice", "For a distribution p on parses its entropy is In order that the maximum entropy principle makes sense all the candidate distribu tions should have finite entropies and this is usually implicitly assumed", "Take Mark Miller and Grenanders 1996 model for example", "First a PCFG distribution p is selected to serve as a reference distribution on parses", "Then by invoking the minimum relative entropy principle which is a variant of the maximum entropy principle the distribution that minimizes Dq ll p  L qT log   L qT log p  Hq T T subject to a set of constraints incorporating contextsensitive features is chosen to be the distribution of the model", "It is then easy to see that the assumption that Hq is finite is necessary", "Conceptually having finite entropy is a basic requirement for a good proba bilistic model because a probability distribution with infinite entropy has too much uncertainty to be informative", "Problems regarding entropies of PCFGs are relatively easy to tackle because they can be studied analytically", "Several authors have reported results on this subject in cluding Miller and OSullivan 1992 who gave analytical results on the rates of en tropies of improper PCFGs", "It is worthwhile to add a few more results on entropies of proper PCFGs", "In this paper we show that the entropies of PCFG distributions im posed by production probabilities assigned by the relative weighted frequency method are finite Section 4 Corollary 2", "In addition to entropy we will also study the moment of sizes of parses", "The mo ment is of statistical interest because it gives information on how sizes of parses are distributed", "For PCFG distributions the first moment of sizes of parses ie the mean size of parses is directly linked with the entropy the mean size of parses is finite if and only if the entropy is The second moment of sizes is another familiar quantity", "The difference between the second moment and the mean squared is the variance of sizes which tells us how scattered sizes of parses are distributed around the mean", "Proposition 2 shows that under distributions imposed by production probabilities as signed by the relative weighted frequency method sizes of parses have finite moment of any order", "13 Gibbs Distributions on Parses and Renormalization of Improper PCFGs", "Besides PCFG distributions a CFG can be equipped with many other types of proba bility distributions", "Among the most widely studied is the Gibbs distribution Mark Miller and Grenander 1996 Mark et al 1996 Mark 1997 Abney 1997", "Gibbs distribu tions arise naturally by invoking the maximum entropy principle", "They are considered to be more powerful than PCFG distributions because they incorporate more features especially contextsensitive features of natural languages whereas PCFG distributions only consider frequencies of production rules", "On the other hand Gibbs distributions are not always superior to PCFG distributions", "A Gibbs distribution with only fre quencies of production rules in parse as its features turns into a PCFG", "More specif ically we will show in Proposition 4 in Section 5 that a CFG equipped with a Gibbs distribution of the form 2 is actually a PCFG and we can get the production probabilities of the PCFG explicitly from the Gibbs form", "The fact that a Gibbs distribution of the form in 2 is imposed by production probabilities has a useful consequence", "Suppose p is an improper PCFG distribution", "If we write the sum of p over all parses as Z and assign to each parse tree a new probability equal to pT   Z then we renormalize p to a Gibbs distribution p on parses", "What 2 implies is that p is also a PCFG distribution Corollary 3", "Moreover in Section 6 we will show that under certain conditions p is subcritical", "There is another issue about the relations between PCFG distributions and Gibbs distributions of the form in 2 from a statistical point of view", "Although PCFG dis tributions are special cases of Gibbs distributions in the sense that the former can be written in the form of the latter PCFG distributions cannot be put in the framework of Gibbs distributions if they have different parameter estimation procedures", "We will compare the maximumlikelihood ML estimation procedures for these two distribu tions", "As will be seen in Section 5 numerically these two estimation procedures are different", "However Corollary 4 shows that they are equivalent in the sense that esti mates by the two procedures impose the same distributions on parses", "For this reason a Gibbs distribution may be considered a generalization of PCFG not only in form but also in a certain statistical sense", "14 Branching Rates of PCFGs", "Because of their contextfree nature PCFG distributions can also be studied from the perspective of stochastic processes", "A PCFG can be described by a random branch ing process Harris 1963 and its asymptotic behavior can be characterized by its branching rate", "A branching process or its corresponding PCFG is called subcritical critical supercritical if its branching rate  1 1  1", "A subcritical PCFG is always proper whereas a supercritical PCFG is always improper", "Many asymptotic properties of supercritical branching processes are established by Miller and OSullivan 1992", "Chi and Geman 1998 proved the properness of PCFG distributions imposed by esti mated production probabilities and around the same time Sanchez and Benedi 1997 established the subcriticality of the corresponding branching processes hence their properness", "In this paper we will explore properties of branching rate further", "First in Proposition 5 we will show that if a PCFG distribution is imposed by production probabilities assigned by the relative weighted frequency method then the PCFG is subcritical", "The result generalizes that of Sanchez and Benedi 1997 and has a less involved proof", "Then in Proposition 7 we will demonstrate that a connected and improper PCFG after being renormalized becomes a subcritical PCFG", "15 Identifiability and Approximation of Production Probabilities", "Returning to the statistical aspect of PCFGs we will discuss the identifiability of pro duction probabilities of PCFGs as well as parameters of Gibbs distributions", "Briefly speaking production probabilities of PCFGs are identifiable which means that differ ent production probabilities always impose different distributions on parses Proposi tion 8", "In contrast for the Gibbs distribution given by 2 the  parameters are not identifiable in fact there are infinitely many different ", "that impose the same Gibbs distribution", "Finally in Proposition 9 we propose a method to approximate production prob abilities", "Perhaps the most interesting part about the result lies in its proof which is largely information theoretic", "We apply the KullbackLeibler divergence which is the information distance between two probability distributions to prove the convergence of the approximation", "Ininformation theory literature the KullbackLeibler divergence is also called the relative entropy", "We also use Lagrange multipliers to solve the con strained minimization problem involved", "Both KullbackLeibler divergence and La grange multipliers method are becoming increasingly useful in statistical modeling eg modeling based on the maximum entropy principle", "16 Summary", "As a simple probabilistic model the PCFG model is applied to problems in linguistics and pattern recognition that do not involve much context sensitivity", "To design sensi ble PCFG distributions for such problems it is necessary to understand some of the statistical properties of the distributions", "On the other hand the PCFG model serves as a basis for more expressive linguistic models", "For example many Gibbs distributions are built upon PCFG distributions by defining pT eU r  PT  z I where p is a PCFG distribution", "Therefore in order for the Gibbs distribution P to have certain desired statistical properties it is necessary for p to have those properties first", "This paper concerns some of the fundamental properties of PCFGs", "However the methods used in the proofs are also useful for the study of statistical issues on other probabilistic models", "This paper proceeds as follows In Section 2 we gather the notations for PCFGs that will be used in the remaining part of the paper", "Section 3 establishes the rel ative weighted frequency method", "Section 4 proves the finiteness of the entropies of PCFG distributions when production probabilities are assigned using the relative weighted frequency method", "In addition finiteness of the moment of sizes of parses are proved", "Section 5 discusses the connections between PCFG distributions and Gibbs distributions on parses", "Renormalization of improper PCFGs is also discussed here", "In Section 6 PCFGs are studied from the random branching process point of view", "Finally in Section 7 identifiability of production probabilities and their approximation are addressed", " 2", "Notations and Definitions In this section we collect the notations and definitions we will use for the remaining part of the paper", "Definition 1 A contextfree grammar CFG G is a quadruple N TR S where N is the set of variables T the set of terminals R the set of production rules and S E N is the start symbol2 Elements of N are also called nonterminal symbols", "N T and R are always 2 Some of our discussion requires that each sentential form have only finitely many parses", "For this", "reason we shall assume that in G there are no null or unit productions", "assumed to be finite", "Let n denote the set of finite parse trees of G an element of which is always denoted as T  For each T E n and each production rule  A  o E R define f A  oT to be the number of occurrences or frequency of the rule in T  and f  A T  to be the number of occurrences of A in T f A T  and f  A  oT are related by aENUT st", "AaER f A  oT", "Define hT  as the height of T which is the number of nonterminal nodes on the longest route from Ts root to its terminal nodes", "Define ITI as the size of T which is the total number of nonterminal nodes in T  For any A E N and any sentential form Y E N U T define n A Y as the number of instances of A in Y Define bl as the length of the sentential form", "Definition 2Let A E T denote that the symbol A occurs in the parse T  If A E T let TA be the left most maximum subtree of T rooted in A which is the subtree of T rooted in A satisfying the condition that if T 1 I TA is also a subtree of T rooted in A then T 1 is either a subtree of TA or a right sibling of TA or a subtree of a right sibling of TA", "Let AT be the root of TA which is the leftmost shallowest instance of A in T  Definition 3 For any two symbols A E N and B E N U T not necessarily different B is said to be reachable from A in G if there is a sequence of symbols A0A1An with Ao  A and An  B and a sequence of sentential forms oo   On1 such that each A  o is a production in R and each o contains the next symbol Al G is called connected if all elements in N U T can be reached from all nonterminal symbols", "We now define the probabilistic version of reachability in CFG", "Suppose p is a distribution on n For any two symbols A E N and B E N U T not necessarily different B is said to be reachable from A in G under p if there is a T E n with p T  0 and there is a subtree T 1 of T such that T 1 is rooted in A and B E T 1 G is called connected under p if all symbols in N U T can be reached from all nonterminal symbols under p Definition 4 A system of production probabilities of G is a function p  R  0 1 such that for any A E N aENUT st", "AaER p A  o  l", "3 We will also use p to represent the PCFG probability distribution on parses im posed by p via the formula pT   IT p A  af AaT ", "AaER 4 Similarly for any estimated system of production probabilities p we will also use p to represent the probability distribution on parses imposed by p We will write pO  as the total probability of all finite parse trees in 0", "Definition 5 Now we introduce a notation in statistics", "Let p be an arbitrary distribution on O and gT  a function of T E 0", "The expected value of g under the distribution p denoted EpgT  is defined as EpgT   pT gT ", "rEl1 Definition 6 All the parse trees we have so far seen are rooted in S It is often useful to investigate subtrees of parses therefore it is necessary to consider trees rooted in symbols other than S We call a tree rooted in A E N a parse tree rooted in A if it is generated from A by the production rules in R Let OA be the set of all finite parse trees with root A Define PA as the probability distribution on OA imposed by a system of production probabilities p via 4", "Also extend the notions of height and size of parses to trees in 0A When we write PAT  we always assume that T is a parse tree rooted in A When p  PA EpgT  equals EpgT   LnA PAT gT ", "We will use pOA  instead of PAOA to denote the total probability of finite parse trees in OA", "With no subscripts 0 and p are assumed to be Os and ps respectively", "For convenience we also extend the notion of trees to terminals", "For each terminal t E T define 01 as the set of the single tree t", "Define p1 t  1 f tl  0 and h t  0", "For this paper we make the following assumptions 1", "For each symbol A  S there is at least one parse T with root S such that", "A E T This will guarantee that each A  S can be reached from S each production rule A  a E R is assumed to have positive probability ie p A  a  0", "This guarantees that there are no useless productions in the PCFG", "3", "Relative Weighted Frequency", "The relative weighted frequency method is motivated by the maximumlikelihood ML estimation of production probabilities", "We shall first give a brief review of ML estimation", "We consider two cases of ML estimation", "In the first case we assume the data are fully observed which means that all the samples are fully observed finite parses trees", "Let T1 Tz", "Tn be the samples", "Then the ML estimate of p A  a is the ratio between the total number of occurrences of the production A  a in the samples and the total number of occurrences of the symbol A in the samples n Lf A  a T p A  a  iln  Lf AT i1 5 Because of the form of the estimator in 5 ML estimation in the full observa tion case is also called relative frequency estimation in computational linguistics", "This simple estimator as shown by Chi and Geman 1998 assigns proper production prob abilities for PCFGs", "In the second case the parse trees are unobserved", "Instead the yields Y1  YT1 1 Yn  YTn which are the lefttoright sequences of terminals of the un known parses T1   ", " Tn form the data", "It can be proved that the ML estimate p is given by n L E J A  aT  IT E fh", "p A  a  iln  L E J A T  IT E Dy  i1 where Dy is the set of all parses with yield Y ie Dy  T E D  Y T   Y", "6 Equation 6 cannot be solved in closed form", "Usually the solution is computed by the EM algorithm with the following iteration Baum 1972 Baker 1979 Dempster Laird and Rubin 1977 n L Ek  f  A  a T  IT E D y Pk1 A  a  iln  L Ek  f A T  IT E Oyj i1 7 Like p in 5 Pk for k  0 impose proper probability distributions on D Chi and Geman 1998", "To unify 6 and 7 expand Ek  f  A  a T  IT E Oy by the definition of expecta tion into Ek f  A  aT  IT E D y  L f A  aT PkT IT E OyJ TE1yj Let A be the set of parses whose yields belong to the data ie A  T  YT  E Y1 Yn For each T E A let y  Y T  and W T   L Pk T IT E Dy  iYy Then we observe that for any production rule A  a n n  LEJf A  a TIT E Oy  Lf A  a TW T ", "il TEA Therefore 7 is transformed into LfA  aT W T Pkla  fA TWT  TEA The ML estimator in 6 can also be written in the above form as can be readily checked by letting A be the set T1   Tn  and WT  for each T E A be the number of occurrences of T in the data", "In addition in both full observation cases and partial observation cases we can divide the weights of WT  by a constant so that their sum is 1", "The above discussion leads us to define a procedure to assign production proba bilities as follows", "First pick an arbitrary finite subset A of nwith every production rule appearing in the trees in A Second assign to each T E A a positive weight WT  such that ITEA WT   1", "Finally define a system of production probabilities p by Lf A  a TW T  p  A  a  TEAf A TWT  8 TEA Because of the similarity between 5 and 8 we call the procedure to assign produc tion probabilities by 8 the relative weighted frequency method", "Proposition 1 Suppose all the symbols of N occur in the parses of A and all the parses have positive weight", "Then the production probabilities given by 8 impose proper distributions on parses", "Proof The proof is almost identical to the one given by Chi and Ceman 1998", "Let qA  p derivation tree rooted in A fails to terminate", "We will show that qs  0 ie derivation trees rooted in S always terminate", "For each A E V let  AT  be the number of non root instances of A in T Given a E V U T  let ai be the ith symbol of the sentential form a For any A E V qA p  LJ LJderivation begins A a and a fails to terminate AaER i L p A  ap  LJa fails to terminate AaER i  L p A  a Lp a fails to terminate AaER L p A  a I n B aq 8  AaER BEV Sum over A E V AEV TEA BEV TEA AEV AaER L qB Lf  B T W T  BEV TEA ie L qA Lif  A T  f AT  W  T  2 0 AEV TEA Clearly for every T E A j AT   f  AT  whenever A  S and j ST   f  ST   1", "Hence q5  0 completing the proof", "D Corollary 1 Under the same assumption of Proposition 1 for each symbol A E N p OA  1", "Proof For any A E N there is a T E A such that A E T  Since p T   0 this implies A is reachable from S under p Using the notation given in Definition 2 we have qs  p A E T and TA fails to terminate p TA fails to terminateIA E T  p A E T  p TA fails to terminateIA E T  0 since q5  0 and p A E T   0", "By the nature of PCFGs the form of TA is distributed according to PA independent of its location in T or of the choice of subtrees elsewhere in T  Therefore the conditional probability of TA failing to terminate given that A occurs in T  equals qA proving that qA  0", "D 4", "Entropy and Moments of Parse Tree Sizes", "In this section we will first show that if production probabilities are assigned by the relative weighted frequency method then they impose PCFG distributions under which parse tree sizes have finite moment of any order", "Based on this result we will then demonstrate that such PCFG distributions have finite entropy and give the explicit form of the entropy", "The mth moment of sizes of parses is given by Er l Tlm  L pT  IT l m TEil  and the entropy of a PCFG distribution p is given by 1 Hp  L p T  log  T  TE fl p To make the proofs more readable we define for any given A  T1   Tn for any  A  a E R and F A  a  Lf A  aT WT TEA FA  I aENUT st", "AaER F A  a  Lf  AT W T  TEA for any A E N that is F A  a is the weighted sum of the number of occurrences of the production rule A  a in A and FA is the weighted sum of the number of occurrences of A in A The relative weighted frequency method given by 8 can be written as F A  a p A  a  FA 9 We have the following simple lemma Lemma 1 For any A E N and LFA  LI TI WT AEN TEA 10 I I BEN Y st", "BYER FS  1 F B  Y n A Y   FA if A  S if A  S 11 If ITEA WT  1 FS 1 should be changed to FS  ITEA WT", "Proof For the first equation AEN AEN TEA TEA AEN TEA For the second equation L L F B  Y n A Y BEN Y st", "BYER L L Lf  B  T W T n A Y B E N  Y s  t  T E A  B   Y  E R L WT L L f  B   T n A Y 12 TEA BEN Y st", " B   Y  E R For each A L L f  B  T n A Y B E N  s  t ", " B    E R is the number of nonroot instances of A in T  When A f S the number of nonroot instances of A in T is equal tof A T ", "Substitute this into 12 to prove 11 for the case A f S The case A  S is similarly proved", "D Proposition 2 Suppose all the symbols in N occur in the parses of A and all parses have posi tive weights", "If the production probabilities p are assigned by the relative weighted frequency method in 8 then for each m E N U O EplT l m  oo", "Proof We shall show that for any A E N if p  PA then EplT l m  oo", "When m  0 this is clearly true", "Now suppose the claim is true for 0   m  lFor each A E N and k E N define M kA  L PAT  IT l m  rErlA h rSk It is easy to check L 1 LITl m p A  apa71  PaL  TL 13 aENUT TITL il AaER TErlo h rSk where for ease of typing we write L for l al For fixed a write L L 1  Lhlm  P IT1I    ITL I   LI Tlm  il il P is a polynomial in I T1I   ", " ITL I each term of which is of the form 14 By induction hypothesis there is a constant C  1 such that for all O  s  m and A E N U T L PA 717l5  EpA IT IS  C rErlA Then for each term with the form given in 14 L h l 81   ", "hlSLPa1 T1   PaLTL   11 L hl 81Pa1 T  cL Tl  TL TjE11oj There are less than Lm  f or terms in P hIh f ", "Hence L P T1      hf p A  apa1 T1   p LTL S f arclI T1 ", "TL TiE00 i h ri S k So we get aENUT st", "AaER lI  L L T mp A  aPa1 T1 Plal ha1 aENUT TJ71 al il st", "AaER Ti Elai h ri Sk  aENUT st", "AaER lal L M kai p A  a", "ENUT il st", "AaER Because the set of production rules is finite the length of a sentential form that occurs on the righthand side of a production rule is upper bounded ie sup a f  for some A E N  A  a E R  oo", "Therefore we can bound  f a  1tcl0 I by a constant say K Then we get ial L M kai p A  a", "15 aENuT il st", "AaER Replace p A  a by F A  a F A then multiply both sides of 15 by F A and sum over all A E N with F A  0", "By 10 and 11 we then get lal L L M kal A  a by 10  rEA AEN aENUT il st", "AaER L L n B aM kBF A  a rEA AEN aENUT BEN st", "AaER n B aF A  a rEA BEN AEN ENUT st", "AaER K L  T WT  L M kBF B  M ks F S  1", "by 11 rEA BfS Because for each A E N M kl A  M kA we get L M kAF A S K L ITI WT  L M kAF A  MksFS  1 AEN rEA AfS ", "M kS S K L ITI WT  00", "rEA Letting k  oo by M kS iE p 5 1T l m we get Ep 5 1 T l m  K IrEA ITIWT  oo", "To complete the induction we shall show for every A E N U T other than S EPA IT i m  oo", "By conditional expectation there is see Definition 2 for the notations A E T and since ps A E T  0", "Because hi  ITI Eps ITAl m IA E T  oo", "As in the proof of Corollary 1 TA is independent of its location and other part of T and is distributed by PA Therefore From Proposition 2 it follows that the mean size of parses is finite under p Since f A  aT   ITI for each production A  a it follows that the mean frequency of f A  a T is finite", "The next proposition gives the explicit form of the mean frequency in terms of the production probabilities assigned by the relative weighted frequency method", "Proposition 3 Under the same conditions of Proposition 2 the mean frequency of the production rule  A  a E R is the weighted sum of the numbers of its occurrences in parses of A with weights WT  ie E pf A  aT   Lf A  aT W T  rEA 17 Proof Fix  A  a E R For each C E N write EC for Epcf  A  a T ", "We shall find the linear relations between EC", "To this end for each T E De let C  1be the production rule applied to T 1s root", "Suppose I is composed of m symbols 11    rm and T1   Tm are the daughter subtrees of T rooted in 11   rm respectively", "Then m f A  aT   x C  1  Lf  A  a Tk  kl where xC     if C  1 A  a otherwise", "Multiply both sides by p T  and sum over all T E De which have C  1 as the production rule applied at the root", "By the definition of PCFG p T   p C  rpr1 p rm and Tk can be any parse in nYk Therefore by factorization we get L xC  Ypr TEc L x  C    Y  p  C    Y  p  r 1     p  r r n  r n p C  YhC  Y IJ pOk  k  l p C YxC Y All p Ok   1by Proposition 1 where Oc stands for the set of trees in which C  Y is the rule applied at the root", "Similarly for each k L f  C Tk PT  TEc rn pC Y L f  C   Tk Ph IJ p r il i p C  YEYk", "Therefore we get L p rf A  aT  TEc m pC Y xc Y  L EYk k  l p C YxC Y  L nBYEYk", "B E N st BE  Sum over all production rules for C The lefthand side totals EC and EC  p C Y xC Y  L n B YEB", "ENUT st", "Cl ER B E N st BEi Replace p C  Y by FC  YFC according to 9", "Then multiply both sides by FC and sum both sides over all C E N We get L FCEC CEN CEN ENuT st", "CER FC  YxC  Y FC  Y L nB YEB CEN ENUT st", "ClER BEN st BEi FA  a  L EB L L FC  YnB Y BEN CEN I st", "CER F A  a  LEBFB  ES By 11 BEN ", "ES  F A  a completing the proof of 17", "D Now we can calculate the entropy of p in terms of production probabilities", "Corollary 2 Under the conditions in Proposition 2 1 1 H p  L F A  a log F A  a  LF A log F A  AaER AEN which is clearly finite", "Proof The calculation goes as follows H p   Lp T log rEl p 1 T  L PT log IJ rEl 1 p A  af Ar  AaER 1 LPT  L f A  aT  log A  a rEl AaER p L Lp T f A  a T  log A 1 a Exchange the order of summation AaER rEl p 1 L Epf A  aT  log A  a AaER p L Lf A  aT WT log F a AaER rEA L Lf A  aT W T  logF A AaER rEA L Lf A  aT W T  log F A  a AaER rEA LF A log FA  L F A  a log FA  a", "D AaER 5", "Gibbs Distributions on Parses and Renormalization of Improper PCFGs", "A Gibbs distribution on parses has the form where Z", " IeU r  and ", "  and UT   UT are constants and functions on nrespectively both indexed by elements in a finite set I The inner product  U  IU is called the potential function of the Gibbs distribution and Z", "is called the partition number for the exponential eU The functions U are usually considered features of parses and the constants  are weights of these features", "The index set I and the functions UT  can take var ious forms", "Among the simplest choices for I is R the set of production rules and correspondingly Ur  f  r  f A  arAaER 18 Given constants X if Z", " oo then we get a Gibbs distribution on parses given by 19 A proper PCFG distribution is a Gibbs distribution of the form in 19", "To see this let AAa  log pA  a for each A  a E R Then p r   which is a Gibbs form", "A Gibbs distribution usually is not a PCFG distribution because its potential function in general includes features other than frequencies of production rules", "What if its potential function only has frequencies as features", "More specifically is the Gibbs distribution in 19 a PCFG distribution", "The next proposition gives a positive answer to this question", "Proposition 4 The Gibbs distribution P", "given by 19 is a PCFG distribution", "That is there are production probabilities p such that for every r E n Pr  II p A  af Aar ", "AaER Proof The Gibbs distributions we have seen so far are only defined for parses rooted in S  By obvious generalization we can define for each nonterminal symbols A the partition number Z A  L e f  r  r E1A and the Gibbs distribution Pr on parses rooted in A For simplicity also define Z t  1 and P1t  1 for each t E T We first show Z A  oo for all A Suppose S  a E R with lal  n Thesum of e f  r  over all r E Ds with S  a being applied at the root is equal to e sa z a1   ", "Z an while less than the sum of e f  r  over all T E Ds which is ZS", "Therefore Since Z", " oo and Z A  0 for all A it follows that Z a is finite", "For any variable A there are variables Ao  SA1", "An  A E N and sentential forms a0  anl  E N U T such that Ai  cil  E R and Ai E ail By the same argument as above we get 1aill ZAi 2 Aai ZDki  kl where afl is the kth element in ail", "By induction Z", "A  oo", "Now for A  a E R with lal  n define 1 p A  a  Z A Aa Z a1 ", "Z an Since Z", "A and Z", "ai are finite p A  a is well defined", "The ps form a system of production probabilities because for each A E N 20 L a  l L IiaTl  l L p A   eAaZ ak   e f T  1 AaER Z", "A AaER kl Z", "A TElA We shall prove by induction on hT  that Pr  IT p A  af AaT ", "AaER When h T  0 T is just a terminal and the equation is obviously true", "Suppose the equation is true for all T E DA with h T  h and all A E N For any T E DA with h T  h let A  J  fJ1 fJm be the production rule applied at the root", "Then 1 1 m PA r   ef T   eA3 e f Tk  ZA ZA kl I where Tk is the daughter subtree of r rooted in fJk", "Each Tk has height  h Hence by induction assumption  PAT  1 m  eA 3 e fh Z", "A kl  leA 3 ft Z fJk  II pB  afBaTk  Z", "A kl BaER  p A  J II II p B  af BaTk  kl BaER IT p B  af  BaT  1 BaER proving P ", "is imposed by p D Proposition 4 has a useful application to the renormalization of improper PCFGs", "Suppose a PCFG distribution p on n  Ds is improper", "We define a new proper distribution p on nby T E f2", "We call p the renormalized distribution of p on nWe can also define the renormalized distribution of PA on DA for each A E N by 21 Comparing p with 19 we see that p is a Gibbs distribution with frequencies of production rules as features", "Therefore by Proposition 4 p is a PCFG distribution and from the proof of Proposition 4 we get Corollary 3", "Corollary 3 Suppose the production probabilities of the improper distribution p are positive for all the production rules", "Then the renormalized distributions p are induced by the production probabilities p A  a  p A  a IIp nBt B a  p A BEN Therefore p on n is a PCFG distribution", "22 Proof The only thing we have not mentioned is that A a  log p A  a are all bounded since p are all positive", "D We have seen that PCFG distributions can be expressed in the form of Gibbs distri butions", "However from the statistical point of view this is not enough for regarding PCFG distributions as special cases of Gibbs distributions", "An important statistical issue about a distribution is the estimation of its parameters", "To equate PCFG distri butions with special cases of Gibbs distributions we need to show that estimators for production probabilities of PCFGs and parameters of Gibbs distributions produce the same results", "Among many estimation procedures the maximumlikelihood ML estimation procedure is commonly used", "In the full observation case if the data is composed of T1   ", " Tn then the estimator for the system of production probabilities is p   p A  a  arg max II II p A  a f  Aar  il AaER 23 subject to L p  A  a  1 AaER for any A E N and the estimator for parameters of Gibbs distributions with  of the form in 19 is A n eUT    arg max  24 ", "il 2", "In addition the ML estimate p in 23 can be analytically solved and the solution is given by Equation 5", "In the partial observation case if Y1    Yn are the observed yields then the esti mators for the two distributions are p   p A  a  arg max II L II p A  af  AaT  25 il Y T Y AaER subject to L p  A  a  1 AaER for any A E N and n eUT    arg max  26 l TE Yi 2", "respectively", "We want to compare the ML estimators for the two distributions and see if they produce the same results in some sense", "Since the parameters p serve as base numbers in PCFG distributions whereas  are exponents in Gibbs distributions to make the comparison sensible we take the logarithms of p and ask whether or not log p and  are the same", "Since the ML estimation procedure for PCFGs involves constrained optimization whereas the estimation procedure for Gibbs distributions only involves unconstrained optimization it is reasonable to suspect log pI Indeed numerically log p and  are different", "For example the estimator 23 only gives one estimate of the system of production probabilities whereas the estimator 24 may yield infinitely many solutions", "Such uniqueness and nonuniqueness of estimates is related to the identifiability of parameters", "We will discuss this in more detail in Section 7", "Despite their numerical differences the ML estimators for PCFG distributions and Gibbs distributions with the form 19 are equivalent in the sense that the estimates produced by the estimators impose the same distributions on parses", "Because of this in the context of ML estimation of parameters we can regard PCFG distributions as special cases of Gibbs distributions", "Corollary 4 If p is the solution of 23 then log p is a solution of ML estimation 24", "Similarly if p is a solution of 25 then log p is a solution of ML estimation 26", "Hence the estimates of production probabilities of PCFG distributions and parameters of Gibbs distributions with the form 19 impose the same distributions on parses", "Proof Suppose ", "is a solution for 24", "By Proposition 4 the Gibbs distribution P5", "is imposed by a system of production probabilities p Then p is the solution of 23", "Let ", " log p ie A  o  log pA  o", "Then ", "impose the same distribution on parses as ", "Therefore ", "are also a solution to 24", "This proves the first half of the result", "The second half is similarly proved", "D 6", "Branching Rates of PCFGs", "In this section we study PCFGs from the perspective of stochastic branching processes", "Adopting the setup given by Miller and OSullivan 1992 we define the mean matrix M of p as a I NI x I NI square matrix with its A Bth entry being the expected number of variables B resulting from rewriting A MAB   p A  onBo", "27 aENUT st", "AaER Clearly M is a nonnegative matrix", "We say B E N can be reached from A E N if for some n  0 M n A B  0 where M n A B is the A Bth element of Mn", "M is irreducible if for any pair AB E N B can be reached from A The corresponding branching process is called connected if M is irreducible Walters 1982", "It is easy to check that these definitions are equivalent to Definition 3", "We need the result below for the study of branching processes", "Theorem 1 PerronFrobenius Let M  mij  be a nonnegative k x k matrix", "1", "There is a nonnegative eigenvalue p such that no eigenvalue of A has", "absolute value greater than p eigenvector v  v1   vk and a nonnegative right column eigenvector 3", "If M is irreducible then p is a simple eigenvalue ie the multiplicity of p is 1 and the corresponding eigenvectors are strictly positive ie U  0 Vi  0 all i", "The eigenvalue p is called the branching rate of the process", "A branching process is called subcritical critical supercritical if p  1  p  1 p  1", "We also say a PCFG is subcritical critical supercritical if its corresponding branching process is When a PCFG is subcritical it is proper", "When a PCFG is supercritical it is improper", "The next result demonstrates that production probabilities assigned by the relative weighted frequency method impose subcritical PCFG distributions", "Proposition 5 For p assigned by the relative weighted frequency method 8 and M by 27 p  1", "28 Proof Let  be the right eigenvector of p as described in item 2 of Theorem 1", "We have M   p", "For each variable A LMA B B  p A", "BEN Therefore BEN aENUT st", "AaER p A  an B a B  p A", "Replacing p A  a by FA  aF A according to 9 I BEN aENUT st", "AaER F A  a F A n B a B  p A", "Multiply both sides by F A and sum over A E N By 11 LF A A  S   p LF A A", "29 AEN AEN We need to show that S  0", "Assume S  0", "Then for any n  0 since Mn  pn  we have LM nl sA A  pnS   0", "AEN For each A E N M nSA A  0", "Because each A E N is reachable from S under p there is n  0 such that M nl sA  0", "So we get  A  0", "Hence   0", "This contradicts the fact that  is a nonnegative eigenvector of M Therefore S   0", "By 29 LF A A  p LF A A  0 AEN AEN This proves p  1", "D We will apply the above result to give another proof of Proposition 2", "Before doing this we need to introduce a spectral theorem which is wellknown in matrix analysis", "Theorem 2 Suppose M is an n x n real matrix", "Let OM be the largest absolute value of Ms eigenvalues", "Then OM  lim IIMn 111n noo where IIMII is the norm of M defined by IIMII  sup IMvl", "liill Now we can prove the following result", "Proposition 6 If M given by 27 has branching rate p  1 then for each m E N U O 30 Proof We repeat the proof of Proposition 2 from Section 4 up to 15", "Then instead of sum ming over A we observe that 15 can be written as M kl A  K  L M  ABM k B BEN Write M k A AE N as Mb which is a vector indexed by A E N We then have where 1 is defined as l", "1 and for twocolumn vectors i and iJ i  iJ means each component of  is  the corresponding component of v Since the components in Kl M and M k are positive the above relation implies Hence we get By induction we get k2 M k  KM jl  M klM 1 jO k2  IMkl S K L IIMlllll  IIMklll lM1l jO 31 By Theorem 2 for any p  p   1 IIMnll  o p n", "Then 31 implies that IMkl is bounded", "Since M k are positive and increasing it follows that M k converge", "D Next we investigate how branching rates of improper PCFGs change after renor malization", "First let us look at a simple example", "Consider the CFG given by 1", "Assign probability p to the first production S  SS and 1 p to the second one S  a", "It was proved that the total probability of parses is minl 1pl", "If p  12 then minl 1p  1  l  p  1 1 implying the PCFG is improper", "To get the renor malized distribution take a parse T with yield am", "Since f S  S S T   m  1 and f  S  aT   m p T   pm11rrThen the renormalized probability of T equals Therefore p is assigned by a system of production probabilities p with p S  SS  1 p  12 and p S  a   p So the renormalized PCFG is subcritical", "More generally we have the following result which says a connected improper PCFG after being renormalized becomes a subcritical PCFG", "Proposition 7 If p is a connected improper PCFG distribution on parses then its renormalized ver sion p is subcritical", "Proof We have O  p rls   1 and we shall first show based on the fact that the PCFG is connected that all O  p OA  1", "Recall the proof of Corollary 1", "There we got the relation qs 2 qAps A E T  where qA is the probability that trees rooted in A fail to terminate", "Because the PCFG is connected S is reachable from A too", "By the same argument we also have qA 2 qspA S E T", "Since both qs and PA S E T   0 qA  0 then p OA  1qA  1", "Similarly we can prove p OA 2 p rl s PA S E T  0", "For each A define generating functions gA as in Harris 1963 Section 22 oENUT BEN st", "AaER where s  sAAEN Write g  gA AEN and define gn  g1l recursively as 32 33 It is easy to see that gA O is the total probability of parses with root A and height 1", "By induction g1  0 is the total probability of parses with root A and height  n", "Therefore gl  o ip rlA   1", "Write r   p OAAEN Then g r  g lim gnl o   lim g gnl o   lim gnl  0  r noo noo noo Therefore r is a nonnegative solution of g s   s It is also the smallest among such solutions", "That is if there is another nonnegative solution r I r then r  r  This is because O  r implies g n 0  g n  r   r for all n  0 and by letting n  oo r  r  Clearly 1is also a solution of g s   s We now renormalize p to get p by 22", "Define generating functions f  fA  of p and Jn  in the same way as 32 and 33", "Then fA s   L p A  a IT sBo oENUT BEN st", "AoER1pA  a IIrnB Ba  IIsBn Ba   aENuT rA BEN BEN st", "AaER 34 For two vectors r   rA and sA write rs for  rAsA  and r s for  rA sA ", "Then 34 can be written f  s   g rs  r Since all rA  p OA are positivef s are well defined by the fractions", "Because r is the smallest nonnegative solution of g s   s by the above equation 1is the only solution of f  s   s in the unit cube", "Since g s   s also has a solution 1 f  s   s has a solution 1 r which is strictly larger than 1", "We want to know how f changes on the line segment connecting 1 and 1 r Let u  1 r  1", "Then u is strictly positive", "Elements on the line segment between 1 and l r can be represented by 1 tu with t E O l", "Define h t  f  l  tu  1 u Then hA t  L p A  a II1  tuB Ba   1tuA", "aENUT BEN st", "AaER 35 Differentiate h at t  0", "Then h 0  M uuwhere M is the mean matrix corresponding to p Every hA t is a convex function", "Then because hA 0  hA l  0 h 0 S 0 which leads to M u S u We now show that for at least one A  M uA  UA", "First of all note that hO  0 only if hA  t is linear", "Assume M u  u which leads to h 0  0 and the linearity of h t", "Together with h O  0 this implies h t  0", "Choose t  0 such that 1 tuA  0 for all A Thenf l  tu  1tu  h t  0", "Therefore 1 tu is a nonnegative solution of f  s   s and is strictly less than 1", "This contradicts the fact that 1 is the smallest nonnegative solution of f s  s Now we have M u S u and 3A st", " M uA  UA", "Because p is connected M is irreducible", "By item 3 of Theorem 1 u is strictly positive and there is a strictly positive left eigenvector v such that vM  pv", "Therefore vM u  vu or pvu  vu", "Hence p  1", "This completes the proof", "D 7", "Identifiability and Approximation of Production Probabilities of PCFGs", "Identifiability of parameters is related to the consistency of estimates both being im portant statistical issues", "Proving the consistency of the ML estimate of a system of production probabilities given in 5 is relatively straightforward", "Consistency in this case means that if p imposes a proper distribution then as the size of the data com posed of independent and identically distributed iid samples goes to infinity with probability one the estimate p converges to p To see this think of the sample parses as taken independently from a branching process governed by p By the contextfree nature of the branching process for A E N each instance of A selects a production A  a by probability p A  a independently of the other instances of A As the size of the data set goes to infinity the number of occurrences of A goes to infinity", "Therefore by the law of large numbers the ratio between the number of occurrences of A  a and the number of occurrences of A which is p A  a converges to p A  a with probability one", "By the consistency of the ML estimate of a system of production probabilities we can prove that production probabilities are identifiable parameters of PCFGs", "Inother words different systems of production probabilities impose different PCFG distribu tions", "Proposition 8 If p1 p2 impose distributions P1 P2 respectively and p1  p2 then Pi  P2", "Proof Assume P1  P2", "Then draw n iid samples from P1", "Because the ML estimator p is consistent as n  oo p  p1 with probability 1", "Because the n iid samples can also be regarded as drawn from P2 with the same argument p  p2 with probability 1", "Hence p1  p2 a contradiction", "D We mentioned in Section 5 that the ML estimators 24 and 26 may produce infinitely many estimates if the Gibbs distributions on parses have the form 19", "This phenomenon of multiple solutions results from the nonidentifiability of parameters of the Gibbs distributions 19 which means that different parameters may yield the same distributions", "To see why parameters of Gibbs distribution 19 are nonidentifiable we note that the frequencies of production rules are linearly dependent L f A  a r AaER L n A af  B  a r BaER if A  S L f  S  a r SaER L n S af  B  aT   1", "BaER Therefore there exists Ao  0 such that for any r Ao f  T   0", "If ", "is a solution for 24 then for any number t  ", " tAo  f  r  ", "f  r  e5tXo J r  ef  r z  z  xtxo ", " P5ixr  P5", "r", "Thus for any t ", " tAo is also a solution for 24", "This shows that the parameters of Gibbs distribution 19 are nonidentifiable", "Finally we consider how to approximate production probabilities by mean fre quencies of productions", "Given iid samples of parses r1   Tn from the distribution imposed by p by the consistency of the ML estimate of p given by 5 n Lf  A  a r n p A  a  in  p A  a Lf  A r n il with probability 1 as n  oo", "If the entropy of the distribution p is finite then for every production rule A  3 E R tf A  JT   Epf A  JT   with probability 1 il p A  a  Epf A  a T  with probability 1 Epf AT    Epf A  a T  p A  a  Epf AT    If the entropy is infinite the above argument does not work because both the numerator and the denominator of the fraction are infinity", "Can we change the fraction a little bit so that it still makes sense and at the same time yields good approximation to p A  a", "One way to do this is to pick a large finite subset O of O and replace the fraction by Epf A  a TIT E O Epf AT IT E o where Epf A  aT IT E O is the conditional expectation of f A  aT  given O which is defined as If A  aT pT  Epf A  a TIT E O  r E", " L pT  Because O is finite the top of the fraction on the righthand side is finite", "Also the bottom of the fraction is positive", "Therefore the conditional expectation of f is finite", "The conditional expectation Epf AT IT E O is similarly defined", "The following result shows that as O expands the approximation gets better", "Proposition 9 Suppose a system of production probabilities p imposes a proper distribution", "Then for any increasing sequence of finite subsets On of O with On i0 ie 01 C 02   ", "C 0 On finite and UOn  0 A  a  lim Epf A  aT IT E On  p noo Epf AT IT E On To prove the proposition we introduce the KullbackLeibler divergence", "For any two probability distributions p and q on 0 the KullbackLeibler divergence between p and q is defined as where O log qg  is defined as O for any qT  0 0", "Dpllq is nonnegative and equal to O if and only if p  q One thing to note is that q need not be proper in order to make Dpl l q nonnegative", "Even when qT  1 it is still true that Dpll q 2 0", "For more about the KullbackLeibler divergence we refer the readers to Cover and Thomas 1991", "The KullbackLeibler divergence has the simple property described below which will be used in the proof of Proposition 9", "Lemma 2 If O is an arbitrary subset of nthen Dpl l q  pO log P O  1 pO log 1 p O   qO 1 qO Proof Consider the KullbackLeibler divergence between the conditional distributions p TI O and q TI O which equals   pTI O 1  pT  pO 0 pTI O  log TIO  O 0 pT  log T   log O 2 0 rEl q p rE1 q q LpT  log pT  2 pO log pO  rE1 q T  q fl Similarly L pT  log pT  2 pO  O log pO  O 2 1  pO log 1 pO   rE11 q T  q fl  fl 1 q fl  The second 2 is because qO  1", "These two inequalities together prove the lemma", "D Proof of Proposition 9 Given n for production probabilities q let Knq be the KullbackLeibler divergence  between the conditional distribution p T l rln and the distribution imposed by q Knq  LpT l rln  log pTIOn  qA  af  Aar  ", "36 AaER We want to find q that minimizes Knq", "This can be achieved by applying the Lagrange multiplier method", "The condition that q is subject to is L q A  a  1 AaER 37 for every A E N There are I NI such constraints", "To incorporate them into the mini mization we consider the function Lq  Knq  L AA  L q A  a  1  AEN AaER where the unknown coefficients  A AE N are called Lagrange multipliers", "The q that minimizes Kn q subject to 37 satisfies L q  0 q A  a   for all A  a E R By simple computation this is equivalent to LJ  A  aTPTI On  Aq A  a", "r EOn Sum both sides over all a E N U T such that A  a E R By the constraints 37 AA  LJ  AT P T IOn ", "r EOn Therefore we prove that if there is a minimizer it has to be Pn where To see that there is a minimizer of Kn q subject to 37 consider the boundary points of the region  q   q A  a  q A  a  0 L q A  a  1 a st", "AaER Any boundary point of the region has a component equal to zero hence for some T E On qT   0 implying Kn q  oo", "Because Kn q is a continuous function Kn must attain its minimum inside the above region and this minimizer as has been shown is Pn We need to show Pn  p Let O  On and apply Lemma 2 to p T IOn  and Pn T ", "Since p Onl On  1 we get O  log pn On   Kn Pn  On the other hand because Pnis the minimizer of Kn Kn Pn   Kn P   log pOn  Because On  0 and p is proper p On  1", "Therefore O  log pn On   log pOn   0", "Hence Pn On  1", "Choose an arbitrary T E 0", "For all n large enough T E On", "Apply Lemma 2 to T and get p T l On  1 p Tl On   0  p T IOn  log PnT IOn   1  pTI On  log l Pn T IOn   Kn Pn   0 p Tl On  1 p Tl On  p T IOn  log PnT IOn   1  pTI On   log 1Pn T IOn   0  r p Tl On   1 n1", " PnT IOn    Together with p T IOn   p T   0 and Pn On  1 this implies lim PnT   lim PnT IOn   lim Pn T IOn   p T ", "n oo n oo n oo This nearly completes the proof", "By the identifiability of production probabilities Pn should converge to p To make the argument more rigorous by compactness of Pn every subsequence of Pn has a limit point", "Let p  be a limit point of a subsequence Pn For any T since Pn r  p r p  r  p r", "By the identifiability of production probabilities p   p Therefore p is the only limit point of Pn This proves Pn  p"]}, "N01-1012": {"title": ["An Algorithm for Aspects of Semantic Interpretation Using an"], "abstract": ["An algorithm for semantic interpretation is explained", "The algorithm is based on predicatesdefined for WordNet verb classes", "The algorithm is driven by the definition of these predicates whose thematic roles are linked to theWordNet ontology for nouns and to the syntactic relations that realize them", "The algorithm has been tested in the identification of the meaning of the verb thematic roles and temporal and spatial adjuncts"], "inroduction": ["The semantic interpretation algorithm ex plained in this paper offers a solution to thefollowing interpretation problems determina tion of the meaning of the verb identification of thematic roles and adjuncts and attachments of prepositional phrases PPs", "An interesting aspect of the algorithm is that the solution ofall these problems is interdependent", "The inter pretation algorithm uses WordNet Miller et al 1993 as its lexical knowledgebase", "Predicatesor verbal concepts have been defined for Word Net verb classes which have been reorganized considerably following the criteria imposed bythe interpretation algorithm", "WordNet ontology for nouns has also undergone some reorga nization and redefinition to conform with the entries in the thematic roles of the predicates", "One of the views that guides this research is that the syntax of many verbs is determined by theirmeaning", "Some verbs that are highly ambigu ous say more than 10 senses and light verbswhich do not lexicalize anything will need spe cial definitions", "Briefly the algorithm is as follows", "For every verb in a sentence WordNet provides a list ofverb synsets for which we have defined predicates", "These predicates can be viewed as contenders for the meaning of the verb", "As syntac tic relations are parsed the interpreter checks each predicate in order to see if the predicatehas a thematic role which is realized by the syn tactic relation", "If so the interpreter records this fact and gets the next syntactic relation", "Thepredicate that realizes the most syntactic rela tions in the sentence is selected as the meaning of the verb", "This paper is organized as follows", "The first part of the paper  sections 2 to 5  explains themethodology for building predicates for Word Net verb classes and the second part  sections 6 to 9  describes the semantic interpretation algorithm testing and conclusions"]}, "N03-1010": {"title": ["Greedy Decoding for Statistical Machine Translation in Almost Linear Time Ulrich Germann USC Information Sciences Institute Marina del Rey CA germannisiedu Abstract We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic   when applied navely to practically linear time1 without sacrificing trans lation quality We achieve this by integrating hypothesis evaluation into hypothesis creation tiling improvements over the translation hypothesis at the end of each search iteration and by imposing restrictions on the amount of word reordering during decoding 1 Introduction Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s Brown et al 1990 1993 Berger et al 1994 1996 Based on the conventions established in Brown et al 1993 these models are commonly referred to as the IBM Models 15 One of the big challenges in building actual MT systems within this framework is that of decoding finding the translation candidate        that maximizes the translation probability"], "abstract": ["We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic  when applied navely to practically linear time1 without sacrificing translation quality", "We achieve this by integrating hypothesis evaluation into hypothesis creation tiling improvements over the translation hypothesis at the end of each search iteration and by imposing restrictions on the amount of word reordering during decoding"], "inroduction": ["Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s Brown et al 1990 1993 Berger et al 1994 1996", "Based on the conventions established in Brown et al", "1993 these models are commonly referred to as the IBM Models 15", "One of the big challenges in building actual MT systems within this framework is that of decoding finding the translation candidate that maximizes the translation probability for the given input  Knight 1999 has shown the problem to be NPcomplete", "Due to the complexity of the task practical MT systems usually do not employ optimal decoders that is decoders that are guaranteed to find an optimal solution within the constraints of the framework but rely on approximative algorithms instead", "Empirical evidence suggests that such algorithms can perform resonably well", "For example Berger et al", "1994 attribute only 5 of the translation errors of their Candide system which uses 1 Technically the complexity is still  However the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs", "a restricted stack search to search errors", "Using the same evaluation metric but different evaluation data Wang and Waibel 1997 report search error rates of 79 and 93 respectively for their decoders", "Och et al", "2001 and Germann et al", "2001 both implemented optimal decoders and benchmarked approximative algorithms against them", "Och et al report word error rates of 6868 for optimal search based on a variant of the A algorithm and 6965 for the most restricted version of a decoder that combines dynamic programming with a beam search Tillmann and Ney 2000", "Germann et al", "2001 compare translations obtained by a multistack decoder and a greedy hillclimbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the travelingsalesman problem cf", "Knight 1999", "Their overall performance metric is the sentence error rate SER", "For decoding with IBM Model 3 they report SERs of about 57 6word sentences and 76 8word sentences for optimal decoding 58 and 75 for stack decoding and 60 and 75 for greedy decoding which is the focus of this paper", "All these numbers suggest that approximative algorithms are a feasible choice for practical applications", "The purpose of this paper is to describe speed improvements to the greedy decoder mentioned above", "While acceptably fast for the kind of evaluation used in Germann et al", "2001 namely sentences of up to 20 words its speed becomes an issue for more realistic applications", "Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 878 segments ca", "25k tokens requires without any of the improvements described in this paper over 440CPU hours using the simpler faster algorithm de scribed below", "We will show that this time can be reduced to ca", "40 minutes without sacrificing translation quality", "In the following we first describe the underlying IBM initial string I do not understand the logic of these people  pick fertilities I not not understand the logic of these people  replace words Je ne pas comprends la logique de ces gens  reorder Je ne comprends pas la logique de ces gens  insert spurious words Je ne comprends pas la logique de ces gens la  Figure 1 How the IBM models model the translation process", "This is a hypothetical example and not taken from any actual training or decoding logs", "models of machine translation Section 2 and our hill climbing algorithm Section 3", "In Section 4 we discuss improvements to the algorithm and its implementation and the effect of restrictions on word reordering"]}, "N03-2035": {"title": ["A ContextSensitive  Homograph Disambiguation"], "abstract": ["Homograph ambiguity is an original issue in TexttoSpeech TTS", "To disambiguate homograph several efficient approaches have been proposed such as partofspeech POS ngram Bayesian classifier decision tree and Bayesianhybrid approaches", "These methods need words orand POS tags surrounding the question homographs in disambiguation", "Some languages such as Thai Chinese and Japanese have no wordboundary delimiter", "Therefore before solving homograph ambiguity we need to identify word boundaries", "In this paper we propose a unique framework that solves both word segmentation and homograph ambiguity problems altogether", "Our model employs both local and long distance contexts which are automatically extracted by a machine learning technique called Winnow"], "inroduction": ["In traditional Thai TTS it consists of four main modules word segmentation graphemetophoneme prosody generation and speech signal processing", "The accuracy of pronunciation in Thai TTS mainly depends on accuracies of two modules word segmentation and graphemetophoneme", "In word segmentation process if word boundaries cannot be identified correctly it leads Thai TTS to the incorrect pronunciation such as a string  which can be separated into two different ways with different meanings and pronunciations", "The first one is eye round pronounced ta0 klom0and the other one is expose wind pronounced tak1 lom0", "In graphemetophoneme mod ule it may produce error pronunciations for a homograph which can be pronounced more than one way such as a word  which can be pronounced phlaw0 or phe0 la0", "Therefore to improve an accuracy of Thai TTS we have to focus on solving the problems of word boundary ambiguity and homograph ambiguity which can be viewed as a disambiguation task", "A number of featurebased methods have been tried for several disambiguation tasks in NLP including decision lists Bayesian hybrids and Winnow", "These methods are superior to the previously proposed methods in that they can combine evidence from various sources in disambiguation", "To apply the methods in our task we treat problems of word boundary and homograph ambiguity as a task of word pronunciation disambiguation", "This task is to decide using the context which was actually intended", "Instead of using only one type of syntactic evidence as in Ngram approaches we employ the synergy of several types of features", "Following previous works 4 6 we adopted two types of features context words and collections", "Contextword feature is used to test for the presence of a particular word within  K words of the target word and collocation test for a pattern of up to L contiguous words andor partofspeech tags surrounding the target word", "To automatically extract the discriminative features from feature space and to combine them in disambiguation we have to investigate an efficient technique in our task", "The problem becomes how to select and combine various kinds of features", "Yarowsky 11 proposed decision list as a way to pool several types of features and to solve the target problem by applying a single strongest feature whatever type it is Golding 3 proposed a Bayesian hybrid method to take into account all available evidence instead of only the strongest one", "The method was applied to the task of contextsentitive spelling correction and was reported to be superior to decision lists", "Later Golding and Roth 4 applied Winnow algorithm in the same task and found that the algorithm performs comparably to the Bayesian hybrid method when using pruned feature sets and is better when using unpruned sets or unfamiliar test set", "In this paper we propose a unified framework in solving the problems of word boundary ambiguity and homograph ambiguity altogether", "Our approach employs both local and longdistance contexts which can be automatically extracted by a machine learning technique", "In this task we employ the machine learning technique called Winnow", "We then construct our system based on the algorithm and evaluate them by comparing with other existing approaches to Thai homograph problems"]}, "N04-1016": {"title": ["The Web as a Baseline Evaluating the Performance of"], "abstract": ["Previous work demonstrated that web counts can be used to approximate bigram frequencies and thus should be useful for a wide variety of NLP tasks", "So far only two generation tasks candidate selection for machine translation and confusionset disambiguation have been tested using webscale data sets", "The present paper investigates if these results generalize to tasks covering both syntax and semantics both generation and analysis and a larger range of ngrams", "For the majority of tasks we fi nd that simple unsupervised models perform better when ngram frequencies are obtained from the web rather than from a large corpus", "However in most cases webbased models fail to outperform more sophisticated stateofthe art models trained on small corpora", "We argue that webbased models should therefore be used as a baseline for rather than an alternative to standard models"], "inroduction": ["Keller and Lapata 2003 investigated the validity of web counts for a range of predicateargument bigrams verb object adjectivenoun and nounnoun bigrams", "They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts a correlate with frequencies obtained from a carefully edited balanced corpus such as the 100M words British National Corpus BNC b correlate with frequencies recreated using smoothing methods in the case of unseen bigrams c reliably predict human plausibility judgments and d yield stateoftheart performance on pseudodisambiguation tasks", "Keller and Lapatas 2003 results suggest that web based frequencies can be a viable alternative to bigram frequencies obtained from smaller corpora or recreated using smoothing", "However they do not demonstrate that realistic NLP tasks can benefi t from web counts", "In order to show this web counts would have to be applied to a diverse range of NLP tasks both syntactic and seman Task n POS Ling Type MT candidate select", "12 V N Sem Generation Spelling correction 123 Any SynSem Generation Adjective ordering 12 Adj Sem Generation Compound bracketing 12 N Syn Analysis Compound interpret", "123 N P Sem Analysis Countability detection 12 N Det Sem Analysis Table 1 Overview of the tasks investigated in this paper n size of ngram POS parts of speech Ling linguistic knowledge Type type of task tic involving analysis eg disambiguation and generation eg selection among competing outputs", "Also it remains to be shown that the webbased approach scales up to larger ngrams eg trigrams and to combinations of different parts of speech Keller and Lapata 2003 only tested bigrams involving nouns verbs and adjectives", "Another important question is whether webbased methods which are by defi nition unsupervised can be competitive alternatives to supervised approaches used for most tasks in the literature", "This paper aims to address these questions", "We start by using web counts for two generation tasks for which the use of large data sets has shown promising results a target language candidate selection for machine translation Grefenstette 1998 and b context sensitive spelling correction Banko and Brill 2001ab", "Then we investigate the generality of the webbased approach by applying it to a range of analysis and generations tasks involving both syntactic and semantic knowledge c ordering of prenominal adjectives d compound noun bracketing e compound noun interpretation and f noun count ability detection", "Table 1 gives an overview of these tasks and their properties", "In all cases we propose a simple unsupervised ngram based model whose parameters are estimated using web counts", "We compare this model both against a baseline same model but parameters estimated on the BNC and against stateoftheart models from the literature which are either supervised ie use annotated training data or unsupervised but rely on taxonomies to recreate missing counts"]}, "N06-1017": {"title": ["Unknown word sense detection as outlier detection"], "abstract": ["We address the problem of unknown word sense detection the identification of corpus occurrences that are not covered by a given sense inventory", "We model this as an instance of outlier detection using a simple nearest neighborbased approach to measuring the resemblance of a new item to a training set", "In combination with a method that alleviates data sparseness by sharing training data across lemmas the approach achieves a precision of 077 and recall of 082"], "inroduction": ["If a system has seen only positive examples how does it recognize a negative example", "This is the problem addressed by outlier detection also called novelty detection1 Markou and Singh 2003a Markou and Singh 2003b Marsland 2003 to detect novel or unknown items that differ from all the seen training data", "Outlier detection approaches typically derive some model of normal objects from the training set and use a distance measure and a threshold to detect abnormal items", "In this paper we apply outlier detection techniques to the task of unknown sense detection the identification of corpus occurrences that are not covered by a given sense inventory", "The training set 1 The term novelty detection is also used for the distinction of novel and repeated information in information retrieval a different if related topic", "Figure 1 Wrong assignment due to missing sense from the Hound of the Baskervilles Ch", "14 against which new occurrences are compared will consist of senseannotated text", "Unknown sense detection is related to word sense disambiguation WSD and to word sense discrimination Schu tze 1998 but differs from both", "In WSD all senses are assumed known and the task is to select one of them while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them and all training instances regardless of the sense to which they belong are modeled as one group of known data", "Unknown sense detection also differs from word sense discrimination where no sense inventory is given and the task is to group occurrences into senses", "In unknown sense detection the model respects the given word senses", "The main motivation for this study comes from shallow semantic parsing by which we mean a combination of WSD and the automatic assignment of semantic roles to free text", "In cases where a sense is missing from the inventory WSD will wrongly assign one of the existing senses", "Figure 1 shows an example a sentence from the Hound of the Baskervilles analyzed by the SH A L M A N E S E R Erk and Pado 2006 shallow semantic parser", "The analysis is based on FrameNet Baker et al 1998 a resource that lists senses and semantic roles for English expressions", "FrameNet is lacking a sense of expectation or being mentally prepared for the verb prepare so prepared has been assigned the sense CO O K I N G C R E AT I O N a possible but improbable analysis2", "Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing eg for drawing inferences", "Unknown sense detection can prevent such mistakes", "All sense inventories face the problem of missing senses either because of their small overall size as is the case for some nonEnglish WordNets or when they encounter domainspecific senses", "Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing but the method we propose is applicable to any sense inventory that has annotated data in particular it is also applicable to WordNet", "In this paper we model unknown sense detection as outlier detection using a simple Nearest Neighborbased method Tax and Duin 2000 that compares the local probability density at each test item with that of its nearest training item", "To our knowledge there exists no other approach to date to the problem of detecting unknown senses", "There are however approaches to the complementary problem of determining the closest known sense for unknown words Widdows 2003 Curran 2005 Burchardt et al 2005 which can be viewed as the logical next step after unknown sense detection", "Plan of the paper", "After a brief sketch of FrameNet in Section 2 we describe the experimental setup used throughout this paper in Section 3", "Section 4 tests whether a very simple model suffices for detecting unknown senses a threshold on confidence scores returned by the SH A L M A N E S E R WSD 2 Unfortunately the semantic roles have been misassigned by the system", "The word I should fill the FO O D role while for a hound could be assigned the optional RE C E I V E R role", "system", "The result is that recall is much too low", "Section 5 introduces the NNbased outlier detection approach that we use in section 6 for unknown sense detection with better results than in the first experiment but still low recall", "Section 7 repeats the experiment of section 6 with added training data making use of the fact that one semantic class in FrameNet typically pertains to several lemmas and achieving a marked improvement in results"]}, "N06-1037": {"title": ["Exploring Syntactic Features for Relation Extraction using"], "abstract": ["This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction", "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel", "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous bestreported featurebased methods on the 24 ACE relation subtypes", "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types"], "inroduction": ["Relation extraction is a subtask of information extraction that finds various predefined semantic relations such as location affiliation rival etc between pairs of entities in text", "For example the sentence George Bush is the president of the United States conveys the semantic relation President between the entities George Bush PER and the United States GPE a GeoPolitical Entity  an entity with land and a government ACE 2004", "Prior featurebased methods for this task Kambhatla 2004 Zhou et al 2005 employed a large amount of diverse linguistic features varyingfrom lexical knowledge entity mention informa tion to syntactic parse trees dependency trees and semantic features", "Since a parse tree contains rich syntactic structure information in principle the features extracted from a parse tree should contribute much more to performance improvement for relation extraction", "However it is reported Zhou et al 2005 Kambhatla 2004 that hierarchical structured syntactic features contributes less to performance improvement", "This may be mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features", "As an alternative kernel methods Collins and Duffy 2001 provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees", "But to our surprise the sole tworeported dependency tree kernels for relation extraction on the ACE corpus Bunescu and Mooney 2005 Culotta and Sorensen 2004 showed much lower performance than the featurebased methods", "One may ask are the syntactic tree features very useful for relation extraction", "Can tree kernel methods effectively capture the syntactic tree features and other various features that have been proven useful in the featurebased methods", "In this paper we demonstrate the effectiveness of the syntactic tree features for relation extraction and study how to capture such features via a convolution tree kernel", "We also study how to select the optimal feature space eg the set of subtrees to represent relation instances to optimize the system performance", "The experimental results show that the convolution tree kernel plus entity features achieves slightly better performance than the previous bestreported featurebased methods", "It also shows that our method significantly outperforms the two dependency tree kernels Bunescu and Mooney 2005 Culotta and Sorensen 2004 on the 5 ACE relation types", "The rest of the paper is organized as follows", "In Section 2 we review the previous work", "Section 3 discusses our tree kernel based learning algorithm", "288 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL pages 288295 New York June 2006", "Qc 2006 Association for Computational Linguistics Section 4 shows the experimental results and compares our work with the related work", "We conclude our work in Section 5"]}, "N07-1015": {"title": ["A Systematic Exploration of the Feature Space for Relation Extraction"], "abstract": ["Relation extraction is the task of finding semantic relations between entities from text", "The stateoftheart methods for relation extraction are mostly based on statistical learning and thus all have to deal with feature selection which can significantly affect the classification performance", "In this paper we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces", "We present a general definition of feature spaces based on a graphic representation of relation instances and explore three different representations of relation instances and features of different complexities within this framework", "Our experiments show that using only basic unit features is generally sufficient to achieve stateoftheart performance while over inclusion of complex features may hurt the performance", "A combination of features of different levels of complexity and from different sentence representations coupled with taskoriented feature pruning gives the best performance"], "inroduction": ["An important information extraction task is relation extraction whose goal is to detect and characterize semantic relations between entities in text", "For example the text fragment hundreds of Palestinians converged on the square contains the located relation between the Person entity hundreds of Palestinians and the BoundedArea entity the square", "Relation extraction has applications in many domains including finding affiliation relations from web pages and finding proteinprotein interactions from biomedical literature", "Recent studies on relation extraction have shown the advantages of discriminative modelbased statistical machine learning approach to this problem", "There are generally two lines of work following this approach", "The first utilizes a set of carefully selected features obtained from different levels of text analysis from partofspeech POS tagging to full parsing and dependency parsing Kambhatla 2004 Zhao and Grishman 2005 Zhou et al 20051", "The second line of work designs kernel functions on some structured representation sequences or trees of the relation instances to capture the similarity between two relation instances Zelenko et al 2003 Culotta and Sorensen 2004 Bunescu and Mooney 2005a Bunescu and Mooney 2005b Zhang et al 2006a Zhang et al 2006b", "Of particular interest among the various kernels proposed are the convolution kernels Bunescu and Mooney 2005b Zhang et al 2006a because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature", "Apart from their computational efficiency convolution kernels also implicitly correspond to some feature space", "Therefore both lines of work rely on an appropriately de 1 Although Zhao and Grishman 2005 defined a number of kernels for relation extraction the method is essentially similar to featurebased methods", "113 Proceedings of NAACL HLT 2007 pages 113120 Rochester NY April 2007", "Qc 2007 Association for Computational Linguistics fined set of features", "As in any learning problem the choice of features can affect the performance significantly", "Despite the importance of feature selection there has not been any systematic exploration of the feature space for relation extraction and the choices of features in existing work are somewhat arbitrary", "In this paper we conduct a systematic study of the feature space for relation extraction and evaluate the effectiveness of different feature subspaces", "Our motivations are twofold", "First based on previous studies we want to identify and characterize the types of features that are potentially useful for relation extraction and define a relatively complete and structured feature space that can be systematically explored", "Second we want to compare the effectiveness of different features", "Such a study can guide us to choose the most effective feature set for relation extraction or to design convolution kernels in the most effective way", "We propose and define a unified graphic representation of the feature space and experiment with three feature subspaces corresponding to sequences syntactic parse trees and dependency parse trees", "Experiment results show that each subspace is effective by itself with the syntactic parse tree subspace being the most effective", "Combining the three subspaces does not generate much improvement", "Within each feature subspace using only the basic unit features can already give reasonably good performance", "Adding more complex features may not improve the performance much or may even hurt the performance", "Taskoriented heuristics can be used to prune the feature space and when appropriately done can improve the performance", "A combination of features of different levels of complexity and from different sentence representations coupled with taskoriented feature pruning gives the best performance"]}, "N07-1024": {"title": ["Hybrid Models for Semantic Classification of"], "abstract": ["This paper addresses the problem of classifying Chinese unknown words into finegrained semantic categories defined in a Chinese thesaurus", "We describe three novel knowledgebased models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways", "We then combine two of the knowledgebased models with a corpusbased model which classifies unknown words using contextual information", "Experiments show that the knowledgebased models outperform previous methods on the same task but the use of contextual information does not further improve performance"], "inroduction": ["Research on semantic annotation has focused primarily on word sense disambiguation WSD ie the task of determining the appropriate sense for each instance of a polysemous word out of a set of senses defined for the word in some lexicon", "Much less work has been done on semantic classification of unknown words ie words that are not listed in the lexicon", "However real texts typically contain a large number of unknown words", "Successful classification of unknown words is not only useful for lexical acquisition but also necessary for natural language processing NLP tasks that require semantic annotation", "This paper addresses the problem of classifying Chinese unknown words into finegrained semantic categories defined in a Chinese thesaurus Cilin Mei et al 1984", "This thesaurus classifies over 70000 words into 12 major categories including human A concrete object B time and space C abstract object D attributes E actions F mental activities G activities H physical states I relations J auxiliaries K and honorifics L", "The 12 major categories are further divided into 94 medium categories which in turn are subdivided into 1428 small categories", "Each small category contains synonyms that are close in meaning", "For example under the major category D the medium category Dm groups all words that refer to institutions and the small category Dm05 groups all words that refer to educational institutions eg  xuxio school", "Unknown word classification involves a much larger search space than WSD", "In classifying words into small categories in Cilin the search space for a polysemous known word consists of all the categories the word belongs to but that for an unknown word consists of all the 1428 small categories", "Research on WSD has concentrated on using contextual information which may be limited for infrequent unknown words", "On the other hand Chinese characters carry semantic information that is useful for predicting the semantic properties of the words containing them", "We present three novel knowledgebased models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways and combine two of them with a corpusbased model that uses contextual information to classify unknown words", "Experiments show that the combined knowledgebased model achieves an accuracy of 616 for classifying unknown words into small categories in Cilin but the use of contextual information does not further improve performance", "The rest of the paper is organized as follows", "Section 2 details the three novel knowledge based models proposed for this task", "Section 3 describes a corpusbased model", "Section 4 reports the experiment results of the proposed 188 Proceedings of NAACL HLT 2007 pages 188195 Rochester NY April 2007", "Qc 2007 Association for Computational Linguistics models", "Section 5 compares these results with previous results", "Section 6 concludes the paper and points to avenues for further research"]}, "N09-1007": {"title": ["Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 56 64 Boulder Colorado June 2009 c  2009 Association for Computational Linguistics A Discriminative Latent Variable Chinese Segmenter with Hybrid WordCharacter Information Xu Sun Department of Computer Science University of Tokyo sunxuissutokyoacjp Yaozhong Zhang Department of Computer Science University of Tokyo yaozhongzhangissu tokyoacjp Takuya Matsuzaki Department of Computer Science University of Tokyo matuzakiissutokyoacjp Yoshimasa Tsuruoka School of Computer Science University of Manchester yoshimasatsuruokamanchesteracuk Jun ichi Tsujii Department of Computer Science University of Tokyo Japan School of Computer Science University of Manchester UK National Centre for Text Mining UK tsujiiissutokyoacjp Abstract Conventional approaches to Chinese word segmentation treat the problem as a characterbased tagging task Recently semiMarkov models have been applied to the problem incorporating features based on complete words In this paper we propose an alternative a latent variable model which uses hybrid information based on both word sequences and character sequences We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words eg namedentities Experimental results show that this is indeed the case With this improvement evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature 1 Introduction For most natural language processing tasks words are the basic units to process Since Chinese sentences are written as continuous sequences of characters segmenting a character sequence into a word sequence is the first step for most Chinese processing applications In this paper we study the problem of Chinese word segmentation CWS which aims to find these basic units words1 for a given sentence in Chinese Chinese character sequences are normally ambiguous and outofvocabulary OOV words are a major source of the ambiguity Typical examples of OOV words include named entities eg organization names person names and location names Those named entities may be very long and a difficult case occurs when a long word W W   4 consists of some words which can be separate words on their own in such cases an automatic segmenter may split the OOV word into individual words For example Computer Committee of International Federation of Automatic Control is one of the organization names in the Microsoft Research corpus Its length is 13 and it contains more than 6 individual words but it should be treated as a single word Proper recognition of long OOV words are meaningful not only for word segmentation but also for a variety of other purposes eg fulltext indexing However as is illustrated recognizing long words without sacrificing the performance on short words is challenging Conventional approaches to Chinese word segmentation treat the problem as a characterbased la1Following previous work in this paper words can also refer to multiword expressions including proper names long named entities idioms etc 56 beling task Xue 2003 Labels are assigned to each character in the sentence indicating whether the character xi is the start Labeli  B middle or end of a multicharacter word Labeli  C A popular discriminative model that have been used for this task is the conditional random fields CRFs Lafferty et al 2001 starting with the model of Peng et al 2004 In the Second International Chinese Word Segmentation Bakeoff the second SIGHAN CWS bakeoff Emerson 2005 two of the highest scoring systems in the closed track competition were based on a CRF model Tseng et al 2005 Asahara et al 2005 While the CRF model is quite effective compared with other models designed for CWS it may be limited by its restrictive independence assumptions on nonadjacent labels Although the window can in principle be widened by increasing the Markov order this may not be a practical solution because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order Andrew 2006 To address this difficulty a choice is to relax the Markov assumption by using the semiMarkov con ditional random field model semiCRF Sarawagi and Cohen 2004 Despite the theoretical advantage of semiCRFs over CRFs however some previous studies Andrew 2006 Liang 2005 exploring the use of a semiCRF for Chinese word segmentation did not find significant gains over the CRF ones As discussed in Andrew 2006 the reason may be that despite the greater representational power of the semiCRF there are some valuable features that could be more naturally expressed in a characterbased labeling model For example on a CRF model one might use the feature the current character xi is X and the current label Labeli is C  This feature may be helpful in CWS for generalizing to new words For example it may rule out certain word boundaries if X were a character that normally occurs only as a suffix but that combines freely with some other basic forms to create new words This type of features is slightly less natural in a semiCRF since in that case local features yi yi1 x are defined on pairs of adjacent words That is to say information about which characters are not on boundaries is only implicit Notably ex cept the hybrid MarkovsemiMarkov system in Andrew 20062 no other studies using the semiCRF Sarawagi and Cohen 2004 Liang 2005 Daume III and Marcu 2005 experimented with features of segmenting nonboundaries In this paper instead of using semiMarkov models we describe an alternative a latent variable model to learn long range dependencies in Chi nese word segmentation We use the discriminative probabilistic latent variable models DPLVMs Morency et al 2007 Petrov and Klein 2008 which use latent variables to carry additional infor mation that may not be expressed by those original labels and therefore try to build more complicated or longer dependencies This is especially meaning ful in CWS because the used labels are quite coarse Labely  BC where B signifies beginning a word and C signifies the continuation of a word3 For example by using DPLVM the aforementioned feature may turn to the current character xi is X  Labeli  C and LatentV ariablei  LV   The current latent variable LV may strongly depend on the previous one or many latent variables and therefore we can model the long range dependencies which may not be captured by those very coarse labels Also since character and word information have their different advantages in CWS in our latent variable model we use hybrid information based on both character and word sequences 2 A Latent Variable Segmenter"], "abstract": ["Conventional approaches to Chinese word segmentation treat the problem as a character based tagging task", "Recently semiMarkov models have been applied to the problem incorporating features based on complete words", "In this paper we propose an alternative a latent variable model which uses hybrid information based on both word sequences and character sequences", "We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words eg namedentities", "Experimental results show that this is indeed the case", "With this improvement evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature"], "inroduction": ["For most natural language processing tasks words are the basic units to process", "Since Chinese sentences are written as continuous sequences of characters segmenting a character sequence into a word sequence is the rst step for most Chinese processing applications", "In this paper we study the problem of Chinese word segmentation CWS which aims to nd these basic units words1 for a given sentence in Chinese", "Chinese character sequences are normally ambiguous and outofvocabulary OOV words are a major source of the ambiguity", "Typical examples of OOV words include named entities eg organization names person names and location names", "Those named entities may be very long and a dif cult case occurs when a long word W W   4 consists of some words which can be separate words on their own in such cases an automatic segmenter may split the OOV word into individual words", "For example Computer Committee of International Federation of Automatic Control is one of the organization names in the Microsoft Research corpus", "Its length is 13 and it contains more than 6 individual words but it should be treated as a single word", "Proper recognition of long OOV words are meaningful not only for word segmentation but also for a variety of other purposes eg fulltext indexing", "However as is illustrated recognizing long words without sacric ing the performance on short words is challenging", "Conventional approaches to Chinese word segmentation treat the problem as a characterbased la 1 Following previous work in this paper words can also refer to multiword expressions including proper names long named entities idioms etc 56 Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 5664 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics beling task Xue 2003", "Labels are assigned to each character in the sentence indicating whether the character xi is the start Labeli  B middle or end of a multicharacter word Labeli  C ", "A popular discriminative model that have been used for this task is the conditional random elds CRFs Lafferty et al 2001 starting with the model of Peng et al", "2004", "In the Second International Chinese Word Segmentation Bakeoff the second SIGHAN CWS bakeoff Emerson 2005 two of the highest scoring systems in the closed track competition were based on a CRF model Tseng et al 2005 Asahara et al 2005", "While the CRF model is quite effective compared with other models designed for CWS it may be limited by its restrictive independence assumptions on nonadjacent labels", "Although the window can in principle be widened by increasing the Markov order this may not be a practical solution because the complexity of training and decoding a linear chain CRF grows exponentially with the Markov order Andrew 2006", "To address this difculty a choice is to relax the Markov assumption by using the semiMarkov conditional random eld model semiCRF Sarawagi and Cohen 2004", "Despite the theoretical advantage of semiCRFs over CRFs however some previous studies Andrew 2006 Liang 2005 exploring the use of a semiCRF for Chinese word segmentation did not nd signicant gains over the CRF ones", "As discussed in Andrew 2006 the reason may be that despite the greater representational power of the semiCRF there are some valuable features that could be more naturally expressed in a characterbased labeling model", "For example on a CRF model one might use the feature the current character xi is X and the current label Labeli is C ", "This feature may be helpful in CWS for generalizing to new words", "For example it may rule out certain word boundaries if X were a character that normally occurs only as a sufx but that combines freely with some other basic forms to create new words", "This type of features is slightly less natural in a semiCRF since in that case local features yi yi1 x are dened on pairs of adjacent words", "That is to say information about which characters are not on boundaries is only implicit", "Notably except the hybrid MarkovsemiMarkov system in An drew 20062 no other studies using the semiCRF Sarawagi and Cohen 2004 Liang 2005 Daume III and Marcu 2005 experimented with features of segmenting nonboundaries", "In this paper instead of using semiMarkov models we describe an alternative a latent variable model to learn long range dependencies in Chinese word segmentation", "We use the discriminative probabilistic latent variable models DPLVMs Morency et al 2007 Petrov and Klein 2008 which use latent variables to carry additional information that may not be expressed by those original labels and therefore try to build more complicated or longer dependencies", "This is especially meaningful in CWS because the used labels are quite coarse Labely  B C  where B signies beginning a word and C signies the continuation of a word3 For example by using DPLVM the aforementioned feature may turn to the current character xi is X  Labeli  C  and LatentV ariablei  LV ", "The current latent variable LV may strongly depend on the previous one or many latent variables and therefore we can model the long range dependencies which may not be captured by those very coarse labels", "Also since character and word information have their different advantages in CWS in our latent variable model we use hybrid information based on both character and word sequences"]}, "N09-1048": {"title": ["Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 424 432 Boulder Colorado June 2009 c  2009 Association for Computational Linguistics SemiSupervised Lexicon Mining from Parenthetical Expressions in Monolingual Web Pages Xianchao Wu  Naoaki Okazaki Jun ichi Tsujii Computer Science Graduate School of Information Science and Technology University of Tokyo 731 Hongo Bunkyoku Tokyo 1138656 Japan School of Computer Science University of Manchester National Centre for Text Mining NaCTeM Manchester Interdisciplinary Biocentre 131 Princess Street Manchester M1 7DN UK wxc okazaki tsujiiissutokyoacjp Abstract This paper presents a semisupervised learning framework for mining ChineseEnglish lexicons from large amount of Chinese Web pages The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis We classify parenthetical translations into bilingual abbreviations transliterations and translations A frequencybased term recognition approach is applied for extracting bilingual abbreviations A selftraining algorithm is proposed for mining transliteration and translation lexicons In which we employ available lexicons in terms of morpheme levels ie phoneme correspondences in transliteration and grapheme eg suffix stem and prefix correspondences in translation The experimental results verified the effectiveness of our approaches 1 Introduction Bilingual lexicons as lexical or phrasal parallel corpora are widely used in applications of multilingual language processing such as statistical machine translation SMT and crosslingual information retrieval However it is a timeconsuming task for constructing largescale bilingual lexicons by hand There are many facts cumber the manual de velopment of bilingual lexicons such as the continuous emergence of neologisms eg new technical terms personal names abbreviations etc the difficulty of keeping up with the neologisms for lexicographers etc In order to turn the facts to a better way one of the simplest strategies is to automatically mine largescale lexicons from corpora such as the daily updated Web Generally there are two kinds of corpora used for automatic lexicon mining One is the purely monolingual corpora wherein frequencybased expectationmaximization EM refer to Dempster et al 1977 algorithms and cognate clues play a central role Koehn and Knight 2002 Haghighi et al 2008 presented a generative model based on canonical correlation analysis in which monolingual features such as the context and orthographic substrings of words were taken into account The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as cooccurrence frequency and context are popularly employed Cheng et al 2004 Shao and Ng 2004 Cao et al 2007 Lin et al 2008 In this paper we focus on a special type of comparable corpus parenthetical translations The issue is motivated by the observation that Web pages and technical papers written in Asian languages eg Chinese Japanese sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses This is considered to be a traditional way to annotate new terms personal names or other named entities with their English translations expressed in brackets Formally a parenthetical translation can be expressed by the following pattern f1 f2  fJ e1 e2  eI 1 Here f1 f2  fJ fJ1  the preparenthesis text de notes the word sequence of some language other than English and e1 e2  eI eI1 the inparenthesis text denotes the word sequence of English We separate parenthetical translations into three categories 1httpenwikipediaorgwikiMain Page 424 Type Examples with translations in italic      GCOSto Global Climate Observing System GCOS        Shipton Tilmanbrand will be among ShiptonTilman ShiptonTilman  Cancelbotstime bomb Cancelbots Cancelbots       Bradford University the English Bradford University Bradford University that holds lessons in Hongkong Abbreviation Transliteration Translation Mixture Table 1 Parenthetical translation categories and examples extracted from Chinese Web pages Mixture stands for the mixture of translation University and transliteration Bradford    denotes the left boundary of fJ1  bilingual abbreviation transliteration and translation Table 1 illustrates examples of these categories We address several characteristics of parenthetical translations that differ from traditional comparable corpora The first is that they only appear in monolingual Web pages or documents and the context information of eI1 is unknown Second frequency and word number of eI1 are frequently small This is because parenthetical translations are only used when the authors thought that fJ1 contained some neologisms which deserved further explanation in another popular language eg English Thus traditional context based approaches are not applicable and frequency based approaches may yield low recall while with high precision Furthermore cognate clues such as orthographic features are not ap plicable between language pairs such as English and Chinese Parenthetical translation mining faces the following issues First we need to distinguish parenthetical translations from parenthetical expressions since parenthesis has many functions eg defining abbreviations elaborations ellipsis citations anno tations etc other than translation Second the left boundary denoted as  in Table 1 of the preparenthesis text need to be determined to get rid of the unrelated words Third we need further distinguish different translation types such as bilingual abbreviation the mixture of translation and transliteration as shown in Table 1 In order to deal with these problems supervised Cao et al 2007 and unsupervised Li et al 2008 methods have been proposed However supervised approaches are restricted by the quality and quantity of manually constructed training data and unsupervised approaches are totally frequencybased without using any semantic clues In contrast we propose a semisupervised framework for mining parenthetical translations We apply a monolingual abbreviation extraction approach to bilingual abbrevia tion extraction We construct an Englishsyllable to Chinesepinyin transliteration model which is self trained using phonemic similarity measurements We further employ our cascaded translation model Wu et al 2008 which is self trained based on morphemelevel translation similarity This paper is organized as follows We briefly review the related work in the next section Our system framework and selftraining algorithm is de scribed in Section 3 Bilingual abbreviation extraction self trained transliteration models and cascaded translation models are described in Section 4 5 and 6 respectively In Section 7 we evaluate our mined lexicons by Wikipedia We conclude in Sec tion 8 finally 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other largescale corpora Shao and Ng 2004 presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies combining both transliteration and context information"], "abstract": ["This paper presents a semisupervised learning framework for mining ChineseEnglish lexicons from large amount of Chinese Web pages", "The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis", "We classify parenthetical translations into bilingual abbreviations transliterations and translations", "A frequencybased term recognition approach is applied for extracting bilingual abbreviations", "A selftraining algorithm is proposed for mining transliteration and translation lexicons", "In which we employ available lexicons in terms of morpheme levels ie phoneme correspondences in transliteration and grapheme eg suffix stem and prefix correspondences in translation", "The experimental results verified the effectiveness of our approaches"], "inroduction": ["Bilingual lexicons as lexical or phrasal parallel corpora are widely used in applications of multilingual language processing such as statistical machine translation SMT and crosslingual information retrieval", "However it is a timeconsuming task for constructing largescale bilingual lexicons by hand", "There are many facts cumber the manual development of bilingual lexicons such as the continuous emergence of neologisms eg new technical terms personal names abbreviations etc the difficulty of keeping up with the neologisms for lexicographers etc In order to turn the facts to a better way one of the simplest strategies is to automatically mine largescale lexicons from corpora such as the daily updated Web", "Generally there are two kinds of corpora used for automatic lexicon mining", "One is the purely monolingual corpora wherein frequencybased expectationmaximization EM refer to Dempster et al 1977 algorithms and cognate clues play a central role Koehn and Knight 2002", "Haghighi et al", "2008 presented a generative model based on canonical correlation analysis in which monolingual features such as the context and orthographic substrings of words were taken into account", "The other is multilingual parallel and comparable corpora eg Wikipedia1 wherein features such as co occurrence frequency and context are popularly employed Cheng et al 2004 Shao and Ng 2004 Cao et al 2007 Lin et al 2008", "In this paper we focus on a special type of comparable corpus parenthetical translations", "The issue is motivated by the observation that Web pages and technical papers written in Asian languages eg Chinese Japanese sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses", "This is considered to be a traditional way to annotate new terms personal names or other named entities with their English translations expressed in brackets", "Formally a parenthetical translation can be expressed by the following pattern f1 f2  fJ e1 e2  eI ", "1 Here f1 f2  fJ f J  the preparenthesis text de notes the word sequence of some language other than English and e1 e2  eI eI  the inparenthesis text denotes the word sequence of English", "We separate parenthetical translations into three categories 1 httpenwikipediaorgwikiMain Page 424 Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 424432 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics Type Examples with translations in italic Abbreviation      GCOS to Global Climate Observing System GCOS Transliteration     ShiptonTilman brand will be among ShiptonTilman ShiptonTilman Translation  Cancelbots time bomb Cancelbots Cancelbots Mixture        Bradford University the English Bradford University Bradford University that holds lessons in Hongkong Table 1 Parenthetical translation categories and examples extracted from Chinese Web pages", "Mixture stands for the mixture of translation University and transliteration Bradford", "  denotes the left boundary of f J  bilingual abbreviation transliteration and translation", "Table 1 illustrates examples of these categories", "We address several characteristics of parenthetical translations that differ from traditional comparable corpora", "The first is that they only appear in monolingual Web pages or documents and the context approaches are restricted by the quality and quantity of manually constructed training data and unsupervised approaches are totally frequencybased without using any semantic clues", "In contrast we propose a semisupervised framework for mining parenthetical translations", "We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction", "We construct an Englishsyllable to Chinesepinyin transliteration model which is self trained using phonemic similarity measurements", "We further employ our cascaded translation model Wu et al 2008 which is selftrained based on morphemelevel translation similarity", "This paper is organized as follows", "We briefly review the related work in the next section", "Our system framework and selftraining algorithm is described in Section 3", "Bilingual abbreviation extraction selftrained transliteration models and cascaded translation models are described in Section 4 5 and 6 respectively", "In Section 7 we evaluate ourmined lexicons by Wikipedia", "We conclude in Sec information of eI is unknown", "Second frequency tion 8 finally", "and word number of eI are frequently small", "This is because parenthetical translations are only used"]}, "N09-1057": {"title": ["More than Words Syntactic Packaging and Implicit Sentiment"], "abstract": ["Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text", "In this paper we introduce a new approach to the problem that focuses not on lexical indicators but on the syntactic packaging of ideas which is well suited to investigating the identification of implicit sentiment or perspective", "We establish a strong predictive connection between linguistically well motivated features and implicit sentiment and then show how computational approximations of these features can be used to improve on existing stateoftheart sentiment classification results"], "inroduction": ["As Pang and Lee 2008 observe the last several years have seen a land rush in research on sentiment analysis and opinion mining with a frequent emphasis on the identification of opinions in evaluative text such as movie or product reviews", "However sentiment also may be carried implicitly by statements that are not only nonevaluative but not even visibly subjective", "Consider for example the following two descriptions of the same invented event 1a On November 25 a soldier veered his jeep into a crowded market and killed three civilians", "b On November 25 a soldiers jeep veered into a crowded market causing three civilian deaths", "Both descriptions appear on the surface to be objective statements and they use nearly the same words", "Lexically the sentences first clauses differ only in the difference between s and his to express the relationship between the soldier and the jeep and in the second clauses both kill and death are terms with negative connotations at least according to the General Inquirer lexicon Stone 1966", "Yet the descriptions clearly differ in the feelings they evoke if the soldier were being tried for his role in what happened on November 25 surely the prosecutor would be more likely to say 1a to the jury and the defense attorney 1b rather than the reverse1 Why then should a description like 1a be perceived as less sympathetic to the soldier than 1b", "If the difference is not in the words it must be in the way they are put together that is the structure of the sentence", "In Section 2 we offer a specific hypothesis about the connection between structure and implicit sentiment we suggest that the relationship is mediated by a set of grammatically relevant semantic properties well known to be important cross linguistically in characterizing the interface between syntax and lexical semantics", "In Section 3 we validate this hypothesis by means of a human ratings study showing that these properties are highly predictive of human sentiment ratings", "In Section 4 we introduce observable proxies for underlying semantics OPUS a practical way to approximate the relevant semantic properties automatically as features in a supervised learning setting", "In Section 5 we show that these features improve on the existing state ofthe art in automatic sentiment classification", "Sec This work was done while the first author was a student in the Department of Linguistics University of Maryland", "1 We refer readers not sharing this intuition to Section 3", "503 Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL pages 503511 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics tions 6 and 7 discuss related work and summarize"]}, "N09-2003": {"title": ["Efficient Extraction of Oraclebest Translations from Hypergraphs"], "abstract": ["Hypergraphs are used in several syntax inspired methods of machine translation to compactly encode exponentially many translation hypotheses", "The hypotheses closest to given reference translations therefore cannot be found via brute force particularly for popular measures of closeness such as BLEU", "We develop a dynamic program for extracting the so called oraclebest hypothesis from a hyper graph by viewing it as the problem of finding the most likely hypothesis under an ngram language model trained from only the reference translations", "We further identify and remove massive redundancies in the dynamic program state due to the sparsity of ngrams present in the reference translations resulting in a very efficient program", "We present run time statistics for this program and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components"], "inroduction": ["A hypergraph as demonstrated by Huang and Chi ang 2007 is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation MT system eg Koehn et al", "2003 or a syntax based MT system eg Chiang 2007", "While the hypergraph represents a very large set of translations it is quite possible that some desired translations eg the reference translations are not contained in the hypergraph due to pruning or inherent deficiency of the translation model", "In this case one is often required to find the translations in the hypergraph that are most similar to the desired translations with similarity computed via some automatic metric such as BLEU Papineni et al 2002", "Such maximally similar translations will be called oracle best translations and the process of extracting them oracle extraction", "Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph and the exponentially large number of hypotheses makes a bruteforce linear search intractable", "Therefore efficient algorithms that can exploit the structure of the hypergraph are required", "We present an efficient oracle extraction algorithm which involves two key ideas", "Firstly we view the oracle extraction as a bottomup model scoring process on a hypergraph where the model is trained on the reference translations", "This is similar to the algorithm proposed for a lattice by Dreyer et al", "2007", "Their algorithm however requires maintaining a separate dynamic programming state for each distinguished sequence of state words and the number of such sequences can be huge making the search very slow", "Secondly therefore we present a novel lookahead technique called equivalent oraclestate maintenance to merge multiple states that are equivalent for similarity computation", "Our experiments show that the equivalent oracle state maintenance technique significantly speeds up more than 40 times the oracle extraction", "Efficient oracle extraction has at least three important applications in machine translation", "Discriminative Training In discriminative training the objective is to tune the model parameters eg weights of a perceptron model or conditional random field such that the reference translations are preferred over competitors", "However the reference translations may not be reachable by the translation system in which case the oraclebest hypotheses should be substituted in training", "9 Proceedings of NAACL HLT 2009 Short Papers pages 912 Boulder Colorado June 2009", "Qc 2009 Association for Computational Linguistics System Combination In a typical system combination task eg Rosti et al", "2007 each component system produces a set of translations which are then grafted to form a confusion network", "The confusion network is then rescored often employing additional language models to select the final translation", "When measuring the goodness of a hypothesis in the confusion network one requires its score under each component system", "However some translations in the confusion network may not be reachable by some component systems in which case a systems score for the most similar reachable translation serves as a good approximation", "Multisource Translation In a multisource translation task Och and Ney 2001 the input is given in multiple source languages", "This leads to a situation analogous to system combination except that each component translation system now corresponds to a specific source language"]}, "N09_csl2013": {"title": ["Senselevel Subjectivity in a Multilingual Setting"], "abstract": ["Recent research on English word sense subjectivity has shown that the subjective aspect of an entity is a characteristic that is better delineated at the sense level instead of the traditional word level", "In this paper we seek to explore whether senses aligned across languages exhibit this trait consistently and if this is the case we investigate how this property can be leveraged in an automatic fashion", "We first conduct a manual annotation study to gauge whether the subjectivity trait of a sense can be robustly transferred across language boundaries", "An automatic framework is then introduced that is able to predict subjectivity labeling for unseen senses using either cross lingual or multilingual training enhanced with bootstrapping", "We show that the multilingual model consistently outperforms the crosslingual one with an accuracy of over 73 across all iterations", "Keywords Sentiment and text classification multilingual subjectivity analysis sense level subjectivity Email addresses carmenbaneagmailcom Carmen Banea radacsuntedu Rada Mihalcea wiebecspittedu Janyce Wiebe Preprint submitted to Computer Speech and Language December 27 2012"], "inroduction": ["Sentiment and subjectivity analysis seeks to automatically identify opinions beliefs speculations emotions sentiments and other private states in natural text Wiebe et al 2005", "Quirk et al", "1985 define a private state as a state that does not lend itself to an objective external validation or in other words a person may be observed to assert that God exists but not to believe that God exists", "Belief is in this sense private p 1181", "In the field of natural language processing researchers have used the term subjectivity analysis to denote identifying private states in text namely separating objective from subjective instances while sentiment or polarity analysis further refines the subjective text into positive negative or neutral", "Sentiment and subjectivity analysis has stemmed into a prolific area of research mainly due to the fact that numerous text processing applications stand to gain from incorporating sentiment dimensions into their models including automatic expressive texttospeech synthesis Alm et al 1990 tracking sentiment timelines in online forums and news Balog et al 2006 Lloyd et al 2005 and mining opinions from product reviews Hu and Liu 2004", "In many natural language processing tasks subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data", "Research that benefited from this additional layering ranges from question answering Yu and Hatzivassiloglou 2003 to conversation summarization Carenini et al 2008 text semantic analysis Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 and lexical substitution Su and Markert 2010", "In experiments carried out on English Wiebe and Mihalcea 2006 have shown that the most robust subjectivity delineation occurs at sense and not at word level", "Following this more finegrained perspective Esuli and Sebastiani 2006 and Andreevskaia and Bergler 2006 have proposed methods to embed senselevel automatic sentiment annotations objectiveneutral negative and positive over the English WordNet structure Miller 1995 using its relationships synonymy antonymy meronymy etc", "On the other hand noticing the scarcity of hand crafted senselevel subjectivitypolarity lexica Markert and Su 2008 have explored ways to infer them from data annotated at either the word or sentence level", "Senselevel subjectivity and crosslingual subjectivity and sentiment analysis have received considerable attentions in recent years yet our paper explores the area that lies at the intersection of these two topics", "To our knowledge this area has not been formally investigated and while the techniques may be similar to those applied in sentiment and subjectivity analysis at the sentence or the review level our work explores the more difficult task of senselevel subjectivity which also involves deep semantic aspects of the language", "The manual annotation study we performed for this task cross lingual senselevel subjectivity annotations as well as the methods we proposed crosslingual and multilingual learning using dictionaries in multiple languages are novel to our knowledge", "This work seeks to answer the following questions", "First for word senses aligned across languages is their subjectivity content consistent or in other words does a subjective sense in language A map to a subjective sense in language B and similarly for an objective sense", "Second can we employ a multilingual framework that can automatically discover new subjectiveobjective senses starting with a limited amount of annotated data", "We seek to answer the first question by conducting a manual annotation study in Section 2", "For the second question we propose two models see Section 3 one cross lingual and one multilingual which are able to simultaneously use information extracted from several languages when making subjectivity senselevel predictions"]}, "N09prod": {"title": ["Sentiment Analysis with Multisource Product Reviews"], "abstract": ["More and more product reviews emerge on Ecommerce sites and microblog systems nowadays", "This information is useful for consumers to know the others opinion on the products before purchasing or companies who want to learn the public sentiment of their products", "In order to effectively utilize this information this paper has done some sentiment analysis on these multisource reviews", "For one thing a binary classification framework based on the aspects of product is proposed", "Both explicit and implicit aspect is considered and multiple kinds of feature weighing and classifiers are compared in our framework", "For another we use several machine learning algorithms to classify the product reviews in microblog systems into positive negative and neutral classes and find OVASVMs perform best", "Part of our work in this paper has been applied in a Chinese Product Review Mining System", "Keywords product review sentiment analysis microblog SVM"], "inroduction": ["With the development of Internet more and more customers get used to purchasing products on Ecommerce sites such as 360buy1 and Newegg2", "They also write reviews on the products after using them which produce a large number of reviews on the Internet", "In addition microblog a system that allows users to post messages of no more than 140 words and share information instantaneously based on the relationship between users is under rapid development such as Twitter3 Sina microblog4 and Tencent microblog5", "A lot of microblogs contain latest product reviews", "Reviews from the above two large data sources contain much useful information for users and companies", "Users can make better purchasing decisions based on these reviews while companies can also analyze customers satisfaction according to these reviews and further improve the quality of their products", "Since there is a mass of 1 httpwww360buycom 2 httpwwwneweggcomcn 3 httpwwwtwittercom 4 httpweibocom 5 httptqqcn DS", "Huang et al", "Eds ICIC 2012 LNCS 7389 pp", "301308 2012", " SpringerVerlag Berlin Heidelberg 2012 product reviews and a single user cannot read all of them automatically mining the reviews from multiple sources is particularly important", "Most reviews in Chinese Ecommerce sites are labeled with advantage or disadvantage which is naturally suitable for binary classification", "The stateofart research in Chinese sentiment analysis mainly focuses on the whole review classification while customers often desire a more detailed understanding of products", "For example they want to know others opinion on the battery of a cell phone", "Therefore we propose a framework of sentiment classification at aspect level to solve this problem", "In our framework not only explicit but also implicit aspects are taken into account", "To our knowledge no implicit aspect discovery work of product review in Chinese language has been reported before", "For the reviews of each aspect the unigram features of words are used as text features", "We also compare the performance of three feature weighing strategies three reduction dimension and three classification approaches", "The sentiment analysis for reviews of products on microblogs is in its infancy", "Besides the microblogs that express opinion on the products some microblogs only give some statements relative to the products which contain no sentiment polarity or are neutral", "Therefore in this paper we exploit linear regression multiclass classification twostage classification and Mincut model optimization to classify the product related microblogs into three classes and compare the performance of these methods"]}, "N10-1019": {"title": ["Using Mostly Native Data to Correct Errors in Learners Writing"], "abstract": ["We present results from a range of experiments on article and preposition error correction for nonnative speakers of English", "We first compare a language model and error specific classifiers all trained on large English corpora with respect to their performance in error detection and correction", "We then combine the language model and the classifiers in a metaclassification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier", "The metaclassifier in turn is trained on errorannotated learner data optimizing the error detection and correction performance on this domain", "The metaclassification approach results in substantial gains over the classifier only and languagemodelonly scenario", "Since the metaclassifier requires errorannotated data for training we investigate how much training data is needed to improve results over the baseline of not using a metaclassifier", "All evaluations are conducted on a large error annotated corpus of learner English"], "inroduction": ["Research on the automatic correction of grammatical errors has undergone a renaissance in the past decade", "This is at least in part based on the recognition that nonnative speakers of English now outnumber native speakers by 21 in some estimates so any tool in this domain could be of tremendous value", "While earlier work in both native and nonnative error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors see Heift and Schulze 2005 for a detailed overview more recent approaches have been based on datadriven methods", "The majority of the datadriven methods use a classification technique to determine whether a word is used appropriately in its context continuing the tradition established for contextual spelling correction by Golding 1995 and Golding and Roth 1996", "The words investigated are typically articles and prepositions", "They have two distinct advantages as the subject matter for investigation They are a closed class and they comprise a substantial proportion of learners errors", "The investigation of preposition corrections can even be narrowed further amongst the more than 150 English prepositions the usage of the ten most frequent prepositions accounts for 82 of preposition errors in the 20 million word Cambridge University Press Learners Corpus", "Learning correct article use is most difficult for native speakers of an L1 that does not overtly mark definiteness and indefiniteness as English does", "Prepositions on the other hand pose difficulties for language learners from all L1 backgrounds Dalgish 1995 Bitchener et al 2005", "Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the prepositionarticle", "Different systems typically vary along three dimensions choice of features choice of classifier and choice of training data", "Features range from words and morphological information Knight and Chander 1994 to the inclusion of partofspeech tags Minnen et al 2000 Han et al 2004 2006 Chodorow et al 2007 Gamon et al 2008 2009 Izumi et al 2003 2004 Tetrault and Chodorow 2008 to features based on linguistic analysis and on WordNet Lee 2004 DeFelice and Pulman 2007 2008", "Knight and Chander 1994 and Gamon et al", "2008 used decision tree classifiers but in general maximum entropy classifiers have become the classification 163 Human Language Technologies The 2010 Annual Conference of the North American Chapter of the ACL pages 163171 Los Angeles California June 2010", "Qc 2010 Association for Computational Linguistics algorithm of choice", "Training data are normally drawn from sizeable corpora of native English text British National Corpus for DeFelice and Pulman 2007 2008 Wall Street Journal in Knight and Chander 1994 a mix of Reuters and Encarta in Gamon et al", "2008 2009", "In order to partially address the problem of domain mismatch between learners writing and the newsheavy data sets often used in datadriven NLP applications Han et al", "2004 2006 use 315 million words from the MetaMetrics corpus a diverse corpus of fiction nonfiction and textbooks categorized by reading level", "In addition to the classification approach to error detection there is a line of research  going back to at least Atwell 1987  that uses language models", "The idea here is to detect errors in areas where the language model score is suspiciously low", "Atwell 1987 uses a partofspeech tag language model to detect errors Chodorow and Leacock 2000 use mutual information and chi square statistics to identify unlikely function word and partofspeech tag sequences Turner and Charniak 2007 employ a language model based on a generative statistical parser and Stehouwer and van Zaanen 2009 investigate a diverse set of language models with different backoff strategies to determine which choice from a set of confusable words is most likely in a given context", "Gamon et al", "2008 2009 use a combination of errorspecific classifiers and a large generic language model with hand tuned heuristics for combining their scores to maximize precision", "Finally Yi et al", "2008 and Her met et al", "2008 use ngram counts from the web as a language model approximation to identify likely errors and correction candidates"]}, "N10-1040": {"title": ["Improving  PhraseBased Translation with Prototypes of Short Phrases"], "abstract": ["We investigate methods of generating additional bilingual phrase pairs for a phrase based decoder by translating short sequences of source text", "Because our translation task is more constrained we can use a model that employs more linguistically rich features than a traditional decoder", "We have implemented an example of this approach", "Experimental results suggest that the phrase pairs produced by our method are useful to the decoder and lead to improved sentence translations"], "inroduction": ["Recently there have been a number of successful attempts at improving phrasebased statistical machine translation by exploiting linguistic knowledge such as morphology partofspeech tags and syntax", "Many translation models use such knowledge before decoding Xia and McCord 2004 and during decoding Birch et al 2007 Gimpel and Smith 2009 Koehn and Hoang 2007 Chiang et al 2009 but they are limited to simpler features for practical reasons often restricted to conditioning leftto right on the target sentence", "Traditionally nbest rerankers Shen et al 2004 have applied expensive analysis after the translation process on both the source and target side though they suffer from being limited to whatever is on the nbest list Hasan et al 2007", "We argue that it can be desirable to pretranslate parts of the source text before sentencelevel decoding begins using a richer model that would typically be out of reach during sentencelevel decoding", "In this paper we describe a particular method of generating additional bilingual phrase pairs for a new source text using what we call phrase prototypes which are are learned from bilingual training data", "Our goal is to generate improved translations of relatively short phrase pairs to provide the SMT decoder with better phrasal choices", "We validate the idea through experiments on ArabicEnglish translation", "Our method produces a 13 BLEU score increase 33 relative on a test set"]}, "N12-1006": {"title": ["Machine Translation of Arabic Dialects"], "abstract": ["Arabic Dialects present many challenges for machine translation not least of which is the lack of data resources", "We use crowdsourcing to cheaply and quickly build LevantineEnglish and EgyptianEnglish parallel corpora consisting of 11M words and 380k words respectively", "The dialectal sentences are selected from a large corpus of Arabic web text and translated using Amazons Mechanical Turk", "We use this data to build Dialectal Arabic MT systems and find that small amounts of dialectal data have a dramatic impact on translation quality", "When translating Egyptian and Levantine test sets our Dialectal Arabic MT system performs 63 and 70 BLEU points higher than a Modern Standard Arabic MT system trained on a 150Mword ArabicEnglish parallel corpus"], "inroduction": ["The Arabic language is a wellknown example of diglossia Ferguson 1959 where the formal variety of the language which is taught in schools and used in written communication and formal speech religion politics etc differs significantly in its grammatical properties from the informal varieties that are acquired natively which are used mostly for verbal communication", "The spoken varieties of the Arabic language which we refer to collectively as Dialectal Arabic differ widely among themselves depending on the geographic distribution and the socioeconomic conditions of the speakers and they diverge from the formal variety known as Modern Standard Arabic MSA Embarki and Ennaji 2011", "Significant differences in the phonology morphology lexicon and even syntax render some of these varieties mutually incomprehensible", "The use of Dialectal Arabic has traditionally been confined to informal personal speech while writ ing has been done almost exclusively using MSA or its ancestor Classical Arabic", "This situation is quickly changing however with the rapid proliferation of social media in the Arabicspeaking part of the world where much of the communication is composed in dialect", "The focus of the Arabic NLP research community which has been mostly on MSA is turning towards dealing with informal communication with the introduction of the DARPA BOLT program", "This new focus presents new challenges the most obvious of which is the lack of dialectal linguistic resources", "Dialectal text which is usually user generated is also noisy and the lack of standardized orthography means that users often improvise spelling", "Dialectal data also includes a wider range of topics than formal data genres such as newswire due to its informal nature", "These challenges require innovative solutions if NLP applications are to deal with Dialectal Arabic effectively", "In this paper  We describe a process for cheaply and quickly developing parallel corpora for LevantineEnglish and EgyptianEnglish using Amazons Mechanical Turk crowdsourcing service 3", " We use the data to perform a variety of machine translation experiments showing the impact of morphological analysis the limited value of adding MSA parallel data the usefulness of crossdialect training and the effects of translating from dialect to MSA to English 4", "We find that collecting dialect translations has a low cost 003word and that relatively small amounts of data has a dramatic impact on translation quality", "When trained on 15M words of dialectal data our system performs 63 to 70 BLEU points higher than when it is trained on 100 times more MSA data from a mismatching domain", "49 2012 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies pages 4959", "Montreal Canada June 38 2012", "Qc 2012 Association for Computational Linguistics"]}, "N12-1045": {"title": ["2012 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies pages 407416 Montreal Canada June 38 2012 c2012 Association for Computational Linguistics A Hierarchical Dirichlet Process Model for Joint PartofSpeech and Morphology Induction Kairit Sirts Institute of Cybernetics at Tallinn University of Technology kairitsirtsphoniocee Tanel Aluma e Institute of Cybernetics at Tallinn University of Technology tanelalumaephoniocee Abstract In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations The model is essentially an infinite HMM that infers the number of states from data Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided The evaluation on multilingual data shows that the model produces stateoftheart results on POS induction 1 Introduction Nonparametric Bayesian modeling has recently be come very popular in natural language processing NLP mostly because of its ability to provide priors that are especially suitable for tasks in NLP Teh 2006 Using nonparametric priors enables to treat the size of the model as a random variable with its value to be induced during inference which makes its use very appealing in models that need to decide upon the number of states The task of unsupervised parts ofspeech POS tagging has been under research in numerous papers for overview see Christodoulopoulos et al 2010 Most of the POS induction models use the structure of hidden Markov model HMM Rabiner 1989 that requires the knowledge about the number of hidden states corresponding to the number of tags in advance According to our considerations supplying this information is not desirable for two opposing reasons 1 it injects into the system a piece of knowledge which in a truly unsupervised setting would be unavailable and 2 the number of POS tags used is somewhat arbitrary anyway because there is no common consensus of what should be the true number of tags in each language and therefore it seems unreasonable to constrain the model with such a number instead of learning it from the data Unsupervised morphology learning is another popular task that has been extensively studied by many authors Here we are interested in learning concatenative morphology of words meaning the substrings of the word corresponding to morphemes that when concatenated will give the lexical representation of the word type For the rest of the paper we will refer to this task as morphological segmentation Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn 2011 Lee et al 2010 BergKirkpatrick et al 2010 Clark 2003 Christodoulopoulos et al 2011 and this approach has been empirically proved to be helpful Christodoulopoulos et al 2010 In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem We will show that the unsupervised learning of both of these tasks in the same 407 model will lead to better results than learning both tasks separately with the gold standard data of the other task provided We will also demonstrate that our model produces stateoftheart results on POS tagging As opposed to the compared methods our model also induces the number of tags from data In the following section 2 gives the overview of the Dirichlet Processes section 3 describes the model setup followed by the description of inference procedures in section 4 experimental results are presented in section 5 section 6 summarizes the previous work and last section concludes the paper 2 Background"], "abstract": ["In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations", "The model is essentially an infinite HMM that infers the number of states from data", "Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step", "We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided", "The evaluation on multilingual data shows that the model produces stateoftheart results on POS induction"], "inroduction": ["Nonparametric Bayesian modeling has recently become very popular in natural language processing NLP mostly because of its ability to provide priors that are especially suitable for tasks in NLP Teh 2006", "Using nonparametric priors enables to treat the size of the model as a random variable with its value to be induced during inference which makes its use very appealing in models that need to decide upon the number of states", "The task of unsupervised partsofspeech POS tagging has been under research in numerous papers for overview see Christodoulopoulos et al 2010", "Most of the POS induction models use the structure of hidden Markov model HMM Rabiner 1989 that requires the knowledge about the number of hidden states corresponding to the number of tags in advance", "According to our considerations supplying this information is not desirable for two opposing reasons 1 it injects into the system a piece of knowledge which in a truly unsupervised setting would be unavailable and 2 the number of POS tags used is somewhat arbitrary anyway because there is no common consensus of what should be the true number of tags in each language and therefore it seems unreasonable to constrain the model with such a number instead of learning it from the data", "Unsupervised morphology learning is another popular task that has been extensively studied by many authors", "Here we are interested in learning concatenative morphology of words meaning the substrings of the word corresponding to morphemes that when concatenated will give the lexical representation of the word type", "For the rest of the paper we will refer to this task as morphological segmentation", "Several unsupervised POS induction systems make use of morphological features Blunsom and Cohn 2011 Lee et al 2010 BergKirkpatrick et al 2010 Clark 2003 Christodoulopoulos et al 2011 and this approach has been empirically proved to be helpful Christodoulopoulos et al 2010", "In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis", "In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem", "We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies pages 407416", "Montreal Canada June 38 2012", "Qc 2012 Association for Computational Linguistics model will lead to better results than learning both tasks separately with the gold standard data of the other task provided", "We will also demonstrate that be served on that table", "The predictive probability distribution over dishes for the ith customer is nk   our model produces stateoftheart results on POS tagging", "As opposed to the compared methods our P xi  k xi  H    1   pH k  2 model also induces the number of tags from data", "In the following section 2 gives the overview of the Dirichlet Processes section 3 describes the model setup followed by the description of inference procedures in section 4 experimental results are presented in section 5 section 6 summarizes the previous work and last section concludes the paper"]}, "N13-1093": {"title": ["Exploiting the Scope of Negations and Heterogeneous Features"], "abstract": ["This paper presents an approach that exploits the scope of negation cues for relation extraction RE without the need of using any specifically annotated dataset for building a separate negation scope detection classifier", "New features are proposed which are used in two different stages", "These also include nontarget entity specific features", "The proposed RE approach outperforms the previous state of the art for drugdrug interaction DDI extraction"], "inroduction": ["Negation is a linguistic phenomenon where a negation cue eg not can alter the meaning of a particular text segment or of a fact", "This text segment or fact is said to be inside the scope of that negation cue", "In the context of RE there is not much work that aims to exploit the scope of negations1 The only work on RE that we are aware of is SanchezGraillet and Poesio 2007 where they used various heuristics to extract negative protein interaction", "Despite the recent interest on automatically detecting the scope of negation2 till now there seems to be no empirical evidence supporting its exploitation for the purpose of RE", "Even if we could manage to obtain highly accurate automatically detected 1 In the context of event extraction a closely related task of", "RE there have been efforts in BioNLP shared tasks of 2009 and 2011 for nonmandatory subtask of event negation detection 3 participants in 2009 2 in 2011 Kim et al 2009 Kim et al 2011", "The participants approached the subtask using either predefined patterns or some heuristics"]}, "N13-1095": {"title": ["Distant Supervision for Relation Extraction with an Incomplete Knowledge Base"], "abstract": ["Distant supervision heuristically labeling a corpus using a knowledge base has emerged as a popular choice for training relation extractors", "In this paper we show that a significant number of negative examples generated by the labeling process are false negatives because the knowledge base is incomplete", "Therefore the heuristic for generating negative examples has a serious flaw", "Building on a stateoftheart distantlysupervised extraction algorithm we proposed an algorithm that learns from only positive and unlabeled labels at the pairofentity level", "Experimental results demonstrate its advantage over existing algorithms"], "inroduction": ["Relation Extraction is a wellstudied problem Miller et al 2000 Zhou et al 2005 Kambhatla 2004 Min et al 2012a", "Recently Distant Supervision DS Craven and Kumlien 1999 Mintz et al 2009 has emerged to be a popular choice for training relation extractors without using manually labeled data", "It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base KB", "This method significantly reduces human efforts for relation extraction", "The labeling heuristic has a serious flaw", "Knowledge bases are usually highly incomplete", "For exam 1 An occurrence of a pair of entities with the source sentence", "ple 938 of persons from Freebase2 have no place of birth and 785 have no nationality section 3", "Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation all the corresponding relation mentions are considered negative examples3 This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation", "The number of such false negative matches even exceeds the number of positive pairs by 3 to 10 times leading to a significant problem for training", "Previous approaches Riedel et al 2010 Hoffmann et al 2011 Surdeanu et al 2012 bypassed this problem by heavily undersampling the negative class", "We instead deal with a learning scenario where we only have entitypair level labels that are either positive or unlabeled", "We proposed an extension to Surdeanu et al", "2012 that can train on this dataset", "Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS", "Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions"]}, "N13-1104": {"title": ["Proceedings of NAACLHLT 2013 pages 837846"], "abstract": ["In naturallanguage discourse related events tend to appear near each other to describe a larger scenario", "Such structures can be formalized by the notion of a frame aka template which comprises a set of related events and prototypical participants and event transitions", "Identifying frames is a prerequisite for information extraction and natural language generation and is usually done manually", "Methods for inducing frames have been proposed recently but they typically use ad hoc procedures and are difficult to diagnose or extend", "In this paper we propose the first probabilistic approach to frame induction which incorporates frames events and participants as latent topics and learns those frame and event transitions that best explain the text", "The number of frame components is inferred by a novel application of a splitmerge method from syntactic parsing", "In endtoend evaluations from text to induced frames and extracted facts our method produces stateoftheart results while substantially reducing engineering effort"], "inroduction": ["Events with causal or temporal relations tend to occur near each other in text", "For example a BOMBING scenario in an article on terrorism might begin with a DETONATION event in which terrorists set off a bomb", "Then a DAMAGE event might ensue to describe the resulting destruction and any casualties followed by an INVESTIGATION event This research was undertaken during the authors internship at Microsoft Research", "covering subsequent police investigations", "Afterwards the BOMBING scenario may transition into a CRIMINALPROCESSING scenario which begins with police catching the terrorists and proceeds to a trial sentencing etc A common set of participants serves as the event arguments eg the agent or subject of DETONATION is often the same as the theme or object of INVESTIGATION and corresponds to a PERPETRATOR", "Such structures can be formally captured by the notion of a frame aka template scenario which consists of a set of events with prototypical transitions as well as a set of slots representing the common participants", "Identifying frames is an explicit or implicit prerequisite for many NLP tasks", "Information extraction for example stipulates the types of events and slots that are extracted for a frame or template", "Online applications such as dialogue systems and personalassistant applications also model users goals and subgoals using framelike representations", "In naturallanguage generation frames are often used to represent contents to be expressed as well as to support surface realization", "Until recently frames and related representations have been manually constructed which has limited their applicability to a relatively small number of domains and a few slots within a domain", "Furthermore additional manual effort is needed after the frames are defined in order to extract frame components from text eg in annotating examples and designing features to train a supervised learning model", "This paradigm makes generalizing across tasks difficult and might suffer from annotator biasRecently there has been increasing interest in au 837 Proceedings of NAACLHLT 2013 pages 837846 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics tomatically inducing frames from text", "A notable example is Chambers and Jurafsky 2011 which first clusters related verbs to form frames and then clusters the verbs syntactic arguments to identify slots", "While Chambers and Jurafsky 2011 represents a major step forward in frame induction it is also limited in several aspects", "The clustering used ad hoc steps and customized similarity metrics as well as an additional retrieval step from a large external text corpus for slot generation", "This makes it hard to replicate their approach or adapt it to new domains", "Lacking a coherent model it is also difficult to incorporate additional linguistic insights and prior knowledge", "In this paper we present PROFINDER PRObabilistic Frame INDucER the first probabilistic approach to frame induction", "PROFINDER defines a joint distribution over the words in a document and their frame assignments by modeling frame and event transitions correlations among events and slots and their surface realizations", "Given a set of documents PROFINDER outputs a set of induced frames with learned parameters as well as the most probable frame assignments that can be used for event and entity extraction", "The numbers of events and slots are dynamically determined by a novel application of the splitmerge approach from syntactic parsing Petrov et al 2006", "In endtoend evaluations from text to entity extraction using standard MUC and TAC datasets PROFINDER achieved stateoftheart results while significantly reducing engineering effort and requiring no external data"]}, "N13-1110": {"title": ["Proceedings of NAACLHLT 2013 pages 897906"], "abstract": ["Coreference resolution systems rely heavily on string overlap eg Google Inc and Google performing badly on mentions with very different words opaque mentions like Google and the search giant", "Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall", "We present a new unsupervised method for mining opaque pairs", "Our intuition is to restrict distributional semantics to articles about the same event thus promoting referential match", "Using an English comparable corpus of tech news we built a dictionary of opaque coreferent mentions only 3 are in WordNet", "Our dictionary can be integrated into any coreference system it increases the performance of a stateoftheart system by 1 F1 on all measures and is easily extendable by using news aggregators"], "inroduction": ["Repetition is one of the most common coreferential devices in written text making stringmatch features important to all coreference resolution systems", "In fact the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of stateoftheart systems Recasens and Hovy 2010", "This suggests that opaque mentions ie lexically different such as iPad and the Cupertino slate are a serious problem for modern systems they comprise 65 of the nonpronominal 1 Closest NP with the same gender and number", "errors made by the Stanford system on the CoNLL 2011 data", "Solving this problem is critical for overcoming the recall gap of stateoftheart systems Haghighi and Klein 2010 Stoyanov et al 2009", "Previous systems have turned either to ontologies Ponzetto and Strube 2006 Uryupina et al 2011 Rahman and Ng 2011 or distributional semantics Yang and Su 2007 Kobdani et al 2011 Bansal and Klein 2012 to help solve these errors", "But neither semantic similarity nor hypernymy are the same as coreference Microsoft and Google are distributionally similar but not coreferent people is a hypernym of both voters and scientists but the people can corefer with the voters but is less likely to corefer with the scientists", "Thus ontologies lead to precision problems and to recall problems like missing NE descriptions eg Apple and the iPhone maker and metonymies eg agreement and wording while distributional systems lead to precision problems like coreferring Microsoft and the Mountain View giant because of their similar vector representation release software update", "We increase precision by drawing on the intuition that referents that are both similar and participate in the same event are likely to corefer", "We restrict dis tributional similarity to collections of articles that discuss the same event", "In the following two documents on the Nexus One from different sources we take the subjects of the identical verb release Google and the Mountain View giantas coreferent", "Document 1 Google has released a software update", "Document 2 The Mountain View giant released an update", "Based on this idea we introduce a new unsupervised method that uses verbs in comparable corpora 897 Proceedings of NAACLHLT 2013 pages 897906 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics as pivots for extracting the hard cases of coreference resolution and build a dictionary of opaque coreferent mentions ie the dictionary entries are pairs of mentions", "This dictionary is then integrated into the Stanford coreference system Lee et al 2011 resulting in an average 1 improvement in the F1 score of all the evaluation measures", "Our work points out the importance of context to decide whether a specific mention pair is coreferent", "On the one hand we need to know what semantic relations are potentially coreferent eg content and video", "On the other we need to distinguish contexts that are compatible for coreference1 and 2a from those that are not1 and 2b", "1 Elemental helps those big media entities process content across a full slate of mobile devices", "2 a Elemental provides the picks and shovels to b make video work across multiple devices", "Elemental is powering the video for HBO Go", "Our dictionary of opaque coreferent pairs is our solution to the first problem and we report on some preliminary work on context compatibility to address the second problem"]}, "N13-1118": {"title": ["Unsupervised Metaphor  Identification Using Hierarchical  Graph"], "abstract": ["We present a novel approach to automatic metaphor identification that discovers both metaphorical associations and metaphorical expressions in unrestricted text", "Our system first performs hierarchical graph factorization clustering HGFC of nouns and then searches the resulting graph for metaphorical connections between concepts", "It then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions", "In contrast to previous work our method is fully unsupervised", "Despite this fact it operates with an encouraging precision 069 and recall 061", "Our approach is also the first one in NLP to exploit the cognitive findings on the differences in or ganisation of abstract and concrete concepts in the human brain"], "inroduction": ["Metaphor has traditionally been viewed as a form of linguistic creativity that gives our expression more vividness distinction and artistism", "While this is true on the surface the mechanisms of metaphor have a much deeper origin in our reasoning", "Today metaphor is widely understood as a cognitive phenomenon operating at the level of mental processes whereby one concept or domain is systematically viewed in terms of the properties of another Lakoff and Johnson 1980", "Consider the examples 1 He shot down all of my arguments and 2 He attacked every weak point in my argument", "They demonstrate a metaphorical mapping of the concept of argument to that of war", "The argument which is the target concept is viewed in terms of a battle or a war the source concept", "The existence of such a link allows us to systematically describe arguments using the war terminology thus leading to a number of metaphorical expressions", "Lakoff and Johnson call such generalisations a sourcetarget domain mapping or conceptual metaphor", "The ubiquity of metaphor in language has been established in a number of corpus studies Cameron 2003 Martin 2006 Steen et al 2010 Shutova and Teufel 2010 and the role it plays in human reasoning has been confirmed in psychological experiments Thibodeau and Boroditsky 2011", "This makes metaphor an important research area for computational and cognitive linguistics and its automatic processing indispensable for any semantics oriented NLP application", "The problem of metaphor modeling is gaining interest within NLP with a growing number of approaches exploiting statistical techniques Mason 2004 Gedigian et al 2006 Shutova 2010 Shutova et al 2010 Turney et al 2011 Shutova et al 2012", "Compared to more traditional approaches based on handcoded knowledge Fass 1991 Martin 1990 Narayanan 1997 Narayanan 1999 Feldman and Narayanan 2004 Barnden and Lee 2002 Agerri et al 2007 these more recent methods tend to have a wider coverage as well as be more efficient accurate and robust", "However even the statistical metaphor processing approaches so far often focused on a limited domain or a subset of phenomena Gedigian et al 2006 Krishnakumaran and Zhu 2007 and only addressed one of the metaphor processing sub tasks identification of metaphorical mappings Mason 2004 or identification of metaphorical expressions Shutova et al 2010 Turney et al 2011", "In this paper we present the first computational method 978 Proceedings of NAACLHLT 2013 pages 978988 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics that identifies the generalisations that govern the production of metaphorical expressions ie conceptual metaphors and then uses these generalisations to identify metaphorical expressions in unrestricted text", "As opposed to previous works on statistical metaphor processing that were supervised or semisupervised and thus required training data our method is fully unsupervised", "It relies on building a hierarchical graph of concepts connected by their association strength using hierarchical clustering and then searching for metaphorical links in this graph", "Shutova et al", "2010 introduced the hypothesis of clustering by association and claimed that in the course of distributional noun clustering abstract concepts tend to cluster together if they are associated with the same source domain while concrete concepts cluster by meaning similarity", "We share this intuition but take this idea a significant step further", "Our approach is theoretically grounded in the cognitive science findings suggesting that abstract and concrete concepts are organised differently in the human brain Crutch and Warrington 2005 Binder et al 2005 WiemerHastings and Xu 2005 Huang et al 2010 Crutch and Warring ton 2010 Adorni and Proverbio 2012", "According to Crutch and Warrington 2005 these differences emerge from their general patterns of relation with other concepts", "However most NLP systems to date treat abstract and concrete concepts as identical", "In contrast we incorporate this distinction into our model by creating a network or a graph of concepts and automatically learning the different patterns of association of abstract and concrete concepts with other concepts", "We expect that while concrete concepts would tend to naturally organise into a treelike structure with more specific terms descending from the more general terms abstract concepts would exhibit a more complex pattern of associations", "Consider the example in Figure 1", "The figure schematically shows a small portion of the graph describing the concepts of mechanism concrete political system and relationship abstract at two levels of generality", "One can see from this graph that if concrete concepts such as bike or engine tend to be connected to only one concept at the higher level in the hierarchy mechanism abstract concepts may have multiple higherlevel associates the literal ones and the metaphorical ones", "For ex ample the abstract concept of democracy is literally associated with a more general concept of political system as well as metaphorically associated with the concept of mechanism", "Such multiple associations are due to the fact that political systems are metaphorically viewed as mechanisms they can function break they can be oiled etc We often discuss them using mechanism terminology and thus a corpusbased distributional learning approach would learn that they share features with political systems from their literal uses as well as with mechanisms from their metaphorical uses as shown next to the respective graph edges in the figure", "Our system discovers such association patterns within the graph and uses them to identify metaphorical connections between the concepts", "To the best of our knowledge our method is the first one to use a hierarchical clustering model for the metaphor processing task", "The original graph of concepts is built using hierarchical graph factorization clustering HGFC Yu et al 2006 of nouns yielding a network of clusters with different levels of generality", "The weights on the edges of the graph indicate association between the clusters concepts", "HGFC has not been previously employed for noun clustering in NLP but showed successful results in the verb clustering task Sun and Korhonen 2011", "In summary our system 1 builds a graph of concepts using HGFC 2 traverses it to find metaphorical associations between clusters using weights on the edges of the graph 3 generates lists of salient features for the metaphorically connected clusters and 4 searches the British National Corpus BNC Burnard 2007 for metaphorical expressions describing the target domain concepts using the verbs from the set of salient features", "We evaluated the performance of the system with the aid of human judges in precision and recalloriented settings", "In addition we compared its performance to that of two baselines an unsupervised baseline using agglomerative clustering AGG and a supervised baseline built upon WordNet Fellbaum 1998 WN"]}, "N13-1140": {"title": ["Proceedings of NAACL HLT 2013 pages 12061215 Atlanta Georgia 914 June 2013 c  2013 Association for Computational Linguistics KnowledgeRich Morphological Priors for Bayesian Language Models Victor Chahuneau Noah A Smith Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh PA 15213 USA vchahunenasmithcdyercscmuedu Abstract We present a morphologyaware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages This relaxes the word independence assumption and enables sharing of statistical strength across for example stems or inflectional paradigms in different contexts Our model can be used in virtually any scenario where multinomial distributions over words would be used We obtain stateoftheart results in language modeling word alignment and unsupervised morphological disambiguation for a variety of morphologically rich languages 1 Introduction Despite morphological phenomena s salience in most human languages many NLP systems treat fully inflected forms as the atomic units of language By assuming independence of lexical stems various surface forms this avoidance approach exacerbates the problem of data sparseness If it is employed at all morphological analysis of text tends to be treated as a preprocessing step to other NLP modules While this latter disambiguation approach helps address data sparsity concerns it has substantial drawbacks it requires supervised learning from expertannotated corpora and determining the optimal morphological granularity is laborintensive Habash and Sadat 2006 Neither approach fully exploits the finitestate transducer FST technology that has been so successful for modeling the mapping between surface forms and their morphological analyses Karttunen and Beesley 2005 and the mature collections of high quality transducers that already exist for many languages eg Turkish Russian Arabic Much linguistic knowledge is encoded in such FSTs In this paper we develop morphologyaware nonparametric Bayesian language models that bring together handwritten FSTs with statistical modeling and require no tokenlevel annotation The sparsity issue discussed above is addressed by hierarchical priors that share statistical strength across different inflections of the same stem by backing off to word formation models that piece together morphemes using FSTs Furthermore because of the nonparametric formulation of our models the regular morphological patterns found in the long tail of word types will rely more heavily on deeper analysis while frequent and idiosyncratically behaved forms are modeled opaquely Our prior can be used in virtually any generative model of language as a replacement for multinomial distributions over words bringing morphological awareness to numerous applications For various morphologically rich languages we show that  our model can provide rudimentary unsupervised disambiguation for a highly ambiguous analyzer  integrating morphology into ngram language models allows better generalization to unseen words and can improve the performance of applications that are truly open vocabulary and  bilingual word alignment models also benefit greatly from sharing translation information 1206 across stems We are particularly interested in lowresource scenarios where one has to make the most of the small quantity of available data and overcoming data sparseness is crucial If analyzers exist in such settings they tend to be highly ambiguous and an notated data for learning to disambiguate are also likely to be scarce or nonexistent Therefore in our experiments with Russian we compare two analyzers a rapidlydeveloped guesser which models regular inflectional paradigms but contains no lexicon or irregular forms and a highquality analyzer 2 Word Models with Morphology In this section we describe a generative model of word formation based on PitmanYor processes that generates word types using a finitestate morphological generator At a high level the process first produces lexicons of stems and inflectional patterns then it generates a lexicon of inflected forms using the finitestate generator Finally the inflected forms are used to generate observed data Different independence assumptions can be made at each of these levels to encode beliefs about where stems inflections and surface forms should share statistical strength"], "abstract": ["We present a morphologyaware nonparametric Bayesian model of language whose prior distribution uses manually constructed finite state transducers to capture the word formation processes of particular languages", "This relaxes the word independence assumption and enables sharing of statistical strength across for example stems or inflectional paradigms in different contexts", "Our model can be used in virtually any scenario where multinomial distributions over words would be used", "We obtain stateoftheart results in language modeling word alignment and unsupervised morphological disambiguation for a variety of morphologically rich languages"], "inroduction": ["Despite morphological phenomenas salience in most human languages many NLP systems treat fully inflected forms as the atomic units of language", "By assuming independence of lexical stems various surface forms this avoidance approach exacerbates the problem of data sparseness", "If it is employed at all morphological analysis of text tends to be treated as a preprocessing step to other NLP modules", "While this latter disambiguation approach helps address data sparsity concerns it has substantial drawbacks it requires supervised learning from expertannotated corpora and determining the optimal morphological granularity is laborintensive Habash and Sadat 2006", "Neither approach fully exploits the finitestate transducer FST technology that has been so successful for modeling the mapping between surface forms and their morphological analyses Karttunen and Beesley 2005 and the mature collections of high quality transducers that already exist for many languages eg Turkish Russian Arabic", "Much linguistic knowledge is encoded in such FSTs", "In this paper we develop morphologyaware non parametric Bayesian language models that bring together handwritten FSTs with statistical modeling and require no tokenlevel annotation", "The sparsity issue discussed above is addressed by hierarchical priors that share statistical strength across different inflections of the same stem by backing off to word formation models that piece together morphemes using FSTs", "Furthermore because of the nonparametric formulation of our models the regular morphological patterns found in the long tail of word types will rely more heavily on deeper analysis while frequent and idiosyncratically behaved forms are modeled opaquely", "Our prior can be used in virtually any generative model of language as a replacement for multinomial distributions over words bringing morphological awareness to numerous applications", "For various morphologically rich languages we show that our model can provide rudimentary unsuper vised disambiguation for a highly ambiguous analyzer  integrating morphology into ngram language models allows better generalization to unseen words and can improve the performance of applications that are truly open vocabulary and  bilingual word alignment models also benefit greatly from sharing translation information 1206 Proceedings of NAACLHLT 2013 pages 12061215 Atlanta Georgia 914 June 2013", "Qc 2013 Association for Computational Linguistics across stems", "We are particularly interested in lowresource scenarios where one has to make the most of the small quantity of available data and overcoming data sparseness is crucial", "If analyzers exist in such settings they tend to be highly ambiguous and annotated data for learning to disambiguate are also likely to be scarce or nonexistent", "Therefore in our experiments with Russian we compare two analyzers a rapidlydeveloped guesser which models regular inflectional paradigms but contains no lexicon or irregular forms and a highquality analyzer"]}, "N15-1071": {"title": ["Subsentential Sentiment on a Shoestring"], "abstract": ["Sentiment analysis has undergone a shift from documentlevel analysis where labels expresses the sentiment of a whole document or whole sentence to subsentential approaches which assess the contribution of individual phrases in particular including the composition of sentiment terms and phrases such as negators and intensifiers", "Starting from a small sentiment treebank modeled after the Stanford Sentiment Treebank of Socher et al", "2013 we investigate suitable methods to perform compositional sentiment classification for German in a datascarce setting harnessing crosslingual methods as well as existing generaldomain lexical resources"], "inroduction": ["In sentiment classification we find a general tendency from documentlevel classification towards more finegrained approaches that yield a more detailed appraisal of the judgement performed in the text  in particular using composition over syntactic structure to get a more detailed approach over phrases", "For English movie reviews work using the Stanford Sentiment Treebank SSTb has shown that such subsentential sentiment information can yield approaches with both very high accuracy Socher et al 2013 Dong et al 2014 Hall et al 2014 and precise information about the role of each phrase  information which can subsequently used for extracting or summarizing the sentiment expressed in the text", "The effort for creating a sentiment treebank such as the SSTb however seems prohibitive if we wanted to create such a resource for each pair of relevant domain and language Compared to documentlevel annotations for sentiment which are easy to come by eg star ratings annotating individual syntactic phrases requires considerable effort", "The main focus of this paper is the question if and how it is possible to reach sensible performance for compositional sentiment classification when we only have limited resources to spend on an inlanguage indomain sentiment treebank", "For this goal we use a new resource the Heidelberg Sentiment Treebank HeiST which is a Germanlanguage counterpart to the Stanford Sentiment Treebank in the sense that it makes explicit the composition of sentiment expression over syntactic phrases", "Our experiments on HeiST provide a direct comparison of different techniques for harnessing crosslingual crossdomain or crosstask information and are the first of this kind to specifically target compositional sentiment analysis", "Figure 1 next page shows a schematic overview of the experiments beyond supervised baseline experiments using SVM classification and a supervised RNTN model section 3 we evaluated cross lingual projection section 4 lexiconbased approaches section 5 as well as semisupervised approaches based on word clusters section 6"]}, "P01-1005": {"title": ["Scaling to Very Very Large Corpora for"], "abstract": ["The amount of readily available online text has reached hundreds of billions of words and continues to grow", "Yet for most core natural language tasks algorithms continue to be optimized tested and compared after training on corpora consisting of only one million words or less", "In this paper we evaluate the performance of different learning methods on a prototypical natural language disambiguation task confusion set disambiguation when trained on orders of magnitude more labeled data than has previously been used", "We are fortunate that for this particular application correctly labeled training data is free", "Since this will often not be the case we examine methods for effectively exploiting very large corpora when labeled data comes at a cost"], "inroduction": ["Machine learning techniques which automatic ally learn linguistic information from online text corpora have been applied to a number of natural language problems throughout the last decade", "A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora", "While the amount of available online text has been increasing at a dramatic rate the size of training corpora typically used for learning has not", "In part this is due to the standardization of data sets used within the field as well as the potentially large cost of annotating data for those learning methods that rely on labeled text", "The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed and relatively small data sets", "Yet since we now have access to significantly more data one has to wonder what conclusions that have been drawn on small data sets may carry over when these learning methods are trained using much larger corpora", "In this paper we present a study of the effects of data size on machine learning for natural language disambiguation", "In particular we study the problem of selection among confusable words using orders of magnitude more training data than has ever been applied to this problem", "First we show learning curves for four different machine learning algorithms", "Next we consider the efficacy of voting sample selection and partially unsupervised learning with large training corpora in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost"]}, "P03-1028": {"title": ["Closing the Gap LearningBased Information Extraction Rivaling"], "abstract": ["In this paper we present a learning approach to the scenario template task of information extraction where information filling one template could come from multiple sentences", "When tested on the MUC4 task our learning approach achieves accuracy competitive to the best of the MUC4 systems which were all built with manually engineered rules", "Our analysis reveals that our use of full parsing and stateoftheart learning algorithms have contributed to the good performance", "To our knowledge this is the first research to have demonstrated that a learning approach to the fullscale information extraction task could achieve performance rivaling that of the knowledge engineering approach"], "inroduction": ["The explosive growth of online texts written in natural language has prompted much research into information extraction IE the task of automatically extracting specific information items of interest from natural language texts", "The extracted information is used to fill database records also known as templates in the IE literature", "Research efforts on IE tackle a variety of tasks", "They include extracting information from semi structured texts such as seminar announcements rental and job advertisements etc as well as from free texts such as newspaper articles Soderland 1999", "IE from semistructured texts is easier than from free texts since the layout and format of a semistructured text provide additional useful clues AYACUCHO 19 JAN 89  TODAY TWO PEOPLE WERE WOUNDED WHEN A BOMB EXPLODED IN SAN JUAN BAUTISTA MUNICIPALITY", "OFFICIALS SAID THAT SHINING PATH MEMBERS WERE RESPONSIBLE FOR THE ATTACK ", "POLICE SOURCES STATED THAT THE BOMB ATTACK INVOLVING THE SHINING PATH CAUSED SERIOUS DAMAGES ", "Figure 1 Snippet of a MUC4 document to aid in extraction", "Several benchmark data sets have been used to evaluate IE approaches on semi structured texts Soderland 1999 Ciravegna 2001 Chieu and Ng 2002a", "For the task of extracting information from free texts a series of Message Understanding Conferences MUC provided benchmark data sets for evaluation", "Several subtasks for IE from free texts have been identified", "The named entity NE task extracts person names organization names location names etc The template element TE task extracts information centered around an entity like the acronym category and location of a company", "The template relation TR task extracts relations between entities", "Finally the fullscale IE task the scenario template ST task deals with extracting generic information items from free texts", "To tackle the full ST task an IE system needs to merge information from multiple sentences in general since the information needed to fill one template can come from multiple sentences and thus discourse processing is needed", "The fullscale ST task is considerably harder than all the other IE tasks or subtasks outlined above", "As is the case with many other natural language processing NLP tasks there are two main approaches to IE namely the knowledgeengineering approach and the learning approach", "Most early IE systems adopted the knowledgeengineering ap 0 MESSAGE ID TST3MUC40014", "1 MESSAGE TEMPLATE 1"]}, "P03-1039": {"title": ["Chunkbased Statistical Translation Taro Watanabe  Eiichiro Sumita and Hiroshi G Okuno tarowatanabe eiichirosumitaatrcojp  ATR Spoken Language Translation  Department of Intelligence Science Research Laboratories and Technology 222 Hikaridai Keihanna Science City Graduate School of Informatics Kyoto Uniersity Kyoto 6190288 JAPAN Kyoto 6068501 JAPAN Abstract This paper describes an alternative trans lation model based on a text chunk under the framework of statistical machine translation The translation model suggested here first performs chunking Then each word in a chunk is translated Finally translated chunks are reordered Under this scenario of translation modeling we have experimented on a broadcoverage JapaneseEnglish traveling corpus and achieved improved performance 1 Introduction The framework of statistical machine translation formulates the problem of translating a source sentence in a language J into a target language E as the maximization problem of the conditional probability E  argmaxE PEJ The application of the Bayes Rule resulted in  E  argmaxE PEPJE The former term PE is called a language model representing the likelihood of E The latter term PJE is called a translation model representing the generation probability from E into J As an implementation of PJE the word alignment based statistical translation Brown et al 1993 has been successfully applied to similar language pairs such as FrenchEnglish and German English but not to drastically different ones such as Japanese English This failure has been due to the limited representation by word alignment and the weak model structure for handling complicated word correspondence This paper provides a chunk based statistical translation as an alternative to the word alignment based statistical translation The translation process inside the translation model is structured as follows A source sentence is first chunked and then each chunk is translated into target language with local word alignments Next translated chunks are reordered to match the target language constraints Based on this scenario the chunk based statistical translation model is structured with several components and trained by a variation of the EMalgorithm A translation experiment was carried out with a decoder based on the lefttoright beam search It was observed that the translation quality improved from 465 to 521 in BLEU score and from 592 to 651 in subjective evaluation The next section briefly reviews the word alignment based statistical machine translation Brown et al 1993 Section 3 discusses an alternative ap proach a chunkbased translation model ranging from its structure to training procedure and decoding algorithm Then Section 4 provides experimental results on JapanesetoEnglish translation in the traveling domain followed by discussion 2 Word Alignment Based Statistical Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A allowing one tomany generation from each source word Figure 1 illustrates an example of English and Japanese sentences E and J with sample word alignments In this example  show1 has generated two words  mise5 and tekudasai6 E  NULL0 show1 me2 the3 one4 in5 the6 window7 J  uindo1 no2 shinamono3 o4 mise5 tekudasai6 A   7 0 4 0 1 1  Figure 1 Example of word alignment Under this word alignment assumption the translation model PJE can be further decomposed without approximation PJE   A PJ AE"], "abstract": ["This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation", "The translation model suggested here first performs chunking", "Then each word in a chunk is translated", "Finally translated chunks are reordered", "Under this scenario of translation modeling we have experimented on a broad coverage JapaneseEnglish traveling corpus and achieved improved performance"], "inroduction": ["The framework of statistical machine translation formulates the problem of translating a source sentence in a language J into a target language E as the maximization problem of the conditional probability E  argmaxE PEJ", "The application of the BayesRule resulted in E  argmaxE PEPJE", "The for mer term PE is called a language model representing the likelihood of E The latter term PJE is called a translation model representing the generation probability from E into JAs an implementation of PJE the word align ment based statistical translation Brown et al 1993 has been successfully applied to similar language pairs such as FrenchEnglish and German English but not to drastically dierent ones such as JapaneseEnglish", "This failure has been due to the limited representation by word alignment and the weak model structure for handling complicated word correspondence", "This paper provides a chunkbased statistical translation as an alternative to the word alignment based statistical translation", "The translation process inside the translation model is structured as follows", "A source sentence is first chunked and then each chunk is translated into target language with local word alignments", "Next translated chunks are reordered to match the target language constraints", "Based on this scenario the chunkbased statistical translation model is structured with several components and trained by a variation of the EM algorithm", "A translation experiment was carried out with a decoder based on the lefttoright beam search", "It was observed that the translation quality improved from 465 to 521 in BLEU score and from 592 to 651 in subjective evaluation", "The next section briefly reviews the word alignment based statistical machine translation Brown et al 1993", "Section 3 discusses an alternative approach a chunkbased translation model ranging from its structure to training procedure and decoding algorithm", "Then Section 4 provides experimental results on JapanesetoEnglish translation in the traveling domain followed by discussion"]}, "P04-1046": {"title": ["Building Verb Predicates A Computational View"], "abstract": ["A method for the definition of verb predicates is proposed", "The definition of the predicates isessentially tied to a semantic interpretation algorithm that determines the predicate for theverb its semantic roles and adjuncts", "As predicate definitions are complete they can be tested by running the algorithm on some sentences andverifying the resolution of the predicate semantic roles and adjuncts in those sentences", "The predicates are defined semiautomatically with the help of a software environment that uses several sections of a corpus to provide feedback for the definition of the predicates and then forthe subsequent testing and refining of the definitions", "The method is very flexible in adding anew predicate to a list of already defined predicates for a given verb", "The method builds on an existing approach that defines predicates for WordNet verb classes and that plans to definepredicates for every English verb", "The definitions of the predicates and the semantic interpretation algorithm are being used to automatically create a corpus of annotated verb predicates semantic roles and adjuncts"], "inroduction": ["This paper deals with the definition of verbpredicates which will make possible the determination of verb meaning semantic roles adjuncts and attachment and meaning of postverbal PPs", "Hence the adequacy of the defini tions is measured by comparing the output of asemantic interpretation algorithm with the so lution of those semantic interpretation tasks on sentences randomly taken from any corpus or typed by a user at the console", "The algorithmthus must provide immediate feedback by test ing the definitions on these randomly selected sentences", "In Gomez 2001 generic predicates have been defined for WordNet 16 henceforthWN verb classes Fellbaum 1998", "The se mantic roles of the predicates are linked to the selectional restrictions categories in WordNetontology for nouns and the grammatical relations that realize them", "The selectional restric tions of the predicates are solidly grounded on the WordNet ontology for nouns Miller 1998 whose upper level ontology has been modified and rearranged Gomez 2004 based on thefeedback obtained by testing the predicate def initionsHowever we have found out that the ini tial idea in that work of defining predicates for a WN verb class which will be also valid for most of the verbs under that class has proven to be too optimistic because many of the verb forms included under that class realize theirsemantic roles by different selectional restric tions and grammatical relations", "This is due to the fact that many of the verbs under a given class have been grouped in many instances bysome kind of implication or troponymy Fellbaum 1998 rather that by sharing seman tic roles in a hierarchy of predicates", "Even in many cases some of the verbs in the same synset list differ semantically and syntactically between them", "For instance the third sense of ampquotgainampquot in WN is ampquotprofit gain benefit derive benefit fromampquot The theme the thing obtained is syntactically realized by the PP from NPfor two of the verbs listed in that synset ampquotbene fitampquot and ampquotprofitampquot while the theme for ampquotgainampquot is realized by a direct object", "The differences in grammatical relations are even more prevailing within the verbs under one given class", "But in those cases in which the verb polysemy is not high one predicate definition for the verbclass may apply to many of the verb forms un der that class", "Notwithstanding these problems WN verb classes have provided an important basis for the construction of a general ontology of predicates that will cover every English verb", "Moreover if one considers that there are 5752 verbs in WordNet 16 having only one sense and2199 verbs having exactly two senses the pred icates that have been constructed for the verb classes of 7951 verbs are very close to be done", "However the method explained in this paper deviates considerably from the WN approach to constructing verb meaning", "In particular iteschews the synset list in favor of predicate def initions for individual verbs as opposed to a listof synonymous verbs and allows for an easy in tegration of a new predicate into a list of already defined predicates for a given verb", "Briefly the algorithm Gomez 2001 that tests the predicates is as follows", "For every verb in a sentence we provide a list of predicates for that verb", "These predicates can be viewed as contenders for the meaning of the verb", "The goals of the algorithm are to select one predicate from that list thus determining the sense of the verb identify its semantic roles and adjuncts and attach postverbal PPs", "All these tasks aresimultaneously achieved", "For each grammati cal relation CR in the clause starting with the NP complements and for every predicate in the list of predicates the algorithm checks if the predicate explains the CR", "A predicate explainsa CR if there is a semantic role in the predi cate realized by the grammatical relation and the selectional restrictions of the semantic role subsume the ontological category of the head noun of the grammatical relation", "This process is repeated for each CR in the clause and each predicate in the list of predicates for the verb of the clause", "Then the predicate that explains the most CRs is selected as the meaning of the verb", "The semantic roles of the predicate have been identified as a result of this process", "In case of ties the predicate that has the greatest number of semantic roles realized is selected", "Every grammatical relation that has not been mapped to a semantic role must be an adjunct or an NP modifier", "The entries for adjuncts are stored in the root node action or description for stative verbs and are inherited by all predicates in those categories", "Adjuncts are identified after the predicate of the verb has been determined because adjuncts are not part of the argument structure of the predicateIn the next section we will show how to de fine predicates for individual verbs as different from WN verb classes how these predicates for individual verbs can reuse entries in the generic predicates for WordNet verb classes and howthey are integrated into the ontology of pred icates that have been defined with the help ofWN verb classes", "Section 3 provides a discus sion of the semiautomatic construction of thepredicates", "Section 4 gives a view of the upper level ontology of predicates section 5 discusses the testing and sections 6 and 7 related research and conclusions respectively"]}, "P05-1004": {"title": ["Supersense Tagging of Unknown Nouns using Semantic Similarity"], "abstract": [], "inroduction": ["The limited coverage of lexicalsemantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words", "Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET", "Ciaramita and Johnson 2003 present a tagger which uses synonym set glosses as annotated training examples", "We describe an unsupervised approach based on vectorspace similarity which does not require annotated examples but significantly outperforms their tagger", "We also demonstrate the use of an extremely large shallowparsed corpus for calculating vectorspace semantic similarity"]}, "P05-1020": {"title": ["Proceedings of the 43rd Annual Meeting of the ACL pages 157164"], "abstract": ["In this paper we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems", "We propose a set of partitionbased features to learn a ranking model for distinguishing good and bad partitions", "Our approach compares favorably to two stateoftheart coreference systems when evaluated on three standard coreference data sets"], "inroduction": ["Recent research in coreference resolution  the problem of determining which noun phrases NPs in a text or dialogue refer to which realworld entity  has exhibited a shift from knowledge based approaches to datadriven approaches yielding learningbased coreference systems that rival their handcrafted counterparts in performance eg Soon et al", "2001 Ng and Cardie 2002b Strube et al", "2002 Yang et al", "2003 Luo et al", "2004", "The central idea behind the majority of these learning based approaches is to recast coreference resolution as a binary classification task", "Specifically a classifier is first trained to determine whether two NPs in a document are coreferring or not", "A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs with one cluster for each set of coreferent NPs", "Though reasonably successful this standard approach is not as robust as one may think", "First de sign decisions such as the choice of the learning algorithm and the clustering procedure are apparently critical to system performance but are often made in an adhoc and unprincipled manner that may be suboptimal from an empirical point of view", "Second this approach makes no attempt to search through the space of possible partitions when given a set of NPs to be clustered employing instead a greedy clustering procedure to construct a partition that may be far from optimal", "Another potential weakness of this approach concerns its inability to directly optimize for clustering level accuracy the coreference classifier is trained and optimized independently of the clustering procedure to be used and hence improvements in classification accuracy do not guarantee corresponding improvements in clusteringlevel accuracy", "Our goal in this paper is to improve the robustness of the standard approach by addressing the above weaknesses", "Specifically we propose the following procedure for coreference resolution given a set of NPs to be clustered 1 use preselected learning based coreference systems to generate candidate partitions of the NPs and then 2 apply an automatically acquired ranking model to rank these candidate hypotheses selecting the best one to be the final partition", "The key features of this approach are Minimal human decision making", "In contrast to the standard approach our method obviates to a large extent the need to make tough or potentially suboptimal design decisions1 For instance if we 1 We still need to determine the coreference systems to be employed in our framework however", "Fortunately the choice of is flexible and can be as large as we want subject to the 157 Proceedings of the 43rd Annual Meeting of the ACL pages 157164 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics cannot decide whether learner is better to use than learner in a coreference system we can simply create two copies of the system with one employing and the other  and then add both into our pre selected set of coreference systems", "Generation of multiple candidate partitions", "Although an exhaustive search for the best partition is not computationally feasible even for a document with a moderate number of NPs our approach explores a larger portion of the search space than the standard approach via generating multiple hypotheses making it possible to find a potentially better partition of the NPs under consideration", "Optimization for clusteringlevel accuracy via ranking", "As mentioned above the standard approach trains and optimizes a coreference classifier without necessarily optimizing for clusteringlevel accuracy", "In contrast we attempt to optimize our ranking model with respect to the target coreference scoring function essentially by training it in such a way that a higher scored candidate partition according to the scoring function would be assigned a higher rank see Section 32 for details", "Perhaps even more importantly our approach provides a general framework for coreference resolution", "Instead of committing ourselves to a particular resolution method as in previous approaches our framework makes it possible to leverage the strengths of different methods by allowing them to participate in the generation of candidate partitions", "We evaluate our approach on three standard coreference data sets using two different scoring metrics", "In our experiments our approach compares favorably to two stateoftheart coreference systems adopting the standard machine learning approach outperforming them by as much as 47 on the three data sets for one of the performance metrics"]}, "P05-1021": {"title": ["Proceedings of the 43rd Annual Meeting of the ACL pages 165172"], "abstract": ["In this paper we focus on how to improve pronoun resolution using the statistics based semantic compatibility information", "We investigate two unexplored issues that influence the effectiveness of such information statistics source and learning framework", "Specifically we for the first time propose to utilize the web and the twincandidate model in addition to the previous combination of the corpus and the singlecandidate model to compute and apply the semantic information", "Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twincandidate learning model and significantly improve the resolution of neutral pronouns"], "inroduction": ["Semantic compatibility is an important factor for pronoun resolution", "Since pronouns especially neutral pronouns carry little semantics of their own the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphors context based on the statistics that the corresponding predicateargument tuples occur in a particular large corpus", "Consider the example given in the work of Dagan and Itai 1990 1 They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2", "For anaphor it1 the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus", "A similar pattern could also be observed for it2", "So far the corpusbased semantic knowledge has been successfully employed in several anaphora resolution systems", "Dagan and Itai 1990 proposed a heuristicsbased approach to pronoun resolution", "It determined the preference of candidates based on predicateargument frequencies", "Recently Bean and Riloff 2004 presented an unsupervised approach to coreference resolution which mined the coreferring NP pairs with similar predicate arguments from a large corpus using a bootstrapping method", "However the utility of the corpusbased semantics for pronoun resolution is often argued", "Kehler et al", "2004 for example explored the usage of the corpusbased statistics in supervised learning based systems and found that such information did not produce apparent improvement for the overall pronoun resolution", "Indeed existing learningbased approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge eg Mitkov 1998 Soon et al", "2001 Strube and Muller 2003", "Could the relatively noisy semantic knowledge give us further system improvement", "In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information", "We propose to enhance the utility of the statisticsbased knowledge from two aspects Statistics source", "Corpusbased knowledge usually suffers from data sparseness problem", "That is many predicateargument tuples would be unseen even in a large corpus", "A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL pages 165172 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics web", "It is believed that the size of the web is thousands of times larger than normal large corpora and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicateargument bigrams Keller and Lapata 2003", "So far the web has been utilized in nominal anaphora resolution Modjeska et al 2003 Poesio et al 2004 to determine the semantic relation between an anaphor and candidate pair", "However to our knowledge using the web to help pronoun resolution still remains unexplored", "Learning framework", "Commonly the predicate argument statistics is incorporated into anaphora resolution systems as a feature", "What kind of learning framework is suitable for this feature", "Previous approaches to anaphora resolution adopt the single candidate model in which the resolution is done on an anaphor and one candidate at a time Soon et al 2001 Ng and Cardie 2002", "However as the purpose of the predicateargument statistics is to evaluate the preference of the candidates in semantics it is possible that the statisticsbased semantic feature could be more effectively applied in the twin candidate Yang et al 2003 that focusses on the preference relationships among candidates", "In our work we explore the acquisition of the semantic compatibility information from the corpus and the web and the incorporation of such semantic information in the singlecandidate model and the twincandidate model", "We systematically evaluate the combinations of different statistics sources and learning frameworks in terms of their effectiveness in helping the resolution", "Results on the MUC data set show that for neutral pronoun resolution in which an anaphor has no specific semantic category the webbased semantic information would be the most effective when applied in the twincandidate model Not only could such a system significantly improve the baseline without the semantic feature it also outperforms the system with the combination of the corpus and the singlecandidate model by 115 success", "The rest of this paper is organized as follows", "Section 2 describes the acquisition of the semantic com and finally Section 5 gives the conclusion"]}, "P05-1045": {"title": ["Incorporating Nonlocal Information into Information"], "abstract": ["Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference but this makes them unable to fully account for the long distance structure that is prevalent in language use", "We show how to solve this dilemma with Gibbs sampling a simple Monte Carlo method used to perform approximate inference in factored probabilistic models", "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs CMMs and CRFs it is possible to incorporate nonlocal structure while preserving tractable inference", "We use this technique to augment an existing CRFbased information extraction system with longdistance dependency models enforcing label consistency and extraction template consistency constraints", "This technique results in an error reduction of up to 9 over stateoftheart systems on two established information extraction tasks"], "inroduction": ["Most statistical models currently used in natural language processing represent only local structure", "Although this constraint is critical in enabling tractable model inference it is a key limitation in many tasks since natural language contains a great deal of non local structure", "A general method for solving this problem is to relax the requirement of exact inference substituting approximate inference algorithms instead thereby permitting tractable inference in models with nonlocal structure", "One such algorithm is Gibbs sampling a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model including sequence models and probabilistic context free grammars Geman and Ge man 1984", "Although Gibbs sampling is widely used elsewhere there has been extremely little use of it in natural language processing1 Here we use it to add nonlocal dependencies to sequence models for information extraction", "Statistical hidden state sequence models such as Hidden Markov Models HMMs Leek 1997 Freitag and McCallum 1999 Conditional Markov Models CMMs Borthwick 1999 and Conditional Random Fields CRFs Lafferty et al 2001 are a prominent recent approach to information extraction tasks", "These models all encode the Markov property decisions about the state at a particular position in the sequence can depend only on a small local window", "It is this property which allows tractable computation the Viterbi Forward Backward and Clique Calibration algorithms all become intractable without it", "However information extraction tasks can benefit from modeling nonlocal structure", "As an example several authors see Section 8 mention the value of enforcing label consistency in named entity recognition NER tasks", "In the example given in Figure 1 the second occurrence of the token Tanjug is mis labeled by our CRFbased statistical NER system because by looking only at local evidence it is unclear whether it is a person or organization", "The first occurrence of Tanjug provides ample evidence that it is an organization however and by enforcing label consistency the system should be able to get it right", "We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure and demonstrate that this technique yields significant improvements on two established IE tasks", "1 Prior uses in NLP of which we are aware include Kim et al", "1995 Della Pietra et al", "1997 and Abney 1997", "363 Proceedings of the 43rd Annual Meeting of the ACL pages 363370 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics the news agency Tanjug reported   ", "airport  Tanjug said  Figure 1 An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset"]}, "P05-1051": {"title": ["Improving Name Tagging by"], "abstract": ["Information extraction systems incorporate multiple stages of linguistic analysis", "Although errors are typically compounded from stage to stage it is possible to reduce the errors in one stage by harnessing the results of the other stages", "We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger", "We use an Nbest approach to generate multiple hypotheses and have them reranked by subsequent stages of processing", "We obtained thereby a reduction of 24 in spurious and incorrect name tags and a reduction of 14 in missed tags"], "inroduction": ["Systems which extract relations or events from a document typically perform a number of types of linguistic analysis in preparation for information extraction", "These include name identification and classification parsing or partial parsing semantic classification of noun phrases and coreference analysis", "These tasks are reflected in the evaluation tasks introduced for MUC6 named entity coreference template element and MUC7 template relation", "In most extraction systems these stages of analysis are arranged sequentially with each stage using the results of prior stages and generating a single analysis that gets enriched by each stage", "This provides a simple modular organization for the extraction systemUnfortunately each stage also introduces a cer tain level of error into the analysis", "Furthermore these errors are compounded  for example errors in name recognition may lead to errors in parsing", "The net result is that the final output relations or events may be quite inaccurate", "This paper considers how interactions between the stages can be exploited to reduce the error rate", "For example the results of coreference analysis or relation identification may be helpful in name classification and the results of relation or event extraction may be helpful in coreference", "Such interactions are not easily exploited in a simple sequential model  if name classification is performed at the beginning of the pipeline it cannot make use of the results of subsequent stages", "It may even be difficult to use this information implicitly by using features which are also used in later stages because the representation used in the initial stages is too limitedTo address these limitations some recent sys tems have used more parallel designs in which a single classifier incorporating a wide range of features encompasses what were previously several separate stages Kambhatla 2004 Zelenko et al 2004", "This can reduce the compounding of errors of the sequential design", "However it leads to a very large feature space and makes it difficult to select linguistically appropriate features for particular analysis tasks", "Furthermore because these decisions are being made in parallel it becomes much harder to express interactions between the levels of analysis based on linguistic intuitions", "411 Proceedings of the 43rd Annual Meeting of the ACL pages 411418 Ann Arbor June 2005", "Qc 2005 Association for Computational Linguistics In order to capture these interactions more explicitly we have employed a sequential design in which multiple hypotheses are forwarded from each stage to the next with hypotheses being reranked and pruned using the information from later stages", "We shall apply this design to show how named entity classification can be improved by feedback from coreference analysis and relation extraction", "We shall show that this approach can capture these interactions in a natural and efficient manner yielding a substantial improvement in name identification and classification"]}, "P05-1053": {"title": ["Exploring Various Knowledge in Relation Extraction"], "abstract": [], "inroduction": ["Extracting semantic relationships between entities is challenging", "This paper investigates the incorporation of diverse lexical syntactic and semantic knowledge in featurebased relation extraction using SVM", "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement", "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking", "We also demonstrate how semantic information such as WordNet and Name List can be used in featurebased relation extraction to further improve the performance", "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously bestreported systems on the 24 ACE relation subtypes and significantly outperforms tree kernelbased systems by over 20 in Fmeasure on the 5 ACE relation types"]}, "P06-1005": {"title": ["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 3340"], "abstract": ["We present an approach to pronoun resolution based on syntactic paths", "Through a simple bootstrapping procedure we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities", "This path information enables us to handle previously challenging resolution instances and also robustly addresses traditional syntactic coreference constraints", "Highly coreferent paths also allow mining of precise probabilistic gendernumber information", "We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier", "Significant gains in performance are observed on several datasets"], "inroduction": ["Pronoun resolution is a difficult but vital part of the overall coreference resolution task", "In each of the following sentences a pronoun resolution system must determine what the pronoun his refers to 1 John needs his friend", "2 John needs his support", "In 1 John and his corefer", "In 2 his refers to some other perhaps previously evoked entity", "Traditional pronoun resolution systems are not designed to distinguish between these cases", "They lack the specific world knowledge required in the second instance  the knowledge that a person does not usually explicitly need his own support", "We collect statistical pathcoreference information from a large automaticallyparsed corpus to address this limitation", "A dependency path is defined as the sequence of dependency links between two potentially coreferent entities in a parse tree", "A path does not include the terminal entities for example John needs his support and He needs their support have the same syntactic path", "Our algorithm determines that the dependency path linking the Noun and pronoun is very likely to connect coreferent entities for the path Noun needs pronouns friend while it is rarely coreferent for the path Noun needs pronouns support This likelihood can be learned by simply counting how often we see a given path in text with an initial Noun and a final pronoun that are from the samedifferent gendernumber classes", "Cases such as John needs her support or They need his support are much more frequent in text than cases where the subject noun and pronoun terminals agree in gendernumber", "When there is agreement the terminal nouns are likely to be coreferent", "When they disagree they refer to different entities", "After a sufficient number of occurrences of agreement or disagreement there is a strong statistical indication of whether the path is coreferent terminal nouns tend to refer to the same entity or noncoreferent nouns refer to different entities", "We show that including path coreference information enables significant performance gains on three thirdperson pronoun resolution experiments", "We also show that coreferent paths can provide the seed information for bootstrapping other even more important information such as the gendernumber of noun phrases"]}, "P06-1011": {"title": ["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 81 88 Sydney July 2006 c  2006 Association for Computational Linguistics Extracting Parallel SubSentential Fragments from NonParallel Corpora Dragos Stefan Munteanu University of Southern California Information Sciences Institute 4676 Admiralty Way Suite 1001 Marina del Rey CA 90292 dragosisiedu Daniel Marcu University of Southern California Information Sciences Institute 4676 Admiralty Way Suite 1001 Marina del Rey CA 90292 marcuisiedu Abstract We present a novel method for extracting parallel subsentential fragments from comparable nonparallel bilingual corpora By analyzing potentially similar sentence pairs using a signal processinginspired approach we detect which segments of the source sentence are translated into segments in the target sentence and which are not This method enables us to extract useful machine translation training data even from very nonparallel corpora which contain no parallel sentence pairs We evaluate the quality of the extracted data by showing that it improves the performance of a stateoftheart statistical machine translation system 1 Introduction Recently there has been a surge of interest in the automatic creation of parallel corpora Several researchers Zhao and Vogel 2002 Vogel 2003 Resnik and Smith 2003 Fung and Cheung 2004a Wu and Fung 2005 Munteanu and Marcu 2005 have shown how fairly goodquality parallel sentence pairs can be automatically extracted from comparable corpora and used to improve the performance of machine translation MT systems This work addresses a major bottleneck in the de velopment of Statistical MT SMT systems the lack of sufficiently large parallel corpora for most language pairs Since comparable corpora exist in large quantities and for many languages  tens of thousands of words of news describing the same events are produced daily  the ability to exploit them for parallel data acquisition is highly beneficial for the SMT field Comparable corpora exhibit various degrees of parallelism Fung and Cheung 2004a describe corpora ranging from noisy parallel to compara ble and finally to very nonparallel Corpora from the last category contain   disparate very nonparallel bilingual documents that could either be on the same topic ontopic or not  This is the kind of corpora that we are interested to exploit in the context of this paper Existing methods for exploiting comparable corpora look for parallel data at the sentence level However we believe that very nonparallel corpora have none or few good sentence pairs most of their parallel data exists at the subsentential level As an example consider Figure 1 which presents two news articles from the English and Romanian editions of the BBC The articles report on the same event the one year anniversary of Ukraine s Orange Revolution have been published within 25 minutes of each other and express overlapping content Although they are  ontopic  these two documents are nonparallel In particular they contain no parallel sentence pairs methods designed to extract full parallel sentences will not find any useful data in them Still as the lines and boxes from the figure show some parallel fragments of data do exist but they are present at the subsentential level In this paper we present a method for extracting such parallel fragments from comparable corpora Figure 2 illustrates our goals It shows two sentences belonging to the articles in Figure 1 and highlights and connects their parallel fragments Although the sentences share some common meaning each of them has content which is not translated on the other side The English phrase reports the BBC  s Helen Fawkes in Kiev as well 81 Figure 1 A pair of comparable nonparallel documents Figure 2 A pair of comparable sentences as the Romanian one De altfel vorbind inaintea aniversarii have no translation correspondent either in the other sentence or anywhere in the whole document Since the sentence pair contains so much untranslated text it is unlikely that any parallel sentence detection method would consider it useful And even if the sentences would be used for MT training considering the amount of noise they contain they might do more harm than good for the systems performance The best way to make use of this sentence pair is to extract and use for training just the translated highlighted fragments This is the aim of our work Identifying parallel subsentential fragments is a difficult task It requires the ability to recognize translational equivalence in very noisy environments namely sentence pairs that express different although overlapping content However a good solution to this problem would have a strong impact on parallel data acquisition efforts Enabling the exploitation of corpora that do not share parallel sentences would greatly increase the amount of comparable data that can be used for SMT 2 Finding Parallel SubSentential Fragments in Comparable Corpora"], "abstract": ["We present a novel method for extracting parallel subsentential fragments from comparable nonparallel bilingual corpora", "By analyzing potentially similar sentence pairs using a signal processing inspired approach we detect which segments of the source sentence are translated into segments in the target sentence and which are not", "This method enables us to extract useful machine translation training data even from very nonparallel corpora which contain no parallel sentence pairs", "We evaluate the quality of the extracted data by showing that it improves the performance of a stateoftheart statistical machine translation system"], "inroduction": ["Recently there has been a surge of interest in the automatic creation of parallel corpora", "Several researchers Zhao and Vogel 2002 Vogel 2003 Resnik and Smith 2003 Fung and Cheung 2004a Wu and Fung 2005 Munteanu and Marcu 2005 have shown how fairly goodquality parallel sentence pairs can be automatically extracted from comparable corpora and used to improve the performance of machine translation MT systems", "This work addresses a major bottleneck in the development of Statistical MT SMT systems the lack of sufficiently large parallel corpora for most language pairs", "Since comparable corpora exist in large quantities and for many languages  tens of thousands of words of news describing the same events are produced daily  the ability to exploit them for parallel data acquisition is highly beneficial for the SMT field", "Comparable corpora exhibit various degrees of parallelism", "Fung and Cheung 2004a describe corpora ranging from noisy parallel to comparable and finally to very nonparallel", "Corpora from the last category contain  disparate very non parallel bilingual documents that could either be on the same topic ontopic or not", "This is the kind of corpora that we are interested to exploit in the context of this paper", "Existing methods for exploiting comparable corpora look for parallel data at the sentence level", "However we believe that very nonparallel corpora have none or few good sentence pairs most of their parallel data exists at the subsentential level", "As an example consider Figure 1 which presents two news articles from the English and Romanian editions of the BBC", "The articles report on the same event the oneyear anniversary of Ukraines Orange Revolution have been published within 25 minutes of each other and express overlapping content", "Although they are ontopic these two documents are nonparallel", "In particular they contain no parallel sentence pairs methods designed to extract full parallel sentences will not find any useful data in them", "Still as the lines and boxes from the figure show some parallel fragments of data do exist but they are present at the subsentential level", "In this paper we present a method for extracting such parallel fragments from comparable corpora", "Figure 2 illustrates our goals", "It shows two sentences belonging to the articles in Figure 1 and highlights and connects their parallel fragments", "Although the sentences share some common meaning each of them has content which is not translated on the other side", "The English phrase reports the BBCs Helen Fawkes in Kiev as well 81 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 8188 Sydney July 2006", "Qc 2006 Association for Computational Linguistics Figure 1 A pair of comparable nonparallel documents Figure 2 A pair of comparable sentences", "as the Romanian one De altfel vorbind inaintea aniversarii have no translation correspondent either in the other sentence or anywhere in the whole document", "Since the sentence pair contains so much untranslated text it is unlikely that any parallel sentence detection method would consider it useful", "And even if the sentences would be used for MT training considering the amount of noise they contain they might do more harm than good for the systems performance", "The best way to make use of this sentence pair is to extract and use for training just the translated highlighted fragments", "This is the aim of our work", "Identifying parallel subsentential fragments is a difficult task", "It requires the ability to recognize translational equivalence in very noisy environments namely sentence pairs that express different although overlapping content", "However a good solution to this problem would have a strong impact on parallel data acquisition efforts", "Enabling the exploitation of corpora that do not share parallel sentences would greatly increase the amount of comparable data that can be used for SMT"]}, "P06-1016": {"title": ["Modeling Commonality among Related Classes in Relation Extraction"], "abstract": ["This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes", "For each class in the hierarchy either manually predefined or automatically clustered a linear dis criminative function is determined in a top down way using a perceptron algorithm with the lowerlevel weight vector derived from the upperlevel weight vector", "As the upperlevel class normally has much more positive training examples than the lowerlevel class the corresponding linear discriminative function can be determined more reliably", "The upper level discriminative function then can effectively guide the discriminative function learning in the lowerlevel which otherwise might suffer from limited training data", "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 56 and 51 in Fmeasure on least and medium frequent relations respectively", "It also shows that our system outperforms the previous bestreported system by 27 in Fmeasure on the 24 subtypes using the same feature set"], "inroduction": ["With the dramatic increase in the amount of textual information available in digital archives and the WWW there has been growing interest in techniques for automatically extracting information from text", "Information Extraction IE is such a technology that IE systems are expected to identify relevant information usually of predefined types from text documents in a certain domain and put them in a structured format", "According to the scope of the ACE program ACE 20002005 current research in IE has three main objectives Entity Detection and Tracking EDT Relation Detection and Characterization RDC and Event Detection and Characterization EDC", "This paper will focus on the ACE RDC task which detects and classifies various semantic relations between two entities", "For example we want to determine whether a person is at a location based on the evidence in the context", "Extraction of semantic relationships between entities can be very useful for applications such as question answering eg to answer the query Who is the president of the United States", "One major challenge in relation extraction is due to the data sparseness problem Zhou et al 2005", "As the largest annotated corpus in relation extraction the ACE RDC 2003 corpus shows that different subtypestypes of relations are much unevenly distributed and a few relation subtypes such as the subtype Founder under the type ROLE suffers from a small amount of annotated data", "Further experimentation in this paper please see Figure 2 shows that most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size", "Given the relative large size of this corpus it will be timeconsuming and very expensive to further expand the corpus with a reasonable gain in performance", "Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes it will be still far beyond practice for those minor sub types given the much unevenly distribution among different relation subtypes", "While various machine learning approaches such as generative modeling Miller et al 2000 maximum entropy Kambhatla 2004 and support vector machines Zhao and Grisman 2005 Zhou et al 2005 have been applied in the relation extraction task no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations", "This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes", "Through organizing various classes hierarchically a linear discriminative function is determined for each class in a top down way using a perceptron algorithm with the lowerlevel weight vector derived from the upperlevel weight vector", "Evaluation on the ACE RDC 2003 corpus shows that the hierarchical 121 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 121128 Sydney July 2006", "Qc 2006 Association for Computational Linguistics strategy achieves much better performance than the flat strategy on least and mediumfrequent relations", "It also shows that our system based on the hierarchical strategy outperforms the previous bestreported system", "The rest of this paper is organized as follows", "Section 2 presents related work", "Section 3 describes the hierarchical learning strategy using the perceptron algorithm", "Finally we present experimentation in Section 4 and conclude this paper in Section 5"]}, "P06-1017": {"title": ["Relation Extraction Using Label Propagation Based Semisupervised"], "abstract": ["Shortage of manually labeled data is an obstacle to supervised relation extraction methods", "In this paper we investigate a graph based semisupervised learning algorithm a label propagation LP algorithm for relation extraction", "It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph and tries to obtain a labeling function to satisfy two constraints 1 it should be fixed on the labeled nodes 2 it should be smooth on the whole graph", "Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available and it also performs better than bootstrap ping for the relation extraction task"], "inroduction": ["Relation extraction is the task of detecting and classifying relationships between two entities from text", "Many machine learning methods have been proposed to address this problem eg supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithms Hasegawa et al 2004", "Supervised methods for relation extraction perform well on the ACE Data but they require a large amount of manually labeled relation instances", "Unsupervised methods do not need the definition of relation types and manually labeled data but they cannot detect relations between entity pairs and its result cannot be directly used in many NLP tasks since there is no relation type label attached to each instance in clustering result", "Considering both the availability of a large amount of untagged corpora and direct usage of extracted relations semi supervised learning methods has received great attention", "DIPRE Dual Iterative Pattern Relation Expansion Brin 1998 is a bootstrappingbased system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations", "Snowball Agichtein and Gravano 2000 is another system that used bootstrap ping techniques for extracting relations from unstructured text", "Snowball shares much in common with DIPRE including the employment of the boot strapping framework as well as the use of pattern matching to extract new candidate relations", "The third system approaches relation classification problem with bootstrapping on top of SVM proposed by Zhang 2004", "This system focuses on the ACE sub problem RDC and extracts various lexical and syntactic features for the classification task", "However Zhang 2004s method doesnt actually detect re laitons but only performs relation classification between two entities given that they are known to be related", "Bootstrapping works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled data using a model learned from augmented labeled data in previous iteration", "It 129 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 129136 Sydney July 2006", "Qc 2006 Association for Computational Linguistics can be found that the affinity information among unlabeled examples is not fully explored in this boot strapping process", "Recently a promising family of semisupervised learning algorithm is introduced which can effectively combine unlabeled data with labeled data in learning process by exploiting manifold structure cluster structure in data Belkin and Niyogi 2002 Blum and Chawla 2001 Blum et al 2004 Zhu and Ghahramani 2002 Zhu et al 2003", "These graphbased semisupervised methods usually define a graph where the nodes represent labeled and unlabeled examples in a dataset and edges may be weighted reflect the similarity of examples", "Then one wants a labeling function to satisfy two constraints at the same time 1 it should be close to the given labels on the labeled nodes and 2 it should be smooth on the whole graph", "This can be expressed in a regularization framework where the first term is a loss function and the second term is a regularizer", "These methods differ from traditional semi supervised learning methods in that they use graph structure to smooth the labeling function", "To the best of our knowledge no work has been done on using graph based semisupervised learning algorithms for relation extraction", "Here we investigate a label propagation algorithm LP Zhu and Ghahramani 2002 for relation extraction task", "This algorithm works by representing labeled and unlabeled examples as vertices in a connected graph then propagating the label information from any vertex to nearby vertices through weighted edges iteratively finally inferring the labels of unlabeled examples after the propagation process converges", "In this paper we focus on the ACE RDC task1"]}, "P06-1104": {"title": ["A Composite Kernel to Extract Relations between Entities with"], "abstract": ["This paper proposes a novel composite kernel for relation extraction", "The composite kernel consists of two individual kernels an entity kernel that allows for entityrelated features and a convolution parse tree kernel that models syntactic information of relation examples", "The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction", "Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering and can also easily scale to include more features", "Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms previous two dependency tree kernels for relation extraction"], "inroduction": ["The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text", "The research on relation extraction has been promoted by the Message Understanding Conferences MUCs MUC 1987 1998 and Automatic Content Extraction ACE program ACE 20022005", "According to the ACE Program an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities", "For example the sentence Bill Gates is chairman and chief software architect of Microsoft Corporation conveys the ACEstyle relation EMPLOYMENTexec between the entities Bill Gates PERSONName and Microsoft Corporation ORGANIZATION", "Commercial", "In this paper we address the problem of relation extraction using kernel methods Schlkopf and Smola 2001", "Many featurebased learning algorithms involve only the dotproduct between feature vectors", "Kernel methods can be regarded by replacing the dotproduct with a kernel function between two vectors or even between two objects", "A kernel function is a similarity function satisfying the properties of being symmetric and positivedefinite", "Recently kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects", "For example the kernels for structured natural language data such as parse tree kernel Collins and Duffy 2001 string kernel Lodhi et al 2002 and graph kernel Suzuki et al 2003 are example instances of the well known convolution kernels1 in NLP", "In relation extraction typical work on kernel methods includes Zelenko et al", "2003 Culotta and Sorensen 2004 and Bunescu and Mooney 2005", "This paper presents a novel composite kernel to explore diverse knowledge for relation extraction", "The composite kernel consists of an entity kernel and a convolution parse tree kernel", "Our study demonstrates that the composite kernel is very effective for relation extraction", "It alsoshows without the need for extensive feature en gineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively", "An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels", "Evaluation on the ACE corpus shows that our method outperforms the previous best reported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features", "The rest of the paper is organized as follows", "In Section 2 we review the previous work", "Section 3 discusses our composite kernel", "Section 4 reports the experimental results and our observations", "Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure", "by Haussler 1999 in the machine learning field", "This framework defines a kernel between input objects by applying convolution subkernels that are the kernels for the decompositions parts of the objects", "825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 825832 Sydney July 2006", "Qc 2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration", "We conclude our work and indicate the future work in Section 6"]}, "P06-1117": {"title": ["Semantic Role Labeling via FrameNet VerbNet and PropBank"], "abstract": ["This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources FrameNet VerbNet and PropBank", "The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs", "We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes", "The PropBank corpus which is tightly connected to the VerbNet lexicon is used to increase the verb coverage and also to test the effectiveness of our approach", "The results indicate that our model is an interesting step towards the design of more robust semantic parsers"], "inroduction": ["During the last years a noticeable effort has been devoted to the design of lexical resources that can provide the training ground for automatic semantic role labelers", "Unfortunately most of the systems developed until now are confined to the scope of the resource used for training", "A very recent example in this sense was provided by the CONLL 2005 shared task Carreras and Marquez 2005 on PropBank PB Kingsbury and Palmer 2002 role labeling", "The systems that participated in the task were trained on the Wall Street Journal corpus WSJ and tested on portions of WSJ and Brown corpora", "While the best Fmeasure recorded on WSJ was 80 on the Brown corpus the Fmeasure dropped below 70", "The most significant causes for this performance decay were highly ambiguous and unseen predicates ie predicates that do not have training examples", "The same problem was again highlighted by the results obtained with and without the frame information in the Senseval3 competition Litkowski 2004 of FrameNet Johnson et al 2003 role labeling task", "When such information is not used by the systems the performance decreases by 10 percent points", "This is quite intuitive as the semantics of many roles strongly depends on the focused frame", "Thus we cannot expect a good performance on new domains in which this information is not available", "A solution to this problem is the automatic frame detection", "Unfortunately our preliminary experiments showed that given a FrameNet FN predicateargument structure the task of identifying the associated frame can be performed with very good results when the verb predicates have enough training examples but becomes very challenging otherwise", "The predicates belonging to new application domains ie not yet included in FN are especially problematic since there is no training data available", "Therefore we should rely on a semantic context alternative to the frame Giuglea and Moschitti 2004", "Such context should have a wide coverage and should be easily derivable from FN data", "A very good candidate seems to be the Intersective Levin class ILC Dang et al 1998 that can be found as well in other predicate resources like PB and VerbNet VN Kipper et al 2000", "In this paper we have investigated the above claim by designing a semiautomatic algorithm that assigns ILCs to FN verb predicates and by carrying out several semantic role labeling SRL experiments in which we replace the frame with the ILC information", "We used support vector ma 929 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 929936 Sydney July 2006", "Qc 2006 Association for Computational Linguistics chines Vapnik 1995 with a polynomial kernels to learn the semantic role classification and b Tree Kernels Moschitti 2004 for learning both frame and ILC classification", "Tree kernels were applied to the syntactic trees that encode the subcategorization structures of verbs", "This means that although FN contains three types of predicates nouns adjectives and verbs we only concentrated on the verb predicates and their roles", "The results show that 1 ILC can be derived with high accuracy for both FN and Probank and 2 ILC can replace the frame feature with almost no loss in the accuracy of the SRL systems", "At the same time ILC provides better predicate coverage as it can also be learned from other corpora eg PB", "In the remainder of this paper Section 2 summarizes previous work done on FN automatic role detection", "It also explains in more detail why models based exclusively on this corpus are not suitable for freetext parsing", "Section 3 focuses on VN and PB and how they can enhance the robustness of our semantic parser", "Section 4 describes the mapping between frames and ILCs whereas Section 5 presents the experiments that support our thesis", "Finally Section 6 summarizes the conclusions"]}, "P06-1141": {"title": ["An Effective TwoStage Model for Exploiting NonLocal Dependencies in"], "abstract": ["This paper shows that a simple twostage approach to handle nonlocal dependencies in Named Entity Recognition NER can outperform existing approaches that handle nonlocal dependencies while being much more computationally efficient", "NER systems typically use sequence models for tractable inference but this makes them unable to capture the long distance structure present in text", "We use a Conditional Random Field CRF based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF", "Using features capturing nonlocal dependencies from the same document our approach yields a 126 relative error reduction on the F1 score over stateofthe art NER systems using localinformation alone when compared to the 93 relative error reduction offered by the best systems that exploit nonlocal information", "Our approach also makes it easy to incorporate nonlocal information from other documents in the test corpus and this gives us a 133 error reduction over NER systems using localinformation alone", "Additionally our running time for inference is just the inference time of two sequential CRFs which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference"], "inroduction": ["Named entity recognition NER seeks to locate and classify atomic elements in unstructured text into predefined entities such as the names of persons organizations locations expressions of times quantities monetary values percentages etc A particular problem for Named Entity RecognitionNER systems is to exploit the presence of useful information regarding labels assigned at a long distance from a given entity", "An example is the labelconsistency constraint that if our text has two occurrences of New York separated by other tokens we would want our learner to encourage both these entities to get the same label", "Most statistical models currently used for Named Entity Recognition use sequence models and thereby capture local structure", "Hidden Markov Models HMMs Leek 1997 Freitag and McCallum 1999 Conditional Markov Models CMMs Borthwick 1999 McCallum et al 2000 and Conditional Random Fields CRFs Lafferty et al 2001 have been successfully employed in NER and other information extraction tasks", "All these models encode the Markov property ie labels directly depend only on the labels assigned to a small window around them", "These models exploit this property for tractable computation as this allows the ForwardBackward Viterbi and Clique Calibration algorithms to become tractable", "Although this constraint is essential to make exact inference tractable it makes us unable to exploit the nonlocal structure present in natural language", "Label consistency is an example of a nonlocal dependency important in NER", "Apart from label consistency between the same token sequences we would also like to exploit richer sources of dependencies between similar token sequences", "For example as shown in Figure 1 we would want it to encourage Einstein to be labeled Person if there is strong evidence that Albert Einstein should be labeled Person", "Sequence models unfortu 1121 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 11211128 Sydney July 2006", "Qc 2006 Association for Computational Linguistics told that Albert Einstein proved   ", "on seeing Einstein at the Figure 1 An example of the label consistency problem", "Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label so as to improve the chance that both are labeled PERSON", "nately cannot model this due to their Markovian assumption", "Recent approaches attempting to capture non local dependencies model the nonlocal dependencies directly and use approximate inference algorithms since exact inference is in general not tractable for graphs with nonlocal structure", "Bunescu and Mooney 2004 define a Relational Markov Network RMN which explicitly models longdistance dependencies and use it to represent relations between entities", "Sutton and McCallum 2004 augment a sequential CRF with skipedges ie edges between different occurrences of a token in a document", "Both these approaches use loopy belief propagation Pearl 1988 Yedidia et al 2000 for approximate inference", "Finkel et al", "2005 handset penalties for inconsistency in entity labeling at different occurrences in the text based on some statistics from training data", "They then employ Gibbs sampling Geman and Geman 1984 for dealing with their local feature weights and their nonlocal penalties to do approximate inference", "We present a simple twostage approach where our second CRF uses features derived from the output of the first CRF", "This gives us the advantage of defining a rich set of features to model nonlocal dependencies and also eliminates the need to do approximate inference since we do not explicitly capture the nonlocal dependencies in a single model like the more complex existing approaches", "This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRFs in contrast Finkel et al", "2005 reported an increase in running time by a factor of 30 over the sequential CRF with their Gibbs sampling approximate inference", "In all our approach is simpler yields higher F1 scores and is also much more computationally efficient than existing approaches modeling non local dependencies"]}, "P06-2012": {"title": ["Unsupervised Relation Disambiguation Using Spectral Clustering"], "abstract": ["This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts", "It works by calculating eigen vectors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors", "Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods"], "inroduction": ["In this paper we address the task of relation extraction which is to find relationships between name entities in a given context", "Many methods have been proposed to deal with this task including supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithm Hasegawa et al 2004", "Among these methods supervised learning is usually more preferred when a large amount of labeled training data is available", "However it is timeconsuming and laborintensive to manually tag a large amount of training data", "Semisupervised learning methods have been put forward to minimize the corpus annotation requirement", "Most of semisupervised methods employ the bootstrapping framework which only need to predefine some initial seeds for any particular relation and then bootstrap from the seeds to acquire the relation", "However it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal number of them", "Compared with supervised and semisupervised methods Hasegawa et al", "2004s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels", "Hasegawa et al", "2004s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities", "However the drawback of hierarchical clustering is that it required providing cluster number by users", "Furthermore clustering is performed in original high dimensional space which may induce nonconvex clusters hard to identified", "This paper presents a novel application of spectral clustering technique to unsupervised relation extraction problem", "It works by calculating eigenvec tors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensional space and then performing cluster number estimation on a transformed space defined by the first few eigen vectors", "This method may help us find nonconvex clusters", "It also does not need to predefine the number of the context clusters or prespecify the similarity threshold for the clusters as Hasegawa et al", "2004s method", "The rest of this paper is organized as follows", "Section 2 formulates unsupervised relation extraction and presents how to apply the spectral clustering 89 Proceedings of the COLINGACL 2006 Main Conference Poster Sessions pages 8996 Sydney July 2006", "Qc 2006 Association for Computational Linguistics technique to resolve the task", "Then section 3 reports experiments and results", "Finally we will give a conclusion about our work in section 4"]}, "P06-2094": {"title": ["Proceedings of the COLINGACL 2006 Main Conference Poster Sessions pages 731 738 Sydney July 2006 c  2006 Association for Computational Linguistics OnDemand Information Extraction Satoshi Sekine Computer Science Department New York University 715 Broadway 7th floor New York NY 10003  USA sekinecsnyuedu"], "abstract": ["At present adapting an Information Extraction system to new topics is an expensive and slow process requiring some knowledge engineering for each new topic", "We propose a new paradigm of Information Extraction which operates on demand in response to a users query", "Ondemand Information Extraction ODIE aims to completely eliminate the customization effort", "Given a users query the system will automatically create patterns to extract salient relations in the text of the topic and build tables from the extracted information using paraphrase discovery technology", "It relies on recent advances in pattern discovery paraphrase discovery and extended named entity tagging", "We report on experimental results in which the system created useful tables for many topics demonstrating the feasibility of this approach"], "inroduction": ["Most of the worlds information is recorded passed down and transmitted between people in text form", "Implicit in most types of text are regularities of information structure  events which are reported many times about different individuals in different forms such as layoffs or mergers and acquisitions in news articles", "The goal of information extraction IE is to extract such information to make these regular structures explicit in forms such as tabular databases", "Once the information structures are explicit they can be processed in many ways to mine information to search for specific information to generate graphical displays and other summaries", "However at present a great deal of knowledge for automatic Information Extraction must be coded by hand to move a system to a new topic", "For example at the later MUC evaluations system developers spent one month for the knowledge engineering to customize the system to the given test topic", "Research over the last decade has shown how some of this knowledge can be obtained from annotated corpora but this still requires a large amount of annotation in preparation for a new task", "Improving portability  being able to adapt to a new topic with minimal effort  is necessary to make Information Extraction technology useful for real users and we be lieve lead to a breakthrough for the application of the technology", "We propose Ondemand information extraction ODIE a system which automatically identifies the most salient structures and extracts the information on the topic the user demands", "This new IE paradigm becomes feasible due to recent developments in machine learning for NLP in particular unsupervised learning methods and it is created on top of a range of basic language analysis tools including POS taggers dependency analyzers and extended Named Entity taggers"]}, "P06-2123": {"title": ["Proceedings of the COLINGACL 2006 Main Conference Poster Sessions pages 961 968 Sydney July 2006 c  2006 Association for Computational Linguistics Subwordbased Tagging for Confidencedependent Chinese Word Segmentation Ruiqiang Zhang12 and Genichiro Kikui and Eiichiro Sumita12 1National Institute of Information and Communications Technology 2ATR Spoken Language Communication Research Laboratories 222 Hikaridai Seiikacho Sorakugun Kyoto 6190288 Japan ruiqiangzhangeiichirosumitaatrjp Abstract We proposed a subword based tagging for Chinese word segmentation to improve the existing characterbased tagging The subwordbased tagging was implemented using the maximum entropy MaxEnt and the conditional random fields CRF methods We found that the proposed subwordbased tagging outperformed the characterbased tagging in all compara tive experiments In addition we proposed a confidence measure approach to combine the results of a dictionarybased and a subwordtaggingbased segmentation This approach can produce an ideal tradeoff between the invocaulary rate and outofvocabulary rate Our techniques were evaluated using the test data from Sighan Bakeoff 2005 We achieved higher Fscores than the best results in three of the four corpora PKU0951 CITYU0950 and MSR0971 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades Segmentation performance has been improved significantly from the earliest maximal match dictionarybased approaches to HMM based Zhang et al 2003 ap proaches and recent stateoftheart machine learning approaches such as maximum entropy MaxEnt Xue and Shen 2003 support vector machine Now the second author is affiliated with NTT SVM Kudo and Matsumoto 2001 conditional random fields CRF Peng and McCallum 2004 and minimum error rate training Gao et al 2004 By analyzing the top results in the first and second Bakeoffs Sproat and Emerson 2003 and Emerson 2005 we found the top results were produced by direct or indirect use of socalled IOB tagging which converts the problem of word segmentation into one of character tagging so that partofspeech tagging approaches can be used for word segmentation This approach was also called  LMR Xue and Shen 2003 or BIES Asahara et al 2005 tagging Under the scheme each character of a word is labeled as B if it is the first character of a multiplecharacter word or  I otherwise and  O if the character functioned as an independent word For example   whole Beijing city is labeled as   O B  I I  Thus the training data in word sequences are turned into IOBlabeled data in character sequences which are then used as the training data for tagging For new test data word boundaries are determined based on the results of tagging While the IOB tagging approach has been widely used in Chinese word segmentation we found that so far all the existing implementations were using characterbased IOB tagging In this work we propose a subword based IOB tagging which assigns tags to a predefined lexicon subset consisting of the most frequent multiplecharacter words in addition to single Chinese characters If only Chinese characters are used the subwordbased IOB tagging is downgraded to a characterbased one Taking the same example mentioned above       is la961 beled as   O   B  I in the subwordbased tagging where B is labeled as one unit We will give a detailed description of this approach in Section 2 There exists a clear weakness with the IOB tagging approach It yields a very low invocabulary rate Riv in return for a higher outofvocabulary OOV rate Roov In the results of the closed test in Bakeoff 2005 Emerson 2005 the work of Tseng et al 2005 using CRFs for the IOB tagging yielded a very high Roov in all of the four corpora used but the Riv rates were lower While OOV recognition is very important in word segmentation a higher IV rate is also desired In this work we propose a confidence measure approach to lessen this weakness By this approach we can change the Roov and Riv and find an optimal tradeoff This approach will be described in Section 23 In addition we illustrate our word segmentation process in Section 2 where the subword based tagging is described by the MaxEnt method Section 3 presents our experimental results The effects using the MaxEnts and CRFs are shown in this section Section 4 describes current stateoftheart methods with Chinese word segmentation with which our results were compared Section 5 provides the con cluding remarks and outlines future goals 2 Chinese word segmentation framework Our word segmentation process is illustrated in Fig 1 It is composed of three parts a dictionarybased Ngram word segmentation for segmenting IV words a maximum entropy subword based tagger for recognizing OOVs and a confidencedependent word disambiguation used for merging the results of both the dictionarybased and the IOBtaggingbased An example exhibiting each steps results is also given in the figure"], "abstract": ["We proposed a subwordbased tagging for Chinese word segmentation to improve the existing characterbased tagging", "The subwordbased tagging was implemented using the maximum entropy MaxEnt and the conditional random fields CRF methods", "We found that the proposed subwordbased tagging outperformed the characterbased tagging in all comparative experiments", "In addition we proposed a confidence measure approach to combine the results of a dictionarybased and a subwordtaggingbased segmentation", "This approach can produce an ideal tradeoff between the invocaulary rate and outofvocabulary rate", "Our techniques were evaluated using the test data from Sighan Bakeoff 2005", "We achieved higher Fscores than the best results in three of the four corpora PKU0951 CITYU0950 and MSR0971"], "inroduction": ["Many approaches have been proposed in Chinese word segmentation in the past decades", "Segmentation performance has been improved significantly from the earliest maximal match dictionarybased approaches to HMMbased Zhang et al 2003 approaches and recent stateoftheart machine learning approaches such as maximum entropy MaxEnt Xue and Shen 2003 support vector machine  Now the second author is affiliated with NTT", "SVM Kudo and Matsumoto 2001 conditional random fields CRF Peng and McCallum 2004 and minimum error rate training Gao et al 2004", "By analyzing the top results in the first and second Bakeoffs Sproat and Emerson 2003 and Emerson 2005 we found the top results were produced by direct or indirect use of socalled IOB tagging which converts the problem of word segmentation into one of character tagging so that partofspeech tagging approaches can be used for word segmentation", "This approach was also called LMR Xue and Shen 2003 or BIES Asahara et al 2005 tagging", "Under the scheme each character of a word is labeled as B if it is the first character of a multiplecharacter word or I otherwise and O if the character functioned as an independent word", "For example whole Beijing city is labeled as O B I I", "Thus the training data in word sequences are turned into IOBlabeled data in character sequences which are then used as the training data for tagging", "For new test data word boundaries are determined based on the results of tagging", "While the IOB tagging approach has been widely used in Chinese word segmentation we found that so far all the existing implementations were using characterbased IOB tagging", "In this work we propose a subwordbased IOB tagging which assigns tags to a predefined lexicon subset consisting of the most frequent multiplecharacter words in addition to single Chinese characters", "If only Chinese characters are used the subwordbased IOB tagging is downgraded to a characterbased one", "Taking thesame example mentioned above  is la 961 Proceedings of the COLINGACL 2006 Main Conference Poster Sessions pages 961968 Sydney July 2006", "Qc 2006 Association for Computational Linguistics beled as O B I in the subwordbased tagging where B is labeled as one unit", "We will give a detailed description of this approach in Section 2", "There exists a clear weakness with the IOB tagging approach It yields a very low invocabulary rate Riv in return for a higher outofvocabulary OOV rate Roov", "In the results of the closed test in Bakeoff 2005 Emerson 2005 the work of Tseng et al 2005 using CRFs for the IOB tagging yielded a very high Roov in all of the four corpora used but the Riv rates were lower", "While OOV recognition is very important in word segmentation a higher IV rate is also desired", "In this work we propose a confidence measure approach to lessen this weakness", "By this approach we can change the Roov and Riv and find an optimal tradeoff", "This approach will be described in Section 23", "In addition we illustrate our word segmentation process in Section 2 where the subwordbased tagging is described by the MaxEnt method", "Section 3 presents our experimental results", "The effects using the MaxEnts and CRFs are shown in this section", "Section 4 describes current stateoftheart methods with Chinese word segmentation with which our results were compared", "Section 5 provides the con input  XDQJLQJKXQ OLYHV LQ HLMLQJFLW Dictionarybased word segmentation       XDQJ LQJ KXQ OLYHV LQ HLMLQJFLW Subwordbased IOB tagging 094 098 091 209 2089 094 098 XDQJ LQJ KXQ OLYHV2 LQ2 HLMLQJ FLW Confidencebased disambiguation 075 078 073 2092 2091 095 098 XDQJ LQJ KXQ OLYHV2 LQ2 HLMLQJ FLW output     XDQJLQJKXQ OLYHV LQ HLMLQJFLW Figure 1 Outline of word segmentation process W  wt0 wt1 wt2   ", "wtM  which satisfies wt0  c0   ", "ct0  wt1  ct0 1   ", "ct1 wti  cti1 1   ", "cti  wtM  ctM1 1   ", "ctM ti  ti1 0  ti  N 0  i  M such that W  arg max PW C  arg max PW PCW  cluding remarks and outlines future goals", "W W  arg max Pwt0 wt1   ", "wtM c0   ", "ct0  wt0  W"]}, "P06-3008": {"title": ["Discursive Usage of Six Chinese Punctuation Marks"], "abstract": ["Both rhetorical structure and punctuation have been helpful in discourse processing", "Based on a corpus annotation project this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts Colon Dash Ellipsis Exclamation Mark Question Mark and Semicolon", "The rhetorical patterns of these marks are compared against patterns around cue phrases in general", "Results show that these Chinese punctuation marks though fewer in number than cue phrases are easy to identify have strong correlation with certain relations and can be used as distinctive indicators of nuclearity in Chinese texts"], "inroduction": ["Rhetorical structure has been proven useful in NLP projects such as text generation summarization machine translation and essay scoring", "Automatic discourse parsing remains an elusive task however despite much rulebased research on lexical cues such as anaphora and conjunctions", "Parsing through machine learning has encountered a bottleneck due to limited resourcesthere is only one English RST treebank publicly available and one RSTannotated German corpus on its way", "Punctuation marks PMs have been proven useful in RST annotation as well as in many other NLP tasks such as PartofSpeech tagging Word Sense Disambiguation Nearduplicate detection bilingual alignment eg Chuang and Yeh 2005 etc Dale 1991 noticed the role of PMs in determining rhetorical relations", "Say 1998 did a study on their roles in English discourse structure", "Marcu 1997 and CorstonOliver 1998 based their automatic discourse parser partially on PMs and other orthographical cues", "Tsou et al", "1999 and Chan et al", "2000 use PMs to disambiguate candidate Discourse Markers for a Chinese summarization system", "Reitter 2003 also used PMs to distinguish ATTRIBUTION and ELABORATION relations in his Featurerich SVM rhetorical analysis system", "All these inspired us to survey on the rhetorical patterns around Chinese PMs so as to provide more direct a priori scores for the coarse rhetorical analyzer by Zhang et al", "2000 in their hybrid summarization system", "This paper is organized into 5 parts Section 2 gives an overview of a Chinese RST treebank under construction and a survey on the syntax of six main PMs in the corpus Colon Dash Ellipses Exclamation Mark Question Mark and Semicolon", "Section 3 reports rhetorical patterns around these PMs", "Section 4 is a discussion on the effectiveness of these PMs in comparison with Chinese cue phrases", "Section 5 is a summary and Section 6 directions for future work"]}, "P06128": {"title": ["2011 Seventh International Conference on Computational Intelligence and Security"], "abstract": ["Linear text segmentation aims at dividing a long text into several topical segments", "It is beneficial to many natural language processing tasks such as information retrieval and document summarization", "In this article an efficient linear text segmentation algorithm based on hierarchical agglomerative clustering is presented", "The proposed linear text segmentation algorithm is implemented without auxiliary knowledge base parameter setting and user involvement", "Experimental results show that the proposed linear text segmentation algorithm not only provides linear time computational complexity but also provides comparable segmentation accuracy with several well known linear text segmentation algorithms", "Keywordstext segmentation hierarchical agglomerative clustering computational intelligence NLP application"], "inroduction": ["The purpose of linear text segmentation is to divide a long text into several segments each of which corresponds to a topic and consists of consecutive sentences or paragraphs", "In other words the task of linear text segmentation is to identify topic boundaries within a long text", "Linear text segmentation algorithms are widely used as an essential step in many natural language processing tasks such as information retrieval and document summarization", "In information retrieval to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved 1", "It not only provides more accurate information to the user but also reduces the users burden to read the whole document", "In document summarization a long document is often divided into topics and then each topic is summarized independently 2", "A text segmentation algorithm is usually applied as the first step of these tasks", "Segmentation accuracy and computational complexity are two critical issues in linear text segmentation algorithm design", "In the past many wellknown linear text segmentation algorithms have been proposed 3 4 5 6", "978076954584411 2600  2011 IEEE DOI 101109CIS2011240 1081 Although it has been proven that these algorithms either improve the segmentation accuracy or provide efficient processing of linear text segmentation they can hardly deal with both issues at the same time", "Some algorithms provide better segmentation accuracy but suffer from expensive computational complexity some algorithms provide efficient processing of linear text segmentation but suffer from lower segmentation accuracy 7", "In recent years some algorithms such as TSF 7 are proposed to provide high segmentation accuracy in a reasonable processing time", "However most of the algorithms rely heavily on a training phase or manpower to designate parameters used in the algorithms", "The training phase is time consuming while to designate parameters by manpower increases users burden", "Moreover it is difficult to determine manually suitable parameters", "To tackle the problems mentioned above a novel efficient linear text segmentation algorithm based on Hierarchical Agglomerative Clustering HAC is presented in this article", "The proposed linear text segmentation algorithm dynamically designates parameters according to the metadata of a text and hence neither training data nor user involvement is needed", "In other words the algorithm requires only the given text no other information or preprocessing is needed", "It not only reduces the users burden but also increases the utility of the proposed linear text segmentation algorithm", "Segmentation accuracy of the proposed linear text segmentation algorithm is evaluated with a most commonly used test collection created by Choi 3", "Also the computational complexity of the proposed algorithm is analyzed", "Experimental results show that the proposed algorithm not only provides comparable segmentation accuracy with several wellknown linear text segmentation algorithms but also provides a linear time computational complexity without any auxiliary knowledge base parameter setting or user involvement", "Existing text segmentation algorithms can be divided into two major categories 3", "The first category is lexical cohesion methods", "The second category is multisource methods", "Lexical cohesion methods detect cohesion using word stem repetition context vectors entity repetition semantic similarity word distance model and word frequency model", "Multisource methods combine lexical cohesion with other indicators of topic shift including cue phrase prosodic features reference syntax and lexical attraction using decision trees and probabilistic models", "In the literatures some efficient linear text segmentation algorithms are proposed such as TextTiling 5", "TextTiling is a wellknown linear time text segmentation algorithm proposed by Hearst", "It uses a sliding window approach to segment a text", "The similarities between adjacent blocks within the text are computed to detect topic changes", "The computed similarities are smoothed and used to identify topic boundaries by a cutoff function", "Although TextTiling is an efficient linear text segmentation method the complexity is linear time it suffers from lower segmentation accuracy", "There are some more complex linear text segmentation algorithms such as C99 3", "Sentencesimilarity matrix consists of similarities between all sentences within a text is frequently adopted in these algorithms", "For example C99 proposed by Choi 3 segments a text by combining a rank matrix transformed from the sentencesimilarity matrix and divisive clustering", "Some algorithms apply the similarity matrix to detect the optimal topic boundaries by using dynamic programming 4 6", "Owing to the cost of constructing the sentencesimilarity matrix is On2 where n represents the number of sentences in a text such algorithms suffer from higher computational complexity 7", "To tackle the problems observed above Kern and Granitzer proposed an efficient linear text segmentation algorithm called TSF 7", "Similar to TextTiling TSF identifies topic boundaries using a sliding window", "The differences between TSF and TextTiling are 1 in TSF a term vector is built per sentence instead of merging all terms within a block into one term vector in TextTiling 2 both the inner similarity of a block of sentences and the outer similarity of adjacent blocks of sentences are considered when evaluating a potential segmentation position3 In TSF the smoothing happens implicitly by using the average of the sentence similarities instead of smoothing the similarity between adjacent blocks in TextTiling", "It has been proven that TSF has an On computational complexity and provides a comparable accuracy when compared with several higher computational complexity algorithms", "However two parameters need to be provided by the user say the size of block and the threshold to identify candidate topic boundaries", "It may increase the users burden and the parameters provided may not always be suitable to reflect the linear time complexity ie On", "Especially no parameter setting is required", "III", "TEXT SEGMENTATION BASED ON HIERARCHICAL AGGLOMERATIVE CLUSTERING TSHAC In this section an efficient linear text segmentation algorithm called TSHAC which considers both computational complexity and segmentation accuracy is proposed", "The process of TSHAC consists of 4 steps", "At first a long text is preprocessed tokenization stopword removal and stemming are conducted to construct the vocabulary of the text", "After text preprocessing the text can be represented as vectors each of which represents a sentence within the text", "A part of sentence similarities are then computed to construct the sentencesimilarity matrix", "Finally the optimal topic boundaries are identified by the proposed algorithm", "A Text Preprocessing In general the process of text preprocessing can be divided into 3 stages 1 tokenization 2 stopword removal 3 stemming", "Punctuation is firstly removed from a long text and the sentences within the text are then converted into a stream of words", "Subsequently generic stopwords are removed", "The rest of words are stemmed and regarded as the vocabulary of the long text", "In 2003 Ji et al 6 devise a new idea of document dependent stopword removal", "In contrast to the fixed set of generic stopwords documentdependent stopwords are the words that are useful in discriminating among several different documents but are rather harmful in detecting subtopics in a document", "Both generic stopword removal and documentdependent stopword removal are adopted in our proposed linear text segmentation algorithm", "B Text Representation Given a long text T a sentence sj contained in T can be represented as a vector in order s j  w1 j  w2 j  wij wkj  where wij represents the word weight of the ith word in sj k represents the total number of words in the vocabulary V constructed from the long text", "In our proposed algorithm wij can be computed by any formula such as TFxIDF that the word weight can be suitably represented", "As the core contribution of the proposed algorithm is the process of hierarchical agglomerative clustering for linear text segmentation here we use the same formula as in TSF for comparison purpose", "The word weight wij is computed by a variant of TFxIDF 7 real metadata", "Based on the above observations an efficient linear text segmentation algorithm based on Hierarchical wij  termFreqij tokenCount j log docCount docFreqi  1   1  Agglomerative Clustering HAC 8 will be presented in the next section", "The proposed linear text segmentation where termFreqij represents the occurrences of word i in sentence j tokenContj represents the total number of words from a corpus docFreqi represents the total number of documents contain word i C SentenceSimilarity Matrix Construction In previous researches the sentencesimilarity matrix is usually constructed by computing the similarities of all pairs of sentences within a text", "The similarity of two sentences is usually computed by cosine measurement  wik w jk  Co sin es  s   k Segmentation Hierarchical Agglomerative Clustering HAC 8 is a bottomup hierarchical clustering method", "It has been successfully applied to many applications", "In TSHAC a sentence is used as the basic processing unit", "For more efficient process of linear text segmentation in the first step a text is roughly divided into several blocks of sentences by the method mentioned in the last subsection", "Each block is then regarded as an independent cluster", "In each merging stage the similarities between a cluster and its neighbors are measured by the following formula i j 2 2 ik jk k k 2    sims m  s n  sm ci sn c j  1 where si represents the ith sentence and sj represents the jth sentence in text T respectively", "As mentioned in Section II the construction of such a sentencesimilarity matrix costs Simci  c j    ci    c j   STDEV On2 where n represents the number of sentences", "To reduce the cost TSHAC computes only the similarities alongside the main diagonal according to the block size instead of the complete sentencesimilarity matrix", "At first the similarities between each sentence and its neighbors are computed", "If the similarity between two adjacent sentences is equal to 0 the sentences are divided into different blocks", "Hence text T is roughly divided into several blocks each of which is composed of consecutive sentences", "Subsequently the number of sentences in the largest block is regarded as the block size", "The similarities alongside the diagonal of the sentencesimilarity matrix are then computed according to the block size", "where Simci cj represents the similarities of cluster i and cluster j The first part of 1 represents the lexical cohesion", "The numerator is the total similarities of sentences from ci with the sentences from cj", "The denominator is used to normalize the lexical cohesion to the range 0 1", "The second part of 1 represents the standard deviation of the cluster size ie number of sentences in a cluster after ci and cj are merged into a group", "In general cases the number of sentences in each cluster is similar", "Based on this observation smaller standard deviation of the cluster size is preferred 9", "The standard deviation of the cluster size is computed by the following equation For example assume that the block size is 2 only the elements i j under i  1 2  7 and j  i  1 i  2 i  3 ie 2  block size  1 where j  7 are computed", "Each element i j in the sentencesimilarity represents the STDEV  cluster  clusterj    j 1   cluster similarity between sentences i and j ie Cosinesi sjBecause all the elements in the diagonal of the sentence similarity matrix are equal to 1 and the sentencesimilarity matrix is symmetric only the upper triangle of the sentence similarity matrix needs to be computed", "An example of the sentencesimilarity matrix is shown as Fig", "1", "The construction of the sentencesimilarity matrix is similar to TSF 7", "However in the proposed algorithm the block size is dynamically determined according to the metadata of a text instead of statically provided by the user", "where  represents the average number of sentences within clusters", "Because the sentences within a text are located in order only adjacent clusters need to be considered in each merging stage ie only Simci1 ci and Simci ci1 need to be computed", "The nearest pair of adjacent clusters is then merged into a new cluster", "The merging process is repeated until all clusters are merged into one universal cluster", "A hierarchical cluster tree is then constructed as shown in Fig", "2", "1 2 3 4 5 6 7 1 2 3 4 5 6 7 01 02 04 06 07 08 c1 c2 c3 c4 c5 c6 c7 Figure 2", "An example of a hierarchical cluster tree", "Figure 1", "Upper triangle of the sentencesimilarity matrix", "In order to divide the text into several topical segments after the hierarchical cluster tree is constructed the input data objects can be divided into several clusters each of which represents a topic by cutting the hierarchical cluster tree at a feasible height", "A fitness function is proposed to evaluate each of the potential height hi of the hierarchical cluster tree constructed as follow seg 1  dissimilarity j contain less than or equal to 2 sentences is also considered", "A lower proportion of short segments is preferred in our fitness function", "E The Proposed TSHAC Algorithm The complete process of TSHAC is summarized as Fig", "3", "IV", "PERFORMANCE EVALUATIONS To evaluate the performance of TSHAC a publicly available test corpus is adopted", "The test corpus was created f hi   j 1  seg  STDEV i   seg len 2 2  s e g by Choi 3 and has been commonly used in previous researches", "The test corpus consists of 700 samples", "A sample is a concatenation of ten text segments", "A segment is As mention in previous researches 9 when dividing a long text into several topical segments the sentences within a topical segment should cover the same subtopic", "Moreover sentences among different segments should belong to different subtopics", "Therefore both the average sentence similarity within each segment inner similarity and the average sentence similarity between two consecutive segments outer similarity are considered in the fitness function", "The inner similarity and the outer similarity are then combined as dissimilarity 7 as follows the first n sentences of a randomly selected document from the Brown corpus", "The 700 samples are divided into 4 sets according to the range of the number of sentences", "Table I shows the statistics of the test corpus n represents the number of sentences", "To compare with several wellknown text segmentation algorithms including TextTiling 5 C99 3 U00 10 TopSeg02 11 AniDiffDynProg03 6 and TSF 7 the commonly used segmentation metric proposed by Beeferman 12 is adopted sim inner  sim outer perrorref hyp k   dissimilarity j  j j sim inner  where pmissref hyp diffe rent ref s egments k  pdiff ref s egmentsre f k   p false alar mref hyp  same ref segment k  psame ref s egmentref  k    B pre  B pre     B post  B post  The aim of the segmentation metric is to compute the error probability of a randomly chosen pair of words at distance k sim inner  j j j 2 j and words apart that is inconsistently classified", "ref represents the tru e se g men tation and hy p repr e se n t s t h e prop o sed sim j   B j B j  The proposed TSHAC algorithm Input A long text", "outer  pre  post  Output Several topical segments", "Step 1", "Preprocess a text by tokenization generic stopword removal where  represents average pairwise sentence similarities of documentdependent stopword removal and stemming", "two blocks pre i represents the block of sentences that Step 2", "Deter mine block size by the method mentioned in Section IIIC", "precede the potential boundary B post represents the block Step 3", "Compute the similarities of sentences alongside the diagonal of sentences that succeed the potential boundary", "The higher dissimilarityj implies higher feasibility of the potential boundary", "On the other hand the standard deviation of the segment length as shown in 3 is also considered in the fitness function", "Because the number of sentences in different segment is usually similar 9 a lower standard deviation value of segment size is preferred when we design the fitness function", " seg  seg j   of the sentencesimilarity matrix with the block size", "Step 4", "Segment the text by the following steps Step 41", "The text is roughly divided into several groups by the method mentioned in Section IIIC and each group is regarded as an independent cluster", "Step 42", "Compute the similarities between adjacent clusters by 1", "Step 43", "Merge the pair of clusters to achieve the greatest similarity", "Step 44", "Repeat Step 42 and Step 43 until only one universal cluster left", "Step 45", "Cut the hierarchical cluster tree by the predesigned fitness function shown as 2 to achieve the highest fitness value", "STDEVi  j 1  seg 3 Figure 3", "The porposed TSHAC algorithm", "Finally a topic segment usually contains several sentences that could well describe a topic", "To avoid topic segments to be too short the number of segments which TABLE I STATISTICS OF THE TEST CORPUS segmentation", "The error probability is composed of the miss and the false alarm probabilities", "The miss probability is a conditional probability that the randomly chosen pair of words spans a segment boundary for the true segmentation but lies in the same segment for the proposed segmentation", "The false alarm probability is a conditional probability that the randomly chosen pair of words lies in the same segment for the true segmentation but spans a segment boundary for the proposed segmentation", "The lower error probability implies higher segmentation accuracy", "A Computational Complexity Analysis As mentioned above the construction of the complete similarity matrix cost On2 where n is the number of sentences in a text", "To reduce the time complexity in the proposed algorithm only the similarities alongside the diagonal are computed with the block size", "Therefore the time complexity is Onk where n represents the number of sentences and k represents the block size", "In this article the proposed linear text segmentation algorithm is based on HAC", "Unlike the general process of HAC only the similarities between each cluster and its neighbors are computed at each merging stage", "Therefore in contrast to the general HAC which takes On2 the proposed TSHAC algorithm compute only the similarities between the newly merged cluster and its two neighbors in each merging stage and hence it takes only On", "Moreover the roughly grouping of sentences into blocks will further improve the efficiency of the proposed linear text segmentation algorithm", "The actual complexity of the HAC process will be less than or equal to On", "According to the above analysis TSHAC is a linear time algorithm of linear text segmentation", "B Experimental Results Table II shows the error probability of TSHAC and several wellknown text segmentation algorithms", "Among these algorithms TextTiling TSF and TSHAC are linear time algorithms and the others are nonlinear time algorithms 7", "The error probability of the wellknown linear text segmentation algorithms are collected from the literatures 3 7", "The experimental results in Table II are reported when the number of segments is unknown in advance", "From Table II the proposed linear text segmentation algorithm ie TSHAC outperforms the linear time algorithm TextTiling and the more complex algorithms C99", "TSHAC also provides comparable results with other algorithms", "Moreover unlike TSF which requires manpower to designate parameters used TSHAC provides a fully automatic process for linear text segmentation without auxiliary knowledge base parameter setting or user involvement", "V CONCLUSION AND FUTURE WORKS The contributions of this article include 1 Propose a high performance linear text segmentation algorithm based on HAC 2 The proposed linear text segmentation algorithm TABLE II", "EXPERIMENTAL RESULTS AND COMPARISON 3 1 1 3 5 6 8 9 1 1 T e x t T i l i n g 4 6  4 4  4 3  4 8  C 9 9 1 3  1 8  1 0  1 0  U 0 0 1 1  1 3  6  6  T o p S e g 10", "74  7", "44  7", "95  6", "75  An iDi ffD yn Pro g03 6", "0  7", "1  5", "3  4", "3  T S F 9", "0  9", "3  6", "8  9", "2  T S H A C 9", "84  9", "26  8", "23  7", "54  achieves comparable segmentation accuracy with efficient processing time 3 The parameters used in the process of linear text segmentation are automatically designated without auxiliary knowledge base parameter setting or user involvement", "Therefore the proposed linear text segmentation algorithm is feasible for segmenting long texts in real time especially when user involvement is unavailable", "Furthermore the proposed algorithm not only provides topical segments but also provides hierarchical discourse structure of the text if necessary", "In the future we plan to implement the proposed linear text segmentation algorithm in some real world applications"]}, "P07-1058": {"title": ["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 456463 Prague Czech Republic June 2007 c  2007 Association for Computational Linguistics Instancebased Evaluation of Entailment Rule Acquisition Idan Szpektor Eyal Shnarch Ido Dagan Dept of Computer Science Bar Ilan University Ramat Gan Israel szpektisheydagancsbiuacil Abstract Obtaining large volumes of inference knowledge such as entailment rules has become a major factor in achieving robust semantic processing While there has been substantial research on learning algorithms for such knowledge their evaluation methodology has been problematic hindering further research We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels The methodology is used to compare two state of the art learning algorithms ex posing critical issues for future progress 1 Introduction In many NLP applications such as Question Answering QA and Information Extraction IE it is crucial to recognize that a particular target meaning can be inferred from different text variants For example a QA system needs to identify that Aspirin lowers the risk of heart attacks can be inferred from Aspirin prevents heart attacks in order to answer the question  What lowers the risk of heart attacks   This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework Dagan et al 2006 A major obstacle for further progress in semantic inference is the lack of broadscale knowledgebases for semantic variability patterns BarHaim et al 2006 One prominent type of inference knowledge representation is inference rules such as paraphrases and entailment rules We define an entailment rule to be a directional relation between two templates text patterns with variables eg X prevent Y  X lower the risk of Y   The lefthandside template is assumed to entail the righthand side template in certain contexts under the same variable instantiation Paraphrases can be viewed as bidirectional entailment rules Such rules capture basic inferences and are used as building blocks for more complex entailment inference For example given the above rule the answer  Aspirin can be identified in the example above The need for largescale inference knowledgebases triggered extensive research on automatic acquisition of paraphrase and entailment rules Yet the current precision of acquisition algorithms is typically still mediocre as illustrated in Table 1 for DIRT Lin and Pantel 2001 and TEASE Szpektor et al 2004 two prominent acquisition algorithms whose outputs are publicly available The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research The prominent approach in the literature for evaluating rules termed here the rulebased approach is to present the rules to human judges asking whether each rule is correct or not However it is difficult to explicitly define when a learned rule should be considered correct under this methodology and this was mainly left undefined in previous works As the criterion for evaluating a rule is not well defined using this approach often caused low agreement between human judges Indeed the standards for evaluation in this field are lower than other fields many papers 456 dont report on human agreement at all and those that do report rather low agreement levels Yet it is crucial to reliably assess rule correctness in order to measure and compare the performance of different algorithms in a replicable manner Lacking a good evaluation methodology has become a barrier for further advances in the field In order to provide a welldefined evaluation methodology we first explicitly specify when entailment rules should be considered correct following the spirit of their usage in applications We then propose a new instancebased evaluation approach Under this scheme judges are not presented only with the rule but rather with a sample of sentences that match its left hand side The judges then assess whether the rule holds under each specific example A rule is considered correct only if the percentage of examples assessed as correct is sufficiently high We have experimented with a sample of input verbs for both DIRT and TEASE Our results show significant improvement in human agreement over the rulebased approach It is also the first comparison between such two stateoftheart algorithms which showed that they are comparable in precision but largely complementary in their coverage Additionally the evaluation showed that both algorithms learn mostly one directional rules rather than symmetric paraphrases While most NLP ap plications need directional inference previous acquisition works typically expected that the learned rules would be paraphrases Under such an expectation unidirectional rules were assessed as incorrect underestimating the true potential of these algorithms In addition we observed that many learned rules are context sensitive stressing the need to learn contextual constraints for rule applications 2 Background Entailment Rules and their Evaluation"], "abstract": ["Obtaining large volumes of inference knowledge such as entailment rules has become a major factor in achieving robust semantic processing", "While there has been substantial research on learning algorithms for such knowledge their evaluation methodology has been problematic hindering further research", "We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels", "The methodology is used to compare two state of the art learning algorithms exposing critical issues for future progress"], "inroduction": ["In many NLP applications such as Question Answering QA and Information Extraction IE it is crucial to recognize that a particular target meaning can be inferred from different text variants", "For example a QA system needs to identify that Aspirin lowers the risk of heart attacks can be inferred from Aspirin prevents heart attacks in order to answer the question What lowers the risk of heart attacks", "This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework Dagan et al 2006", "A major obstacle for further progress in semantic inference is the lack of broadscale knowledge bases for semantic variability patterns BarHaim et al 2006", "One prominent type of inference knowledge representation is inference rules such as para 456 phrases and entailment rules", "We define an entail ment rule to be a directional relation between two templates text patterns with variables eg X prevent Y  X lower the risk of Y ", "The lefthand side template is assumed to entail the righthand side template in certain contexts under the same variable instantiation", "Paraphrases can be viewed as bidirectional entailment rules", "Such rules capture basic inferences and are used as building blocks for more complex entailment inference", "For example given the above rule the answer Aspirin can be identified in the example above", "The need for largescale inference knowledge bases triggered extensive research on automatic acquisition of paraphrase and entailment rules", "Yet the current precision of acquisition algorithms is typically still mediocre as illustrated in Table 1 for DIRT Lin and Pantel 2001 and TEASE Szpek tor et al 2004 two prominent acquisition algorithms whose outputs are publicly available", "The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research", "The prominent approach in the literature for evaluating rules termed here the rulebased approach is to present the rules to human judges asking whether each rule is correct or not", "However it is difficult to explicitly define when a learned rule should be considered correct under this methodology and this was mainly left undefined in previous works", "As the criterion for evaluating a rule is not well defined using this approach often caused low agreement between human judges", "Indeed the standards for evaluation in this field are lower than other fields many papers Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 456463 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics dont report on human agreement at all and those that do report rather low agreement levels", "Yet it is crucial to reliably assess rule correctness in order to measure and compare the performance of different algorithms in a replicable manner", "Lacking a good evaluation methodology has become a barrier for further advances in the field", "In order to provide a welldefined evaluation methodology we first explicitly specify when entail ment rules should be considered correct following the spirit of their usage in applications", "We then propose a new instancebased evaluation approach", "Under this scheme judges are not presented only with the rule but rather with a sample of sentences that match its left hand side", "The judges then assess whether the rule holds under each specific example", "A rule is considered correct only if the percentage of examples assessed as correct is sufficiently high", "We have experimented with a sample of input verbs for both DIRT and TEASE", "Our results show significant improvement in human agreement over the rulebased approach", "It is also the first comparison between such two stateoftheart algorithms which showed that they are comparable in precision but largely complementary in their coverage", "Additionally the evaluation showed that both algorithms learn mostly onedirectional rules rather than symmetric paraphrases", "While most NLP applications need directional inference previous acquisition works typically expected that the learned rules would be paraphrases", "Under such an expectation unidirectional rules were assessed as incorrect underestimating the true potential of these algorithms", "In addition we observed that many learned rules are context sensitive stressing the need to learn contextual constraints for rule applications"]}, "P07-1061": {"title": ["Finding document topics for improving topic segmentation"], "abstract": ["Topic segmentation and identification are often tackled as separate problems whereas they are both part of topic analysis", "In this article we study how topic identification can help to improve a topic segmenter based on word reiteration", "We first present an unsupervised method for discovering the topics of a text", "Then we detail how these topics are used by segmentation for finding topical similarities between text segments", "Finally we show through the results of an evaluation done both for French and English the interest of the method we propose"], "inroduction": ["In this article we address the problem of linear topic segmentation which consists in segmenting documents into topically homogeneous segments that does not overlap each other", "This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as Hearst 1994", "One criterion for classifying topic segmentation systems is the kind of knowledge they depend on", "Most of them only rely on surface features of documents word reiteration in Hearst 1994 Choi 2000 Utiyama and Isahara 2001 Galley et al 2003 or discourse cues in Passonneau and Lit man 1997 Galley et al 2003", "As such systems do not require external knowledge they are not sensitive to domains but they are limited by the type of documents they can be applied to lexical reiteration is reliable only if concepts are not too frequently ex 480 pressed by several means synonyms etc and discourse cues are often rare and corpusspecific", "To overcome these difficulties some systems make use of domainindependent knowledge about lexical cohesion a lexical network built from a dictionary in Kozima 1993 a thesaurus in Morris and Hirst 1991 a large set of lexical co occurrences collected from a corpus in Choi et al 2001", "To a certain extent these lexical networks enable topic segmenters to exploit a sort of concept reiteration", "However their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high", "The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents", "Such topic models are generally built from a large set of example documents as in Yam ron et al 1998 Blei and Moreno 2001 or in one component of Beeferman et al 1999", "These statistical topic models enable segmenters to improve their precision but they also restrict their scope", "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination Job bins and Evett 1998 combined word recurrence cooccurrences and a thesaurus Beeferman et al 1999 relied on both lexical modeling and discourse cues Galley et al 2003 made use of word reiteration through lexical chains and discourse cues", "The work we report in this article takes place in the first category we have presented", "It does not rely on any a priori knowledge and exploits word usage rather than discourse cues", "More precisely we present a new method for enhancing the results Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 480487 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics of segmentation systems based on word reiteration without relying on any external knowledge"]}, "P07-1066": {"title": ["BilingualLSA Based LM Adaptation for Spoken Language Translation"], "abstract": ["We propose a novel approach to crosslingual language model LM adaptation based on bilingual Latent Semantic Analysis bLSA", "A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a onetoone topic correspondence during training", "Using the proposed bLSA framework crosslingual LM adaptation can be performed by first inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language Ngram LM via marginal adaptation", "The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language", "On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27 for a unigram LM and up to 136 for a 4gram LM", "Furthermore the proposed approach consistently improved machine translation quality on both speech and text based adaptation"], "inroduction": ["Language model adaptation is crucial to numerous speech and translation tasks as it enables higher level contextual information to be effectively incorporated into a background LM improving recognition or translation performance", "One approach is 520 to employ Latent Semantic Analysis LSA to capture indomain word unigram distributions which are then integrated into the background Ngram LM", "This approach has been successfully applied in automatic speech recognition ASR Tam and Schultz 2006 using the Latent Dirichlet Allocation LDA Blei et al 2003", "The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution", "For LM adaptation the topic mixture weights are estimated based on indomain adaptation text eg ASR hypotheses", "The adapted mixture weights are then used to interpolate a topic dependent unigram LM which is finally integrated into the background Ngram LM using marginal adaptation Kneser et al 1997 In this paper we propose a framework to perform LM adaptation across languages enabling the adaptation of a LM from one language based on the adaptation text of another language", "In statistical machine translation SMT one approach is to apply LM adaptation on the target language based on an initial translation of input references Kim and Khudanpur 2003 Paulik et al 2005", "This scheme is limited by the coverage of the translation model and overall by the quality of translation", "Since this approach only allows to apply LM adaptation after translation available knowledge cannot be applied to extend the coverage", "We propose a bilingual LSA model bLSA for crosslingual LM adaptation that can be applied before translation", "The bLSA model consists of two LSA models one for each side of the language trained on parallel document corpora", "The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 520527 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics the latent topic of the source and target LSA models can be assumed to be a onetoone correspondence and thus share a common latent topic space since the training corpora consist of bilingual parallel data", "For instance say topic 10 of the Chinese LSA model is about politics", "Then topic 10 of the English LSA model is set to also correspond to politics and so forth", "During LM adaptation we first infer the topic mixture weights from the source text using the source LSA model", "Then we transfer the inferred mixture weights to the target LSA model and thus obtain the target LSA marginals", "The challenge is to enforce the onetoone topic correspon dence", "Our proposal is to share common variational ASR hypo Chinese ASR ChineseEnglish SMT Chinese Ngram LM English Ngram LM Adapt Adapt Topic distribution Chinese LSA English LSA Chinese text English text ChineseEnglish Parallel document corpus MT hypo Dirichlet posteriors over the topic mixture weights of a document pair in the LDAstyle model", "The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion rather than to require manual interaction", "Since the topic space is language independent our approach supports topic transfer in multiple language pairs in ON where N is the number of languages", "Related work includes the Bilingual Topic Admixture Model BiTAM for word alignment proposed by Zhao and Xing 2006", "Basically the BiTAM model consists of topicdependent transla tion lexicons modeling P rce k where c e and k denotes the source Chinese word target English word and the topic index respectively", "On the other hand the bLSA framework models P rck and P rek which is different from the BiTAM model", "By their different modeling nature the bLSA model usually supports more topics than the BiTAM model", "Another work by Kim and Khudanpur 2004 employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection", "We organize the paper as follows In Section 2 we introduce the bLSA framework including Latent DirichletTree Allocation LDTA Tam and Schultz 2007 as a correlated LSA model bLSA training and crosslingual LM adaptation", "In Section 3 we present the effect of LM adaptation on word perplexity followed by SMT experiments reported in BLEU on both speech and text input in Section 33", "Section 4 describes conclusions and fu Figure 1 Topic transfer in bilingual LSA model", "ture works"]}, "P07-1067": {"title": ["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 528535"], "abstract": ["Semantic relatedness is a very important factor for the coreference resolution task", "To obtain this semantic information corpus based approaches commonly leverage patterns that can express a specific semantic relation", "The patterns however are designed manually and thus are not necessarily the most effective ones in terms of accuracy and breadth", "To deal with this problem in this paper we propose an approach that can automatically find the effective patterns for coreference resolution", "We explore how to automatically discover and evaluate patterns and how to exploit the patterns to obtain the semantic relatedness information", "The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution"], "inroduction": ["Semantic relatedness is a very important factor for coreference resolution as noun phrases used to refer to the same entity should have a certain semantic relation", "To obtain this semantic information previous work on reference resolution usually leverages a semantic lexicon like WordNet Vieira and Poesio 2000 Harabagiu et al 2001 Soon et al 2001 Ng and Cardie 2002", "However the drawback of WordNet is that many expressions especially for proper names word senses and semantic relations are not available from the database Vieira and Poesio 2000", "In recent years increasing interest has 528 been seen in mining semantic relations from large text corpora", "One common solution is to utilize a pattern that can represent a specific semantic relation eg X such as Y  for isa relation and X and other Y  for otherrelation", "Instantiated with two given noun phrases the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness Markert et al 2003 Modjeska et al 2003 Poesio et al 2004", "However in the previous pattern based approaches the selection of the patterns to represent a specific semantic relation is done in an ad hoc way usually by linguistic intuition", "The manually selected patterns nevertheless are not necessarily the most effective ones for coreference resolution from the following two concerns  Accuracy", "Can the patterns eg X such as Y  find as many NP pairs of the specific semantic relation eg isa as possible with a high precision", " Breadth", "Can the patterns cover a wide variety of semantic relations not just isa by which coreference relationship is realized", "For example in some annotation schemes like ACE BeijingChina are coreferential as the capital and the country could be used to represent the government", "The pattern for the common is a relation will fail to identify the NP pairs of such a capitalcountry relation", "To deal with this problem in this paper we propose an approach which can automatically discover effective patterns to represent the semantic relations Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 528535 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics for coreference resolution", "We explore two issues in our study 1 How to automatically acquire and evaluate the patterns", "We utilize a set of coreferential NP pairs as seeds", "For each seed pair we search a large corpus for the texts where the two noun phrases co occur and collect the surrounding words as the surface patterns", "We evaluate a pattern based on its commonality or association with the positive seed pairs", "2 How to mine the patterns to obtain the semantic relatedness information for coreference resolution", "We present two strategies to exploit the patterns choosing the top best patterns as a set of pattern features or computing the reliability of semantic relatedness as a single feature", "In either strategy the obtained features are applied to do coreference resolution in a supervisedlearning way", "To our knowledge our work is the first effort that systematically explores these issues in the coreference resolution task", "We evaluate our approach on ACE data set", "The experimental results show that the pattern based semantic relatedness information is helpful for the coreference resolution", "The remainder of the paper is organized as follows", "Section 2 gives some related work", "Section 3 introduces the framework for coreference resolution", "Section 4 presents the model to obtain the pattern based semantic relatedness information", "Section 5 discusses the experimental results", "Finally Section 6 summarizes the conclusions"]}, "P07-1068": {"title": ["Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 536543"], "abstract": ["This paper examines whether a learning based coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the Penn Treebank in which the noun phrases are labeled with their semantic classes", "Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2 in Fmeasure over one that exploits heuristically computed semantic class knowledge", "In addition the induced knowledge improves the accuracy of common noun resolution by 26"], "inroduction": ["In the past decade knowledgelean approaches have significantly influenced research in noun phrase NP coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document", "In knowledgelean approaches coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process eg Mitkov 1998 Tetreault 2001", "While these approaches have been reasonably successful see Mitkov 2002 Kehler et al", "2004 speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance", "In fact semantics plays a crucially important role in the resolution of common NPs allowing us to identify the coreference relation between two lexically dissimilar common nouns eg talks 536 and negotiations and to eliminate George W Bush from the list of candidate antecedents of the city for instance", "As a result researchers have readopted the oncepopular knowledgerich approach investigating a variety of semantic knowledge sources for common noun resolution such as the semantic relations between two NPs eg Ji et al", "2005 their semantic similarity as computed using WordNet eg Poesio et al", "2004 or Wikipedia Ponzetto and Strube 2006 and the contextual role played by an NP see Bean and Riloff 2004", "Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class SC of an NP which can be used to disallow coreference between semantically incompatible NPs", "However learningbased resolvers have not been able to benefit from having an SC agreement feature presumably because the method used to compute the SC of an NP is too simplistic while the SC of a proper name is computed fairly accurately using a named entity NE recognizer many resolvers simply assign to a common noun the first ie most frequent WordNet sense as its SC eg Soon et al", "2001 Markert and Nissim 2005", "It is not easy to measure the accuracy of this heuristic but the fact that the SC agreement feature is not used by Soon et als decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this firstsense heuristic", "Motivated in part by this observation we examine whether automatically induced semantic class knowledge can improve the performance of a learningbased coreference resolver reporting evaluation results on the commonlyused ACE corefer Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 536543 Prague Czech Republic June 2007", "Qc 2007 Association for Computational Linguistics ence corpus", "Our investigation proceeds as follows", "Train a classifier for labeling the SC of an NP", "In ACE we are primarily concerned with classifying an NP as belonging to one of the ACE semantic classes", "For instance part of the ACE Phase 2 evaluation involves classifying an NP as PER SO N O R G A N IZATIO N G PE a geographicalpolitical region FAC ILITY LO C ATIO N or OTH ER S We adopt a corpusbased approach to SC determination recasting the problem as a sixclass classification task", "Derive two knowledge sources for coreference resolution from the induced SCs", "The first knowledge source KS is semantic class agreement SCA", "Following Soon et al", "2001 we represent SCA as a binary value that indicates whether the induced SCs of the two NPs involved are the same or not", "The second KS is mention which is represented as a binary value that indicates whether an NP belongs to one of the five ACE SCs mentioned above", "Hence the mention value of an NP can be readily derived from its induced SC the value is N O if its SC is OTH ER S and Y ES otherwise", "This KS could be useful for ACE coreference since ACE is concerned with resolving only NPs that are mentions", "Incorporate the two knowledge sources in a coreference resolver", "Next we investigate whether these two KSs can improve a learningbased baseline resolver that employs a fairly standard feature set", "Since 1 the two KSs can each be represented in the resolver as a constraint for filtering nonmentions or disallowing coreference between semantically incompatible NPs or as a feature and 2 they can be applied to the resolver in isolation or in combination we have eight ways of incorporating these KSs into the baseline resolver", "In our experiments on the ACE Phase 2 coreference corpus we found that 1 our SC induction method yields a significant improvement of 2 in accuracy over Soon et als firstsense heuristic method as described above 2 the coreference resolver that incorporates our induced SC knowledge by means of the two KSs mentioned above yields a significant improvement of 2 in Fmeasure over the resolver that exploits the SC knowledge computed by Soon et als method 3 the mention KS when used in the baseline resolver as a constraint improves the resolver by approximately 57 in F measure and 4 SCA when employed as a feature by the baseline resolver improves the accuracy of common noun resolution by about 58"]}, "P07-2007": {"title": ["Proceedings of the ACL 2007 Demo and Poster Sessions pages 25 28 Prague June 2007 c  2007 Association for Computational Linguistics"], "abstract": ["This paper describes the latest version of speechtospeech translation systems developed by the team of NICTATR for over twenty years", "The system is now ready to be deployed for the travel domain", "A new noisesuppression technique notably improves speech recognition performance", "Corpusbased approaches of recognition translation and synthesis enable coverage of a wide variety of topics and portability to other languages"], "inroduction": ["Speech recognition speech synthesis and machine translation research started about half a century ago", "They have developed independently for a long time until speechtospeech translation research was proposed in the 1980s The feasibility of speechtospeech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult", "After groundbreaking work for two decades corpusbased speech and language processing technology have recently enabled the achievement of speechtospeech translation that is usable in the real world", "This paper introduces at ACL 2007 the state oftheart speechtospeech translation system developed by NICTATR Japan"]}, "P07-3016": {"title": ["Clustering Hungarian Verbs on the Basis of Complementation Patterns"], "abstract": ["Our paper reports an attempt to apply an unsupervised clustering algorithm to a Hungarian treebank in order to obtain semantic verb classes", "Starting from the hypothesis that semantic metapredicates underlie verbs syntactic realization we investigate how one can obtain semantically motivated verb classes by automatic means", "The 150 most frequent Hungarian verbs were clustered on the basis of their complementation patterns yielding a set of basic classes and hints about the features that determine verbal subcategorization", "The resulting classes serve as a basis for the subsequent analysis of their alternation behavior"], "inroduction": ["For over a decade automatic construction of wide coverage structured lexicons has been in the center of interest in the natural language processing community", "On the one hand structured lexical databases are easier to handle and to expand because they allow making generalizations over classes of words", "On the other hand interest in the automatic acquisition of lexical information from corpora is due to the fact that manual construction of such resources is timeconsuming and the resulting database is difficult to update", "Most of the work in the field of acquisition of verbal lexical properties aims at learning subcategorization frames from corpora eg", "Pereira et al 1993 Briscoe and Carroll 1997 Sass 2006", "However semantic group ing of verbs on the basis of their syntactic distribution or other quantifiable features has also gained attention Schulte im Walde 2000 Schulte im Walde and Brew 2002 Merlo and Stevenson 2001 Dorr and Jones 1996", "The goal of these investigations is either the validation of verb classes based on Levin 1993 or finding algorithms for the categorization of new verbs", "Unlike these projects we report an attempt to cluster verbs on the basis of their syntactic properties with the further goal of identifying the semantic classes relevant for the description of Hungarian verbs alternation behavior", "The theoretical grounding of our clustering attempts is provided by the socalled Semantic Base Hypothesis Levin 1993 Koenig et al 2003", "It is founded on the observation that semantically similar verbs tend to occur in similar syntactic contexts leading to the assumption that verbal semantics determines argument structure and the surface realization of arguments", "While in English semantic argument roles are mapped to configurational positions in the tree structure Hungarian codes complement structure in its highly rich nominal inflection system", "Therefore we start from the examination of casemarked NPs in the context of verbs", "The experiment discussed in this paper is the first stage of an ongoing project for finding the semantic verb classes which are syntactically relevant in Hungarian", "As we do not have presuppositions about which classes have to be used we chose an unsupervised clustering method described in Schulte im Walde 2000", "The 150 most frequent Hungarian verbs were categorized according to their comp 91 Proceedings of the ACL 2007 Student Research Workshop pages 9196 Prague June 2007", "Qc 2007 Association for Computational Linguistics lementation structures in a syntactically annotated corpus the Szeged Treebank Csendes et al 2005", "We are seeking the answer to two questions 1", "Are the resulting clusters semantically coherent", "thus reinforcing the Semantic Base Hypothesis"]}, "P08-1078": {"title": ["Proceedings of ACL08 HLT pages 683691 Columbus Ohio USA June 2008 c  2008 Association for Computational Linguistics Contextual Preferences Idan Szpektor Ido Dagan Roy BarHaim Department of Computer Science BarIlan University Ramat Gan Israel szpektidaganbarhaircsbiuacil Jacob Goldberger School of Engineering BarIlan University Ramat Gan Israel goldbejengbiuacil Abstract The validity of semantic inferences depends on the contexts in which they are applied We propose a generic framework for handling contextual considerations within applied inference termed Contextual Preferences This framework defines the various context aware components needed for inference and their relationships Contextual preferences extend and generalize previous notions such as selectional preferences while experiments show that the extended framework allows improving inference quality on real application data 1 Introduction Applied semantic inference is typically concerned with inferring a target meaning from a given text For example to answer Who wrote Idomeneo   Question Answering QA systems need to infer the target meaning  Mozart wrote Idomeneo from a given text Mozart composed Idomeneo Following common Textual Entailment terminology Giampiccolo et al 2007 we denote the target meaning by h for hypothesis and the given text by t A typical applied inference operation is matching Sometimes h can be directly matched in t in the example above if the given sentence would be literally Mozart wrote Idomeneo Generally the target meaning can be expressed in t in many different ways Indirect matching is then needed using inference knowledge that may be captured through rules termed here entailment rules In our example Mozart wrote Idomeneo can be inferred using the rule  X compose Y  X write Y   Recently several algorithms were proposed for automatically learning entailment rules and paraphrases viewed as bidirectional entailment rules Lin and Pantel 2001 Ravichandran and Hovy 2002 Shinyama et al 2002 Szpektor et al 2004 Sekine 2005 A common practice is to try matching the structure of h or of the lefthand side of a rule r within t However context should be considered to allow valid matching For example suppose that to find acquisitions of companies we specify the target template hypothesis a hypothesis with variables X acquire Y   This h should not be matched in  children acquire language quickly  because in this context Y is not a company Similarly the rule X charge Y  X accuse Y  should not be applied to This store charged my account  since the assumed sense of  charge in the rule is different than its sense in the text Thus the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference Context matching at inference time was often approached in an application specific manner Harabagiu et al 2003 Patwardhan and Riloff 2007 Recently some generic methods were proposed to handle context sensitive inference Dagan et al 2006 Pantel et al 2007 Downey et al 2007 Connor and Roth 2007 but these usually treat only a single aspect of context matching see Section 6 We propose a comprehensive framework for handling various contextual considerations termed Contextual Preferences It extends and generalizes previous work defining the needed contextual components and their relationships We also present and implement concrete representation models and un683 supervised matching methods for these components While our presentation focuses on semantic inference using lexicalsyntactic structures the proposed framework and models seem suitable for other common types of representations as well We applied our models to a test set derived from the ACE 2005 event detection task a standard Information Extraction IE benchmark We show the benefits of our extended framework for textual inference and present component wise analysis of the results To the best of our knowledge these are also the first unsupervised results for event argument extraction in the ACE 2005 dataset 2 Contextual Preferences"], "abstract": ["The validity of semantic inferences depends on the contexts in which they are applied", "We propose a generic framework for handling contextual considerations within applied inference termed Contextual Preferences", "This framework defines the various contextaware components needed for inference and their relationships", "Contextual preferences extend and generalize previous notions such as selectional preferences while experiments show that the extended framework allows improving inference quality on real application data"], "inroduction": ["Applied semantic inference is typically concerned with inferring a target meaning from a given text", "For example to answer Who wrote Idomeneo Question Answering QA systems need to infer the target meaning Mozart wrote Idomeneo from a given text Mozart composed Idomeneo", "Following common Textual Entailment terminology Giampiccolo et al 2007 we denote the target meaning by h for hypothesis and the given text by t A typical applied inference operation is matching", "Sometimes h can be directly matched in t in the example above if the given sentence would be literally Mozart wrote Idomeneo", "Generally the target meaning can be expressed in t in many different ways", "Indirect matching is then needed using inference knowledge that may be captured through rules termed here entailment rules", "In our example Mozart wrote Idomeneo can be inferred using the rule X compose Y  X write Y ", "Recently several algorithms were proposed for automatically learning entailment rules and paraphrases viewed as bidirectional entailment rules Lin and Pantel 2001 Ravichandran and Hovy 2002 Shinyama et al 2002 Szpektor et al 2004 Sekine 2005", "A common practice is to try matching the structure of h or of the lefthandside of a rule r within t However context should be considered to allow valid matching", "For example suppose that to find acquisitions of companies we specify the target template hypothesis a hypothesis with variables X acquire Y ", "This h should not be matched in children acquire language quickly because in this context Y is not a company", "Similarly the rule X charge Y  X accuse Y  should not be applied to This store charged my account since the assumed sense of charge in the rule is different than its sense in the text", "Thus the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference", "Context matching at inference time was often approached in an applicationspecific manner Harabagiu et al 2003 Patwardhan and Riloff 2007", "Recently some generic methods were proposed to handle contextsensitive inference Dagan et al 2006 Pantel et al 2007 Downey et al 2007 Connor and Roth 2007 but these usually treat only a single aspect of context matching see Section 6", "We propose a comprehensive framework for handling various contextual considerations termed Contextual Preferences", "It extends and generalizes previous work defining the needed contextual components and their relationships", "We also present and implement concrete representation models and un 683 Proceedings of ACL08 HLT pages 683691 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics supervised matching methods for these components", "While our presentation focuses on semantic inference using lexicalsyntactic structures the proposed framework and models seem suitable for other common types of representations as well", "We applied our models to a test set derived from the ACE 2005 event detection task a standard Information Extraction IE benchmark", "We show the benefits of our extended framework for textual inference and present componentwise analysis of the results", "To the best of our knowledge these are also the first unsupervised results for event argument extraction in the ACE 2005 dataset"]}, "P08-1090": {"title": ["Proceedings of ACL08 HLT pages 789797"], "abstract": ["Handcoded scripts were used in the 197080s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge", "We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text", "A narrative event chain is a partially ordered set of events related by a common protagonist", "We describe a three step process to learning narrative event chains", "The first uses unsupervised distributional methods to learn narrative relations between events sharing corefer ring arguments", "The second applies a temporal classifier to partially order the connected events", "Finally the third prunes and clusters selfcontained chains from the space of events", "We introduce two evaluations the narrative cloze to evaluate event relatedness and an order coherence task to evaluate narrative order", "We show a 36 improvement over baseline for narrative prediction and 25 for temporal coherence"], "inroduction": ["This paper induces a new representation of structured knowledge called narrative event chains or narrative chains", "Narrative chains are partially ordered sets of events centered around a common protagonist", "They are related to structured sequences of participants and events that have been called scripts Schank and Abelson 1977 or Fillmorean frames", "These participants and events can be filled in and instantiated in a particular text situation to draw inferences", "Chains focus on a single actor to facili tate learning and thus this paper addresses the three tasks of chain induction narrative event induction temporal ordering of events and structured selection pruning the event space into discrete sets", "Learning these prototypical schematic sequences of events is important for rich understanding of text", "Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization coreference resolution and question answering", "For example Schank and Abelson 1977 proposed that understanding text about restaurants required knowledge about the Restaurant Script including the participants Customer Waiter Cook Tables etc the events constituting the script entering sitting down asking for menus etc and the various preconditions ordering and results of each of the constituent actions", "Consider these two distinct narrative chains", "accused X W joined X claimed W served X argued W oversaw dismissed X W resigned It would be useful for question answering or textual entailment to know that X denied  is also a likely event in the left chain while  replaces W temporally follows the right", "Narrative chains such as Firing of Employee or Executive Resigns offer the structure and power to directly infer these new subevents by providing critical background knowledge", "In part due to its complexity automatic induction has not been addressed since the early non statistical work of Mooney and DeJong 1985", "The first step to narrative induction uses an entity based model for learning narrative relations by fol 789 Proceedings of ACL08 HLT pages 789797 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics lowing a protagonist", "As a narrative progresses through a series of events each event is characterized by the grammatical role played by the protagonist and by the protagonists shared connection to surrounding events", "Our algorithm is an unsupervised distributional learning approach that uses core ferring arguments as evidence of a narrative relation", "We show using a new evaluation task called narrative cloze that our protagonistbased method leads to better induction than a verbonly approach", "The next step is to order events in the same narrative chain", "We apply work in the area of temporal classification to create partial orders of our learned events", "We show using a coherencebased evaluation of temporal ordering that our partial orders lead to better coherence judgements of real narrative instances extracted from documents", "Finally the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains"]}, "P08-2021": {"title": [""], "abstract": ["Given several systems automatic translations of the same sentence we show how to combine them into a confusion network whose various paths represent composite translations that could be considered in a subsequent rescoring step", "We build our confusion networks using the method of Rosti et al", "2007 but instead of forming alignments using the tercom script Snover et al 2006 we create alignments that minimize invWER Leusch et al 2003 a form of edit distance that permits properly nested block movements of substrings", "Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better in terms of BLEU and TER than those in tercombased confusion networks"], "inroduction": ["Large improvements in machine translation MT may result from combining different approaches to MT with mutually complementary strengths", "Systemlevel combination of translation outputs is a promising path towards such improvements", "Yet there are some significant hurdles in this path", "One must somehow align the multiple outputsto identify where different hypotheses reinforce each other and where they offer alternatives", "One must then This work was partially supported by the DARPA GALE program Contract No HR00110620001", "Also we would like to thank the IBM Rosetta team for the availability of several MT system outputs", "use this alignment to hypothesize a set of new composite translations and select the best composite hypothesis from this set", "The alignment step is difficult because different MT approaches usually reorder the translated words differently", "Training the selection step is difficult because identifying the best hypothesis relative to a known reference translation means scoring all the composite hypotheses of which there may be exponentially many", "Most MT combination methods do create an exponentially large hypothesis set representing it as a confusion network of strings in the target language eg English", "A confusion network is a lattice where every node is on every path ie each time step presents an independent choice among several phrases", "Note that our contributions in this paper could be applied to arbitrary lattice topologies", "For example Bangalore et al", "2001 show how to build a confusion network following a multistring alignment procedure of several MT outputs", "The procedure used primarily in biology Thompson et al 1994 yields monotone alignments that minimize the number of insertions deletions and substitutions", "Unfortunately monotone alignments are often poor since machine translations particularly from different models can vary significantly in their word order", "Thus when Matusov et al", "2006 use this procedure they deterministically reorder each translation prior to the monotone alignment", "The procedure described by Rosti et al", "2007 has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate TER to guide the alignment", "TER is defined as the minimum number of inser 81 Proceedings of ACL08 HLT Short Papers Companion Volume pages 8184 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics tions deletions substitutions and block shifts between two strings", "A remarkable feature of that procedure is that it performs the alignment of the output translations i without any knowledge of the translation model used to generate the translations and ii without any knowledge of how the target words in each translation align back to the source words", "In fact it only requires a procedure for creating pairwise alignments of translations that allow appropriate reorderings", "For this Rosti et al", "2007 use the tercom script Snover et al 2006 which uses a number of heuristics as well as dynamic programming for finding a sequence of edits insertions deletions substitutions and block shifts that convert an input string to another", "In this paper we show that one can build better confusion networks in terms of the best translation possible from the confusion network when the pairwise alignments are computed not by tercom which approximately minimizes TER but instead by an exact minimization of invWER Leusch et al 2003 which is a restricted version of TER that permits only properly nested sets of block shifts and can be computed in polynomial time", "The paper is organized as follows a summary of TER tercom and invWER is presented in Section"]}, "P09-1045": {"title": ["Reducing semantic drift with bagging and distributional similarity"], "abstract": ["Iterative bootstrapping algorithms are typically compared using a single set of handpicked seeds", "However we demonstrate that performance varies greatly depending on these seeds and favourable seeds for one algorithm can perform very poorly with others making comparisons unreliable", "We exploit this wide variation with bagging sampling from automatically extracted seeds to reduce semantic drift", "However semantic drift still occurs in later iterations", "We propose an integrated distributional similarity filter to identify and censor potential semantic drifts ensuring over 10 higher precision when extracting large semantic lexicons"], "inroduction": ["Iterative bootstrapping algorithms have been proposed to extract semantic lexicons for N L P tasks with limited linguistic resources", "Bootstrapping was initially proposed by Riloff and Jones 1999 and has since been successfully applied to extracting general semantic lexicons Riloff and Jones 1999 Thelen and Riloff 2002 biomedical entities Yu and Agichtein 2003 facts Pasca et al 2006 and coreference data Yang and Su 2007", "Bootstrapping approaches are attractive because they are domain and language independent require minimal linguistic preprocessing and can be applied to raw text and are efficient enough for terascale extraction Pasca et al 2006", "Bootstrapping is minimally supervised as it is initialised with a small number of seed instances of the information to extract", "For semantic lexicons these seeds are terms from the category of interest", "The seeds identify contextual patterns that express a particular semantic category which in turn recognise new terms Riloff and Jones 1999", "Unfortunately semantic drift often occurs when ambiguous or erroneous terms andor patterns are introduced into and then dominate the iterative process Curran et al 2007", "Bootstrapping algorithms are typically compared using only a single set of handpicked seeds", "We first show that different seeds cause these algorithms to generate diverse lexicons which vary greatly in precision", "This makes evaluation unreliable  seeds which perform well on one algorithm can perform surprisingly poorly on another", "In fact random goldstandard seeds often outperform seeds carefully chosen by domain experts", "Our second contribution exploits this diversity we have identified", "We present an unsupervised bagging algorithm which samples from the extracted lexicon rather than relying on existing gazetteers or handselected seeds", "Each sample is then fed back as seeds to the bootstrapper and the results combined using voting", "This both improves the precision of the lexicon and the robustness of the algorithms to the choice of initial seeds", "Unfortunately semantic drift still dominates in later iterations since erroneous extracted terms andor patterns eventually shift the categorys direction", "Our third contribution focuses on detecting and censoring the terms introduced by semantic drift", "We integrate a distributional similarity filter directly into W M E B McIntosh and Curran 2008", "This filter judges whether a new term is more similar to the earlier or most recently extracted terms a sign of potential semantic drift", "We demonstrate these methods for extracting biomedical semantic lexicons using two bootstrap ping algorithms", "Our unsupervised bagging approach outperforms carefully handpicked seeds by  10 in later iterations", "Our distributionalsimilarity filter gives a similar performance im provement", "This allows us to produce large lexicons accurately and efficiently for domainspecific language processing", "396 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 396404 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP"]}, "P09-1068": {"title": ["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 602610"], "abstract": ["We describe an unsupervised system for learning narrative schemas coherent sequences or sets of events arrestedPOLICESUSPECT convicted JUDGE SUSPECT whose arguments are filled with participant semantic roles defined over words JUDGE  judge jury court POLICE  police agent authorities", "Unlike most previous work inevent structure or semantic role learning our system does not use supervised techniques handbuilt knowledge or predefined classes of events or roles", "Our unsupervised learning algorithm uses corefer ring arguments in chains of verbs to learn both rich narrative event structure and argument roles", "By jointly addressing both tasks we improve on previous results in narrativeframe learning and induce rich framespecific semantic roles"], "inroduction": ["be learned", "Even unsupervised attempts to learn semantic roles have required a pre defined set of roles Grenager and Manning 2006 and often a handlabeled seed corpus Swier and Stevenson 2004 He and Gildea 2006", "In this paper we describe our attempts to learn scriptlike information about the world including both event structures and the roles of their participants but without pre defined frames roles or tagged corpora", "Consider the following Narrative Schema to be defined more formally later", "The events on the left follow a set of participants through a series of connected events that constitute a narrative Events Roles This paper describes a new approach to event semantics that jointly learns event relations and their A search B A arrest B B plead C A  Police B  Suspect C  Plea participants from unlabeled corpora", "The early years of natural language processing NLP took a topdown approach to language D acquit B D convict B D sentence B D  Jury understanding using representations like scripts Schank and Abelson 1977 structured representations of events their causal relationships and their participants and frames to drive interpretation of syntax and word use", "Knowledge structures such as these provided the interpreter rich information about many aspects of meaning", "The problem with these rich knowledge structures is that the need for hand construction specificity and domain dependence prevents robust and flexible language understanding", "Instead modern work on understanding has focused on shallower representations like semantic roles which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank Palmer et al 2005 and Framenet Baker et al 1998", "Unfortunately creating these supervised corpora is an expensive and difficult multiyear effort requiring complex decisions about the exact set of roles to Being able to robustly learn sets of related events left and framespecific role information about the argument types that fill them right could assist a variety of NLP applications from question answering to machine translation", "Our previous work Chambers and Jurafsky 2008 relied on the intuition that in a coherent text any two events that are about the same participants are likely to be part of the same story or narrative", "The model learned simple aspects of narrative structure narrative chains by extracting events that share a single participant the protagonist", "In this paper we extend this work to represent sets of situationspecific events not unlike scripts caseframes Bean and Riloff 2004 and FrameNet frames Baker et al 1998", "This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema while the shared arguments across verbs can provide rich information for inducing semantic roles", "602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 602610 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP"]}, "P09-1074": {"title": ["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 656664"], "abstract": ["We aim to shed light on the stateoftheart in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions the assumptions made in evaluation methodologies and inherent differences in text corpora", "First we examine three subproblems that play a role in coreference resolution named entity recognition anaphoricity determination and coreference element detection", "We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task", "Second we measure the performance of a stateoftheart coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets"], "inroduction": ["As is common for many natural language processing problems the stateoftheart in noun phrase NP coreference resolution is typically quantified based on system performance on manually annotated text corpora", "In spite of the availability of several benchmark data sets eg MUC6 1995 ACE NIST 2004 and their use in many formal evaluations as a field we can make surprisingly few conclusive statements about the stateofthe art in NP coreference resolution", "In particular it remains difficult to assess the effectiveness of different coreference resolution approaches even in relative terms", "For example the 915 Fmeasure reported by McCallum and Well ner 2004 was produced by a system using perfect information for several linguistic subproblems", "In contrast the 713 Fmeasure reported by Yang et al", "2003 represents a fully automatic endtoend resolver", "It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation", "Results vary widely across data sets", "Coreference resolution scores range from 8590 on the ACE 2004 and 2005 data sets to a much lower 60 70 on the MUC 6 and 7 data sets eg Soon et al", "2001 and Yang et al", "2003", "What accounts for these differences", "Are they due to properties of the documents or domains", "Or do differences in the coreference task definitions account for the differences in performance", "Given a new text collection and domain what level of performance should we expect", "We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by stateoftheart systems", "Except for some fairly general statements for example that proper names are easier to resolve than pronouns which are easier than common nouns there has been little analysis of which aspects of the problem have achieved success and which remain elusive", "The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resolution", "For our investigations we employ a stateoftheart classificationbased NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets", "We hypothesize that performance variation within and across coreference resolvers is at least in part a function of 1 the sometimes unstated assumptions in evaluation methodologies and 2 the relative difficulty of the benchmark text corpora", "With these in mind Section 3 first examines three subproblems that play an important role in coreference resolution named entity recognition anaphoricity determination and coreference element detection", "We quantitatively measure the impact of each of these subproblems on coreference resolution performance as a whole", "Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of stateof theart resolvers while improvements to named entity recognition likely offer little gains", "Our results also confirm that the assumptions adopted in 656 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 656664 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP MUC ACE Relative Pronouns no yes Gerunds no yes Nested nonNP nouns yes no Nested NEs no GPE  LOC premod Semantic Types all 7 classes only Singletons no yes Table 1 Coreference Definition Differences for MUC and ACE", "GPE refers to geopolitical entities", "some evaluations dramatically simplify the resolution task rendering it an unrealistic surrogate for the original problem", "In Section 4 we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes", "Our goals are twofold to measure the level of performance of stateoftheart coreference resolvers on different types of anaphora and to develop a quantitative measure for estimating coreference resolution performance on new data sets", "We introduce a coreference performance prediction CPP measure and show that it accurately predicts the performance of our coreference resolver", "As a side effect of our research we provide a new set of muchneeded benchmark results for coreference resolution under common sets of fullyspecified evaluation assumptions"]}, "P09-1098": {"title": ["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 870 878 Suntec Singapore 27 August 2009 c  2009 ACL and AFNLP Mining Bilingual Data from the Web with Adaptively Learnt Patterns"], "abstract": ["Mining bilingual data including bilingual sentences and terms1 from the Web can benefit many NLP applications such as machine translation and cross language information retrieval", "In this paper based on the observation that bilingual data in many web pages appear collectively following similar patterns an adaptive patternbased bilingual data mining method is proposed", "Specifically given a web page the method contains four steps 1 pre processing parse the web page into a DOM tree and segment the inner text of each node into snippets 2 seed mining identify potential translation pairs seeds using a word based alignment model which takes both translation and transliteration into consideration 3 pattern learning learn generalized patterns with the identified seeds 4 pattern based mining extract all bilingual data in the page using the learned patterns", "Our experiments on Chinese web pages produced more than 75 million pairs of bilingual sentences and more than 5 million pairs of bilingual terms both with over 80 accuracy"], "inroduction": ["Bilingual data including bilingual sentences and bilingual terms are critical resources for building many applications such as machine translation Brown 1993 and cross language information retrieval Nie et al 1999", "However most existing bilingual data sets are i not adequate for their intended uses ii not uptodate iii apply only to limited domains", "Because its very hard and expensive to create a large scale bilin 1 In this paper terms refer to proper nouns technical terms", "movie names and so on", "And bilingual termssentences mean termssentences and their translations", "gual dataset with human effort recently many researchers have turned to automatically mining them from the Web", "If the content of a web page is written in two languages we call the page a Bilingual Web Page", "Many such pages exist in nonEnglish web sites", "Most of them have a primary language usually a nonEnglish language and a secondary language usually English", "The content in the secondary language is often the translation of some primary language text in the page", "Since bilingual web pages are very common in nonEnglish web sites mining bilingual data from them should be an important task", "However as far as we know there is no publication available on mining bilingual sentences directly from bilingual web pages", "Most existing methods for mining bilingual sentences from the Web such as Nie et al 1999 Resnik and Smith 2003 Shi et al 2006 try to mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment methods", "As to mining term translations from bilingual web pages Cao et al", "2007 and Lin et al", "2008 proposed two different methods to extract term translations based on the observation that authors of many bilingual web pages especially those whose primary language is Chinese Japanese or Korean sometimes annotate terms with their English translations inside a pair of parentheses like c1c2cne1 e2  em c1c2cn is a primary language term and e1 e2  em is its English translation", "Actually in addition to the parenthesis pattern there is another interesting phenomenon that in many bilingual web pages bilingual data appear collectively and follow similar surface patterns", "Figure 1 shows an excerpt of a page which introduces different kinds of dogs2", "The page provides 2 httpwwwchinapetnet 870 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 870878 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP a list of dog names in both English and Chinese", "Note that those bilingual names do not follow the parenthesis pattern", "However most of them are identically formatted as NumberEnglish nameChinese nameEndOfLine", "One exceptional pair 1Alaskan Malamute      differs only slightly", "Furthermore there are also many pages containing consistently formatted bilingual sentences see Figure 2", "The page3 lists the claimed 200 most common oral sentences in English and their Chinese translations to facilitate English learning", "Figure 1", "Consistently formatted term translation pairs Figure 2", "Consistently formatted sentence translation pairs People create such web pages for various reasons", "Some online stores list their products in two languages to make them understandable to foreigners", "Some pages aim to help readers with foreign language learning", "And in some pages where foreign names or technical terms are mentioned the authors provide the translations for disambiguation", "For easy reference from now on we will call pages which contain many consistently formatted translation pairs Collective Bi lingual Pages", "According to our estimation at least tens of millions of collective bilingual pages exist in Chinese web sites", "Most importantly each such page usually contains a large amount of bilingual 3 httpculbeelinkcom200602052021119shtml data", "This shows the great potential of bilingual data mining", "However the mining task is not straightforward for the following reasons 1 The patterns vary in different pages so its impossible to mine the translation pairs using predefined templates 2 Some pages contain consistently formatted texts in two languages but they are not translation pairs 3 Not all translations in a collective bilingual page necessarily follow an exactly consistent format", "As shown in Figure 1 the ten translation pairs are supposed to follow the same pattern however due to typos the pattern of the first pair is slightly different", "Because of these difficulties simply using a classifier to extract translation pairs from adjacent bilingual texts in a collective bilingual page may not achieve satisfactory results", "Therefore in this paper we propose a patternbased approach learning patterns adaptively from collective bilingual pages instead of using the parenthesis pattern then using the learned patterns to extract translation pairs from corresponding web pages", "Specifically our approach contains four steps 1 Preprocessing parse the web page into a DOM tree and segment the inner text of each node into snippets 2 Seed mining identify potential translation pairs seeds using an alignment model which takes both translation and transliteration into consideration 3 Pattern learning learn generalized patterns with the identified seeds 4 Pattern based mining extract all bilingual data in the page using the learnt patterns", "Let us take mining bilingual data from the text shown in Figure 1 as an example", "Our method identifies Boxer  and Eskimo Dog   as two potential translation pairs based on a dictionary and a transliteration model Step 2 above", "Then we learn a generalized pattern that both pairs follow as BulletNumberPunctuationEnglish termChinese termEndOfLine Step 3 above", "Finally we apply it to match in the entire text and get all translation pairs following the pattern Step 4 above", "The remainder of this paper is organized as follows", "In Section 2 we list some related work", "The overview of our mining approach is presented in Section 3", "In Section 4 we give de tailed introduction to each of the four modules in our mining approach", "The experimental results are reported in Section 5 followed by our conclusion and some future work in Section 6", "Please note that in this paper we describe our method using example bilingual web pages in English and Chinese however the method can be applied to extract bilingual data from web pages written in any other pair of languages such as Japanese and English Korean and English etc"]}, "P09-1113": {"title": ["Distant supervision for relation extraction without labeled data"], "abstract": ["Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small handlabeled corpora", "We investigate an alternative paradigm that does not require labeled corpora avoiding the domain dependence of ACE style algorithms and allowing the use of corpora of any size", "Our experiments use Freebase a large semantic database of several thousand relations to provide distant supervision", "For each pair of entities that appears in some Freebase relation we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier", "Our algorithm combines the advantages of supervised IE combining 400000 noisy pattern features in a probabilistic classifier and unsupervised IE extracting large numbers of relations from large corpora of any domain", "Our model is able to extract 10000 instances of 102 relations at a precision of 676", "We also analyze feature performance showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression"], "inroduction": ["At least three learning paradigms have been applied to the task of extracting relational facts from text for example learning that a person is employed by a particular organization or that a geographic entity is located in a particular region", "In supervised approaches sentences in a corpus are first handlabeled for the presence of entities and the relations between them", "The NIST Automatic Content Extraction ACE RDC 2003 and 2004 corpora for example include over 1000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations totaling 16771 relation instances", "ACE systems then extract a wide variety of lexical syntactic and semantic features and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence optionally combining relation men tions Zhou et al 2005 Zhou et al 2007 Surdeanu and Ciaramita 2007", "Supervised relation extraction suffers from a number of problems however", "Labeled training data is expensive to produce and thus limited in quantity", "Also because the relations are labeled on a particular corpus the resulting classifiers tend to be biased toward that text domain", "An alternative approach purely unsupervised information extraction extracts strings of words between entities in large amounts of text and clusters and simplifies these word strings to produce relationstrings Shinyama and Sekine 2006 Banko et al 2007", "Unsupervised approaches can use very large amounts of data and extract very large numbers of relations but the resulting relations may not be easy to map to relations needed for a particular knowledge base", "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning Brin 1998 Riloff and Jones 1999 Agichtein and Gravano 2000 Ravichandran and Hovy 2002 Etzioni et al 2005 Pennacchiotti and Pantel 2006 Bunescu and Mooney 2007 Rozenfeld and Feldman 2008", "These seeds are used with a large corpus to extract a new set of patterns which are used to extract more instances which are used to extract more patterns in an iterative fashion", "The resulting patterns often suffer from low precision and semantic drift", "We propose an alternative paradigm distant supervision that combines some of the advantages of each of these approaches", "Distant supervision is an extension of the paradigm used by Snow et al", "2005 for exploiting WordNet to extract hyper nym isa relations between entities and is similar to the use of weakly labeled data in bioinformatics Craven and Kumlien 1999 Morgan et al 1003 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 10031011 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP Relation name New instance locationlocationcontains locationlocationcontains musicartistorigin peopledeceased personplace of death peoplepersonnationality peoplepersonplace of birth bookauthorworks written businesscompanyfounders peoplepersonprofession Paris Montmartre Ontario Fort Erie Mighty Wagon Cincinnati Fyodor Kamensky Clearwater Marianne Yvonne Heemskerk Netherlands Wavell Wayne Hinds Kingston Upton Sinclair Lanny Budd WWE Vince McMahon Thomas Mellon judge Table 1 Ten relation instances extracted by our system that did not appear in Freebase", "2004", "Our algorithm uses Freebase Bollacker et al 2008 a large semantic database to provide distant supervision for relation extraction", "Freebase contains 116 million instances of 7300 relations between 9 million entities", "The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way", "Since there may be many sentences containing a given entity pair we can extract very large numbers of potentially noisy features that are combined in a logistic regression classifier", "Thus whereas the supervised training paradigm uses a small labeled corpus of only 17000 relation instances as training data our algorithm can use much larger amounts of data more text more relations and more instances", "We use 12 million Wikipedia articles and 18 million instances of 102 relations connecting 940000 entities", "In addition combining vast numbers of features in a large classifier helps obviate problems with bad features", "Because our algorithm is supervised by a database rather than by labeled text it does not suffer from the problems of overfitting and domaindependence that plague supervised systems", "Supervision by a database also means that unlike in unsupervised approaches the output of our classifier uses canonical names for relations", "Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities", "Because our algorithm can use large amounts of unlabeled data a pair of entities may occur multiple times in the test set", "For each pair of entities we aggregate the features from the many different sentences in which that pair appeared into a single feature vector allowing us to provide our classifier with more information resulting in more accurate labels", "Table 1 shows examples of relation instances extracted by our system", "We also use this system to investigate the value of syntactic versus lexi cal word sequence features in relation extraction", "While syntactic features are known to improve the performance of supervised IE at least using clean handlabeled ACE data Zhou et al 2007 Zhou et al 2005 we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE", "Most previous research in bootstrapping or unsupervised IE has used only simple lexical features thereby avoiding the computational expense of parsing Brin 1998 Agichtein and Gravano 2000 Etzioni et al 2005 and the few systems that have used unsupervised IE have not compared the performance of these two types of feature"]}, "P09-1114": {"title": ["MultiTask Transfer Learning for WeaklySupervised Relation Extraction"], "abstract": ["Creating labeled training data for relation extraction is expensive", "In this paper we study relation extraction in a special weaklysupervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types", "Observing that different relation types can share certain common structures we propose to use a multitask learning method coupled with human guidance to address this weaklysupervised relation extraction problem", "The proposed framework models the commonality among different relation types through a shared weight vector enables knowledge learned from the auxiliary relation types to be transferred to the target relation type and allows easy control of the tradeoff between precision and recall", "Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods"], "inroduction": ["Relation extraction is the task of detecting and characterizing semantic relations between entities from free text", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides stateoftheart solutions to the problem Culotta and Sorensen 2004 Zhou et al 2005 Bunescu and Mooney 2005 Qian et al 2008", "However supervised learning heavily relies on a sufficient amount of labeled data for training which is not always available in practice due to the laborintensive nature of human annotation", "This problem is especially serious for relation ex traction because the types of relations to be extracted are highly dependent on the application domain", "For example when working in the financial domain we may be interested in the employment relation but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation and thus have to create training data for the new relation type", "However is the old training data really useless", "Inspired by recent work on transfer learning and domain adaptation in this paper we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weaklysupervised setting where only a few seed instances of the new relation type are available", "While transfer learning was proposed more than a decade ago Thrun 1996 Caruana 1997 its application in natural language processing is still a relatively new territory Blitzer et al 2006 Daume III 2007 Jiang and Zhai 2007a Arnold et al 2008 Dredze and Crammer 2008 and its application in relation extraction is still unexplored", "Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures which can possibly be transferred from the old types to the new type", "We therefore propose to use a general multitask learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together", "By treating classification of different relation types as related tasks the learning framework can naturally model the common syntactic structures among different relation types in a principled manner", "It also allows us to introduce human guidance in separating the common model component from the typespecific components", "The framework naturally transfers the knowledge learned from the old relation types to the new relation type and helps improve the recall of the relation extractor", "We also exploit ad 1012 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP pages 10121020 Suntec Singapore 27 August 2009", "Qc 2009 ACL and AFNLP ditional human knowledge about the entity type constraints on the relation arguments which can usually be derived from the definition of a relation type", "Imposing these constraints further improves the precision of the final relation extractor", "Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods improving the average F1 measure from 01532 to 04132 when only 10 seed instances of the new relation type are used"]}, "P09-2063": {"title": ["Proceedings of the ACLIJCNLP 2009 Conference Short Papers pages 249252 Suntec Singapore 4 August 2009 c2009 ACL and AFNLP Introduction of a new paraphrase generation tool based on MonteCarlo sampling Jonathan Chevelu 12 Thomas Lavergne Yves Lepage 1 Thierry Moudenc 2"], "abstract": ["We propose a new specifically designed method for paraphrase generation based on MonteCarlo sampling and show how this algorithm is suitable for its task", "Moreover the basic algorithm presented here leaves a lot of opportunities for future improvement", "In particular our algorithm does not constraint the scoring function in opposite to Viterbi based decoders", "It is now possible to use some global features in paraphrase scoring functions", "This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation"], "inroduction": ["A paraphrase generation system is a program which given a source sentence produces a different sentence with almost the same meaning", "Paraphrase generation is useful in applications to choose between different forms to keep the most appropriate one", "For instance automatic summary can be seen as a particular paraphrasing task Barzilay and Lee 2003 with the aim of selecting the shortest paraphrase", "Paraphrases can also be used to improve natural language processing NLP systems", "CallisonBurch et al 2006 improved machine translations by augmenting the coverage of patterns that can be translated", "Similarly Sekine 2005 improved information retrieval based on pattern recognition by introducing paraphrase generation", "In order to produce paraphrases a promising approach is to see the paraphrase generation problem as a translation problem where the target language is the same as the source language Quirk et al 2004 Bannard and CallisonBurch 2005", "A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT", "Most paraphrase generation tools use some standard SMT decoding algorithms Quirk et al 2004 or some offtheshelf decoding tools like MOSES Koehn et al 2007", "The goal of a decoder is to find the best path in the lattice produced from a paraphrase table", "This is basically achieved by using dynamic programming and especially the Viterbi algorithm associated with beam searching", "However decoding algorithms were designed for translation not for paraphrase generation", "Although lefttoright decoding is justified for translation it may not be necessary for paraphrase generation", "A paraphrase generation tool usually starts with a sentence which may be very similar to some potential solution", "In other words there is no need to translate all of the sentences", "Moreover decoding may not be suitable for noncontiguous transformation rules", "In addition dynamic programming imposes an incremental scoring function to evaluate the quality of each hypothesis", "For instance it cannot capture some scattered syntactical dependencies", "Improving on this major issue is a key point to improve paraphrase generation systems", "This paper first presents an alternative to decoding that is based on transformation rule application in section 2", "In section 3 we propose a paraphrase generation method for this paradigm based on an algorithm used in twoplayer games", "Section 4 briefly explain experimental context and its associated protocol for evaluation of the proposed system", "We compare the proposed algorithm with a baseline system in section 5", "Finally in section 6 we point to future research tracks to improve paraphrase generation tools"]}, "P10-1020": {"title": ["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 186 195 Uppsala Sweden 1116 July 2010 c2010 Association for Computational Linguistics Entitybased local coherence modelling using topological fields Jackie Chi Kit Cheung and Gerald Penn Department of Computer Science University of Toronto Toronto ON M5S 3G4 Canada jcheunggpenncstorontoedu Abstract One goal of natural language generation is to produce coherent text that presents information in a logical order In this pa per we show that topological fields which model highlevel clausal structure are an important component of local coherence in German First we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata 2008 more than grammatical role and simple clausal order information do particularly when manual annotations of this information are not available Then we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text and show that the added coherence component improves performance slightly though not statistically significantly 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling which measures the coherence of a document by examining the similarity between neighbouring text spans The entitybased approach in particular considers the occurrences of noun phrase entities in a document Barzilay and Lapata 2008 Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization Barzilay and Lee 2004 and genre classification Barzilay and Lapata 2008 Previous work on English a language with relatively fixed word order has identified factors that contribute to local coherence such as the grammatical roles associated with the entities There is good reason to believe that the importance of these factors vary across languages For instance freerwordorder languages exhibit word order patterns which are dependent on discourse factors relating to information structure in addition to the grammatical roles of nominal arguments of the main verb We thus expect word order information to be particularly important in these languages in discourse analysis which includes coherence modelling For example Strube and Hahn 1999 introduce Functional Centering a variant of Centering Theory which utilizes information status distinctions between hearerold and hearernew entities They apply their model to pronominal anaphora resolution identifying potential antecedents of subsequent anaphora by considering syntactic and word order information classifying constituents by their familiarity to the reader They find that their approach correctly resolves more pronominal anaphora than a grammatical rolebased ap proach which ignores word order and the difference between the two approaches is larger in German corpora than in English ones Unfortunately their criteria for ranking potential antecedents require complex syntactic information in order to classify whether proper names are known to the hearer which makes their algorithm hard to automate Indeed all evaluation is done manually We instead use topological fields a model of clausal structure which is indicative of information structure in German but shallow enough to be automatically parsed at high accuracy We test the hypothesis that they would provide a good complement or alternative to grammatical roles in local coherence modelling We show that they are superior to grammatical roles in a sentence ordering experiment and in fact outperforms simple wordorder information as well We further show that these differences are particularly large when manual syntactic and grammatical role an186 Millionen von Mark verschwendet   der Senat jeden Monat weil er   sparen will LK MF VCVF LK MF S NF S The senate wastes millions of marks each month because it wants to save Figure 1 The clausal and topological field structure of a German sentence Notice that the subordinate clause receives its own topology notations are not available We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting We add contextual features using topological field transitions to the model of Filippova and Strube 2007b and achieve a slight improvement over their model in a con stituent ordering task though not statistically significantly We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling 2 Background and Related Work"], "abstract": ["One goal of natural language generation is to produce coherent text that presents information in a logical order", "In this paper we show that topological fields which model highlevel clausal structure are an important component of local coherence in German", "First we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata 2008 more than grammatical role and simple clausal order information do particularly when manual annotations of this information are not available", "Then we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text and show that the added coherence component improves performance slightly though not statistically significantly"], "inroduction": ["One type of coherence modelling that has captured recent research interest is local coherence modelling which measures the coherence of a document by examining the similarity between neigh bouring text spans", "The entitybased approach in particular considers the occurrences of noun phrase entities in a document Barzilay and Lap ata 2008", "Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization Barzilay and Lee 2004 and genre classification Barzilay and Lap ata 2008", "Previous work on English a language with relatively fixed word order has identified factors that contribute to local coherence such as the grammatical roles associated with the entities", "There is good reason to believe that the importance of these factors vary across languages", "For instance freer wordorder languages exhibit word order patterns which are dependent on discourse factors relating to information structure in addition to the grammatical roles of nominal arguments of the main verb", "We thus expect word order information to be particularly important in these languages in discourse analysis which includes coherence modelling", "For example Strube and Hahn 1999 introduce Functional Centering a variant of Centering Theory which utilizes information status distinctions between hearerold and hearernew entities", "They apply their model to pronominal anaphora resolution identifying potential antecedents of subsequent anaphora by considering syntactic and word order information classifying constituents by their familiarity to the reader", "They find that their approach correctly resolves more pronominal anaphora than a grammatical rolebased approach which ignores word order and the difference between the two approaches is larger in German corpora than in English ones", "Unfortunately their criteria for ranking potential antecedents require complex syntactic information in order to classify whether proper names are known to the hearer which makes their algorithm hard to automate", "Indeed all evaluation is done manually", "We instead use topological fields a model of clausal structure which is indicative of information structure in German but shallow enough to be automatically parsed at high accuracy", "We test the hypothesis that they would provide a good complement or alternative to grammatical roles in local coherence modelling", "We show that they are superior to grammatical roles in a sentence ordering experiment and in fact outperforms simple wordorder information as well", "We further show that these differences are particularly large when manual syntactic and grammatical role an 186 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 186195 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics S VF LK MF NF S LK MF VC Millionen von Mark verschwendet der Senat jeden Monat weil er sparen will", "The senate wastes millions of marks each month because it wants to save Figure 1 The clausal and topological field structure of a German sentence", "Notice that the subordinate clause receives its own topology", "notations are not available", "We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting", "We add contextual features using topological field transitions to the model of Filippova and Strube 2007b and achieve a slight improvement over their model in a constituent ordering task though not statistically significantly", "We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling"]}, "P10-1068": {"title": ["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 659 670 Uppsala Sweden 1116 July 2010 c2010 Association for Computational Linguistics Towards robust multitool tagging An OWLDLbased approach Christian Chiarcos University of Potsdam Germany chiarcosunipotsdamde Abstract This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis It is shown how annotations created by seven NLP tools are mapped onto toolindependent descriptions that are defined with reference to an ontology of linguistic annotations and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way For morphosyntactic parts of speech and morphological annotations of three German corpora the resulting merged sets of ontological descriptions are evaluated in comparison to ontological representation of existing reference annotations 1 Motivation and overview NLP systems for higherlevel operations or complex annotations often integrate redundant modules that provide alternative analyses for the same linguistic phenomenon in order to benefit from their respective strengths and to compensate for their respective weaknesses eg in parsing Crysmann et al 2002 or in machine translation Carl et al 2000 The current trend to parallel and distributed NLP architectures Aschenbrenner et al 2006 Gietz et al 2006 Egner et al 2007 Lu  s and de Matos 2009 opens the possibility of ex ploring the potential of redundant parallel annotations also for lower levels of linguistic analysis This paper evaluates the potential benefits of such an approach with respect to morphosyntax parts of speech pos and morphology in German In comparison to English German shows a rich and polysemous morphology and a considerable number of NLP tools are available making it a promising candidate for such an experiment Previous research indicates that the integration of multiple part of speech taggers leads to more accurate analyses So far however this line of research focused on tools that were trained on the same corpus Brill and Wu 1998 Halteren et al 2001 or that specialize to different subsets of the same tagset Zavrel and Daelemans 2000 Tufis  2000 Borin 2000 An even more substantial increase in accuracy and detail can be expected if tools are combined that make use of different an notation schemes For this task ontologies of linguistic annotations are employed to assess the linguistic infor mation conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and toolindependent way The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles the NEGRA corpus Skut et al 1998 the TIGER corpus Brants et al 2002 and the Potsdam Commentary Corpus Stede 2004 PCC 2 Ontologies and annotations Various repositories of linguistic annotation terminology have been developed in the last decades ranging from early texts on annotation standards Bakker et al 1993 Leech and Wilson 1996 over relational data base models Bickel and Nichols 2000 Bickel and Nichols 2002 to more recent formalizations in OWLRDF or with OWLRDF export eg the General Ontology of Linguistic Description Farrar and Langendoen 2003 GOLD the ISO TC37SC4 Data Category Registry Ide and Romary 2004 Kemps659 Snijders et al 2009 DCR the OntoTag ontology Aguado de Cea et al 2002 or the Typological Database System ontology Saulwick et al 2005 TDS Despite their common level of representation however these efforts have not yet converged into a unified and generally accepted ontology of linguistic annotation terminology but rather different resources are maintained by different communities so that a considerable amount of disagreement between them and their respective definitions can be observed1 Such conceptual mismatches and incompatibilities between existing terminological repositories have been the motivation to develop the OLiA architecture Chiarcos 2008 that employs a shallow Reference Model to mediate between ontological models of annotation schemes and several existing terminology repositories incl GOLD the DCR and OntoTag When an annotation receives a representation in the OLiA Reference Model it is thus also interpretable with respect to other linguistic ontologies Therefore the findings for the OLiA Reference Model in the experiments described below entail similar results for an application of GOLD or the DCR to the same task"], "abstract": ["This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis", "It is shown how annotations created by seven NLP tools are mapped onto tool independent descriptions that are defined with reference to an ontology of linguistic annotations and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way", "For morphosyntactic parts of speech and morphological annotations of three German corpora the resulting merged sets of ontological descriptions are evaluated in comparison to ontological representation of existing reference annotations"], "inroduction": ["NLP systems for higherlevel operations or complex annotations often integrate redundant modules that provide alternative analyses for the same linguistic phenomenon in order to benefit from their respective strengths and to compensate for their respective weaknesses eg in parsing Crysmann et al 2002 or in machine translation Carl et al 2000", "The current trend to parallel and distributed NLP architectures Aschenbrenner et al 2006 Gietz et al 2006 Egner et al 2007 Lus and de Matos 2009 opens the possibility of exploring the potential of redundant parallel annotations also for lower levels of linguistic analysis", "This paper evaluates the potential benefits of such an approach with respect to morphosyntax parts of speech pos and morphology in German In comparison to English German shows a rich and polysemous morphology and a considerable number of NLP tools are available making it a promising candidate for such an experiment", "Previous research indicates that the integration of multiple part of speech taggers leads to more accurate analyses", "So far however this line of research focused on tools that were trained on the same corpus Brill and Wu 1998 Halteren et al 2001 or that specialize to different subsets of the same tagset Zavrel and Daelemans 2000 Tufis 2000 Borin 2000", "An even more substantial increase in accuracy and detail can be expected if tools are combined that make use of different annotation schemes", "For this task ontologies of linguistic annotations are employed to assess the linguistic information conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and toolindependent way", "The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles the NEGRA corpus Skut et al 1998 the TIGER corpus Brants et al 2002 and the Potsdam Commentary Corpus Stede 2004 PCC"]}, "P10-1115": {"title": ["CrossLingual Latent Topic Extraction"], "abstract": ["Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way", "One common deficiency of existing topic models though is that they would not work well for extracting crosslingual latent topics simply because words in different languages generally do not cooccur with each other", "In this paper we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages", "Specifically we propose a new topic model called Probabilistic CrossLingual Latent Semantic Analysis PCLSA which extends the Probabilistic Latent Semantic Analysis PLSA model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary", "Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract crosslingual latent topics from multilingual text data"], "inroduction": ["As a robust unsupervised way to perform shallow latent semantic analysis of topics in text probabilistic topic models Hofmann 1999a Blei et al 2003b have recently attracted much attention", "The common idea behind these models is the following", "A topic is represented by a multinomial word distribution so that words characterizing a topic generally have higher probabilities than other words", "We can then hypothesize the existence of multiple topics in text and define a generative model based on the hypothesized topics", "By fitting the model to text data we can obtain an estimate of all the word distributions corresponding to the latent topics as well as the topic distributions in text", "Intuitively the learned word distributions capture clusters of words that cooccur with each other probabilistically", "Although many topic models have been proposed and shown to be useful see Section 2 for more detailed discussion of related work most of them share a common deficiency they are designed to work only for monolingual text data and would not work well for extracting crosslingual latent topics ie topics shared in text data in two different natural languages", "The deficiency comes from the fact that all these models rely on cooccurrences of words forming a topical cluster but words in different language generally do not cooccur with each other", "Thus with the existing models we can only extract topics from text in each language but cannot extract common topics shared in multiple languages", "In this paper we propose a novel topic model called Probabilistic CrossLingual Latent Semantic Analysis PCLSA model which can be used to mine shared latent topics from unaligned text data in different languages", "PCLSA extends the Probabilistic Latent Semantic Analysis PLSA model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary", "The dictionarybased constraints are key to bridge the gap of different languages and would force the captured cooccurrences of words in each language by PCLSA to be synchronized so that related words in the two languages would have similar probabilities", "PCLSA can be estimated efficiently using the General Expectation Maximization GEM algorithm", "As a topic extraction algorithm PCLSA would take a pair of unaligned document sets in different languages and a bilingual dictionary as input and output a set of aligned word distributions in both languages that can characterize the shared topics in the two languages", "In addition it also outputs a topic cov 1128 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 11281137 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics erage distribution for each language to indicate the relative coverage of different shared topics in each language", "To the best of our knowledge no previous work has attempted to solve this topic extraction problem and generate the same output", "The closest existing work to ours is the MuTo model proposed in BoydGraber and Blei 2009 and the JointLDA model published recently in Jagaralamudi and Daume III 2010", "Both used a bilingual dictionary to bridge the language gap in a topic model", "However the goals of their work are different from ours in that their models mainly focus on mining crosslingual topics of matching word pairs and discovering the correspondence at the vocabulary level", "Therefore the topics extracted using their model cannot indicate how a common topic is covered differently in the two languages because the words in each word pair share the same probability in a common topic", "Our work focuses on discovering correspondence at the topic level", "In our model since we only add a soft constraint on word pairs in the dictionary their probabilities in common topics are generally different naturally capturing which shows the different variations of a common topic in different languages", "We use a crosslingual news data set and a review data set to evaluate PCLSA", "We also propose a crosscollection likelihood measure to quantitatively evaluate the quality of mined topics", "Experimental results show that the PCLSA model can effectively extract crosslingual latent topics from multilingual text data and it outperforms a baseline approach using the standard PLSA on text data in each language"]}, "P10-1124": {"title": ["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 1220 1229 Uppsala Sweden 1116 July 2010 c2010 Association for Computational Linguistics Global Learning of Focused Entailment Graphs Jonathan Berant TelAviv University TelAviv Israel jonatha6posttauacil Ido Dagan BarIlan University RamatGan Israel dagancsbiuacil Jacob Goldberger BarIlan University RamatGan Israel goldbejengbiuacil Abstract We propose a global algorithm for learning entailment relations between predicates We define a graph structure over predicates that represents entailment relations as directed edges and use a global transitivity constraint on the graph to learn the optimal set of edges by formulating the optimization problem as an Integer Linear Program We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept and show that our global algorithm improves perfor mance by more than 10 over baseline algorithms 1 Introduction The Textual Entailment TE paradigm Dagan et al 2009 is a generic framework for applied semantic inference The objective of TE is to recognize whether a target meaning can be inferred from a given text For example a Question Answering system has to recognize that alcohol affects blood pressure is inferred from alcohol reduces blood pressure to answer the question What affects blood pressure  TE systems require extensive knowledge of en tailment patterns often captured as entailment rules rules that specify a directional inference relation between two text fragments when the rule is bidirectional this is known as paraphrasing An important type of entailment rule refers to propositional templates ie propositions comprising a predicate and arguments possibly replaced by variables The rule required for the previous example would be X reduce Y  X affect Y Because facts and knowledge are mostly expressed by propositions such entailment rules are central to the TE task This has led to active research on broadscale acquisition of entailment rules for predicates eg Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2008 Previous work has focused on learning each entailment rule in isolation However it is clear that there are interactions between rules A prominent example is that entailment is a transitive relation and thus the rules  X  Y  and  Y  Z imply the rule  X  Z  In this paper we take advantage of these global interactions to improve entailment rule learning First we describe a structure termed an entailment graph that models entailment relations between propositional templates Section 3 Next we show that we can present propositions according to an entailment hierarchy derived from the graph and suggest a novel hierarchical presentation scheme for corpus propositions referring to a target concept As in this application each graph focuses on a single concept we term those focused entailment graphs Section 4 In the core section of the paper we present an algorithm that uses a global approach to learn the entailment relations of focused entailment graphs Section 5 We define a global function and look for the graph that maximizes that function under a transitivity constraint The optimization problem is formulated as an Integer Linear Program ILP and solved with an ILP solver We show that this leads to an optimal solution with respect to the global function and demonstrate that the algorithm outperforms methods that utilize only local information by more than 10 as well as methods that employ a greedy optimization algorithm rather than an ILP solver Section 6 2 Background Entailment learning Two information types have primarily been utilized to learn entailment rules between predicates lexicographic resources and distributional similarity resources Lexicographic 1220 resources are manuallyprepared knowledge bases containing information about semantic relations between lexical items WordNet Fellbaum 1998 by far the most widely used resource specifies relations such as hyponymy derivation and entailment that can be used for semantic inference Budanitsky and Hirst 2006 WordNet has also been exploited to automatically generate a training set for a hyponym classifier Snow et al 2005 and we make a similar use of WordNet in Section"], "abstract": ["We propose a global algorithm for learning entailment relations between predicates", "We define a graph structure over predicates that represents entailment relations as directed edges and use a global transitivity constraint on the graph to learn the optimal set of edges by formulating the optimization problem as an Integer Linear Program", "We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept and show that our global algorithm improves performance by more than 10 over baseline algorithms"], "inroduction": ["The Textual Entailment TE paradigm Dagan et al 2009 is a generic framework for applied semantic inference", "The objective of TE is to recognize whether a target meaning can be inferred from a given text", "For example a Question Answering system has to recognize that alcohol affects blood pressure is inferred from alcohol reduces blood pressure to answer the question What affects blood pressure TE systems require extensive knowledge of entailment patterns often captured as entailment rules rules that specify a directional inference relation between two text fragments when the rule is bidirectional this is known as paraphrasing", "An important type of entailment rule refers to propositional templates ie propositions comprising a predicate and arguments possibly replaced by variables", "The rule required for the previous example would be X reduce Y  X affect Y", "Be cause facts and knowledge are mostly expressed by propositions such entailment rules are central to the TE task", "This has led to active research on broadscale acquisition of entailment rules for predicates eg", "Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2008", "Previous work has focused on learning each entailment rule in isolation", "However it is clear that there are interactions between rules", "A prominent example is that entailment is a transitive relation and thus the rules X  Y  and Y  Z  imply the rule X  Z ", "In this paper we take advantage of these global interactions to improve entailment rule learning", "First we describe a structure termed an entail ment graph that models entailment relations between propositional templates Section 3", "Next we show that we can present propositions according to an entailment hierarchy derived from the graph and suggest a novel hierarchical presentation scheme for corpus propositions referring to a target concept", "As in this application each graph focuses on a single concept we term those focused entailment graphs Section 4", "In the core section of the paper we present an algorithm that uses a global approach to learn the entailment relations of focused entailment graphs Section 5", "We define a global function and look for the graph that maximizes that function under a transitivity constraint", "The optimization problem is formulated as an Integer Linear Program ILP and solved with an ILP solver", "We show that this leads to an optimal solution with respect to the global function and demonstrate that the algorithm outperforms methods that utilize only local information by more than 10 as well as methods that employ a greedy optimization algorithm rather than an ILP solver Section 6"]}, "P10-1142": {"title": ["Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics pages 13961411"], "abstract": ["The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade", "This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago"], "inroduction": ["Noun phrase NP coreference resolution the task of determining which NPs in a text or dialogue refer to the same realworld entity has been at the core of natural language processing NLP since the 1960s", "NP coreference is related to the task of anaphora resolution whose goal is to identify an antecedent for an anaphoric NP ie an NP that depends on another NP specifically its antecedent for its interpretation see van Deemter and Kibble 2000 for a detailed discussion of the difference between the two tasks", "Despite its simple task definition coreference is generally considered a difficult NLP task typically involving the use of sophisticated knowledge sources and inference procedures Charniak 1972", "Computational theories of discourse in particular focusing see Grosz 1977 and Sidner 1979 and centering Grosz et al", "1983 1995 have heavily influenced coreference research in the 1970s and 1980s leading to the development of numerous centering algorithms see Walker et al", "1998", "The focus of coreference research underwent a gradual shift from heuristic approaches to machine learning approaches in the 1990s", "This shift can be attributed in part to the advent of the statistical NLP era and in part to the public availability of annotated coreference corpora produced as part of the MUC6 1995 and MUC7 1998 conferences", "Learningbased coreference research has remained vibrant since then with results regularly published not only in general NLP conferences but also in specialized conferences eg the biennial Discourse Anaphora and Anaphor Resolution Colloquium DAARC and workshops eg the series of Bergen Workshop on Anaphora Resolution WAR", "Being inherently a clustering task coreference has also received a lot of attention in the machine learning community", "Fifteen years have passed since the first paper on learningbased coreference resolution was published Connolly et al 1994", "Our goal in this paper is to provide NLP researchers with a survey of the major milestones in supervised coreference research focusing on the computational models the linguistic features the annotated corpora and the evaluation metrics that were developed in the past fifteen years", "Note that several leading coreference researchers have published books eg Mitkov 2002 written survey articles eg Mitkov 1999 Strube 2009 and delivered tutorials eg Strube 2002 Ponzetto and Poesio 2009 that provide a broad overview of coreference research", "This survey paper aims to complement rather than supersede these previously published materials", "In particular while existing survey papers discuss learningbased coreference research primarily in the context of the influential mentionpair model we additionally survey recently proposed learningbased coreference models which attempt to address the weaknesses of the mentionpair model", "Due to space limitations however we will restrict our discussion to the most commonly investigated kind of coreference relation the identity relation for NPs excluding coreference among clauses and bridging references eg partwhole and setsubset relations"]}, "P10-2025": {"title": ["Word Alignment with Synonym Regularization"], "abstract": ["We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model", "Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language", "We design a generative model for word alignment that uses synonym information as a regularization term", "The experimental results show that our proposed method significantly improves word alignment quality"], "inroduction": ["Word alignment is an essential step in most phrase and syntax based statistical machine translation SMT", "It is an inference problem of word correspondences between different languages given parallel sentence pairs", "Accurate word alignment can induce high quality phrase detection and translation probability which leads to a significant improvement in SMT performance", "Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner Vo gel et al 1996 Och and Ney 2003 Fraser and Marcu 2007", "One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus", "This monolingual knowledge makes it easier to determine corresponding words correctly", "For instance functional words in one language tend to correspond to functional words in another language Deng and Gao 2007 and the syntactic dependency of words in each language can help the alignment process Ma et al 2008", "It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality", "A large number of monolingual lexical semantic resources such as WordNet Miller 1995 have been constructed in more than fifty languages Sagot and Fiser 2008", "They include word level relations such as synonyms hypernyms and hyponyms", "Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language", "In this paper we explore a method for using synonym information effectively to improve word alignment quality", "In general synonym relations are defined in terms of word sense not in terms of word form", "In other words synonym relations are usually context or domain dependent", "For instance head and chief are synonyms in contexts referring to working environment while head and forefront are synonyms in contexts referring to physical positions", "It is difficult however to imagine a context where chief and forefront are synonyms", "Therefore it is easy to imagine that simply replacing all occurrences of chief and forefront with head do sometimes harm with word alignment accuracy and we have to model either the context or senses of words", "We propose a novel method that incorporates synonyms from monolingual resources in a bilingual word alignment model", "We formulate a synonym pair generative model with a topic variable and use this model as a regularization term with a bilingual word alignment model", "The topic variable in our synonym model is helpful for disambiguating the meanings of synonyms", "We extend HMBiTAM which is a HMMbased word alignment model with a latent topic with a novel synonym pair generative model", "We applied the proposed method to an EnglishFrench word alignment task and successfully improved the word 137 Proceedings of the ACL 2010 Conference Short Papers pages 137141 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics translation probability from e to f under the kth topic p f e z  k ", "T  Tii  is a state tran sition probability of a first order Markov process", "Fig", "1 shows a graphical model of HMBiTAM", "The total likelihood of bilingual sentence pairsE F  can be obtained by marginalizing out la tent variables z a and  p F E     f p F E z a   d 1 z a Figure 1 Graphical model of HMBiTAM alignment quality"]}, "P10-2066": {"title": ["Distributional Similarity vs PU Learning for Entity Set Expansion"], "abstract": ["Distributional similarity is a classic technique for entity set expansion where the system is given a set of seed entities of a particular class and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds", "This paper shows that a machine learning model called positive and unlabeled learning PU learning can model the set expansion problem better", "Based on the test results of 10 corpora we show that a PU learning technique outperformed distributional similarity significantly"], "inroduction": ["The entity set expansion problem is defined as follows Given a set S of seed entities of a particular class and a set D of candidate entities eg extracted from a text corpus we wish to determine which of the entities in D belong to S In other words we expand the set S based on the given seeds", "This is clearly a classification problem which requires arriving at a binary decision for each entity in D belonging to S or not", "However in practice the problem is often solved as a ranking problem ie ranking the entities in D based on their likelihoods of belonging to S The classic method for solving this problem is based on distributional similarity Pantel et al 2009 Lee 1998", "The approach works by comparing the similarity of the surrounding word distributions of each candidate entity with the seed entities and then ranking the candidate entities using their similarity scores", "In machine learning there is a class of semi supervised learning algorithms that learns from positive and unlabeled examples PU learning for short", "The key characteristic of PU learning is that there is no negative training example available for learning", "This class of algorithms is less known to the natural language processing NLP community compared to some other semi supervised learning models and algorithms", "PU learning is a twoclass classification model", "It is stated as follows Liu et al 2002 Given a set P of positive examples of a particular class and a set U of unlabeled examples containing hidden positive and negative cases a classifier is built using P and U for classifying the data in U or future test cases", "The results can be either binary decisions whether each test case belongs to the positive class or not or a ranking based on how likely each test case belongs to the positive class represented by P Clearly the set expansion problem can be mapped into PU learning exactly with S and D as P and U respectively", "This paper shows that a PU learning method called SEM Liu et al 2002 outperforms distributional similarity considerably based on the results from 10 corpora", "The experiments involved extracting named entities eg product and organization names of the same type or class as the given seeds", "Additionally we also compared SEM with a recent method called Bayesian Sets Ghahramani and Heller 2005 which was designed specifically for set expansion", "It also does not perform as well as PU learning", "We will explain why PU learning performs better than both methods in Section 5", "We believe that this finding is of interest to the NLP community", "359 Proceedings of the ACL 2010 Conference Short Papers pages 359364 Uppsala Sweden 1116 July 2010", "Qc 2010 Association for Computational Linguistics There is another approach used in the Web environment for entity set expansion", "It exploits Web page structures to identify lists of items using wrapper induction or other techniques", "The idea is that items in the same list are often of the same type", "This approach is used by Google Sets Google 2008 and BooWa", "Wang and Cohen 2008", "However as it relies on Web page structures it is not applicable to general free texts"]}, "P10111": {"title": ["PDF  PS wwwgelbukhcom"], "abstract": ["two different problems thematic segmentation into rather large topically self contained sections and splitting into paragraphs ie lexicogrammatical segmentation of lower level", "In this paper we consider the latter problem", "We propose a method of reasonably splitting text into paragraph based on a text cohesion measure", "Specifically we propose a method of quantitative evaluation of text cohesion based on a large linguistic resource  a collocation network", "At each step our algorithm compares word occurrences in a text against a large DB of collocations and semantic links between words in the given natural language", "The procedure consists in evaluation of the cohesion function its smoothing normalization and comparing with a specially constructed threshold"], "inroduction": ["In the recent decade automatic text segmentation became a popular research area 4 13 15 17 19", "In most cases thematic segmentation is considered ie the borders to be searched subdivide the text to rather long thematically selfcontained parts", "In contrast to most works in the area in this paper we propose a method for a lowlevel lexicogrammatical segmentation", "The difference between these two segmentation tasks can be explained as follows", "A good application of thematic segmentation is automatic extraction of thematically relevant parts from a long unstructured file", "When a file is too long for the user to read it through completely a computer tool  a segmentation program  is quite handy", "Another application of such segmentation is consulting a novice author on a better splitting hisher large and not yet brushed scitech text to balanced and thematically diverse sectionsAs the main tool for thematic segmentation the sets of terms belonging to each po tential segment are considered", "For example the words most frequently used in the whole text are selected the stopwords mainly functional are discarded and then the similarity between adjacent potential segments is measured across the potential border as the cosine coefficient of occurrence numbers of the rest content words", "In such a task the segmentation of the lower level ie the division of text into sentences and paragraphs is supposed to have been done", "Thus paragraphs are considered as minimal text units with already determined lengths measured in words or sentences and terminological content", "However this is by itself a problem faced by every author namely the problem of optimally splitting the text into paragraphs", "One might consider rational splitting text into paragraphs a component of general education at school in writing correct texts", "However numerous manuscripts of masterlevel students show that this component of school education is not efficient many people who have to write scitech texts do not do it well", "Specifically grammatically correct texts written by humans are frequently subdivided into paragraphs in a rather arbitrary manner impeding smooth readingAccording to 22 the rational low level structuring of scitech texts is rather diffi cult even for humans", "Besides splitting text into paragraphs it includes other difficult tasks eg introduction of numbered or dotted items", "In this paper we confine ourselves only to the task of splitting text into paragraphs", "It is a commonplace that singling out a paragraph conforms to some grammatical and logical rules that seem to be so far not formalized and thus not computable", "From this point of view the work 21 is an important step to this objective but it supposes the problem of how to represent automatically by logical terms the meaning of any sentence and a text as a whole to have been solved whereas modern computational linguistics only aims at this goal", "In this paper we propose a method of lexicogrammatical segmentation of lower level ie splitting texts into paragraphs", "It is based on the following conjectures Splitting text into paragraphs is determined by current text cohesion", "Cohesive links are clustered within paragraphs whereas the links between them are significantly weaker", "At present text cohesion has no formal definition", "A human considers a text cohesive if it consistently narrates about selected entities persons things relations actions processes properties etc", "At the level of semantic representation of text cohesion is observable in the form of linked terms and predicates of logical types but it is not well explored how one can observe the same links at the surface In such conditions it is worthwhile to suppose that text cohesion can be approximately determined through syntactic pseudosyntactic and semantic links between words in a text", "By pseudosyntactic link we mean links that are similar to syntactic ones but hold between words of different sentences for example the link between chief and demanded in the text She insulted her chief", "He demanded on apology1 Syntactic links are considered as in dependency grammars 14 which arrange words of any sentence in dependency trees", "In the example she hurriedly went through the big forest the words out of parentheses constitute a dependency subtree in this case a chain with the highlighted content words at the ends and the functional auxiliary word in between", "The words within parentheses as well as all other possible words of the sentence are linked into the same tree and other pairs of linked content words can be observed among them such as hurriedly went or big forestSyntactic links between two content words are called collocations2 whereas func tional words only subcategorize them", "Indeed collocations can be of various 1 Formally we define such a link to hold between words a and b not necessarily in the same", "sentence if in the text there is a word c coreferent with a and syntactically linked to b classes the first example above represents a combination of the ruling verb and its prepositional complement while two other examples give combinations of verb or noun with their modifiers", "Semantic links are well known", "They connect synonyms hyponym with a corresponding hyperonym the whole with its part or a word with its semantic derivative like possessor to possessive or to possess", "When occurring in the same text such words are rarely linked syntactically", "Their cooccurrences have other reason", "Namely the anaphoric coreferential entities can be represented in a text not only by direct repetitions and pronouns but also by their synonyms or hyperonyms", "A quantitative measure of cohesion implied by pseudosyntactic and semantic links can be proposed", "This measure experiences fluctuations along the texts with maximums in the middle of the sentences and minimums between them", "Some local minimums are deeper than others", "Just they should be taken as splitting borders", "This paper proposes a method of quantitative evaluation of text cohesion", "It compares word occurrences in a text against a large DB of collocations and semantic links in a given natural language", "Pseudosyntactic links are more important since within segments comparable with paragraphs by length no statistics of relevant terms can be collected", "Taking into account the cooccurrences our method processes cohesion function stage by stage ie recurrently evaluates this function smoothes it normalizes and compares it with a specially constructed threshold"]}, "P101121_p07": {"title": ["BLEUSP INVWER CDER Three improved MT evaluation measures"], "abstract": ["We present three modifications of well established automatic machine translation evaluation measures to improve correlation between those measures and human evaluation", "Following Lin  Och we present an improved version of the BLEU score which uses a smoothed geometric mean for combining different ngram precisions", "We use segment boundary markers to increase the weight of words near the segment boundaries in the BLEU score", "Our second MT evaluation measure is a variant of the WER which allows for block movements but does not demand complete and disjoint coverage of the source sentence", "As this might be problematic if MT systems are tuned on this score we later investigate a linear combination of this measure with PER", "Finally we describe an edit distance similar to TER which also allows for block reordering", "Our measure uses a full search but with the constraint that block operations must be bracketed", "We describe this measure using a Bracketing Transduction Grammar and sketch a polynomialtime algorithm for its calculation", "We also modify the WERlike measures such that they use worddependent substitution costs instead of fixed ones to model the similarity between words", "Experimental comparison of these measures show that our new measures correlate significantly better with human judgment than the original measures"], "inroduction": ["For a couple of reasons automatic evaluation of Machine Translation MT systems is a difficult task mostly because it is difficult to define when a translation is good and when it is bad", "Or which of two given translations is better and which one is worse", "The main reason for this are ambiguities in natural languages Usually there is more than one correct translation for a source sentences there are ambiguities in the choice of synonyms as well as in the order of the words", "Because of the difficulty of this task a multitude of automatic MT evaluation measures have been defined over the last couple of years", "Some of these measures have become well established for example BLEU or TER others are only of medium or small significance", "We expect that in the context of NISTs Metrics MATR evaluation more measures will be added to the pool of evaluation measures", "In a few previous papers the proposed measures seemed to be more of theoretical interest than of practical use While they certainly emphasis important linguistic effects it is not investigated systematically in how far these effects play a role in the difference in quality in different MT systems", "Some other proposed evaluation measures seemed to focus on specific properties and features of the previously generated translations they are trained or optimized on which can but does not need to lead to evaluation measures which are basically classifiers dividing previously good from previously poor systems or easy from difficult source sentences", "If measures with this property are used to tune a typical statistical MT system it can sometimes be observed that the MT system learns to play against this and might even learn to produce translations which show the good features without actually being good translations", "For example Rosti et al", "2007 report such an effect", "This is not to say that all new measures share these problems nor that there is no need for MT evaluation measures which go beyond lexical comparison  quite the opposite", "But these issues were the motivation for us to start from established evaluation measures with known properties especially with regard to tuning and alter them at a few selected points to improve their correlation with human judgment", "This paper is organized as follows In Section 2 we describe some modifications to the BLEU score following Lin and Och 2004 and Leusch et al", "2005", "We present a simple variant of WER in Section 3 called CDER which allows for block transposition similar to TER following Leusch et al", "2006", "This measure can be efficiently calculated exactly without having to resort to shift heuristics or greedy search as in TER", "The tradeoff is that this measure by itself measures basically recall not precision", "To overcome this bias we will later propose a linear combination of this measure and PER in Section 6", "Before this in Section 4 we describe another variant of TER which can be exactly calculated in polynomial time this time by restricting possible shifts to ITG constrains", "This method follows Leusch et al", "2003", "We call this measure IN VWER", "In Section 5 we introduce two simple methods following Leusch et al", "2006 to improve edit operationbased measures like PER WER TER and CDERINVWER by taking into account the lexical difference of words in a substitution operation", "After an experimental evaluation of our three proposed evaluation measures in Section 7 we conclude this paper in Section 8"]}, "P101167": {"title": ["A Random WalkBased Model for Identifying Semantic Orientation"], "abstract": ["Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification text filtering product review analysis survey response analysis and online discussion mining", "We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph and produces a polarity estimate for any given word", "The model can accurately and quickly assign a polarity sign and magnitude to any word", "It can be used both in a semisupervised setting where a training set of labeled words is used and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes", "The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon", "We also show how our method can be used for threeway classification which identifies neutral words in addition to positive and negative words", "Our experiments show that the proposed method outperforms the stateoftheart methods in the semisupervised setting and is comparable to the best reported values in the weakly supervised setting", "In addition the proposed method is faster and does not need a large corpus", "We also present extensions of our methods for identifying the polarity of foreign words and outofvocabulary words", "Microsoft Research Redmond WA USA", "Email hassanammicrosoftcom", "This research was performed while at the University of Michigan", "Department of Electrical Engineering  Computer Science University of Michigan Ann Arbor MI USA", "Email amjbaraumichedu", " Department of Electrical Engineering  Computer Science University of Michigan Ann Arbor MI USA", "Email wanchluumichedu", " Department of Electrical Engineering  Computer Science and School of Information University of Michigan Ann Arbor MI USA", "Email radevumichedu", "Submission received 15 November 2011 revised submission received 10 May 2013 accepted for publication 14 July 2013", "doi101162COLI a 00192  2014 Association for Computational Linguistics"], "inroduction": ["Identifying emotions and attitudes from unstructured text has a variety of possible applications", "For example there has been a large body of work for mining product reputation on the Web Morinaga et al 2002 Turney 2002", "Morinaga et al", "2002 have shown how product reputation mining helps with marketing and customer relation management", "The Google products catalog and many online shopping sites like Amazoncom provide customers not only with comprehensive information and reviews about a product but also with faceted sentiment summaries", "Such systems are all supported by a sentiment lexicon some even in multiple languages", "Another interesting application is mining online discussions", "An enormous number of discussion groups exist on the Web", "Millions of users post content to these groups covering pretty much every possible topic", "Tracking a participant attitude toward different topics and toward other participants is a very important task that makes use of sentiment lexicons", "For example Tong 2001 presented the concept of sentiment timelines", "His system classifies discussion posts about movies as either positive or negative", "This is used to produce a plot of the number of positive and negative sentiment messages over time", "All these applications would benefit from an automatic way of identifying semantic orientation of words", "In this article we study the task of automatically identifying the semantic orientation of any word by analyzing its relations to other words Automatically classifying words as positive negative or neutral enables us to automatically identify the polarity of larger pieces of text", "This could be a very useful building block for systems that mine surveys product reviews and online discussions", "We apply a Markov random walk model to a large semantic relatedness graph producing a polarity estimate for any given word", "Previous work on identifying the semantic orientation of words has addressed the problem as both a semisupervised Takamura Inui and Okumura 2005 and a weakly supervised Turney and Littman 2003 learning problem", "In the semi supervised setting a training set of labeled words is used to train the model", "In the weakly supervised setting only a handful of seeds are used to define the two polarity classes", "Our proposed method can be used both in a semisupervised and in a weakly supervised setting", "Empirical experiments on a labeled set of positive and negative words show that the proposed method outperforms the stateoftheart methods in the semisupervised setting", "The results in the weakly supervised setting are comparable to the best reported values", "The proposed method has the advantages that it is faster and does not need a large training corpus", "The rest of the article is structured as follows", "In Section 2 we review related work on word polarity and subjectivity classification and note applications of the random walk and hitting times framework", "Section 3 presents our method for identifying word polarity", "We describe how the proposed method can be extended to cover foreign languages in Section 4 and outofvocabulary words in Section 5", "Section 6 describes our experimental setup", "We present our conclusions in Section 7"]}, "P101194": {"title": ["A New  Czech Morphological Analyser  ajka"], "abstract": ["new Czech morphological analyser ajka which is based on the algorithmic description of the Czech formal morphology", "First we present two most important wordforming processes in Czech  inflection and derivation", "A brief description of the data structures used for storing morphological information as well as a discussion of the efficient storage of lexical items stem bases of Czech words is included too", "Finally we bring some interesting features of the designed and implemented system ajka together with current statistic data"], "inroduction": ["Typically morphological analysis returns the base form lemma and associates it with all the possible POS partofspeech labels together with all grammatical information for each known word form", "In analytical languages a simple approach can be taken it is enough to list all word forms to catch the most of morphological processes", "In English for example a regular verb has usually only 4 distinct forms and irregular ones have at most 8 forms", "On the other hand the highly inflectional languages like Czech or Finnish present a difficulty for such simple approaches as the expansion of the dictionary is at least an order of magnitude greater1 4", "Specialised finitestate compilers have been implemented 1 which allow the use of specific operations for combining base forms and affixes and applying rules for morphophonological variations 3", "Descriptions of morphological analysers for other languages can be found in 8 11", "Basically there are three major types of wordforming processes  inflection derivation and compounding", "Inflection refers to the systematic modification of a stem by means of prefixes and suffixes", "Inflected forms express morphological distinctions like case or number but do not change meaning or POS", "In contrast the process of derivation usually causes change in meaning and often change of POS", "Compounding deals with the process of merging several word bases to form a new word", "1 As our effective implementation of spellchecker for Czech based on finite state au", "tomata suggests it does not necessarily mean that no application can take advantage of a simple listing of word forms in highly inflecting languages", "Czech belongs to the family of inflectional languages which are characterised by the fact that one morpheme typically an ending carries the values of several grammatical categories together for example an ending of nouns typically expresses a value of grammatical category of case number and gender", "This feature requires a special treatment of Czech words in text processing systems", "To this end we developed a universal morphological analyser which performs the morphological analysis based on dividing all words in Czech texts to their smallest relevant components that we call segments  The notion of segment roughly corresponds to the linguistic concept morpheme  which denotes the smallest meaningful unit of a language", "Presented morphological analyser consists of three major parts a formal description of morphological processes via morphological patterns an assignment of Czech stems to their relevant patterns and a morphological analysis algorithm", "The description of Czech formal morphology is represented by a system of inflectional patterns and sets of endings and it includes lists of segments and their correct combinations", "The assignment of Czech stems to their patterns is contained in the Czech Machine Dictionary 10", "Finally algorithm of morphological analysis using this information splits each word into appropriate segments", "The morphological analyser is being used for lemmatisation and morphological tagging of Czech texts in large corpora as well as for generating correct word forms and also as a spelling checker", "It can also be applied to other problems that arise in the area of processing Czech texts eg creating stop lists for building indexes used in information retrieval systems"]}, "P11-1053": {"title": ["Semisupervised Relation Extraction with Largescale Word Clustering"], "abstract": ["We present a simple semisupervised relation extraction system with largescale word clustering", "We focus on systematically exploring the effectiveness of different clusterbased features", "We also propose several statistical methods for selecting clusters at an appropriate level of granularity", "When training on different sizes of data our semisupervised approach consistently outperformed a stateoftheart supervised baseline system"], "inroduction": ["Relation extraction is an important information extraction task in natural language processing NLP with many practical applications", "The goal of relation extraction is to detect and characterize semantic relations between pairs of entities in text", "For example a relation extraction system needs to be able to extract an Employment relation between the entities US soldier and US in the phrase US soldier", "Current supervised approaches for tackling this problem in general fall into two categories feature based and kernel based", "Given an entity pair and a sentence containing the pair both approaches usually start with multiple level analyses of the sentence such as tokenization partial or full syntactic parsing and dependency parsing", "Then the feature based method explicitly extracts a variety of lexical syntactic and semantic features for statistical learning either generative or discriminative Miller et al 2000 Kambhatla 2004 Boschee et al 2005 Grishman et al 2005 Zhou et al 2005 Jiang and Zhai 2007", "In contrast the kernel based method does not explicitly extract features it designs kernel functions over the structured sentence representations sequence dependency or parse tree to capture the similarities between different relation instances Zelenko et al 2003 Bunescu and Mooney 2005a Bunescu and Mooney 2005b Zhao and Grishman 2005 Zhang et al 2006 Zhou et al 2007 Qian et al 2008", "Both lines of work depend on effective features either explicitly or implicitly", "The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features", "For example unless the example US soldier has previously been seen in the training data it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not", "Because the syntactic feature of the phrase US soldier is simply a nounnoun compound which is quite general the words in it are crucial for extracting the relation", "This motivates our work to use word clusters as additional features for relation extraction", "The assumption is that even if the word soldier may never have been seen in the annotated Employment relation instances other words which share the same cluster membership with soldier such as president and ambassador may have been observed in the Employment instances", "The absence of lexical features can be compensated by 521 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 521529 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics the cluster features", "Moreover word clusters may implicitly correspond to different relation classes", "For example the cluster of president may be related to the Employment relation as in US president while the cluster of businessman may be related to the Affiliation relation as in US businessman", "The main contributions of this paper are we explore the clusterbased features in a systematic way and propose several statistical methods for selecting effective clusters", "We study the impact of the size of training data on cluster features and analyze the performance improvements through an extensive experimental study", "The rest of this paper is organized as follows Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm", "Section 4 describes in detail a stateoftheart supervised baseline system", "Section 5 describes the cluster based features and the cluster selection methods", "Though Boschee et al", "2005 and Chan and Roth 2010 used word clusters in relation extraction they shared the same limitation as the above approaches in choosing clusters", "For example Boschee et al", "2005 chose clusters of different granularities and Chan and Roth 2010 simply used a single threshold for cutting the word hierarchy", "Moreover Boschee et al", "2005 only augmented the predicate typically a verb or a noun of the most importance in a relation in their definition with word clusters while Chan and Roth 2010 performed this for any lexical feature consisting of a single word", "In this paper we systematically explore the effectiveness of adding word clusters to different lexical features", "3 Background", "31 Relation Extraction", "One of the well defined relation extraction tasks is 1 We present experimental results in Section 6 and the Automatic Content Extraction ACE program conclude in Section 7"]}, "P11-1056": {"title": ["Exploiting SyntacticoSemantic Structures for Relation Extraction"], "abstract": ["In this paper we observe that there exists a second dimension to the relation extraction RE problem that is orthogonal to the relation type dimension", "We show that most of these second dimensional structures are relatively constrained and not difficult to identify", "We propose a novel algorithmic approach to RE that starts by first identifying these structures and then within these identifying the semantic type of the relation", "In the real RE problem where relation arguments need to be identified exploiting these structures also allows reducing pipelined propagated errors", "We show that this RE framework provides significant improvement in RE performance"], "inroduction": ["Relation extraction RE has been defined as the task of identifying a given set of semantic binary relations in text", "For instance given the span of text ", "the Seattle zoo   ", " one would like to extract the relation that the Seattle zoo is locatedat Seattle", "RE has been frequently studied over the last few years as a supervised learning task learning from spans of text that are annotated with a set of semantic relations of interest", "However most approaches to RE have assumed that the relations arguments are given as input Chan and Roth 2010 Jiang and Zhai 2007 Jiang 2009 Zhou et al 2005 and therefore offer only a partial solution to the problem", "Conceptually this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest", "However these approaches to RE require a large amount of manually annotated training data to achieve good performance making it difficult to expand the set of target relations", "Moreover as we show these approaches become brittle when the relations arguments are not given but rather need to be identified in the data too", "In this paper we build on the observation that there exists a second dimension to the relation extraction problem that is orthogonal to the relation type dimension all relation types are expressed in one of several constrained syntacticosemantic structures", "As we show identifying where the text span is on the syntacticosemantic structure dimension first can be leveraged in the RE process to yield improved performance", "Moreover working in the second dimension provides robustness to the real RE problem that of identifying arguments along with the relations between them", "For example in the Seattle zoo the entity mention Seattle modifies the noun zoo", "Thus the two mentions Seattle and the Seattle zoo are involved in what we later call a premodifier relation one of several syntacticosemantic structures we identify in Section 3", "We highlight that all relation types can be expressed in one of several syntacticosemantic structures  Premodifiers Possessive Preposition Formulaic and Verbal", "As it turns out most of these structures are relatively constrained and are not difficult to identify", "This suggests a novel algorithmic approach to RE that starts by first identifying these structures and then within these identifying the semantic type of the relation", "Not only does this approach provide significantly improved RE perfor 551 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 551560 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics mance it carries with it two additional advantages", "First leveraging the syntacticosemantic structure is especially beneficial in the presence of small amounts of data", "Second and more important is the fact that exploiting the syntacticosemantic dimension provides several new options for dealing with the full RE problem  incorporating the argument identification into the problem", "We explore one of these possibilities making use of the constrained structures as a way to aid in the identification of the relations arguments", "We show that this already provides significant gain and discuss other possibilities that can be explored", "The contributions of this paper are summarized below  We highlight that all relation types are expressed as one of several syntacticosemantic structures and show that most of these are relatively constrained and not difficult to identify", "Consequently working first in this structural dimension can be leveraged in the RE process to improve performance", " We show that when one does not have a large number of training examples exploiting the syntacticosemantic structures is crucial for RE performance", " We show how to leverage these constrained structures to improve RE when the relations arguments are not given", "The constrained structures allow us to jointly entertain argument can didates and relations built with them as arguments", "Specifically we show that considering argument candidates which otherwise would have been discarded provided they exist in syntacticosemantic structures we reduce error propagation along a standard pipeline RE architecture and that this joint inference process leads to improved RE performance", "In the next section we describe our relation extraction framework that leverages the syntactico semantic structures", "We then present these structures in Section 3", "We describe our mention entity typing system in Section 4 and features for the RE system in Section 5", "We present our RE experiments in Section 6 and perform analysis in Section 7 before concluding in Section 8", "S  premodifier possessive preposition formulaic gold mentions in training data Mtrain Dg  mi  mj   Mtrain  Mtrain  mi in same sentence as mj  i  j  i  j REbase  RE classifier trained on Dg Ds   for each mi  mj   Dg do p  structure inference on mi  mj  using patterns if p  S  mi  mj  was annotated with a S structure Ds  Ds  mi  mj  done REs  RE classifier trained on Ds Output REbase and REs Figure 1 Training a regular baseline RE classifier REbase and a RE classifier leveraging syntactico semantic structures REs "]}, "P11-1062": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 610 619 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics Global Learning of Typed Entailment Rules Jonathan Berant Tel Aviv University Tel Aviv Israel jonatha6posttauacil Ido Dagan BarIlan University RamatGan Israel dagancsbiuacil Jacob Goldberger BarIlan University RamatGan Israel goldbejengbiuacil Abstract Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference In this paper we propose an algorithm that utilizes transitivity constraints to learn a globallyoptimal set of entailment rules for typed predicates We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs We apply the algorithm over a large data set of extracted predicate instances from which a resource of typed entailment rules has been recently released Schoenmackers et al 2010 Our results show that using global transitivity information substantially improves perfor mance over this resource and several baselines and that our scaling methods allow us to increase the scope of global learning of entailmentrule graphs 1 Introduction Generic approaches for applied semantic inference from text gained growing attention in recent years particularly under the Textual Entailment TE framework Dagan et al 2009 TE is a generic paradigm for semantic inference where the objective is to recognize whether a target meaning can be inferred from a given text A crucial component of inference systems is extensive resources of entailment rules also known as inference rules ie rules that specify a directional inference relation between fragments of text One important type of rule is rules that specify entailment relations between predicates and their arguments For example the rule  X annex Y X control Y helps recognize that the text Japan annexed Okinawa answers the question  Which country controls Okinawa Thus acquisition of such knowledge received considerable attention in the last decade Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2009 Schoenmackers et al 2010 Most past work took a local learning approach learning each entailment rule independently of others It is clear though that there are global interactions between predicates Notably entailment is a transitive relation and so the rules A  B and B  C imply A C Recently Berant et al 2010 proposed a global graph optimization procedure that uses Integer Linear Programming ILP to find the best set of entailment rules under a transitivity constraint Imposing this constraint raised two challenges The first of ambiguity transitivity does not always hold when predicates are ambiguous eg X buy Y X acquire Y and X acquire Y  X learn Y but X buy Y 9 X learn Y since these two rules correspond to two different senses of acquire The second challenge is scalability ILP solvers do not scale well since ILP is an NPcomplete problem Berant et al circumvented these issues by learning rules where one of the predicate s arguments is instantiated eg  X reduce nausea X affect nausea which is useful for learning small graphs onthefly given a target con cept such as nausea While rules may be effectively learned when needed their scope is narrow and they are not useful as a generic knowledge resource This paper aims to take global rule learning one step further To this end we adopt the representation suggested by Schoenmackers et al 2010 who learned inference rules between typed predicates ie predicates where the argument types eg city or drug are specified Schoenmackers et al uti610 lized typed predicates since they were dealing with noisy and ambiguous web text Typing predicates helps disambiguation and filtering of noise while still maintaining rules of wideapplicability Their method employs a local learning approach while the number of predicates in their data is too large to be handled directly by an ILP solver In this paper we suggest applying global optimization learning to open domain typed entailment rules To that end we show how to construct a structure termed typed entailment graph where the nodes are typed predicates and the edges represent entailment rules We suggest scaling techniques that allow to optimally learn such graphs over a large set of typed predicates by first decomposing nodes into components and then applying incremental ILP Riedel and Clarke 2006 Using these techniques the obtained algorithm is guaranteed to return an optimal solution We ran our algorithm over the data set of Schoenmackers et al and release a resource of 30000 rules1 that achieves substantially higher recall without harming precision To the best of our knowledge this is the first resource of that scale to use global optimization for learning predicative entailment rules Our evaluation shows that global transitivity improves the F1 score of rule learning by 27 over several baselines and that our scaling techniques allow dealing with larger graphs resulting in improved coverage 2 Background Most work on learning entailment rules between predicates considered each rule independently of others using two sources of information lexico graphic resources and distributional similarity Lexicographic resources are manuallyprepared knowledge bases containing semantic information on predicates A widelyused resource is WordNet Fellbaum 1998 where relations such as synonymy and hyponymy can be used to generate rules Other resources include NomLex Macleod et al 1998 Szpektor and Dagan 2009 and FrameNet Baker and Lowe 1998 Ben Aharon et al 2010 Lexicographic resources are accurate but have 1The resource can be downloaded from httpwwwcstauacilj onatha6homepage filesresources ACL2011Resourcezip low coverage Distributional similarity algorithms use large corpora to learn broader resources by assuming that semantically similar predicates appear with similar arguments These algorithms usually represent a predicate with one or more vectors and use some function to compute argument similarity Distributional similarity algorithms differ in their feature representation Some use a binary representation each predicate is represented by one feature vector where each feature is a pair of arguments Szpektor et al 2004 Yates and Etzioni 2009 This representation performs well but suffers when data is sparse The binaryDIRT representation deals with sparsity by representing a predicate with a pair of vectors one for each argument Lin and Pantel 2001 Last a richer form of representation termed unary has been suggested where a different predicate is defined for each argument Szpektor and Dagan 2008 Different algorithms also differ in their similarity function Some employ symmetric functions geared towards paraphrasing bidirectional entailment while others choose directional measures more suited for entailment Bha gat et al 2007 In this paper We employ several such functions such as Lin Lin and Pantel 2001 and BInc Szpektor and Dagan 2008 Schoenmackers et al 2010 recently used distributional similarity to learn rules between typed predicates where the lefthand side of the rule may contain more than a single predicate horn clauses In their work they used Hearstpatterns Hearst 1992 to extract a set of 29 million argument type pairs from a large web crawl Then they employed several filtering methods to clean this set and au tomatically produced a mapping of 11 million arguments into 156 types Examples for argument type pairs are EXODUS book CHINA coun try and ASTHMA disease Schoenmackers et al then utilized the types the mapped arguments and tuples from TextRunner Banko et al 2007 to generate 10672 typed predicates such as con quercountrycity and common indiseaseplace and learn 30000 rules between these predicates2 In this paper we will learn entailment rules over the same data set which was generously provided by 2The rules and the mapping of arguments into types can be downloaded from httpwwwcswashingtoneduresearch sherlockhornclauses 611 Schoenmackers et al As mentioned above Berant et al 2010 used global transitivity information to learn small entailment graphs Transitivity was also used as an information source in other fields of NLP Taxonomy Induction Snow et al 2006 Coreference Resolution Finkel and Manning 2008 Temporal Information Extraction Ling and Weld 2010 and Unsupervised Ontology Induction Poon and Domingos 2010 Our proposed algorithm applies to any sparse transitive relation and so might be applicable in these fields as well Last we formulate our optimization problem as an Integer Linear Program ILP ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints Scaling ILP is challenging since it is an NPcomplete problem ILP has been extensively used in NLP lately Clarke and Lapata 2008 Martins et al 2009 Do and Roth 2010 3 Typed Entailment Graphs Given a set of typed predicates entailment rules can only exist between predicates that share the same unordered pair of types such as place and country3 Hence every pair of types defines a graph that describes the entailment relations between predicates sharing those types Figure 1 Next we show how to represent entailment rules between typed predicates in a structure termed typed entailment graph which will be the learning goal of our algorithm A typed entailment graph is a directed graph where the nodes are typed predicates A typed predicate is a triple pt1 t2 representing a predicate in natural language p is the lexical realization of the predicate and the types t1 t2 are variables representing argument types These are taken from a set of types T  where each type t  T is a bag of natural language words or phrases Examples for typed predicates are conquercountrycity and containproductmaterial An instance of a typed predicate is a triple pa1 a2 where a1  t1 and a2  t2 are termed arguments For example be common inASTHMAAUSTRALIA is an instance of be common indiseaseplace For brevity we refer 3Otherwise the rule would contain unbound variables to typed entailment graphs and typed predicates as entailment graphs and predicates respectively Edges in typed entailment graphs represent en tailment rules an edge u v means that predicate u entails predicate v If the type t1 is different from the type t2 mapping of arguments is straightforward as in the rule  be find inmaterialproduct  containproductmaterial We term this a twotypes entailment graph When t1 and t2 are equal mapping of arguments is ambiguous we distinguish directmapping edges where the first argument on the lefthandside LHS is mapped to the first argument on the righthandside RHS as in  beatteamteam d  defeatteamteam and reversedmapping edges where the LHS first argument is mapped to the RHS second argument as in  beatteamteam r  lose toteamteam We term this a singletype entailment graph Note that in singletype entailment graphs reversedmapping loops are possible as in  playteamteam r  playteamteam  if team A plays team B then team B plays team A Since entailment is a transitive relation typedentailment graphs are transitive if the edges u v and v w are in the graph so is the edge uw Note that in singletype entailment graphs one needs to consider whether mapping of edges is direct or reversed if mapping of both u v and v w is either direct or reversed mapping of uw is direct otherwise it is reversed Typing plays an important role in rule transitivity if predicates are ambiguous transitivity does not necessarily hold However typing predicates helps disambiguate them and so the problem of ambiguity is greatly reduced 4 Learning Typed Entailment Graphs Our learning algorithm is composed of two steps"], "abstract": ["Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference", "In this paper we propose an algorithm that utilizes transitivity constraints to learn a globallyoptimal set of entailment"], "inroduction": ["as a graph learning problem and suggest methods that scale the algorithm to larger graphs", "We apply the algorithm over a large data set of extracted predicate instances from which a resource of typed entailment rules has been recently released Schoenmackers et al 2010", "Our results show that using global transitivity information substantially improves performance over this resource and several baselines and that our scaling methods allow us to increase the scope of global learning of entailmentrule graphs", "1 Introduction", "Generic approaches for applied semantic inference from text gained growing attention in recent years particularly under the Textual Entailment TE framework Dagan et al 2009", "TE is a generic paradigm for semantic inference where the objective is to recognize whether a target meaning can be inferred from a given text", "A crucial component of inference systems is extensive resources of entailment rules also known as inference rules ie rules that specify a directional inference relation between fragments of text", "One important type of rule is rules that specify entailment relations between predicates and their arguments", "For example the rule X annex Y  X control Y helps recognize that the text Japan annexed Okinawa answers the question Which country controls Okinawa", "Thus acquisition of such knowledge received considerable attention in the last decade Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2009 Schoenmackers et al 2010", "Most past work took a local learning approach learning each entailment rule independently of others", "It is clear though that there are global interactions between predicates", "Notably entailment is a transitive relation and so the rules A  B and B  C imply A  C  Recently Berant et al", "2010 proposed a global graph optimization procedure that uses Integer Linear Programming ILP to find the best set of entail ment rules under a transitivity constraint", "Imposing this constraint raised two challenges", "The first of ambiguity transitivity does not always hold when predicates are ambiguous eg X buy Y  X acquire Y and X acquire Y  X learn Y but X buy Y  Xlearn Y since these two rules correspond to two dif ferent senses of acquire", "The second challenge is scalability ILP solvers do not scale well since ILP is an NPcomplete problem", "Berant et al circumvented these issues by learning rules where one of the predicates arguments is instantiated eg X re duce nausea  X affect nausea which is useful forlearning small graphs onthefly given a target con cept such as nausea", "While rules may be effectively learned when needed their scope is narrow and they are not useful as a generic knowledge resource", "This paper aims to take global rule learning one step further", "To this end we adopt the representation suggested by Schoenmackers et al", "2010 who learned inference rules between typed predicates ie predicates where the argument types eg city or drug are specified", "Schoenmackers et al uti 610 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 610619 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics lized typed predicates since they were dealing with noisy and ambiguous web text", "Typing predicates helps disambiguation and filtering of noise while still maintaining rules of wideapplicability", "Their method employs a local learning approach while the number of predicates in their data is too large to be handled directly by an ILP solver", "In this paper we suggest applying global optimization learning to open domain typed entailment rules", "To that end we show how to construct a structure termed typed entailment graph where the nodes are typed predicates and the edges represent entailment rules", "We suggest scaling techniques that allow to optimally learn such graphs over a large set of typed predicates by first decomposing nodes into components and then applying incremental ILP Riedel and Clarke 2006", "Using these techniques the obtained algorithm is guaranteed to return an optimal solution", "We ran our algorithm over the data set of Schoenmackers et al and release a resource of 30000 rules1 that achieves substantially higher recall without harming precision", "To the best of our knowledge this is the first resource of that scale to use global optimization for learning predicative entailment rules", "Our evaluation shows that global transitivity improves the F1 score of rule learning by27 over several baselines and that our scaling tech niques allow dealing with larger graphs resulting in improved coverage"]}, "P11-1082": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 814824"], "abstract": [], "inroduction": ["Noun phrase NP coreference resolution is the task of determining which NPs in a text or dialogue refer to the same realworld entity", "The difficulty of the task stems in part from its reliance on world knowledge Charniak 1972", "To exemplify consider the following text fragment", "Martha Stewart is hoping people dont run out on her", "The celebrity indicted on charges stemming from   ", "Having the world knowledge that Martha Stewart is a celebrity would be helpful for establishing the coreference relation between the two NPs", "One may argue that employing heuristics such as subject preference or syntactic parallelism which prefers resolving an NP to a candidate antecedent that has the same grammatical role in this example would also allow us to correctly resolve the celebrity Mitkov 814 and Marcu 2005 Ng 2007 and coreference annotated data eg Bengtson and Roth 2008", "While each of these three sources of world knowledge has been shown to improve coreference resolution the improvements were typically obtained by incorporating world knowledge as features into a baseline resolver composed of a rather weak coreference model ie the mentionpair model and a small set of features ie the 12 features adopted by Soon et als 2001 knowledgelean approach", "As a result some questions naturally arise", "First can world knowledge still offer benefits when used in combination with a richer set of features", "Second since automatically extracted world knowledge is typically noisy Ponzetto and Poesio 2009 are recentlydeveloped coreference models more noise tolerant than the mentionpair model and if so can they profit more from the noisily extracted world knowledge", "Finally while different world knowl Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 814824 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics edge sources have been shown to be useful when applied in isolation to a coreference system do they offer complementary benefits and therefore can further improve a resolver when applied in combination", "We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learningbased coreference resolution", "Specifically we 1 derive world knowledge from encyclopedic sources that are under investigated for coreference resolution including FrameNet Baker et al 1998 and YAGO Suchanek et al 2007 in addition to coreferenceannotated data and unannotated data 2 incorporate such knowledge as features into a richer baseline feature set that we previously employed Rahman and Ng 2009 and 3 evaluate their utility using two coreference models the traditional mentionpair model Soon et al 2001 and the recently developed clusterranking model Rahman and Ng 2009", "Our evaluation corpus contains 410 documents which are coreferenceannotated using the ACE annotation scheme as well as the OntoNotes annotation scheme Hovy et al 2006", "By evaluating on two sets of coreference annotations for the same set of documents we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to annotate the documents"]}, "P11-1087": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 865 874 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics A Hierarchical PitmanYor Process HMM for Unsupervised Part of Speech Induction Phil Blunsom Department of Computer Science University of Oxford PhilBlunsomcsoxacuk Trevor Cohn Department of Computer Science University of Sheffield TCohndcsshefacuk Abstract In this work we address the problem of unsupervised partofspeech induction by bringing together several strands of research into a single model We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical PitmanYor processes prior providing an elegant and principled means of incorporating lexical characteristics Central to our approach is a new typebased sampling algorithm for hierarchical PitmanYor models in which we track fractional table counts In an empirical evaluation we show that our model consistently outperforms the current stateoftheart across 10 languages 1 Introduction Unsupervised partofspeech PoS induction has long been a central challenge in computational linguistics with applications in human language learning and for developing portable language processing systems Despite considerable research effort progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al 1992 approach Christodoulopoulos et al 2010 One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar however this limits portability and also comprimises any cognitive conclusions In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing stateoftheart across all our corpora in 10 different languages Moreover the performance of our unsupervised model approaches that of many existing semisupervised systems despite our method not receiving any human input In this paper we present a Bayesian hidden Markov model HMM which uses a nonparametric prior to infer a latent tagging for a sequence of words HMMs have been popular for unsupervised PoS induction from its very beginnings Brown et al 1992 and justifiably so as the most discriminating feature for deciding a word s PoS is its local syntactic context Our work brings together several strands of research including Bayesian nonparametric HMMs Goldwater and Griffiths 2007 PitmanYor language models Teh 2006b Goldwater et al 2006b tagging constraints over word types Brown et al 1992 and the incorporation of morphological features Clark 2003 The result is a nonparametric Bayesian HMM which avoids overfitting contains no free parameters and exhibits good scaling properties Our model uses a hierarchical PitmanYor process PYP prior to affect sophisicated smoothing over the transition and emission distributions This allows the modelling of subword structure thereby capturing tagspecific morphological variation Unlike many existing approaches our model is a principled generative model and does not include any hand tuned language specific features Inspired by previous successful approaches Brown et al 1992 we develop a new typelevel inference procedure in the form of an MCMC sampler with an approximate method for incorporating the complex dependencies that arise between jointly sampled events Our experimental evaluation demonstrates that our model particularly when restricted to a single tag per type produces 865 stateoftheart results across a range of corpora and languages 2 Background Past research in unsupervised PoS induction has largely been driven by two different motivations a task based perspective which has focussed on inducing word classes to improve various applications and a linguistic perspective where the aim is to induce classes which correspond closely to anno tated partofspeech corpora Early work was firmly situtated in the taskbased setting of improving generalisation in language models Brown et al 1992 presented a simple firstorder HMM which restricted word types to always be generated from the same class Though PoS induction was not their aim this restriction is largely validated by empirical analysis of treebanked data and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time allowing very efficient inference using the exchange algorithm This model has been popular for language modelling and bilingual word alignment and an implementation with improved inference called mkcls Och 19991 has become a standard part of statistical machine translation systems The HMM ignores orthographic information which is often highly indicative of a words partofspeech particularly so in morphologically rich languages For this reason Clark 2003 extended Brown et al 1992 s HMM by incorporating a character language model allowing the modelling of limited morphology Our work draws from these models in that we develop a HMM with a one class per tag restriction and include a character level language model In contrast to these previous works which use the maximum likelihood estimate we develop a Bayesian model with a rich prior for smoothing the parameter estimates allowing us to move to a trigram model A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori Smith and Eisner 2005 Haghighi and Klein 2006 Goldwater and Griffiths 2007 Toutanova and Johnson 2008 Ravi and Knight 2009 These systems achieve 1Available from httpfjochcommkclshtml much higher accuracy than fully unsupervised systems though it is unclear whether the tag dictionary assumption has real world application We focus solely on the fully unsupervised scenario which we believe is more practical for text processing in new languages and domains Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data Goldwater and Griffiths 2007 Johnson 2007 Gao and Johnson 2008 These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag Conversely Ganchev et al 2010 developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags Recently Lee et al 2010 combined the one class per word type constraint Brown et al 1992 in a HMM with a Dirichlet prior to achieve both forms of sparsity However this work approximated the derivation of the Gibbs sampler omitting the interdependence between events when sampling from a collapsed model resulting in a model which underperformed Brown et al 1992 s oneclass HMM Our work also seeks to enforce both forms of sparsity by developing an algorithm for typelevel inference under the one class constraint This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior such that our model jointly infers distributions over tag trigrams bigrams and unigrams and whole words and their character level representation This smoothing is critical to ensure adequate generalisation from small data samples Research in language modelling Teh 2006b Goldwater et al 2006a and parsing Cohn et al 2010 has shown that models employing PitmanYor priors can significantly outperform the more frequently used Dirichlet priors especially where complex hierarchical relationships exist between latent variables In this work we apply these advances to unsupervised PoS tagging developing a HMM smoothed using a PitmanYor process prior 866 3 The PYPHMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags t and words w as P tw  L1 l1 P tltl1 tl2Pwltl  where L  w  t and t0  t 1  tL1   are assigned a sentinel value to denote the start or end of the sentence A key decision in formulating such a model is the smoothing of the tag trigram and emission distributions which would otherwise be too difficult to estimate from small datasets Prior work in unsupervised PoS induction has employed simple smoothing techniques such as additive smoothing or Dirichlet priors Goldwater and Griffiths 2007 Johnson 2007 however this body of work has overlooked recent advances in smoothing methods used for language modelling Teh 2006b Goldwater et al 2006b Here we build upon previous work by developing a PoS induction model smoothed with a sophisticated nonparametric prior Our model uses a hierarchical PitmanYor process prior for both the transition and emission distributions encoding a backoff path from complex distributions to successsively simpler ones The use of complex distributions eg over tag trigrams allows for rich expressivity when sufficient evidence is available while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise The PYP has been shown to generate distributions particularly well suited to modelling language Teh 2006a Goldwater et al 2006b and has been shown to be a generalisation of KneserNey smoothing widely recognised as the best smoothing method for language modelling Chen and Goodman 1996"], "abstract": ["In this work we address the problem of unsupervised partofspeech induction by bringing together several strands of research into a single model", "We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical PitmanYor processes prior providing an elegant and principled means of incorporating lexical characteristics", "Central to our approach is a new typebased sampling algorithm for hierarchical PitmanYor models in which we track fractional table counts", "In an empirical evaluation we show that our model consistently outperforms the current stateoftheart across 10 languages"], "inroduction": ["Unsupervised partofspeech PoS induction has long been a central challenge in computational linguistics with applications in human language learning and for developing portable language processing systems", "Despite considerable research effort progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al", "1992 approach Christodoulopoulos et al 2010", "One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar however this limits portability and also comprimises any cognitive conclusions", "In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing stateoftheart across all our corpora in 10 different languages", "Moreover the performance of our unsupervised model approaches 865 that of many existing semisupervised systems despite our method not receiving any human input", "In this paper we present a Bayesian hidden Markov model HMM which uses a nonparametric prior to infer a latent tagging for a sequence of words", "HMMs have been popular for unsupervised PoS induction from its very beginnings Brown et al 1992 and justifiably so as the most discriminating feature for deciding a words PoS is its local syntactic context", "Our work brings together several strands of research including Bayesian nonparametric HMMs Goldwater and Griffiths 2007 PitmanYor language models Teh 2006b Goldwater et al 2006b tagging constraints over word types Brown et al 1992 and the incorporation of morphological features Clark 2003", "The result is a nonparametric Bayesian HMM which avoids overfitting contains no free parameters and exhibits good scaling properties", "Our model uses a hierarchical PitmanYor process PYP prior to affect sophisicated smoothing over the transition and emission distributions", "This allows the modelling of subword structure thereby capturing tagspecific morphological variation", "Unlike many existing approaches our model is a principled generative model and does not include any hand tuned language specific features", "Inspired by previous successful approaches Brown et al 1992 we develop a new type level inference procedure in the form of an MCMC sampler with an approximate method for incorporating the complex dependencies that arise between jointly sampled events", "Our experimental evaluation demonstrates that our model particularly when restricted to a single tag per type produces Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 865874 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics stateoftheart results across a range of corpora and languages"]}, "P11-1125": {"title": ["Machine Translation System Combination by Confusion Forest"], "abstract": ["The stateoftheart system combination method for machine translation MT is based on confusion networks constructed by aligning hypotheses with regard to word similarities", "We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest a packed forest representing alternative trees", "The forest is generated using syntactic consensus among parsed hypotheses First MT outputs are parsed", "Second a context free grammar is learned by extracting a set of rules that constitute the parse trees", "Third a packed forest is generated starting from the root symbol of the extracted grammar through nonterminal rewriting", "The new hypothesis is produced by searching the best derivation in the forest", "Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space"], "inroduction": ["System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields such as speech recognition Fiscus 1997 Mangu et al 2000 or parsing Henderson and Brill 1999", "One of the stateoftheart system combination methods for MT is based on confusion networks which are compact graphbased structures representing multiple hypotheses Bangalore et al 2001", "Confusion networks are constructed based on string similarity information", "First one skeleton or backbone sentence is selected", "Then other hypotheses are aligned against the skeleton forming a lattice with each arc representing alternative word candidates", "The alignment method is either modelbased Matusov et al 2006 He et al 2008 in which a statistical word aligner is used to compute hypothesis alignment or editbased Jayaraman and Lavie 2005 Sim et al 2007 in which alignment is measured by an evaluation metric such as translation error rate TER Snover et al 2006", "The new translation hypothesis is generated by selecting the best path through the network", "We present a novel method for system combination which exploits the syntactic similarity of system outputs", "Instead of constructing a stringbased confusion network we generate a packed forest Billot and Lang 1989 Mi et al 2008 which encodes exponentially many parse trees in a polynomial space", "The packed forest or confusion forest is constructed by merging the MT outputs with regard to their syntactic consensus", "We employ a grammarbased method to generate the confusion forest First system outputs are parsed", "Second a set of rules are extracted from the parse trees", "Third a packed forest is generated using a variant of Earleys algorithm Earley 1970 starting from the unique root symbol", "New hypotheses are selected by searching the best derivation in the forest", "The grammar a set of rules is limited to those found in the parse trees", "Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each nonterminal symbol such as parent and sibling labels using the state representation in Earleys algorithm", "1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 12491257 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation WMT10 in four directions Czech French German Spanishto English CallisonBurch et al 2010 and we found comparable performance to the conventional confusion network based system combination in two language pairs and statistically significant improve  I saw the forest I walked the blue forest I saw the green trees the forest was found a Pairwise alignment using the first starred hypothesis as a skeleton", "ments in the others", "First we will review the stateoftheart method which is a system combination framework based on confusion networks 2", "Then we will introduce I saw   walked the blue  forest green trees  was found a novel system combination method based on con b Confusion network from a fusion forest 3 and present related work in consensus translations 4", "Experiments are presentedin Section 5 followed by discussion and our conclu sion", "I saw   walked the blue green forest trees was  found "]}, "P11-1159": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 1586 1596 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics Improving Arabic Dependency Parsing with Formbased and Functional Morphological Features Yuval Marton"], "abstract": ["We explore the contribution of morphological features  both lexical and inflectional  to dependency parsing of Arabic a morphologically rich language", "Using controlled experiments we find that definiteness person number gender and the undiacritzed lemma are most helpful for parsing on automatically tagged input", "We further contrast the contribution of formbased and functional features and show that functional gender and number eg broken plurals and the related rationality feature improve over formbased features", "It is the first time functional morphological features are used for Arabic NLP"], "inroduction": ["Parsers need to learn the syntax of the modeled language in order to project structure on newly seen sentences", "Parsing model design aims to come up with features that best help parsers to learn the syntax and choose among different parses", "One aspect of syntax which is often not explicitly modeled in parsing involves morphological constraints on syntactic structure such as agreement which often plays an important role in morphologically rich languages", "In this paper we explore the role of morphological features in parsing Modern Standard Arabic MSA", "For MSA the space of possible morphological features is fairly large", "We determine which morphological features help and why", "We also explore going beyond the easily detectable regular formbased surface features by representing functional values for some morphological features", "We expect that representing lexical abstrac tions and inflectional features participating in agreement relations would help parsing quality but other inflectional features would not help", "We further expect functional features to be superior to surface only features", "The paper is structured as follows", "We first present the corpus we use Section 2 then relevant Arabic linguistic facts Section 3 we survey related work Section 4 describe our experiments Section 5 and conclude with an analysis of parsing error types Section 6"]}, "P11-2032": {"title": ["Bayesian Word Alignment for Statistical Machine Translation"], "abstract": ["In this work we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectationmaximization EM", "We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 integrating over all possible parameter values in finding the alignment distribution", "We show that Bayesian inference outperforms EM in all of the tested language pairs domains and data set sizes by up to 299 BLEU points", "We also show that the proposed method effectively addresses the wellknown rare word problem in EMestimated models and at the same time induces a much smaller dictionary of bilingual wordpairs"], "inroduction": ["Word alignment is a crucial early step in the training of most statistical machine translation SMT systems in which the estimated alignments are used for constraining the set of candidates in phrasegrammar extraction Koehn et al 2003 Chiang 2007 Galley et al 2006", "Stateoftheart word alignment models such as IBM Models Brown et al 1993 HMM Vogel et al 1996 and the jointlytrained symmetric HMM Liang et al 2006 contain a large number of parameters eg word translation probabilities that need to be estimated in addition to the desired hidden alignment variables", "The most common method of inference in such models is expectationmaximization EM Dempster et al 1977 or an approximation to EM when exact EM is intractable", "However being a maxi mization eg maximum likelihood ML or maximum a posteriori MAP technique EM is generally prone to local optima and overfitting", "In essence the alignment distribution obtained via EM takes into account only the most likely point in the parameter space but does not consider contributions from other points", "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore 2004 and a number of heuristic changes to the estimation procedure such as smoothing the parameter estimates were shown to reduce the alignment error rate but the effects on translation performance was not reported", "Zhao and Xing 2006 note that the parameter estimation for which they use variational EM suffers from data sparsity and use symmetric Dirichlet priors but they find the MAP solution", "Bayesian inference the approach in this paper have recently been applied to several unsupervised learning problems in NLP Goldwater and Griffiths 2007 Johnson et al 2007 as well as to other tasks in SMT such as synchronous grammar induction Blunsom et al 2009 and learning phrase alignments directly DeNero et al 2008", "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al", "2008 Nguyen et al", "2010 and Chung and Gildea 2009", "The former two works place nonparametric priors also known as cache models on the parameters and utilize Gibbs sampling", "However alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA Xu et al 2008 or by local maximization Nguyen et al 2010", "On the other hand 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 182187 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics Chung and Gildea 2009 apply a sparse Dirichlet prior on the multinomial parameters to prevent over fitting", "They use variational Bayes for inference but they do not investigate the effect of Bayesian inference to word alignment in isolation", "Recently Zhao and Gildea 2010 proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters and their inference method is actually stochastic EM also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence", "The set of alignments for a sentence corpus is denoted by a AThe model parameters consist of a VE  VF ta ble T of word translation probabilities such that tef  P f e", "The joint distribution of the Model1 variables is given by the following generative model3 n approximate the expected counts in the Estep", "Even though they report substantial reductions in align P E F A T  P eP aeP f a e T 1 s J ment error rate the translation BLEU scores do not improve", "Our approach in this paper is fully Bayesian in  n P e I  1J s n t j1 eaj fj 2 which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive sparse prior", "We develop a Gibbs sampler for alignments under IBM Model 1 In the proposed Bayesian setting we treat T as a random variable with a prior P T", "To find a suitable prior for T we rewrite 2 as e VE VFwhich is relevant for the stateoftheart SMT sys tems since 1 Model 1 is used in bootstrapping the parameter settings for EM training of higher P E F AT  n s P I  1J n n t e1 f 1 ef nef 3 VE VF P eorder alignment models and 2 many stateofthe  n n tef Nef n J 4art SMT systems use Model 1 translation probabilities as features in their loglinear model", "We eval e1 f 1 I  1 s uate the inferred alignments in terms of the endto end translation performance where we show the results with a variety of input data to illustrate the general applicability of the proposed technique", "To our knowledge this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance"]}, "P11-2037": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 212216 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics LanguageIndependent Parsing with Empty Elements Shu Cai and David Chiang USC Information Sciences Institute 4676 Admiralty Way Suite 1001 Marina del Rey CA 90292 shucaichiangisiedu Yoav Goldberg Ben Gurion University of the Negev Department of Computer Science POB 653 Be er Sheva 84105 Israel yoavgcsbguacil Abstract We present a  simple languageindependent method for integrating recovery of empty elements into syntactic parsing This method outperforms the best published method we are aware of on English and a recently published method on Chinese 1 Introduction Empty elements in the syntactic analysis of a sentence are markers that show where a word or phrase might otherwise be expected to appear but does not They play an important role in understanding the grammatical relations in the sentence For example in the tree of Figure 2a the first empty element  marks where John would be if believed were in the active voice someone believed   and the second empty element T marks where the manwould be ifwhowere not fronted John was believed to admire who Empty elements exist in many languages and serve different purposes In languages such as Chinese and Korean where subjects and objects can be dropped to avoid duplication empty elements are particularly important as they indicate the position of dropped arguments Figure 1 gives an example of a Chinese parse tree with empty elements The first empty element pro marks the subject of the whole sentence a pronoun inferable from context The second empty element PRO marks the subject of the de pendent VP shsh fl ti ow n The Penn Treebanks Marcus et  al 1993 Xue et al 2005 contain detailed annotations of empty elements Yet most parsing work based on these resources has ignored empty elements with some IP  VP  VP  IP  VP  NP"], "abstract": ["We present a simple languageindependent method for integrating recovery of empty elements into syntactic parsing", "This method outperforms the best published method we are aware of on English and a recently published method on Chinese"], "inroduction": ["NPNONE pro IP ADVP AD  znsh for now VP VV  zhngzh suspend VP NPNONE PRO IP VV  shsh VP NP NN NN Empty elements in the syntactic analysis of a sentence are markers that show where a word or phrase might otherwise be expected to appear but does not", "implement  fl law  tiown clause They play an important role in understanding the grammatical relations in the sentence", "For example in the tree of Figure 2a the rst empty element  marks where John would be if believed were in the active voice someone believed", " and the second empty element T marks where the man would be if who were not fronted John was believed to admire who", "Empty elements exist in many languages and serve different purposes", "In languages such as Chinese and Korean where subjects and objects can be dropped to avoid duplication empty elements are particularly important as they indicate the position of dropped arguments", "Figure 1 gives an example of a Chinese parse tree with empty elements", "The rst empty element pro marks the subject of the whole sentence a pronoun inferable from context", "The second empty element PRO marks the subject of the dependent VP shsh fl tiown", "The Penn Treebanks Marcus et al 1993 Xue et al 2005 contain detailed annotations of empty elements", "Yet most parsing work based on these resources has ignored empty elements with some 212 Figure 1 Chinese parse tree with empty elements marked", "The meaning of the sentence is Implementation of the law is temporarily suspended notable exceptions", "Johnson 2002 studied empty element recovery in English followed by several others Dienes and Dubey 2003 Campbell 2004 Gabbard et al 2006 the best results we are aware of are due to Schmid 2006", "Recently emptyelement recovery for Chinese has begun to receive attention Yang and Xue 2010 treat it as classication problem while Chung and Gildea 2010 pursue several approaches for both Korean and Chinese and explore applications to machine translation", "Our intuition motivating this work is that empty elements are an integral part of syntactic structure and should be constructed jointly with it not added in afterwards", "Moreover we expect emptyelement recovery to improve as the parsing quality improves", "Our method makes use of a strong syntactic model the PCFGs with latent annotation of Petrov et al", "2006 which we extend to predict empty cate Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 212216 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics gories by the use of lattice parsing", "The method is languageindependent and performs very well on both languages we tested it on for English it outperforms the best published method we are aware of Schmid 2006 and for Chinese it outperforms the method of Yang and Xue 20101"]}, "P11-2122": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 693698 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics Using Derivation Trees for Treebank Error Detection Seth Kulick and Ann Bies and Justin Mott Linguistic Data Consortium University of Pennsylvania 3600 Market Street Suite 810 Philadelphia PA 19104 skulickbiesjmottldcupennedu Abstract This work introduces a new approach to checking treebank consistency Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison We report on the result of applying this approach to the Penn Arabic Treebank and how this ap proach leads to high precision of error detection 1 Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research Treebank annotation consisting of syntactic structure with words as the terminals is by its nature more complex and thus more prone to error than other annotation tasks such as partofspeech tagging Recent work has therefore focused on the importance of detecting errors in the treebank Green and Manning 2010 and methods for finding such errors automatically eg Dickinson and Meurers 2003b Boyd et al 2007 Kato and Matsubara 2010 We present here a new approach to this problem that builds upon Dickinson and Meurers 2003b by integrating the perspective on treebank consistency checking and search in Kulick and Bies 2010 The approach in Dickinson andMeurers 2003b has certain limitations and complications that are inherent in examining only strings of words To overcome these problems we recast the search as one of searching for inconsistentlyused elementary trees in a Tree Adjoining Grammarbased form of the treebank This allows consistency checking to be based on structural locality instead of ngrams resulting in improved precision of finding inconsistent treebank annotation allowing for the correction of such inconsistencies in future work 2 Background and Motivation 21 Previous Work  DECCA The basic idea behind the work in Dickinson and Meurers 2003a Dickinson and Meurers 2003b is that strings occurring more than once in a corpus may occur with different  labels taken to be constituent node labels and such differences in labels might be the manifestation of an annotation error Adopting their terminology a  variation nucleus is the string of words with a difference in the annotation label while a  variation ngram is a larger string containing the variation nucleus"], "abstract": ["This work introduces a new approach to checking treebank consistency", "Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity", "This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison", "We report on the result of applying this approach to the Penn Arabic Treebank and how this approach leads to high precision of error detection"], "inroduction": ["The internal consistency of the annotation in a tree bank is crucial in order to provide reliable training and testing data for parsers and linguistic research", "Treebank annotation consisting of syntactic structure with words as the terminals is by its nature more complex and thus more prone to error than other annotation tasks such as partofspeech tagging", "Recent work has therefore focused on the importance of detecting errors in the treebank Green and Manning 2010 and methods for finding such errors automatically eg", "Dickinson and Meurers 2003b Boyd et al 2007 Kato and Matsubara 2010", "We present here a new approach to this problem that builds upon Dickinson and Meurers 2003b by integrating the perspective on treebank consistency checking and search in Kulick and Bies 2010", "The approach in Dickinson and Meurers 2003b has certain limitations and complications that are inherent in examining only strings of words", "To over come these problems we recast the search as one of searching for inconsistentlyused elementary trees in a Tree Adjoining Grammarbased form of the tree bank", "This allows consistency checking to be based on structural locality instead of ngrams resulting in improved precision of finding inconsistent treebank annotation allowing for the correction of such inconsistencies in future work"]}, "P11-2124": {"title": ["Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 704709 Portland Oregon June 1924 2011 c  2011 Association for Computational Linguistics Joint Hebrew Segmentation and Parsing using a PCFG LA Lattice Parser Yoav Goldberg and Michael Elhadad Ben Gurion University of the Negev Department of Computer Science POB 653 Be er Sheva 84105 Israel yoavgelhadadcsbguacil Abstract We experiment with extending a lattice parsing methodology for parsing Hebrew Goldberg and Tsarfaty 2008 Golderg et al 2009 to make use of a stronger syntactic model the PCFG LA Berkeley Parser We show that the methodology is very effective using a small training set of about 5500 trees we construct a parser which parses and segments unsegmented Hebrew text with an Fscore of almost 80 an error reduction of over 20 over the best previous result for this task This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs 1 Introduction Most work on parsing assumes that the lexical items in the yield of a parse tree are fully observed and correspond to space delimited tokens perhaps after a deterministic preprocessing step of tokenization While this is mostly the case for English the situation is different in languages such as Chinese in which word boundaries are not marked and the Semitic languages of Hebrew and Arabic in which various particles corresponding to function words are agglutinated as affixes to content bearing words sharing the same spacedelimited token For example the Hebrew token bcl1 can be interpreted as the single noun meaning  onion or as a sequence of a preposition and a noun bcl meaning in the shadow  In such languages the sequence of lexical 1We adopt here the transliteration scheme of Simaan et al 2001 items corresponding to an input string is ambiguous and cannot be determined using a deterministic procedure In this work we focus on constituency parsing of Modern Hebrew henceforth Hebrew from raw unsegmented text A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process in which the input string is presegmented prior to handing it to a parser The shortcoming of this method as noted by Tsarfaty 2006 is that many segmentation de cisions cannot be resolved based on local context alone Rather they may depend on long distance relations and interact closely with the syntactic struc ture of the sentence Thus segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step Goldberg and Tsarfaty 2008 demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text They experimented with various manual refinements of unlexicalized treebankderived grammars and showed that better grammars contribute to better segmentation accuracies Goldberg et al 2009 showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice parser using an external resource Recently Green and Manning 2010 demonstrated the effectiveness of lattice parsing for parsing Arabic Here we report the results of experiments coupling lattice parsing together with the currently best grammar learning method the Berkeley PCFG LA parser Petrov et al 2006 704 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a languageprocessing perspective are Affixation Common function words are prefixed to the following word These include m from   f who that  h the  wand klike lto and b in  Several such elements may attach together producing forms such as wfmhfmf wfmhfmf andthatfromthesun  Notice that the last part of the token the noun fmf  sun when appearing in isolation can be also interpreted as the sequence fmf  who moved  The linear order of such segmental elements within a token is fixed disallowing the reading wfmhfmf in the previous example However the syntactic relations of these elements with respect to the rest of the sentence is rather free The relativizer f  that  for example may attach to an arbitrarily long relative clause that goes beyond token boundaries To further complicate matters the definite article h the  is not realized in writing when following the particles bin klike and lto Thus the form bbit can be interpreted as either bbit  in house or bhbit  in the house  In addition pronominal elements may attach to nouns verbs adverbs preposi tions and others as suffixes eg lqxnlqxhn  tookthem elihmelihm on them  These affixations result in highly ambiguous token segmentations Relatively free constituent order The ordering of constituents inside a phrase is relatively free This is most notably apparent in the verbal phrases and sentential levels In particular while most sentences follow an SVO order OVS and VSO configurations are also possible Verbal arguments can appear before or after the verb and in many ordering This results in long and flat VP and S structures and a fair amount of sparsity Rich templatic morphology Hebrew has a very productive morphological structure which is based on a roottemplate system The productive morphology results in many distinct word forms and a high outofvocabulary rate which makes it hard to reliably estimate lexical parameters from annotated corpora The roottemplate system combined with the unvocalized writing system and rich affixation makes it hard to guess the morphological analyses of an unknown word based on its prefix and suffix as usually done in other languages Unvocalized writing system Most vowels are not marked in everyday Hebrew text which results in a very high level of lexical and morphological ambiguity Some tokens can admit as many as 15 distinct readings Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns which should agree on Gender and Number and definiteness and between Subjects and Verbs which should agree on Gender and Number 3 PCFG LA Grammar Estimation Klein and Manning 2003 demonstrated that linguistically informed splitting of nonterminal symbols in treebankderived grammars can result in accurate grammars Their work triggered investigations in automatic grammar refinement and statesplitting Matsuzaki et al 2005 Prescher 2005 which was then perfected by Petrov et al 2006 Petrov 2009 The model of Petrov et al 2006 and its publicly available implementation the Berkeley parser2 works by starting with a barebones treebank derived grammar and automatically refining it in splitmergesmooth cycles The learning works by iteratively 1 splitting each nonterminal category in two 2 merging back noneffective splits and 3 smoothing the split nonterminals toward their shared ancestor Each of the steps is followed by an EMbased parameter reestimation This process allows learning tree annotations which capture many latent syntactic interactions At inference time the latent annotations are approximately marginalized out resulting in the approximate most probable unannotated tree according to the refined grammar This parsing methodology is very robust producing state of the art accuracies for English as well as many other languages including German Petrov and Klein 2008 French Candito et al 2009 and Chinese Huang and Harper 2009 among others The grammar learning process is applied to binarized parse trees with 1storder vertical and 0thorder horizontal markovization This means that in 2httpcodegooglecompberkeleyparser"], "abstract": ["We experiment with extending a lattice parsing methodology for parsing Hebrew Goldberg and Tsarfaty 2008 Golderg et al 2009 to make use of a stronger syntactic model the PCFGLA Berkeley Parser", "We show that the methodology is very effective using a small training set of about 5500 trees we construct a parser which parses and segments unsegmented Hebrew text with an Fscore of almost 80 an error reduction of over 20 over the best previous result for this task", "This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs"], "inroduction": ["Most work on parsing assumes that the lexical items in the yield of a parse tree are fully observed and correspond to space delimited tokens perhaps after a deterministic preprocessing step of tokenization", "While this is mostly the case for English the situation is different in languages such as Chinese in which word boundaries are not marked and the Semitic languages of Hebrew and Arabic in which various particles corresponding to function words are agglutinated as affixes to content bearing words sharing the same spacedelimited token", "For example the Hebrew token bcl1 can be interpreted as the single noun meaning onion or as a sequence of a preposition and a noun bcl meaning in the shadow", "In such languages the sequence of lexical 1 We adopt here the transliteration scheme of Simaan et al", "2001 items corresponding to an input string is ambiguous and cannot be determined using a deterministic procedure", "In this work we focus on constituency parsing of Modern Hebrew henceforth Hebrew from raw unsegmented text", "A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process in which the input string is presegmented prior to handing it to a parser", "The shortcoming of this method as noted by Tsarfaty 2006 is that many segmentation decisions cannot be resolved based on local context alone", "Rather they may depend on long distance relations and interact closely with the syntactic structure of the sentence", "Thus segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step", "Goldberg and Tsarfaty 2008 demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text", "They experimented with various manual refinements of unlexicalized treebankderived grammars and showed that better grammars contribute to better segmentation accuracies", "Goldberg et al", "2009 showed that segmentation and parsing ac curacies can be further improved by extending the lexical coverage of a latticeparser using an external resource", "Recently Green and Manning 2010 demonstrated the effectiveness of latticeparsing for parsing Arabic", "Here we report the results of experiments coupling lattice parsing together with the currently best grammar learning method the Berkeley PCFGLA parser Petrov et al 2006", "704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguisticsshortpapers pages 704709 Portland Oregon June 1924 2011", "Qc 2011 Association for Computational Linguistics"]}, "P11-3012": {"title": ["An Error Analysis of Relation Extraction in Social Media Documents"], "abstract": ["Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another eg part of", "This paper presents an analysis of a relation extraction system based on prior work but applied to the JD Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media", "The results are examined on three different subsets of the JDPA Corpus showing that the system performs much worse on documents from certain sources", "The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs"], "inroduction": ["To summarize accurately determine the sentiment or answer questions about a document it is often necessary to be able to determine the relationships between entities being discussed in the document such as partof or memberof", "In the simple sentiment example Example 11 I bought a new car yesterday", "I love the powerful engine", "determining the sentiment the author is expressing about the car requires knowing that the engine is a part of the car so that the positive sentiment being expressed about the engine can also be attributed to the car", "In this paper we examine our preliminary results from applying a relation extraction system to the JD Power and Associates JDPA Sentiment Corpus Kessler et al 2010", "Our system uses lexical features from prior work to classify relations and we examine how the system works on different subsets from the JDPA Sentiment Corpus breaking the source documents down into professionally written reviews blog reviews and social networking reviews", "These three document types represent quite different writing styles and we see significant difference in how the relation extraction system performs on the documents from different sources"]}, "P12-1001": {"title": ["Learning to Translate with Multiple Objectives"], "abstract": ["We introduce an approach to optimize a machine translation MT system on multiple metrics simultaneously", "Different metrics eg BLEU TER focus on different aspects of translation quality our multiobjective approach leverages these diverse aspects to improve overall quality", "Our approach is based on the theory of Pareto Optimality", "It is simple to implement on top of existing singleobjective optimization methods eg MERT PRO and outperforms ad hoc alternatives based on linearcombination of metrics", "We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization"], "inroduction": ["Weight optimization is an important step in building machine translation MT systems", "Discrimi native optimization methods such as MERT Och 2003 MIRA Crammer et al 2006 PRO Hopkins and May 2011 and DownhillSimplex Nelder and Mead 1965 have been influential in improving MT systems in recent years", "These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU which serve as surrogate objective for translation quality", "However we know that a single metric such as BLEU is not enough", "Ideally we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality", "Now at Nara Institute of Science  Technology NAIST While many alternatives have been proposed such a perfect evaluation metric remains elusive", "As a result many MT evaluation campaigns now report multiple evaluation metrics CallisonBurch et al 2011 Paul 2010", "Different evaluation metrics focus on different aspects of translation quality", "For example while BLEU Papineni et al 2002 focuses on wordbased ngram precision METEOR Lavie and Agarwal 2007 allows for stemsynonym matching and incorporates recall", "TER Snover et al 2006 allows arbitrary chunk movements while permutation metrics like RIBES Isozaki et al 2010 Birch et al 2010 measure deviation in word order", "Syntax Owczarzak et al 2007 and semantics Pado et al 2009 also help", "Arguably all these metrics correspond to our intuitions on what is a good translation", "The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics", "Can we really claim that a system is good if it has high BLEU but very low METEOR", "Similarly is a highMETEOR lowBLEU system desirable", "Our goal is to propose a multiobjective optimization method that avoids overfitting to a single metric", "We want to build a MT system that does well with respect to many aspects of translation quality", "In general we cannot expect to improve multiple metrics jointly if there are some inherent trade offs", "We therefore need to define the notion of Pareto Optimality Pareto 1906 which characterizes this tradeoff in a rigorous way and distinguishes the set of equally good solutions", "We will describe Pareto Optimality in detail later but roughly speaking a 1 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 110 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics hypothesis is paretooptimal if there exist no other hypothesis better in all metrics", "The contribution of this paper is twofold  We introduce PMO Paretobased Multi objective Optimization a general approach for learning with multiple metrics", "Existing single objective methods can be easily extended to multiobjective using PMO", " We show that PMO outperforms the alternative singleobjective optimization of linearly combined metrics in multiobjective space 1 09 08 07 06 05 04 03 02 01 0 0 01 02 03 04 05 06 07 08 09 1 metric1 and especially obtains stronger results for metrics that may be difficult to tune individually", "In the following we first explain the theory of Pareto Optimality Section 2 and then use it to build up our proposed PMO approach Section 3", "Experiments on NIST ChineseEnglish and PubMed EnglishJapanese translation using BLEU TER and RIBES are presented in Section 4", "We conclude by discussing related work Section 5 and opportunitieslimitations Section 6"]}, "P12-1013": {"title": ["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 117 125 Jeju Republic of Korea 814 July 2012 c  2012 Association for Computational Linguistics Efficient Treebased Approximation for Entailment Graph Learning Jonathan Berant Ido Dagan Meni Adler  Jacob Goldberger  The Blavatnik School of Computer Science Tel Aviv University  Department of Computer Science BarIlan University  Faculty of Engineering Bar Ilan University jonatha6posttauacil dagangoldbejcsengbiuacil adlermcsbguacil Abstract Learning entailment rules is fundamental in many semanticinference applications and has been an active field of research in recent years In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates termed entailment graphs We first identify that entailment graphs exhibit a treelike property and are very similar to a novel type of graph termed forestreducible graph We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges where each iteration takes linear time We compare our approximation algorithm to a recentlyproposed stateoftheart exact algorithm and show that it is more efficient and scalable both theoretically and empirically while its output quality is close to that given by the optimal solution of the exact algorithm 1 Introduction Performing textual inference is in the heart of many semantic inference applications such as Question Answering QA and Information Extraction IE A prominent generic paradigm for textual inference is Textual Entailment TUE Dagan et al 2009 In TUE the goal is to recognize given two text fragments termed text and hypothesis whether the hypothesis can be inferred from the text For example the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis  The Ottomans attacked Cyprus  Semantic inference applications such as QA and IE crucially rely on entailment rules Ravichandran and Hovy 2002 Shinyama and Sekine 2006 or equivalently inference rules that is rules that de scribe a directional inference relation between two fragments of text An important type of entailment rule specifies the entailment relation between natural language predicates eg the entailment rule X invade Y  X attack Y can be helpful in inferring the aforementioned hypothesis Consequently substantial effort has been made to learn such rules Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2008 Schoenmackers et al 2010 Textual entailment is inherently a transitive relation  that is the rules x  y and y  z imply the rule  x  z   Accordingly Berant et al 2010 formulated the problem of learning entailment rules as a graph optimization problem where nodes are predicates and edges represent entailment rules that respect transitivity Since finding the optimal set of edges respecting transitivity is NPhard they employed Integer Linear Programming ILP to find the exact solution Indeed they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure More recently Berant et al Berant et al 2011 introduced a more efficient exact algorithm which decomposes the graph into connected components and then applies an ILP solver over each component Despite this progress finding the exact solution remains NPhard  the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited Thus scaling their algorithm to data sets with tens of thousands of predicates eg the extractions of Fader et al 2011 is unlikely 117 In this paper we present a novel method for learning the edges of entailment graphs Our method computes much more efficiently an approximate solution that is empirically almost as good as the exact solution To that end we first Section 3 conjecture and empirically show that entailment graphs exhibit a treelike property ie that they can be reduced into a structure similar to a directed forest Then we present in Section 4 our iterative ap proximation algorithm where in each iteration a node is removed and reattached back to the graph in a locallyoptimal way Combining this scheme with our conjecture about the graph structure enables a linear algorithm for node reattachment Section 5 shows empirically that this algorithm is by orders of magnitude faster than the stateoftheart exact algorithm and that though an optimal solution is not guaranteed the area under the precisionrecall curve drops by merely a point To conclude the contribution of this paper is twofold First we define a novel modeling assumption about the treelike structure of entailment graphs and demonstrate its validity Second we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past Finally we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction Snow et al 2006 and Ontology induction Poon and Domingos 2010 and thus our approach may improve scalability in these fields as well 2 Background Until recently work on learning entailment rules be tween predicates considered each rule independently of others and did not exploit global dependencies Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments Lin and Pantel 2001 Szpektor et al 2004 Yates and Etzioni 2009 Schoenmackers et al 2010 Some methods extracted rules from lexicographic resources such as WordNet Szpektor and Dagan 2009 or FrameNet Bob and Rambow 2009 Ben Aharon et al 2010 and others assumed that semantic relations between predicates can be deduced from their cooccurrence in a corpus via manuallyconstructed patterns Chklovski and Pantel 2004 Recently Berant et al 2010 2011 formulated the problem as the problem of learning global entailment graphs In entailment graphs nodes are predicates eg  X attack Y and edges represent entailment rules between them X invade Y  X attack Y For every pair of predicates i j an entailment score wij was learned by training a classifier over distributional similarity features A positive wij indicated that the classifier believes i j and a negative wij indicated that the classifier believes i 9 j Given the graph nodes V corresponding to the predicates and the weighting function w  V  V  R they aim to find the edges of a graph G  VE that maximize the objective  ijE wij under the constraint that the graph is transitive ie for every node triplet i j k if i j  E and j k  E then i k  E Berant et al proved that this optimization problem which we term MaxTransGraph is NPhard and so described it as an Integer Linear Program ILP Let xij be a binary variable indicating the ex istence of an edge i  j in E Then X  xij  i 6 j are the variables of the following ILP for MaxTransGraph argmax"], "abstract": ["Learning entailment rules is fundamental in many semanticinference applications and has been an active field of research in recent years", "In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates termed entailment graphs", "We first identify that entailment graphs exhibit a treelike property and are very similar to a novel type of graph termed forestreducible graph", "We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges where each iteration takes linear time", "We compare our approximation algorithm to a recentlyproposed stateoftheart exact algorithm and show that it is more efficient and scalable both theoretically and empirically while its output quality is close to that given by the optimal solution of the exact algorithm"], "inroduction": ["Performing textual inference is in the heart of many semantic inference applications such as Question Answering QA and Information Extraction IE", "A prominent generic paradigm for textual inference is Textual Entailment TUE Dagan et al 2009", "In TUE the goal is to recognize given two text fragments termed text and hypothesis whether the hypothesis can be inferred from the text", "For example the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus", "Semantic inference applications such as QA and IE crucially rely on entailment rules Ravichandran and Hovy 2002 Shinyama and Sekine 2006 or equivalently inference rules that is rules that describe a directional inference relation between two fragments of text", "An important type of entailment rule specifies the entailment relation between natural language predicates eg the entailment rule X invade Y  X attack Y can be helpful in inferringthe aforementioned hypothesis", "Consequently sub stantial effort has been made to learn such rules Lin and Pantel 2001 Sekine 2005 Szpektor and Dagan 2008 Schoenmackers et al 2010", "Textual entailment is inherently a transitive relation  that is the rules x  y and y  z imply the rule x  z", "Accordingly Berant et al", "2010 formulated the problem of learning entailment rules as a graph optimization problem where nodes are predicates and edges represent entailment rules that respect transitivity", "Since finding the optimal set of edges respecting transitivity is NPhard they employed Integer Linear Programming ILP to find the exact solution", "Indeed they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure", "More recently Berant et al", "Berant et al 2011 introduced a more efficient exact algorithm which decomposes the graph into connected components and then applies an ILP solver over each component", "Despite this progress finding the exact solution remains NPhard  the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited", "Thus scaling their algorithm to data sets with tens of thousands of predicates eg the extractions of Fader et al", "2011 is unlikely", "117 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 117125 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics In this paper we present a novel method for learning the edges of entailment graphs", "Our method computes much more efficiently an approximate solution that is empirically almost as good as the exact solution", "To that end we first Section 3 conjecture and empirically show that entailment graphs exhibit a treelike property ie that they can be reduced into a structure similar to a directed forest", "Then we present in Section 4 our iterative approximation algorithm where in each iteration a node is removed and reattached back to the graph in a locallyoptimal way", "Combining this scheme with our conjecture about the graph structure enables a linear algorithm for node reattachment", "Section 5 shows empirically that this algorithm is by orders of magnitude faster than the stateoftheart exact algorithm and that though an optimal solution is not guaranteed the area under the precisionrecall curve drops by merely a point", "To conclude the contribution of this paper is twofold First we define a novel modeling assumption about the treelike structure of entailment graphs and demonstrate its validity", "Second we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past", "Finally we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction Snow et al 2006 and Ontology induction Poon and Domingos 2010 and thus our approach may improve scalability in these fields as well"]}, "P12-1016": {"title": ["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 146 155"], "abstract": ["1 ll  When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number the output commonly contains morphosyntactic agreement errors", "To address this issue we present a targetside classbased agreement model", "Agreement is promoted by scoring a sequence of finegrained morphosyntactic classes that are predicted during decoding for each translation hypothesis", "For EnglishtoArabic translation our model yields a 104 BLEU average improvement over a stateoftheart baseline", "The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrasebased decoders"], "inroduction": ["Languages vary in the degree to which surface forms reflect grammatical relations", "English is a weakly inflected language it has a narrow verbal paradigm restricted nominal inflection plurals and only the vestiges of a case system", "Consequently translation into Englishwhich accounts for much of the machine translation MT literature Lopez 2008often involves some amount of morphosyntactic dimension ality reduction", "Less attention has been paid to what happens during translation from English richer grammatical features such as gender dual number and overt case are effectively latent variables that must be inferred during decoding", "Consider the output of Google Translate for the simple English sentence in Fig", "1", "The correct translation is a monotone mapping of the input", "However in Arabic SVO word order requires both gender and number agreement between the subject ll the car and verb  go", "The MT system selects the correct verb stem but with masculine inflection", "Although the translation has thecarsgdeffem gosgmasc withspeedsgfem The car goes quickly Figure 1 Ungrammatical Arabic output of Google Translate for the English input The car goes quickly", "The subject should agree with the verb in both gender and number but the verb has masculine inflection", "For clarity the Arabic tokens are arranged lefttoright", "the correct semantics it is ultimately ungrammatical", "This paper addresses the problem of generating text that conforms to morphosyntactic agreement rules", "Agreement relations that cross statistical phrase boundaries are not explicitly modeled in most phrase based MT systems Avramidis and Koehn 2008", "We address this shortcoming with an agreement model that scores sequences of finegrained morpho syntactic classes", "First bound morphemes in translation hypotheses are segmented", "Next the segments are labeled with classes that encode both syntactic category information ie parts of speech and grammatical features such as number and gender", "Finally agreement is promoted by scoring the predicted class sequences with a generative Markov model", "Our model scores hypotheses during decoding", "Unlike previous models for scoring syntactic relations our model does not require bitext annotations phrase table features or decoder modifications", "The model can be implemented using the feature APIs of popular phrasebased decoders such as Moses Koehn et al 2007 and Phrasal Cer et al 2010", "Intuition might suggest that the standard ngram language model LM is sufficient to handle agreement phenomena", "However LM statistics are sparse and they are made sparser by morphological variation", "For EnglishtoArabic translation we achieve a 104 BLEU average improvement by tiling our model on top of a large LM", "146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 146155 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generation because the bitext may not PronFemSg VerbMasc3Pl Prt Conj contain all inflected variants Minkov et al 2007 Toutanova et al 2008 Fraser et al 2012", "However using lexical coverage experiments we show that it there is ample room for translation quality improvements through better selection of forms that already exist in the translation model", "they write will and"]}, "P12-1027": {"title": ["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 253 262 Jeju Republic of Korea 814 July 2012 c  2012 Association for Computational Linguistics Fast Online Training with FrequencyAdaptive Learning Rates for Chinese Word Segmentation and New Word Detection Xu Sun  Houfeng Wang Wenjie Li Department of Computing The Hong Kong Polytechnic University Key Laboratory of Computational Linguistics Peking University Ministry of Education China csxsun cswjlicomppolyueduhk wanghfpkueducn Abstract We present a joint model for Chinese word segmentation and new word detection We present high dimensional new features including wordbased features and enriched edge labeltransition features for the joint modeling As we know training a word segmentation system on largescale datasets is already costly In our case adding high dimensional new features will further slow down the training speed To solve this problem we propose a new training method adaptive online gradient descent based on feature frequency information for very fast online training of the parameters even given largescale datasets with high dimensional features Compared with existing training methods our training method is an order magnitude faster in terms of training time and can achieve equal or even higher accuracies The proposed fast training method is a general purpose optimization method and it is not limited in the specific task discussed in this paper 1 Introduction Since Chinese sentences are written as continuous sequences of characters segmenting a character sequence into words is normally the first step in the pipeline of Chinese text processing The major problem of Chinese word segmentation is the ambiguity Chinese character sequences are normally ambiguous and new words outofvocabulary words are a major source of the ambiguity A typical category of new words is named entities including organization names person names location names and so on In this paper we present high dimensional new features including wordbased features and enriched edge labeltransition features for the joint modeling of Chinese word segmentation CWS and new word detection NWD While most of the stateoftheart CWS systems used semiMarkov conditional random fields or latent variable conditional random fields we simply use a single firstorder conditional random fields CRFs for the joint modeling The semiMarkov CRFs and latent variable CRFs relax the Markov assumption of CRFs to express more complicated dependencies and therefore to achieve higher disambiguation power Alternatively our plan is not to relax Markov assumption of CRFs but to exploit more complicated dependencies via using refined highdimensional features The advantage of our choice is the simplicity of our model As a result our CWS model can be more efficient compared with the heavier systems and with similar or even higher accuracy because of using refined features As we know training a word segmentation system on largescale datasets is already costly In our case adding high dimensional new features will further slow down the training speed To solve this challenging problem we propose a new training method adaptive online gradient descent based on feature frequency information ADF for very fast word segmentation with new word detection even given largescale datasets with high dimensional features In the proposed training method we try to use more refined learning rates Instead of using a single learning rate a scalar for all weights we extend the learning rate scalar to a learning rate vector based on feature frequency information in the updating By doing so each weight has 253 its own learning rate adapted on feature frequency information We will show that this can significantly improve the convergence speed of online learning We approximate the learning rate vector based on feature frequency information in the updating process Our proposal is based on the intuition that a feature with higher frequency in the training process should be with a learning rate that is decayed faster Based on this intuition we will show the formalized training algorithm later We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods and can achieve equal or even higher accuracies The contribution of this work is as follows  We propose a general purpose fast online training method ADF The proposed training method requires only a few passes to complete the training  We propose a joint model for Chinese word segmentation and new word detection  Compared with prior work our system achieves better accuracies on both word segmentation and new word detection 2 Related Work First we review related work on word segmentation and new word detection Then we review popular online training methods in particular stochastic gradient descent SGD"], "abstract": ["We present a joint model for Chinese word segmentation and new word detection", "We present high dimensional new features including wordbased features and enriched edge labeltransition features for the joint modeling", "As we know training a word segmentation system on largescale datasets is already costly", "In our case adding high dimensional new features will further slow down the training speed", "To solve this problem we propose a new training method adaptive online gradient descent based on feature frequency information for very fast online training of the parameters even given largescale datasets with high dimensional features", "Compared with existing training methods our training method is an order magnitude faster in terms of training time and can achieve equal or even higher accuracies", "The proposed fast training method is a general purpose optimization method and it is not limited in the specific task discussed in this paper"], "inroduction": ["Since Chinese sentences are written as continuous sequences of characters segmenting a character sequence into words is normally the first step in the pipeline of Chinese text processing", "The major problem of Chinese word segmentation is the ambiguity", "Chinese character sequences are normally ambiguous and new words out ofvocabulary words are a major source of the ambiguity", "A typical category of new words is named entities including organization names person names location names and so on", "In this paper we present high dimensional new features including wordbased features and enriched edge labeltransition features for the joint modeling of Chinese word segmentation CWS and new word detection NWD", "While most of the stateoftheart CWS systems used semi Markov conditional random fields or latent variable conditional random fields we simply use a single firstorder conditional random fields CRFs for the joint modeling", "The semiMarkov CRFs and latent variable CRFs relax the Markov assumption of CRFs to express more complicated dependencies and therefore to achieve higher disambiguation power", "Alternatively our plan is not to relax Markov assumption of CRFs but to exploit more complicated dependencies via using refined high dimensional features", "The advantage of our choice is the simplicity of our model", "As a result our CWS model can be more efficient compared with the heavier systems and with similar or even higher accuracy because of using refined features", "As we know training a word segmentation system on largescale datasets is already costly", "In our case adding high dimensional new features will further slow down the training speed", "To solve this challenging problem we propose a new training method adaptive online gradient descent based on feature frequency information ADF for very fast word segmentation with new word detection even given largescale datasets with high dimensional features", "In the proposed training method we try to use more refined learning rates", "Instead of using a single learning rate a scalar for all weights we extend the learning rate scalar to a learning rate vector based on feature frequency information in the updating", "By doing so each weight has 253 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 253262 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics its own learning rate adapted on feature frequency information", "We will show that this can significantly improve the convergence speed of online learning", "We approximate the learning rate vector based on feature frequency information in the updating process", "Our proposal is based on the intuition that a feature with higher frequency in the training process should be with a learning rate that is decayed faster", "Based on this intuition we will show the formalized training algorithm later", "We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods and can achieve equal or even higher accuracies", "The contribution of this work is as follows  We propose a general purpose fast online training method ADF", "The proposed training method requires only a few passes to complete the training", " We propose a joint model for Chinese word segmentation and new word detection", " Compared with prior work our system achieves better accuracies on both word segmentation and new word detection"]}, "P12-1048": {"title": ["Translation Model Adaptation for Statistical Machine Translation with"], "abstract": ["To adapt a translation model trained from the data in one domain to another previous works paid more attention to the studies of parallel corpus while ignoring the indomain monolingual corpora which can be obtained more easily", "In this paper we propose a novel approach for translation model adaptation by utilizing indomain monolingual topic information instead of the indomain bilingual corpora which incorporates the topic information into translation probability estimation", "Our method establishes the relationship between the outofdomain bilingual corpus and the indomain monolingual corpora via topic mapping and phrasetopic distribution probability estimation from indomain monolingual corpora", "Experimental result on the NIST ChineseEnglish translation task shows that our approach significantly outperforms the baseline system"], "inroduction": ["In recent years statistical machine translationSMT has been rapidly developing with more and more novel translation models being proposed and put into practice Koehn et al 2003 Och and Ney 2004 Galley et al 2006 Liu et al 2006 Chiang 2007 Chiang 2010", "However similar to other natural language processingNLP tasks SMT systems often suffer from domain adaptation problem during practical applications", "The simple reason is that the underlying statistical models always tend to closely Part of this work was done during the first authors internship at Baidu", "approximate the empirical distributions of the training data which typically consist of bilingual sentences and monolingual target language sentences", "When the translated texts and the training data come from the same domain SMT systems can achieve good performance otherwise the translation quality degrades dramatically", "Therefore it is of significant importance to develop translation systems which can be effectively transferred from one domain to another for example from newswire to weblog", "According to adaptation emphases domain adaptation in SMT can be classified into translation model adaptation and language model adaptation", "Here we focus on how to adapt a translation model which is trained from the largescale outofdomain bilingual corpus for domainspecific translation task leaving others for future work", "In this aspect previous methods can be divided into two categories one paid attention to collecting more sentence pairs by information retrieval technology Hildebrand et al 2005 or synthesized parallel sentences Ueffing et al 2008 Wu et al 2008 Bertoldi and Federico 2009 Schwenk and Senellart 2009 and the other exploited the full potential of existing parallel corpus in a mixturemodeling Foster and Kuhn 2007 Civera and Juan 2007 Lv et al 2007 framework", "However these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora therefore limiting the potential of further translation quality improvement", "In this paper we propose a novel adaptation method to adapt the translation model for domain specific translation task by utilizing indomain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 459468 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics monolingual corpora", "Our approach is inspired by the recent studies Zhao and Xing 2006 Zhao and Xing 2007 Tam et al 2007 Gong and Zhou 2010 Ruiz and Federico 2011 which have shown that a particular translation always appears in some specific topical contexts and the topical context information has a great effect on translation selection", "For example bank often occurs in the sentences related to the economy topic when translated into yinhang and occurs in the sentences related to the geography topic when translated to hean", "Therefore the cooccurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases", "In a monolingual corpus if bank occurs more often in the sentences related to the economy topic than the ones related to the geography topic it is more likely that bank is translated to yinhang than to hean", "With the outofdomain bilingual corpus we first incorporate the topic information into translation probability estimation aiming to quantify the effect of the topical context information on translation selection", "Then we rescore all phrase pairs according to the phrase topic and the wordtopic posterior distributions of the additional indomain monolingual corpora", "As compared to the previous works our method takes advantage of both the indomain monolingual corpora and the outofdomain bilingual corpus to incorporate the topic information into our translation model thus breaking down the corpus barrier for translation quality improvement", "The experimental effect on the performance of SMT system", "Phrase probability measures the cooccurrence frequency of a phrase pair and lexical probability is used to validate the quality of the phrase pair by checking how well its words are translated to each other", "According to the definition proposed by Koehn et al 2003 given a source sentence f  f J  f1   ", " fj    ", " fJ  a target sentence e  eI  e1   ", " ei   ", " eI  and its word alignment a which is a subset of the Cartesian product of word position s a  j i  j  1   ", " J  i  1   ", " I  the phrase pair f e is said to be consistent Och and Ney 2004 with the alignment if and only if 1 there must be at least one word inside one phrase aligned to a word inside the other phrase and 2 no words inside one phrase can be aligned to a word outside the other phrase", "After all consistent phrase pairs are extracted from training corpus the phrase probabilities are estimated as relative frequencies Och and Ney 2004 countf e e f  1  countf  e  e  l Here countf e indicates how often the phrase pair f e occurs in the training corpus", "To obtain the corresponding lexical weight we first estimate a lexical translation probability distri bution wef  by relative frequency from the train ing corpus f e results on the NIST data set demonstrate the effectiveness of our method", "The reminder of this paper is organized as follows Section 2 provides a brief description of translation probability estimation", "Section 3 introduces the adaptation method which incorporates the topic information into the translation model Section cou nt wef   countf e  2 e l Retaining the alignment a between the phrase pair f e the corresponding lexical weight is calculated as 4 describes and discusses the experimental results Section 5 briefly summarizes the recent related work about translation model adaptation", "Finally we end e 1 pw ef a  n i1 jj i  a  weifj  3 jia with a conclusion and the future work in Section 6"]}, "P12-1079": {"title": ["A Topic Similarity Model"], "abstract": ["Previous work using topic model for statistical machine translation SMT explore topic information at the word level", "However SMT has been advanced from wordbased paradigm to phraserulebased paradigm", "We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrasebased translation", "We associate each synchronous rule with a topic distribution and select desirable rules according to the similarity of their topic distributions with given documents", "We show that our model significantly improves the translation performance over the baseline on NIST ChinesetoEnglish translation experiments", "Our model also achieves a better performance and a faster speed than previous approaches that work at the word level"], "inroduction": ["Topic model Hofmann 1999 Blei et al 2003 is a popular technique for discovering the underlying topic structure of documents", "To exploit topic information for statistical machine translation SMT researchers have proposed various topicspecific lexicon translation models Zhao and Xing 2006 Zhao and Xing 2007 Tam et al 2007 to improve translation quality", "Topicspecific lexicon translation models focus on wordlevel translations", "Such models first estimate word translation probabilities conditioned on topics and then adapt lexical weights of phrases  Corresponding author by these probabilities", "However the stateofthe art SMT systems translate sentences by using sequences of synchronous rules or phrases instead of translating word by word", "Since a synchronous rule is rarely factorized into individual words we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level", "Consequently we propose a topic similarity model for hierarchical phrasebased translation Chiang 2007 where each synchronous rule is associated with a topic distribution", "In particular  Given a document to be translated we calculate the topic similarity between a rule and the document based on their topic distributions", "We augment the hierarchical phrasebased system by integrating the proposed topic similarity model as a new feature Section 31", " As we will discuss in Section 32 the similarity between a generic rule and a given source document computed by our topic similarity model is often very low", "We dont want to penalize these generic rules", "Therefore we further propose a topic sensitivity model which rewards generic rules so as to complement the topic similarity model", " We estimate the topic distribution for a rule based on both the source and target side topic models Section 41", "In order to calculate sim ilarities between targetside topic distributions of rules and sourceside topic distributions of given documents during decoding we project 750 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 750758 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics 06 06 06 06 04 04 04 04 02 02 02 02 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30a      opera tional capability b  X1  grands X1 c  X1  give X1 d X1   X2  held talks X1 X2 Figure 1 Four synchronous rules with topic distributions", "Each subgraph shows a rule with its topic distribution where the Xaxis means topic index and the Yaxis means the topic probability", "Notably the rule b and rule c shares the same source Chinese string but they have different topic distributions due to the different English translations", "the targetside topic distributions of rules into the space of sourceside topic model by oneto many projection Section 42", "Experiments on ChineseEnglish translation tasks Section 6 show that our method outperforms the baseline hierarchial phrasebased system by 09 BLE U points", "This result is also 05 points higher and 3 times faster than the previous topicspecific lexicon translation method", "We further show that both the sourceside and targetside topic distributions improve translation quality and their improvements are complementary to each other"]}, "P12-2002": {"title": ["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 6 10 Jeju Republic of Korea 814 July 2012 c  2012 Association for Computational Linguistics Joint Evaluation of Morphological Segmentation and Syntactic Parsing Reut Tsarfaty Joakim Nivre Evelina Andersson Box 635 751 26 Uppsala University Uppsala Sweden tsarfatystplingfiluuse joakimnivre evelinaanderssonlingfiluuse Abstract We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance The protocol uses distancebased metrics defined for the space of trees over lattices Our metrics allow us to precisely quantify the performance gap be tween nonrealistic parsing scenarios assuming gold segmented and tagged input and realistic ones not assuming gold segmentation and tags Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios 1 Introduction A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentences humanperceived interpretation Current stateoftheart parsers assume that the spacedelimited words in the input are the basic units of syntactic analysis Standard evaluation procedures and metrics Black et al 1991 Buchholz and Marsi 2006 accordingly assume that the yield of the parse tree is known in advance This assumption breaks down when parsing morphologically rich languages Tsarfaty et al 2010 where every spacedelimited word may be effectively composed of multiple morphemes each of which having a distinct role in the syntactic parse tree In order to parse such input the text needs to undergo morphological segmentation that is identifying the morphological segments of each word and assigning the corresponding partofspeech PoS tags to them Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty 2006 Cohen and Smith 2007 Goldberg and Tsarfaty 2008 Green and Manning 2010 If the selected segmentation is different from the gold segmentation the gold and parse trees are rendered incomparable and standard evaluation metrics break down Evaluation scenarios restricted to gold input are often used to bypass this problem but as shall be seen shortly they present an overly optimistic upperbound on parser performance This paper presents a full treatment of evaluation in different parsing scenarios using distancebased measures defined for trees over a shared common denominator defined in terms of a lattice structure We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing perfor mance for the Semitic language Modern Hebrew using the best performing systems both constituencybased and dependencybased Tsarfaty 2010 Goldberg 2011a Our experiments demonstrate that for all parsers significant performance gaps between realistic and nonrealistic scenarios crucially depend on the kind of information initially provided to the parser The tool and metrics that we provide are completely general and can straightforwardly apply to other languages treebanks and different tasks 6 tree1 TOP PP IN 0B1 in NP NP DEF 1H2 the NP NN 2CL3 shadow PP POSS 3FL4 of PRN 4HM5 them ADJP DEF 5H6 the JJ 6NEIM7 pleasant tree2 TOP PP IN 0B1 in NP NP NN 1CL2 shadow PP POSS 2FL3 of PRN 3HM4 them VB 4HNEIM5 madepleasant Figure 1 A correct tree tree1 and an incorrect tree tree2 for  BCLM HNEIM indexed by terminal boundaries Erroneous nodes in the parse hypothesis are marked in italics Missing nodes from the hypothesis are marked in bold 2 The Challenge Evaluation for MRLs In morphologically rich languages MRLs substantial information about the grammatical relations be tween entities is expressed at word level using inflectional affixes In particular in MRLs such as Hebrew Arabic Turkish or Maltese elements such as determiners definite articles and conjunction markers appear as affixes that are appended to an open class word Take for example the Hebrew wordtoken BCLM1 which means  in their shadow  This word corresponds to five distinctly tagged elements B  in IN H  the DEF CL  shadow NN FL  ofPOSS HM  they PRN Note that morphological segmentation is not the inverse of concatenation For instance the overt definite article H and the possessor FL show up only in the analysis The correct parse for the Hebrew phrase BCLM HNEIM is shown in Figure 1 tree1 and it presupposes that these segments can be identified and assigned the correct PoS tags However morphological segmentation is nontrivial due to massive wordlevel ambiguity The word BCLM for instance can be segmented into the noun BCL onion and M a genitive suffix  of them or into the prefix B in  followed by the noun CLM  image  2 The multitude of morphological analyses may be encoded in a lattice structure as illustrated in Figure 2 1We use the Hebrew transliteration in Sima an et al 2001 2The complete set of analyses for this word is provided in Goldberg and Tsarfaty 2008 Examples for similar phenomena in Arabic may be found in Green and Manning 2010 Figure 2 The morphological segmentation possibilities of BCLM HNEIM Double circles are word boundaries In practice a statistical component is required to decide on the correct morphological segmentation that is to pick out the correct path through the lattice This may be done based on linear local context Adler and Elhadad 2006 Shacham and Wintner 2007 Bar haim et al 2008 Habash and Rambow 2005 or jointly with parsing Tsarfaty 2006 Goldberg and Tsarfaty 2008 Green and Manning 2010 Either way an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis ultimately providing a parse tree which spans a different yield than the gold terminals In such cases existing evaluation metrics break down To understand why consider the trees in Figure 1 Metrics like PARSEVAL Black et al 1991 calculate the harmonic means of precision and recall on labeled spans  i label j where i j are terminal boundaries Now the NP dominating shadow of them has been identified and labeled correctly in tree2 but in tree1 it spans 2NP 5 and in tree2 it spans  1NP 4  This node will then be counted as an error for tree2 along with its dominated and dominating structure and PARSEVAL will score 0 7 A generalized version of PARSEVAL which con siders i j characterbased indices instead of terminal boundaries Tsarfaty 2006 will fail here too since the missing overt definite article H will cause similar misalignments Metrics for dependencybased evaluation such as ATTACHMENT SCORES Buchholz and Marsi 2006 suffer from similar problems since they assume that both trees have the same nodes  an assumption that breaks down in the case of incorrect morphological segmentation Although great advances have been made in parsing MRLs in recent years this evaluation challenge remained unsolved3 In this paper we present a solution to this challenge by extending TEDEVAL Tsarfaty et al 2011 for handling trees over lattices 3 The Proposal DistanceBased Metrics Input and Output Spaces We view the joint task as a structured prediction function h  X  Y from input space X onto output space Y  Each element x  X is a sequence x  w1     wn of spacedelimited words from a setW  We assume a lexicon LEX distinct fromW  containing pairs of segments drawn from a set T of terminals and PoS categories drawn from a set N of nonterminals LEX   s p s  T  p  N Each word wi in the input may admit multiple morphological analyses constrained by a languagespecific morphological analyzer MA The morphological analysis of an input word MAwi can be represented as a lattice Li in which every arc corresponds to a lexicon entry  s p  The morphological analysis of an input sentence x is then a lattice L obtained through the concatenation of the lattices L1     Ln where MAw1  L1     MAwn  Ln Now let x  w1     wn be a sentence with a morphological analysis lattice MAx  L We define the output space YMAxL for h abbreviated YL as the set of linearlyordered labeled trees such that the yield of LEX entries  s1 p1   sk pk in each tree where si  T and pi  N  and possibly k 6 n corresponds to a path through the lattice L 3A tool that could potentially apply here is SParseval Roark et al 2006 But since it does not respect wordboundaries it fails to apply to such lattices Cohen and Smith 2007 aimed to fix this but in their implementation syntactic nodes internal to word boundaries may be lost without scoring Edit Scripts and Edit Costs We assume a set AADDc i jDELc i jADDs p  i j DEL s p i j of edit operations which can add or delete a labeled node c  N or an entry  s p  LEX which spans the states i j in the lattice L The operations in A are properly constrained by the lattice that is we can only add and delete lexemes that belong to LEX and we can only add and delete them where they can occur in the lattice We assume a function Ca  1 assigning a unit cost to every operation a  A and define the cost of a sequence a1     am as the sum of the costs of all operations in the sequence Ca1  am  m i1 Cai An edit script ESy1 y2  a1     am is a sequence of operations that turns y1 into y2 The treeedit distance is the minimum cost of any edit script that turns y1 into y2 Bille 2005 TEDy1 y2  min ESy1y2 CESy1 y2 DistanceBased Metrics The error of a predicted structure p with respect to a gold structure g is now taken to be the TED cost and we can turn it into a score by normalizing it and subtracting from a unity TEDEVALp g  1 TEDp g p g  2 The term p  g  2 is a normalization factor defined in terms of the worstcase scenario in which the parser has only made incorrect decisions We would need to delete all lexemes and nodes in p and add all the lexemes and nodes of g except for roots An Example Both trees in Figure 1 are contained in YL for the lattice L in Figure 2 If we replace terminal boundaries with lattice indices from Figure 2 we need 6 edit operations to turn tree2 into tree1 deleting the nodes in italic adding the nodes in bold and the evaluation score will be TEDEVALtree2tree1  1 61410  2  07273"], "abstract": ["We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance", "The protocol uses distancebased metrics defined for the space of trees over lattices", "Our metrics allow us to precisely quantify the performance gap between nonrealistic parsing scenarios assuming gold segmented and tagged input and realistic ones not assuming gold segmentation and tags", "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios"], "inroduction": ["A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentences humanperceived interpretation", "Current stateoftheart parsers assume that the space delimited words in the input are the basic units of syntactic analysis", "Standard evaluation procedures and metrics Black et al 1991 Buchholz and Marsi 2006 accordingly assume that the yield of the parse tree is known in advance", "This assumption breaks down when parsing morphologically rich languages Tsarfaty et al 2010 where every spacedelimited word may be effectively composed of multiple morphemes each of which having a distinct role in the syntactic parse tree", "In order to parse such input the text needs to undergo morphological segmentation that is identifying the morphological segments of each word and assigning the corresponding partof speech PoS tags to them", "Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated", "The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence", "One can either select a segmentation path prior to parsing or as has been recently argued one can let the parser pick a segmentation jointly with decoding Tsarfaty 2006 Cohen and Smith 2007 Goldberg and Tsarfaty 2008 Green and Manning 2010", "If the selected segmentation is different from the gold segmentation the gold and parse trees are rendered incomparable and standard evaluation metrics break down", "Evaluation scenarios restricted to gold input are often used to bypass this problem but as shall be seen shortly they present an overly optimistic upper bound on parser performance", "This paper presents a full treatment of evaluation in different parsing scenarios using distancebased measures defined for trees over a shared common denominator defined in terms of a lattice structure", "We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing performance for the Semitic language Modern Hebrew using the best performing systems both constituency based and dependencybased Tsarfaty 2010 Goldberg 2011a", "Our experiments demonstrate that for all parsers significant performance gaps between realistic and nonrealistic scenarios crucially depend on the kind of information initially provided to the parser", "The tool and metrics that we provide are completely general and can straightforwardly apply to other languages treebanks and different tasks", "6 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 610 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics tree1 TOP tree2 TOP PP PP IN 0B1 in NP N P DEF NP ADJP DEF JJ IN 0B1 in NP NN 1CL2 NP V B 4 H N E I M 5PP made pleasant 1H2 the NN 2CL3 shadow POSS 3FL4 PP PRN 4HM5 5H6 the 6NEIM7 pleasant shadow POSS 2FL3 of PRN 3HM4 them of them Figure 1 A correct tree tree1 and an incorrect tree tree2 for BCLM HNEIM indexed by terminal boundaries", "Erroneous nodes in the parse hypothesis are marked in italics", "Missing nodes from the hypothesis are marked in bold"]}, "P12-2023": {"title": ["Topic Models for Dynamic Translation Model Adaptation"], "abstract": ["We propose an approach that biases machine translation systems toward relevant translations based on topicspecific contexts where topics are induced in an unsupervised way using topic models this can be thought of as inducing subcorpora for adaptation without any human annotation", "We use these topic distributions to compute topicdependent lexical weighting probabilities and directly incorporate them into our translation model as features", "Conditioning lexical probabilities on the topic biases translations toward topic relevant output resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline"], "inroduction": ["The performance of a statistical machine translation SMT system on a translation task depends largely on the suitability of the available parallel training data", "Domains eg newswire vs blogs may vary widely in their lexical choices and stylistic preferences and what may be preferable in a general setting or in one domain is not necessarily preferable in another domain", "Indeed sometimes the domain can change the meaning of a phrase entirely", "In a food related context the Chinese sentence      fensi henduo  would mean They have a lot of vermicelli however in an informal Internet conversation this sentence would mean They have a lot of fans", "Without the broader context it is impossible to determine the correct translation in otherwise identical sentences", "This problem has led to a substantial amount of recent work in trying to bias or adapt the translation model TM toward particular domains of interest Axelrod et al 2011 Foster et al 2010 Snover et al 20081 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation", "Matsoukas et al", "2009 introduced assigning a pair of binary features to each training sentence indicating sentences genre and collection as a way to capture domains", "They then learn a mapping from these features to sentence weights use the sentence weights to bias the model probability estimates and subsequently learn the model weights", "As sentence weights were found to be most beneficial for lexical weighting Chiang et al", "2011 extends the same notion of conditioning on provenance ie the origin of the text by removing the separate mapping step directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature estimated from only those sentences that comprise that genre or collection", "The common thread throughout prior work is the concept of a domain", "A domain is typically a hard constraint that is externally imposed and hand labeled such as genre or corpus collection", "For example a sentence either comes from newswire or weblog but not both", "However this poses several problems", "First since a sentence contributes its counts only to the translation table for the source it came from many word pairs will be unobserved for a given table", "This sparsity requires smoothing", "Second we may not know the subcorpora our training 1 Language model adaptation is also prevalent but is not the focus of this work", "115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 115119 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics data come from and even if we do subcorpus may not be the most useful notion of domain for better translations", "We take a finergrained flexible unsupervised approach for lexical weighting by domain", "We induce unsupervised domains from large corpora and we incorporate soft probabilistic domain membership into a translation model", "Unsupervised modeling of the training data produces naturally occurring sub corpora generalizing beyond corpus and genre", "Depending on the model used to select subcorpora we can bias our translation toward any arbitrary distinction", "This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation", "In this work we consider the underlying latent topics of the documents Blei et al 2003", "Topic modeling has received some use in SMT for instance Bilingual LSA adaptation Tam et al 2007 and the BiTAM model Zhao and Xing 2006 which uses a bilingual topic model for learning alignment", "In our case by building a topic distribution for the source side of the training data we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership", "This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics", "We accomplish this by introducing topic dependent lexical probabilities directly as cf e2e cf e  Phrase pair probabilities pef  are computed from these as described in Koehn et al", "2003", "Chiang et al", "2011 showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features fsef  one for each domain s whichcompute a new word translation table psef  esti mated from only those sentences which belong to s csf e2e csf e  where cs is the number of occurrences of the word pair in s Topic Modeling for MT We extend provenance to cover a set of automatically generated topics zn", "Given a parallel training corpus T composed of documents di we build a source side topic model over T  which provides a topic distribution pzndi for zn  1   ", " K  over each document using Latent Dirichlet Allocation LDA Blei et al 2003", "Then we assign pzndi to be the topic distribution for every sentence xj  di thus enforcing topic sharing across sentence pairs in the same document instead of treating them as unrelated", "Computing the topic distribution over a document and assigning it to the sentences serves to tie the sentences together in the document context", "To obtain the lexical probability conditioned on topic distribution we first compute the expected count ezn e f  of a word pair under topic zn features in the translation model and interpolatingthem loglinearly with our other features thus allow ezn e f   pzndi cj e f  1 ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function", "Incorporating these features into our hierarchical phrasebased translation system significantly improved translation per where cj  denotes the number of occurrences of the word pair in sentence xj  and then compute ezn e f  formance by up to 1 BLEU and 3 TER over a strong Chinese to English baseline", "pzn ef   2", "e ezn e f  2"]}, "P12-2031": {"title": ["Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 156 160 Jeju Republic of Korea 814 July 2012 c  2012 Association for Computational Linguistics Crowdsourcing InferenceRule Evaluation Naomi Zeichner BarIlan University RamatGan Israel zeichnernaomigmailcom Jonathan Berant TelAviv University TelAviv Israel jonatha6posttauacil Ido Dagan BarIlan University RamatGan Israel dagancsbiuacil Abstract The importance of inference rules to semantic applications has long been recognized and ex tensive work has been carried out to automatically acquire inferencerule resources However evaluating such resources has turned out to be a nontrivial task slowing progress in the field In this paper we suggest a framework for evaluating inferencerule resources Our framework simplifies a previously proposed instancebased evaluation method that involved substantial annotator training making it suitable for crowdsourcing We show that our method produces a large amount of an notations with high interannotator agreement for a low cost at a short period of time without requiring training expert annotators 1 Introduction Inference rules are an important component in semantic applications such as Question Answering QA Ravichandran and Hovy 2002 and Information Extraction IE Shinyama and Sekine 2006 describing a directional inference relation between two text patterns with variables For example to answer the question  Where was Reagan raised a QA system can use the rule  X brought up in Y X raised in Y to extract the answer from  Reagan was brought up in Dixon  Similarly an IE system can use the rule  X work as Y X hired as Y to ex tract the PERSON and ROLE entities in the hiring event from  Bob worked as an analyst for Dell The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel 2001 Sekine 2005 Schoenmackers et al 2010 and generate knowledge resources for inference systems However despite their potential utilization of inference rule resources is currently somewhat limited This is largely due to the fact that these algorithms often produce invalid rules Thus evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource Unfortunately as evaluating inference rules is hard and costly there is no clear evaluation standard and this has become a slowing factor for progress in the field One option for evaluating inference rule resources is to measure their impact on an end task as that is what ultimately interests an inference system developer However this is often problematic since inference systems have many components that address multiple phenomena and thus it is hard to assess the effect of a single resource An example is the Recognizing Textual Entailment RTE framework Dagan et al 2009 in which given a text T and a textual hypothesis H a system determines whether H can be inferred from T This type of evaluation was established in RTE challenges by ablation tests see RTE ablation tests in ACLWiki and showed that resources impact can vary considerably from one system to another These issues have also been noted by Sammons et al 2010 and LoBue and Yates 2011 A complementary application independent evaluation method is hence necessary Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule Shinyama et al 2002 Sekine 2005 However Szpektor et al 2007 observed that directly judging rules out of context often results in low interannotator agreement To remedy that Szpektor et al 2007 and 156 Bhagat et al 2007 proposed instancebased evaluation in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference This simulates the utility of rules in an application and yields high interannotator agreement Unfortunately their method requires lengthy guidelines and substantial annotator training effort which are time consuming and costly Thus a simple robust and replicable evaluation method is needed Recently crowdsourcing services such as Ama zon Mechanical Turk AMT and CrowdFlower CF1 have been employed for semantic inference annotation Snow et al 2008 Wang and CallisonBurch 2010 Mehdad et al 2010 Negri et al 2011 These works focused on generating and an notating RTE texthypothesis pairs but did not ad dress annotation and evaluation of inference rules In this paper we propose a novel instancebased evaluation framework for inference rules that takes advantage of crowdsourcing Our method substantially simplifies annotation of rule applications and avoids annotator training completely The novelty in our framework is twofold 1 We simplify instancebased evaluation from a complex decision scenario to two independent binary decisions 2 We apply methodological principles that efficiently communicate the definition of the inference relation to untrained crowdsourcing workers Turkers As a case study we applied our method to evaluate algorithms for learning inference rules between predicates We show that we can produce many annotations cheaply quickly at good quality while achieving high interannotator agreement 2 Evaluating Rule Applications As mentioned in instancebased evaluation individual rule applications are judged rather than rules in isolation and the quality of a ruleresource is then evaluated by the validity of a sample of applications of its rules Rule application is performed by finding an instantiation of the rule lefthand side in a corpus termed LHS extraction and then applying the rule on the extraction to produce an instantiation of the rule righthand side termed RHS instantiation For example the rule  X observe YX celebrate Y 1httpswwwmturkcom and httpcrowdflowercom can be applied on the LHS extraction  they observe holidays to produce the RHS instantiation they celebrate holidays  The target of evaluation is to judge whether each rule application is valid or not Following the standard RTE task definition a rule application is considered valid if a human reading the LHS extraction is highly likely to infer that the RHS instanti ation is true Dagan et al 2009 In the aforementioned example the annotator is expected to judge that they observe holidays entails  they celebrate holidays  In addition to this straightforward case two more subtle situations may arise The first is that the LHS extraction is meaningless We regard a proposition as meaningful if a human can easily understand its meaning despite some simple grammatical errors A meaningless LHS extraction usually occurs due to a faulty extraction process eg Table 1 Example 2 and was relatively rare in our case study 4 of output see Section 4 Such rule applications can either be extracted from the sample so that the rulebase is not penalized since the problem is in the extraction procedure or can be used as examples of nonentailment if we are interested in overall performance A second situation is a meaningless RHS instantiation usually caused by rule application in a wrong context This case is tagged as nonentailment for example applying the rule  X observe Y X celebrate Y in the context of the extraction companies observe dress code  Each rule application therefore requires an answer to the following three questions 1 Is the LHS ex traction meaningful 2 Is the RHS instantiation meaningful 3 If both are meaningful does the LHS extraction entail the RHS instantiation 3 Crowdsourcing Previous works using crowdsourcing noted some principles to help get the most out of the serviceWang et al 2012 In keeping with these findings we employ the following principles a Simple tasks The global task is split into simple subtasks each dealing with a single aspect of the problem b Do not assume linguistic knowledge by annotators Task descriptions avoid linguistic terms such as  tense  which confuse workers c Gold standard validation Using CFs builtin methodology 157 Phrase Meaningful Comments 1 Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors 2 A player deposit an No Bad extraction for the rule LHS  X deposit Y 3 humans bring in bed No Wrong context result of applying  X turn in YX bring in Y on humans turn in bed Table 1 Examples of phrase  meaningfulness Note that the comments are not presented to Turkers gold standard GS examples are combined with actual annotations to continuously validate annotator reliability We split the annotation process into two tasks the first to judge phrase meaningfulness Questions 1 and 2 above and the second to judge entailment Question 3 above In Task 1 the LHS extrac tions and RHS instantiations of all rule applications are separated and presented to different Turkers independently of one another This task is simple quick and cheap and allows Turkers to focus on the single aspect of judging phrase meaningfulness Rule applications for which both the LHS extrac tion and RHS instantiation are judged as meaningful are passed to Task 2 where Turkers need to decide whether a given rule application is valid If not for Task 1 Turkers would need to distinguish in Task 2 between nonentailment due to 1 an incorrect rule"], "abstract": ["The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inferencerule resources", "However evaluating such resources has turned out to be a nontrivial task slowing progress in the field", "In this paper we suggest a framework for evaluating inferencerule resources", "Our framework simplifies a previously proposed instancebased evaluation method that involved substantial annotator training making it suitable for crowdsourcing", "We show that our method produces a large amount of annotations with high interannotator agreement for a low cost at a short period of time without requiring training expert annotators"], "inroduction": ["Inference rules are an important component in semantic applications such as Question Answering QA Ravichandran and Hovy 2002 and Information Extraction IE Shinyama and Sekine 2006 describing a directional inference relation between two text patterns with variables", "For example to answer the question Where was Reagan raised a QA system can use the rule X brought up in YX raised in Y to extract the answer from Reagan was brought up in Dixon", "Similarly an IE system can use the rule X work as YX hired as Y to extract the PERSON and ROLE entities in the hiring event from Bob worked as an analyst for Dell", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules Lin and Pantel 2001 Sekine 2005 Schoenmackers et al 2010 and generate knowledge resources for inference systems", "However despite their potential utilization of inference rule resources is currently somewhat limited", "This is largely due to the fact that these algorithms often produce invalid rules", "Thus evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource", "Unfortunately as evaluating inference rules is hard and costly there is no clear evaluation standard and this has become a slowing factor for progress in the field", "One option for evaluating inference rule resources is to measure their impact on an end task as that is what ultimately interests an inference system developer", "However this is often problematic since inference systems have many components that address multiple phenomena and thus it is hard to assess the effect of a single resource", "An example is the Recognizing Textual Entailment RTE framework Dagan et al 2009 in which given a text T and a textual hypothesis H a system determines whether H can be inferred from T This type of evaluation was established in RTE challenges by ablation tests see RTE ablation tests in ACLWiki and showed that resources impact can vary considerably from one system to another", "These issues have also been noted by Sammons et al", "2010 and LoBue and Yates 2011", "A complementary applicationindependent evaluation method is hence necessary", "Some attempts were made to let annotators judge rule correctness directly that is by asking them to judge the correctness of a given rule Shinyama et al 2002 Sekine 2005", "However Szpektor et al", "2007 observed that directly judging rules out of context often results in low interannotator agreement", "To remedy that Szpektor et al", "2007 and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 156160 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics Bhagat et al", "2007 proposed instancebased evaluation in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference", "This simulates the utility of rules in an application and yields high interannotator agreement", "Unfortunately their method requires lengthy guidelines and substantial annotator training effort which are time consuming and costly", "Thus a simple robust and replicable evaluation method is needed", "Recently crowdsourcing services such as Amazon Mechanical Turk AMT and CrowdFlower CF1 have been employed for semantic inference annotation Snow et al 2008 Wang and CallisonBurch 2010 Mehdad et al 2010 Negri et al 2011", "These works focused on generating and annotating RTE texthypothesis pairs but did not address annotation and evaluation of inference rules", "In this paper we propose a novel instancebased evaluation framework for inference rules that takes advantage of crowdsourcing", "Our method substantially simplifies annotation of rule applications and avoids annotator training completely", "The novelty in our framework is twofold 1 We simplify instancebased evaluation from a complex decision scenario to two independent binary decisions", "2 We apply methodological principles that efficiently communicate the definition of the inference relation to untrained crowdsourcing workers Turkers", "As a case study we applied our method to evaluate algorithms for learning inference rules between predicates", "We show that we can produce many annotations cheaply quickly at good quality while achieving high interannotator agreement"]}, "P12-2050": {"title": ["Coarse Lexical Semantic Annotation with Supersenses"], "abstract": ["Lightweight semantic annotation of textcalls for a simple representation ideally without requiring a semantic lexicon to achieve good coverage in the language and domainIn this paper we repurpose WordNets super sense tags for annotation developing specificguidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains", "The resulting corpus has high coverage and was completed quickly with reasonable interannotator agreement"], "inroduction": ["The goal of lightweight semantic annotation of text particularly in scenarios with limited resources and expertise presents several requirements for arepresentation simplicity adaptability to new lan guages topics and genres and coverage", "This paper describes coarse lexical semantic annotationof Arabic Wikipedia articles subject to these con straints", "Traditional lexical semantic representations are either narrow in scope like named entities1 or make reference to a fullfledged lexiconontology which may insufficiently cover the languagedomainof interest or require prohibitive expertise and ef fort to apply2 We therefore turn to supersense tags SSTs 40 coarse lexical semantic classes 25 fornouns 15 for verbs originating in WordNet", "Previ ously these served as groupings of English lexicon 1Some ontologies like those in Sekine et al", "2002 and BBN Identifinder Bikel et al 1999 include a large selection of classes which tend to be especially relevant to proper names", "2Eg a WordNet Fellbaum 1998 sense annotation effortreported by Passonneau et al", "2010 found considerable inter annotator variability for some lexemes FrameNet Baker etal 1998 is limited in coverage even for English and Prop Bank Kingsbury and Palmer 2002 does not capture semanticrelationships across lexemes", "We note that the Omega ontology Philpot et al 2003 has been used for finegrained cross lingual annotation Hovy et al 2006 Dorr et al 2010", "COMMUNICATION GROUP 859  XCJ   AD ACT TIME The Guinness Book of World Records considers the University of AlKaraouine in Fez Morocco established in the year 859 AD the oldest university in the world Figure 1 A sentence from the article Islamic GoldenAge with the supersense tagging from one of two anno tators", "The Arabic is shown lefttoright", "entries but here we have repurposed them as target labels for direct human annotation", "Part of the earliest versions of WordNet the supersense categories originally lexicographer classes were intended to partition all English noun and verb senses into broad groupings or semanticfields Miller 1990 Fellbaum 1990", "More re cently the task of automatic supersense tagging has emerged for English Ciaramita and Johnson 2003 Curran 2005 Ciaramita and Altun 2006 Paa and Reichartz 2009 as well as for Italian Picca et al 2008 Picca et al 2009 Attardi et al 2010 and Chinese Qiu et al 2011 languages with WordNetsmapped to English WordNet3 In principle we be lieve supersenses ought to apply to nouns and verbsin any language and need not depend on the avail ability of a semantic lexicon4 In this work we focuson the noun SSTs summarized in figure 2 and ap plied to an Arabic sentence in figure 1", "SSTs both refine and relate lexical items they capture lexical polysemy on the one handeg3Note that work in supersense tagging used text with fine grained sense annotations that were then coarsened to SSTs", "4The nounverb distinction might prove problematic in some languages", "QJK  considers  JJ k Guinness H AJ book  C JAJ  AP   that forrecords thestandard Ag", " university Q AlKaraouine Ai in Fez H Q   Morocco Ag", " oldest university  Y     in ARTIFACT LOCATION A theworld  ADJAK established J in year IJ    k was where   LOCATION 253 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics pages 253258 Jeju Republic of Korea 814 July 2012", "c2012 Association for Computational Linguistics Crusades  Damascus  Ibn Tolun Mosque  Imam Hussein Shrine  Islamic Golden Age  Islamic History  Ummayad Mosque 434s 16185t 5859m Atom  Enrico Fermi  Light  Nuclear power  Periodic Table  Physics  Muhammad alRazi 777s 18559t 6477m 2004 Summer Olympics  Christiano Ronaldo  Football  FIFA World Cup  Portugal football team  Raul Gonzales  Real Madrid 390s 13716t 5149m", "Computer  Computer Software  Internet  Linux  Richard Stallman  Solaris  X Window System 618s 16992t 5754m Table 1 Snapshot of the supersenseannotated data", "The 7 article titles translated in each domain with total counts of sentences tokens and supersense mentions", "Overall there are 2219 sentences with 65452 tokens and 23239 mentions 13 tokensmention on average", "Counts exclude sentences marked as problematic and mentions marked ", "disambiguating PERSON vs POSSESSION for the noun principaland generalize across lexemes on the othereg principal teacher and student can all be PERSONs", "This lumping property might be expected to give too much latitude to annotators yetwe find that in practice it is possible to elicit reason able interannotator agreement even for a languageother than English", "We encapsulate our interpreta tion of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language our annotators had no prior expertise in linguistics or linguistic annotation", "Finally we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering Li and Roth 2002 to animacy hierarchy representation for corpus linguistics Zaenen et al 2004", "We believe the interpretation of the SSTs adopted here can serveas a single starting point for diverse resource en gineering efforts and applications especially when finegrained sense annotation is not feasible"]}, "P13-1059": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 604614 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics Nameaware Machine Translation Haibo Li Jing Zheng Heng Ji Qi Li Wen Wang  Computer Science Department and Linguistics Department Queens College and Graduate Center City University of New York New York NY USA 10016 lihaiboc hengjicuny liqiearthgmailcom  Speech Technology  Research Laboratory SRI International Menlo Park CA USA 94025 zj wwangspeechsricom Abstract We propose a Nameaware Machine Translation MT approach which can tightly integrate name processing into MT model by jointly annotating parallel corpora extracting nameaware translation grammar and rules adding name phrase table and name translation driven decoding Additionally we also propose a new MT metric to appropriately evaluate the translation quality of informative words by assigning different weights to different words according to their importance values in a document Experiments on ChineseEnglish translation demonstrated the effectiveness of our approach on en hancing the quality of overall translation name translation and word alignment over a highquality MT baseline1 1 Introduction A shrinking fraction of the world s Web pages are written in English therefore the ability to access pages across a range of languages is becoming increasingly important This need can be addressed in part by crosslingual information access tasks such as entity linking McNamee et al 2011 Cassidy et al 2012 event extraction HakkaniTur et al 2007 slot filling Snover et al 2011 and question answering Parton et al 2009 Parton and McKeown 2010 A key bottleneck of highquality crosslingual information access lies in the performance of Machine Translation MT Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information especially names 1Some of the resources and open source programs developed in this work are made freely available for research purpose at httpnlpcsqccunyeduNAMTtgz A typical statistical MT system can only trans late 60 person names correctly Ji et al 2009"], "abstract": ["We propose a Nameaware Machine Translation MT approach which can tightly integrate name processing into MT model by jointly annotating parallel corpora extracting nameaware translation grammar and rules adding name phrase table and name translation driven decoding", "Additionally we also propose a new MT metric to appropriately evaluate the translation quality of informative words by assigning different weights to different words according to their importance values in a document", "Experiments on ChineseEnglish translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation name translation and word alignment over a highquality MT baseline1"], "inroduction": ["A shrinking fraction of the worlds Web pages are written in English therefore the ability to access pages across a range of languages is becoming increasingly important", "This need can be addressed in part by crosslingual information access tasks such as entity linking McNamee et al 2011 Cassidy et al 2012 event extraction HakkaniTur et al 2007 slot filling Snover et al 2011 and question answering Parton et al 2009 Parton and McKeown 2010", "A key bottleneck of high quality crosslingual information access lies in the performance of Machine Translation MT", "Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information especially names", "1 Some of the resources and open source programs developed in this work are made freely available for research purpose at httpnlpcsqccunyeduNAMTtgz A typical statistical MT system can only translate 60 person names correctly Ji et al 2009", "Incorrect segmentation and translation of names which often carry central meanings of a sentence can also yield incorrect translation of long contexts", "Names have been largely neglected in the prior MT research due to the following reasons  The current dominant automatic MT scoring metrics such as Bilingual Evaluation Understudy BLEU Papineni et al 2002 treat all words equally but names have relative low frequency in text about 6 in newswire and only 3 in web documents and thus are vastly outnumbered by function words and common nouns etc", " Name translations pose a greater complexity because the set of names is open and highly dynamic", "It is also important to acknowledge that there are many fundamental differences between the translation of names and other tokens depending on whether a name is rendered phonetically semantically or a mixture of both Ji et al 2009", " The artificial settings of assigning low weights to information translation compared to overall word translation in some large scale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem", "We propose a novel Nameaware MT NAMT approach which can tightly integrate name processing into the training and decoding processes of an endtoend MT pipeline and a new nameaware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document", "Compared to previous methods the novel contributions of our approach are 1", "Tightly integrate joint bilingual name tag", "ging into MT training by coordinating tagged 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 604614 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics names in parallel corpora updating word segmentation word alignment and grammar extraction Section 31"]}, "P13-1062": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 631640 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics Bootstrapping Entity Translation on Weakly Comparable Corpora Taesung Lee and Seungwon Hwang Department of Computer Science and Engineering Pohang University of Science and Technology POSTECH Pohang Republic of Korea elca4u swhwangpostechedu Abstract This paper studies the problem of mining named entity translations from compara ble corpora with some asymmetry Unlike the previous approaches relying on the symmetry found in parallel corpora the proposed method is tolerant to asymmetry often found in comparable corpora by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora Our experimental results on EnglishChinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 016 for organization names and 014 in a low comparability case 1 Introduction Identifying and understanding entities is a crucial step in understanding text This task is more challenging in the presence of multilingual text because translating named entities NEs such as persons locations or organizations is a nontrivial task Early research on NE trans lation used phonetic similarities for example to mine the translation  Mandelson       ManDeErSen with similar sounds However not all NE translations are based on transliterations as shown in Table 1 Some translations especially the names of most organizations are based on semantic equivalences Furthermore names can be abbreviated in one or both languages eg the  World Trade Organization   can be called the WTO     Another challenging example is that a translation can be arbitrary eg Jackie Chan     ChengLong There are many approaches English Chinese World Trade Organization  ShiJieMaoYiZuZhi WTO     ShiMaoZuZhi Jackie Chan   ChengLong Table 1 Examples of nonphonetic translations that deal with some of these challenges Lam et al 2007 Yang et al 2009 eg by combining phonetic similarity and a dictionary However arbitrary translations still cannot be handled by examining the NE pair itself Corpusbased approaches Kupiec 1993 Feng 2004 by mining external signals from a large corpus such as parenthetical translation    Jackie Chan   complement the problem of transliterationbased approaches but the coverage of this approach is limited to popular entities with such evidence The most effective known approach to NE translation has been a holistic framework You et al 2010 Kim et al 2011 You et al 2012 combining transliteration and corpusbased methods In these approaches both 1 arbitrary translations and 2 lesserknown entities can be handled by propagating the translation scores of known entities to lesserknown entities if they cooccur frequently in both corpora For example a lesserknown entity Tom Watson can be translated if Mandelson and Tom Watson cooccur frequently in an English corpus and their Chinese translations also co occur frequently in a Chinese corpus ie if the co occurrences in the two corpora are symmetric A research question we ask in this paper is What if comparable corpora are not comparable enough to support this symmetry assumption We found that this is indeed the case For example even English and Chinese news from the same publisher may have different focus the Chi nese version focuses more on Chinese Olympic 631 teams and Chinese local news In the presence of such asymmetry all previous approaches building upon symmetry quickly deteriorate by propagating false positives For example co occurrence of Mandelson and Tom Watson may not appear in a Chinese corpus which may lead to the translation of Tom Watson into another Chinese entity Gordon Brown which happens to co occur with the Chinese translation of Mandelson Our key contribution is to avoid such false propagation by discerning the semantics of relations For example relations between Mandelson and Tom Watson should be semantically different from Chinese relations between        Gordon Brown and  Mandelson A naive approach would be finding documents with a similar topic such as politics and scientific discovery and allowing propagation only when the topic agrees However we found that a topic is a unit that is too coarse for this task because most articles on Mandelson will invariably fall into the same topic1 In clear contrast we selectively propagate seed translations only when the relations in the two corpora share the same semantics This selective propagation can be especially effective for translating challenging types of entities such as organizations including theWTO used with and without abbreviation in both languages Applying a holistic approach You et al 2012 on organizations leads to poor results 006 in terms of the F1score A naive approach to increase the precision would be to consider multitype cooccurrences hoping that highly precise translations of some type eg persons with an F1score of 069 You et al 2012 can be propagated to boost the precision on organizations In our experiments this naive multitype propagation still leads to an unsatisfactory F1score of 012 Such a low score can be explained by the following example When translating WTO using the co occurrence with Mandelson other cooccurrences such as London Mandelson and EU Mandelson produce a lot of noise because the right translation of WTO does not share much phoneticsemantic similarity Our understanding of relation semantics can distinguish Mandelson was born in London from  Mandelson visited the WTO  to stop false propagations which generates an F1score 025 higher than the existing ap 1The MRR for organization names achieved by a topic modelbased approach was 015 lower than our best proaches More formally we enable selective propagation of seed translations on weakly comparable corpora by 1 clarifying the detailed meaning of relational information of co occurring entities and 2 identifying the contexts of the relational information using statementlevel context comparison In other words we propagate the translation score of a known translation pair to a neighbor pair if the semantics of their relations in English and Chinese corpora are equivalent to accurately propagate the scores For example if we know  Russia     1 and join2 then from a pair of statements  Russia1 joins2 theWTO3 and     1 2     3  we can propagate the translation score of Russia    1 to WTO  3 However we do not exploit a pair of statements Russia joined the WTO and  2   because   2  does not mean join2 Furthermore we mine a similar EnglishChinese document pair that can be found by comparing the entity relationships such as Mandelson visited Moscow  and  Mandelson met Alexei Kudrin  within the English document and the Chinese document to leverage similar contexts to assure that we use symmetric parts For this goal we first extract relations among entities in documents such as visit and join and mine semantically equivalent relations across the languages eg English and Chinese such as join    Once these relation translations are mined similar document pairs can be identified by comparing each constituent relationship among entities using their relations Knowing document similarity improves NE translation and improved NE translation can boost the accuracy of document and relationship similarity This iterative process can continue until convergence To the best of our knowledge our approach is the first to translate a broad range of multilingual relations and exploit them to enhance NE translation In particular our approach leverages semantically similar document pairs to exclude incomparable parts that appear in one language only Our method outperforms the previous approaches in translating NE up to 016 in terms of the mean reciprocal rank MRR for organization names Moreover our method shows robustness with 014 higher MRR than seed translations on less comparable corpora 632 2 Related Work This work is related to two research streams NE translation and semantically equivalent relation mining Entity translation Existing approaches on NE translation can be categorized into 1 transliterationbased 2 corpusbased and 3 hybrid approaches Transliterationbased approaches Wan and Verspoor 1998 Knight and Graehl 1998 are the foundations of many decent methods but they alone suffer from ambiguity eg    and  have the same sounds and cannot handle nontransliterated cases such as Jackie Chan   ChengLong  Some methods Lam et al 2007 Yang et al 2009 rely on meanings of con stituent letters or words to handle organization name translation such as  Bank of China    whose translation is derived from  China    and a bank    However many names often originate from abbreviation such as WTO  hence we cannot always leverage meanings Corpusbased approaches Kupiec 1993 Lin et al 2008 Jiang et al 2009 exploit highquality bilingual evidence such as parenthetical translation eg    Jackie Chan   Lin et al 2008 semistructural patterns Jiang et al 2009 and parallel corpus Kupiec 1993 However the coverage of the corpusbased approaches is limited to popular entities with such bilingual evidences On the other hand our method can cover entities with monolingual occurrences in corpora which significantly improves the coverage The most effective known approach is a holis tic framework that combines those two ap proaches You et al 2012 You et al 2010 Kim et al 2011 You et al 2010 2012 leverage two graphs of entities in each language that are generated from a pair of corpora with edge weights quantified as the strength of the relatedness of en tities Then two graphs are iteratively aligned using the common neighbors of two entities Kim et al 2011 build such graphs using the context similarity measured with a bag of words approach of entities in news corpora to translate NEs How ever these approaches assume the symmetry of the two graphs This assumption holds if two corpora are parallel but such resources are scarce But our approach exploits comparable parts from corpora 0"], "abstract": ["English Chinese This paper studies the problem of mining World Trade Organization  ShiJieMaoYiZuZhi named entity translations from comparable corpora with some asymmetry", "Unlike the previous approaches relying on the symmetry found in parallel corpora the proposed method is tolerant to asymmetry often found in comparable corpora by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora", "Our experimental results on EnglishChinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 016 for organization names and 014 in a low comparability case"], "inroduction": ["Identifying and understanding entities is a crucial step in understanding text", "This task is more challenging in the presence of multilingual text because translating named entities NEs such as persons locations or organizations is a nontrivial task", "Early research on NE translation used phonetic similarities for example to mine the translation Mandelson   ManDeErSen with similar sounds", "However not all NE translations are based on transliterations as shown in Table 1Some translations especially the names of most organizations are based on semantic equivalences", "Furthermore names can be abbreviated in one or both lan guages eg the World Trade Organization   can be called the WTO  ", "Another challenging example is that a translation can be arbitrary eg Jackie Chan    ChengLong", "There are many approaches WTO  ShiMaoZuZhi Jackie Chan  ChengLong Table 1 Examples of nonphonetic translations", "that deal with some of these challenges Lam et al 2007 Yang et al 2009 eg by combining phonetic similarity and a dictionary", "However arbitrary translations still cannot be handled by examining the NE pair itself", "Corpusbased approaches Kupiec 1993 Feng 2004 by mining external signals from a large corpus such as parenthetical translation  Jackie Chan com plement the problem of transliterationbased approaches but the coverage of this approach is limited to popular entities with such evidence", "The most effective known approach to NE translation has been a holistic framework You et al 2010 Kim et al 2011 You et al 2012 combining transliteration and corpusbased methods", "In these approaches both 1 arbitrary translations and 2 lesserknown entities can be handled by propagating the translation scores of known entities to lesserknown entities if they cooccur frequently in both corpora", "For example a lesser known entity Tom Watson can be translated if Mandelson and Tom Watson cooccur frequently in an English corpus and their Chinese translations also cooccur frequently in a Chinese corpus ie if the cooccurrences in the two corpora are symmetric", "A research question we ask in this paper is What if comparable corpora are not comparable enough to support this symmetry assumption", "We found that this is indeed the case", "For example even English and Chinese news from the same publisher may have different focus the Chinese version focuses more on Chinese Olympic 631 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 631640 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics teams and Chinese local news", "In the presence of such asymmetry all previous approaches building upon symmetry quickly deteriorate by propagating false positives", "For example cooccurrence of Mandelson and Tom Watson may not appear in a Chinese corpus which may lead to the translation of Tom Watson into another Chinese entity Gordon Brown which happens to cooccur with the Chinese translation of Mandelson", "Our key contribution is to avoid such false propagation by discerning the semantics of relations", "For example relations between Mandelson and Tom Watson should be semantically differ proaches", "More formally we enable selective propagation of seed translations on weakly comparable corpora by 1 clarifying the detailed meaning of relational information of cooccurring entities and 2 identifying the contexts of the relational information using statementlevel context comparison", "In other words we propagate the translation score of a known translation pair to a neighbor pair if the semantics of their relations in English and Chinese corpora are equivalent to accurately propagate the scores", "For example if we know Russia1 and join2 then from a pair of state ent from Chinese relations between  ments Russia joins the WTO  and  Gordon Brown and  Mandelson", "A 1 2 3 1 naive approach would be finding documents with a similar topic such as politics and scientific discovery and allowing propagation only when the topic agrees", "However we found that a topic is a unit that is too coarse for this task because most articles on Mandelson will invariably fall into the same topic1", "In clear contrast we selectively propagate seed translations only when the relations in the two corpora share the same semantics", "This selective propagation can be especially effective for translating challenging types of entities such as organizations including the WTO used with and without abbreviation in both languages", "Applying a holistic approach You et al 2012 on organizations leads to poor results 006 in terms of the F1score", "A naive approach to increase the precision would be to consider multi type cooccurrences hoping that highly precise translations of some type eg persons with an F1score of 069 You et al 2012 can be propagated to boost the precision on organizations", "In our experiments this naive multitype propagation still leads to an unsatisfactory F1score of 012", "Such a low score can be explained by the following example", "When translating WTO using the cooccurrence with Mandelson other cooccurrences such as London Mandelson and EU Mandelson produce a lot of noise because the right translation of WTO does not share much phoneticsemantic similarity", "Our understanding of relation semantics can distinguish Mandelson was born in London from Mandelson visited the WTO to stop false propagations which generates an F1score 025 higher than the existing ap 1 The MRR for organization names achieved by a topic modelbased approach was 015 lower than our best2 3 we can propagate the trans lation score of Russia 1 to WTO  3", "However we do not exploit a pair of statements Russia joined the WTO and  2  because 2 does not mean join2", "Furthermore we mine a similar EnglishChinese document pair that can be found by comparing the entity relationships such as Mandel son visited Moscow and Mandelson met Alexei Kudrin within the English document and the Chinese document to leverage similar contexts to assure that we use symmetric parts", "For this goal we first extract relations among entities in documents such as visit and join and mine semantically equivalent relations across the languages eg English and Chinese such as join", "Once these relation translations are mined similar document pairs can be identified by comparing each constituent relationship among entities using their relations", "Knowing document similarity improves NE translation and improved NE translation can boost the accuracy of document and relationship similarity", "This iterative process can continue until convergence", "To the best of our knowledge our approach is the first to translate a broad range of multilingual relations and exploit them to enhance NE translation", "In particular our approach leverages semantically similar document pairs to exclude incomparable parts that appear in one language only", "Our method outperforms the previous approaches in translating NE up to 016 in terms of the mean reciprocal rank MRR for organization names", "Moreover our method shows robustness with 014 higher MRR than seed translations on less comparable corpora"]}, "P13-1107": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 10831093 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics Resolving Entity Morphs in Censored Data Hongzhao Huang1 Zhen Wen2 Dian Yu1 Heng Ji1 Yizhou Sun3 Jiawei Han4 He Li5 1Computer Science Department and Linguistics Department Queens College and Graduate Center City University of New York New York NY USA 2IBM T J Watson Research Center Hawthorne NY USA 3College of Computer and Information Science Northeastern University Boston MA USA 4Computer Science Department Univerisity of Illinois at Urbana Champaign Urbana IL USA 5Admaster Inc China hongzhaohuang1yudiandoris1hengjicuny1 liheact5gmailcom zhenwenusibmcom2 yzsunccsneuedu3 hanjillinoisedu4 Abstract In some societies internet users have to create information morphs eg Peace West King to refer to  Bo Xilai to avoid active censorship or achieve other communication goals In this paper we aim to solve a new problem of resolving en tity morphs to their real targets We ex ploit temporal constraints to collect crosssource comparable corpora relevant to any given morph query and identify target candidates Then we propose various novel similarity measurements including surface features meta path based semantic features and social correlation features and combine them in a learningtorank framework Experimental results on Chinese Sina Weibo data demonstrate that our approach is promising and significantly outperforms baseline methods1 1 Introduction Language constantly evolves to maximize communicative success and expressive power in daily social interactions The proliferation of online social media significantly expedites this evolution as new phrases triggered by social events may be disseminated rapidly in social media To automatically analyze such fast evolving language in social media new computational models are demanded In this paper we focus on one particular language evolution that creates new ways to communicate sensitive subjects because of the existence of internet information censorship We call this 1Some of the resources and open source programs developed in this work are made freely available for research purpose at httpnlpcsqccunyeduMorphingtargz phenomenon information morph For example when Chinese online users talk about the former politician  Bo Xilai they use a morph Peace West King instead a historical figure four hundreds years ago who governed the same region as Bo Morph can be considered as a special case of alias used for hiding true entities in malicious environment Hsiung et al 2005 Pantel 2006 However social network plays an important role in generating morphs Usually morphs are generated by harvesting the collective wisdom of the crowd to achieve certain communication goals Aside from the purpose of avoiding censorship other motivations for using morph include expressing sarcasmirony positivenegative sentiment or making descriptions more vivid toward some entities or events Table 1 presents the wide range of cases that are used to create the morphs We can see that a morph can be either a regular term with new meaning or a newly created term Morph Target Motivation Peace West King Bo Xilai Sensitive Blind Man Chen Guangcheng Sensitive Miracle Brother Wang Yongping Irony Kim Fat Kim Joingil Negative Kimchi Country South Korea Vivid Table 1 Morph Examples and Motivations We believe that successful resolution of morphs is a crucial step for automated understanding of the fast evolving social media language which is important for social media marketing Barwise and Meehan 2010 Another application is to help common users without enough backgroundcultural knowledge to understand internet language for their daily use Furthermore our ap proaches can also be applied for satire or other implicit meaning recognition as well as information extraction Bollegala et al 2011 1083 However morph resolution in social media is challenging due to the following reasons First the sensitive real targets that exist in the same data source under active censorship are often au tomatically filtered Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo For example the target  Chen Guangcheng only appears once in Weibo Thus the cooccurrence of a morph and its target is quite low in the vast amount of information in social media Second most morphs were not created based on pronunciations spellings or other encryptions of their original targets Instead they were created according to semantically related entities in historical and cultural narratives eg  Peace West King as morph of  Bo Xilai  and thus very difficult to capture based on typical lexical features Third tweets from TwitterChinese Weibo are short only up to 140 characters and noisy resulting in difficult ex "], "abstract": ["In some societies internet users have to create information morphs eg Peace West King to refer to Bo Xilai to avoid active censorship or achieve other communication goals", "In this paper we aim to solve a new problem of resolving entity morphs to their real targets", "We exploit temporal constraints to collect cross source comparable corpora relevant to any given morph query and identify target candidates", "Then we propose various novel similarity measurements including surface features metapath based semantic features and social correlation features and combine them in a learningtorank framework", "Experimental results on Chinese Sina Weibo data demonstrate that our approach is promising and significantly outperforms baseline methods1"], "inroduction": ["Language constantly evolves to maximize communicative success and expressive power in daily social interactions", "The proliferation of online social media significantly expedites this evolution as new phrases triggered by social events may be disseminated rapidly in social media", "To automatically analyze such fast evolving language in social media new computational models are demanded", "In this paper we focus on one particular language evolution that creates new ways to communicate sensitive subjects because of the existence of internet information censorship", "We call this 1 Some of the resources and open source programs developed in this work are made freely available for research purpose at httpnlpcsqccunyeduMorphingtargz phenomenon information morph", "For example when Chinese online users talk about the former politician Bo Xilai they use a morph Peace West King instead a historical figure four hundreds years ago who governed the same region as Bo", "Morph can be considered as a special case of alias used for hiding true entities in malicious environment Hsiung et al 2005 Pantel 2006", "However social network plays an important role in generating morphs", "Usually morphs are generated by harvesting the collective wisdom of the crowd to achieve certain communication goals", "Aside from the purpose of avoiding censorship other motivations for using morph include expressing sarcasmirony positivenegative sentiment or making descriptions more vivid toward some entities or events", "Table 1 presents the wide range of cases that are used to create the morphs", "We can see that a morph can be either a regular term with new meaning or a newly created term", "Morph Target Motivation Peace West King Bo Xilai Sensitive Blind Man Chen Guangcheng Sensitive Miracle Brother Wang Yongping Irony Kim Fat Kim Joingil Negative Kimchi Country South Korea Vivid Table 1 Morph Examples and Motivations", "We believe that successful resolution of morphs is a crucial step for automated understanding of the fast evolving social media language which is important for social media marketing Bar wise and Meehan 2010", "Another application is to help common users without enough backgroundcultural knowledge to understand internet language for their daily use", "Furthermore our approaches can also be applied for satire or other implicit meaning recognition as well as information extraction Bollegala et al 2011", "1083 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 10831093 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics However morph resolution in social media is challenging due to the following reasons", "First the sensitive real targets that exist in the same data source under active censorship are often automatically filtered", "Table 2 presents the distributions of some examples of morphs and their targets in English Twitter and Chinese Sina Weibo", "For example the target Chen Guangcheng only appears once in Weibo", "Thus the cooccurrence of a morph and its target is quite low in the vast amount of information in social media", "Second most morphs were not created based on pronunciations spellings or other encryptions of their original targets", "Instead they were created according to semantically related entities in historical and cultural narratives eg Peace West King as morph of Bo Xilai and thus very difficult to capture based on typical lexical features", "Third tweets from TwitterChinese Weibo are short only up to the similarity measures to generate global semantic features", " We model social user behaviors and use social correlation to assist in measuring semantic similarities because the users who posted a morph and its corresponding target tend to share similar interests and opinions", "Our experiments demonstrate that the proposed approach significantly outperforms traditional alias detection methods Hsiung et al 2005"]}, "P13-1110": {"title": ["Online Relative Margin Maximization for Statistical Machine Translation"], "abstract": ["Recent advances in largemargin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization such as the spread of the data", "However these solutions are impractical in complex structured prediction problems such as statistical machine translation", "We present an online gradientbased algorithm for relative margin maximization which bounds the spread of the projected data while maximizing the margin", "We evaluate our optimizer on ChineseEnglish and ArabicEnglish translation tasks each with small and large feature sets and show that our learner is able to achieve significant improvements of 122 BLEU and 1743 TER on average over stateoftheart optimizers with the large feature set"], "inroduction": ["The desire to incorporate highdimensional sparse feature representations into statistical machine translation SMT models has driven recent research away from Minimum Error Rate Training MERT Och 2003 and toward other discriminative methods that can optimize more features", "Examples include minimum risk Smith and Eisner 2006 pairwise ranking PRO Hopkins and May 2011 RAMPION Gimpel and Smith 2012 and variations of the margininfused relaxation algorithm MIRA Watanabe et al 2007 Chiang et al 2008 Cherry and Foster 2012", "While the objective function and optimization method vary for each optimizer they can all be broadly described as learning a linear model or parameter vector w which is used to score alternative translation hypotheses", "In every SMT system and in machine learning in general the goal of learning is to find a model that generalizes well ie one that will yield good translations for previously unseen sentences", "However as the dimension of the feature space increases generalization becomes increasingly difficult", "Since only a small portion of all sparse features may be observed in a relatively small fixed set of instances during tuning we are prone to overfit the training data", "An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bi text Tillmann and Zhang 2006 Blunsom et al 2008 Simianer et al 2012 which is usually substantially larger than the tuning set but this is complementary to our goal here of better generalization given a fixed size tuning set", "In order to achieve that goal we need to carefully choose what objective to optimize and how to perform parameter estimation of w for this objective", "We focus on largemargin methods such as SVM Joachims 1998 and passiveaggressive algorithms such as MIRA", "Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one", "This criterion performs well in practice at finding a linear separator in highdimensional feature spaces Tsochantaridis et al 2004 Crammer et al 2006", "Now recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information as in the Second Order Perceptron CesaBianchi et al 2005 Gaussian Margin Machines Crammer et al 2009b confidence weighted learning Dredze and Crammer 2008 AROW Crammer et al 2009a Chiang 2012 and Relative Margin Machines RMM Shivaswamy and Jebara 2009b", "The latter RMM was introduced as an effective and less computationally expensive way to incorporate the spread of the data  second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 11161126 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w Unfortunately not all advances in machine learning are easy to apply to structured prediction problems such as SMT the latter often involve latent variables and surrogate references resulting in loss functions that have not been well explored in machine learning Mcallester and Keshet 2011 Gimpel and Smith 2012", "Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction Shivaswamy and Jebara even where previously MERT was shown to be advantageous 5", "Finally we discuss the spread and other key issues of RM 6 and conclude with discussion of future work 7"]}, "P13-1121": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 12331242"], "abstract": ["In automatic summarization centrality is the notion that a summary should contain the core parts of the source text", "Current systems use centrality along with redundancy avoidance and some sentence compression to produce mostly extractive summaries", "In this paper we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text", "We conduct a series of studies comparing humanwritten model summaries to system summaries at the semantic level of caseframes", "We show that model summaries 1 are more abstractive and make use of more sentence aggregation 2 do not contain as many topical caseframes as system summaries and 3 cannot be reconstructed solely from the source text but can be if texts from indomain documents are added", "These results suggest that substantial improvements are unlikely to result from better optimizing centralitybased criteria but rather more domain knowledge is needed"], "inroduction": ["In automatic summarization centrality has been one of the guiding principles for content selection in extractive systems", "We define centrality to be the idea that a summary should contain the parts of the source text that are most similar or representative of the source text", "This is most transparently illustrated by the Maximal Marginal Relevance MMR system of Carbonell and Goldstein 1998 which defines the summarization objective to be a linear combination of a centrality term and a nonredundancy term", "Since MMR much progress has been made on more sophisticated methods of measuring central ity and integrating it with nonredundancy See Nenkova and McKeown 2011 for a recent survey", "For example term weighting methods such as the signature term method of Lin and Hovy 2000 pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus", "This method is a core component of the most successful sum marization methods Conroy et al 2006", "While extractive methods based on centrality have thus achieved success there has long been recognition that abstractive methods are ultimately more desirable", "One line of work is in text simplification and sentence fusion which focus on the ability of abstraction to achieve a higher compression ratio Knight and Marcu 2000 Barzilay and McKeown 2005", "A less examined issue is that of aggregation and information synthesis", "A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user", "For example a series of related events can be aggregated and expressed as a trend", "The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference", "Instead summarization systems need to make more use of domain knowledge", "We provide evidence for this in a series of studies on the TAC 2010 guided summarization data set that examines how the behaviour of automatic summarizers can or cannot be distinguished from human summarizers", "First we confirm that abstraction is a desirable goal and 1233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 12331242 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics provide a quantitative measure of the degree of sentence aggregation in a summarization system", "Second we show that centralitybased measures are unlikely to lead to substantial progress towards abstractive summarization because current top performing systems already produce summaries that are more central than humans do", "Third we consider how domain knowledge may be useful as a resource for an abstractive system by showing that key parts of model summaries can be reconstructed from the source plus related indomain documents", "Our contributions are novel in the following respects", "First our analyses are performed at the level of caseframes rather at the level of words or syntactic dependencies as in previous work", "Case frames are shallow approximations of semantic roles which are well suited to characterizing a domain by its slots", "Furthermore we take a developmental rather than evaluative perspectiveour goal is not to develop a new evaluation measure as defined by correlation with human responsiveness judgments", "Instead our studies reveal useful criteria with which to distinguish humanwritten and system summaries helping to guide the development of future summarization systems"]}, "P13-1131": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 13311340 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics A Two Level Model for Context Sensitive Inference Rules Oren Melamud  Jonathan Berant  Ido Dagan  Jacob Goldberger Idan Szpektor  Computer Science Department Bar Ilan University  Computer Science Department Stanford University  Faculty of Engineering Bar Ilan University  Yahoo Research Israel melamuodagangoldbejcscsengbiuacil joberantstanfordedu idanyahooinccom Abstract Automatic acquisition of inference rules for predicates has been commonly ad dressed by computing distributional similarity between vectors of argument words operating at the word space level A recent line of work which addresses context sensitivity of rules represented contexts in a latent topic space and computed similarity over topic vectors We propose a novel twolevel model which computes similarities between wordlevel vectors that are biased by topiclevel context representations Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior wordlevel and topiclevel models We also release a first context sensitive inference rule set 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications such as Question Answering QA Ravichandran and Hovy 2002 and Information Extraction IE Shinyama and Sekine 2006 For example the inference rule X treat Y  X relieve Y can be useful to extract pairs of drugs and the illnesses which they relieve or to answer a question like  Which drugs relieve headache Along this vein such inference rules constitute a crucial component in generic modeling of textual inference under the Textual Entailment paradigm Dagan et al 2006 Dinu and Wang 2009 Motivated by these needs substantial research was devoted to automatic learning of inference rules from corpora mostly in an unsupervised distributional setting This research line was mainly initiated by the highlycited DIRT algorithm Lin and Pantel 2001 which learns inference for binary predicates with two argument slots like the rule in the example above DIRT represents a predicate by two vectors one for each of the argument slots where the vector entries correspond to the argument words that occurred with the predicate in the corpus Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors This general scheme was further enhanced in several directions eg directional sim ilarity Bhagat et al 2007 Szpektor and Dagan 2008 and meta classification over similarity values Berant et al 2011 Consequently several knowledge resources of inference rules were released containing the top scoring rules for each predicate Schoenmackers et al 2010 Berant et al 2011 Nakashole et al 2012 The above mentioned methods provide a single confidence score for each rule which is based on the obtained degree of argumentvector similarities Thus a system that applies an inference rule to a text may estimate the validity of the rule application based on the prespecified rule score However the validity of an inference rule may depend on the context in which it is applied such as the context specified by the given predicate s arguments For example  ATT acquire TMobile  ATT purchase TMobile  is a valid application of the rule  X acquire Y  X purchase Y while Children acquire skills  Children purchase skills is not To address this issue a line of works emerged which computes a contextsensitive reliability score for each rule application based on the given context The major trend in contextsensitive inference models utilizes latent or classbased methods for context modeling Pantel et al 2007 Szpektor et al 2008 Ritter et al 2010 Dinu and Lapata 2010b In particular the more recent methods Ritter et al 2010 Dinu and Lapata 2010b modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 cation LDA model Then similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context yielding a context sensitive score for each particular rule application We notice at this point that while contextinsensitive methods represent predicates by argument vectors in the original finegrained word space contextsensitive methods represent them as vectors at the level of latent topics This raises the question of whether such coarsegrained topic vectors might be less informative in determining the semantic similarity between the two predicates To address this hypothesized caveat of prior contextsensitive rule scoring methods we propose a novel generic scheme that integrates wordlevel and topiclevel representations Our scheme can be applied on top of any context insensitive base similarity measure for rule learning which operates at the word level such as Cosine or Lin Lin 1998 Rather than computing a single contextinsensitive rule score we compute a distinct wordlevel similarity score for each topic in an LDA model Then when applying a rule in a given context these different scores are weighed together based on the specific topic distribution under the given context This way we calculate similarity over vectors in the original word space while biasing them towards the given context via a topic model In order to promote replicability and equalterm comparison with our results we based our experiments on publicly available datasets both for unsupervised learning of the evaluated models and for testing them over a random sample of rule applications We apply our twolevel scheme over three stateoftheart context insensitive similarity measures The evaluation compares perfor mances both with the original contextinsensitive measures and with recent LDAbased contextsensitive methods showing consistent and robust advantages of our scheme Finally we release a contextsensitive rule resource comprising over 2000 frequent verbs and one million rules 2 Background and Model Setting This section presents components of prior work which are included in our model and experiments setting the technical preliminaries for the rest of the paper We first present contextinsensitive rule learning based on distributional similarity at the word level and then context sensitive scoring for rule applications based on topiclevel similarity Some further discussion of related work appears in Section 6"], "abstract": ["Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words operating at the word space level", "A recent line of work which addresses context sensitivity of rules represented contexts in a latent topic space and computed similarity over topic vectors", "We propose a novel twolevel model which computes similarities between wordlevel vectors that are biased by topiclevel context representations", "Evaluations on a naturally distributed dataset show that our model significantly outperforms prior wordlevel and topiclevel models", "We also release a first contextsensitive inference rule set"], "inroduction": ["Inference rules for predicates have been identified as an important component in semantic applications such as Question Answering QA Ravichandran and Hovy 2002 and Information Extraction IE Shinyama and Sekine 2006", "For example the inference rule X treat Y  X relieve Y  can be useful to extract pairs of drugs and the illnesses which they relieve or to answer a question like Which drugs relieve headache", "Along this vein such inference rules constitute a crucial component in generic modeling of textual inference under the Textual Entailment paradigm Dagan et al 2006 Dinu and Wang 2009", "Motivated by these needs substantial research was devoted to automatic learning of inference rules from corpora mostly in an unsupervised dis tributional setting", "This research line was mainly initiated by the highlycited DIRT algorithm Lin and Pantel 2001 which learns inference for binary predicates with two argument slots like the rule in the example above", "DIRT represents a predicate by two vectors one for each of the argument slots where the vector entries correspond to the argument words that occurred with the predicate in the corpus", "Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors", "This general scheme was further enhanced in several directions eg directional similarity Bhagat et al 2007 Szpektor and Dagan 2008 and metaclassification over similarity values Berant et al 2011", "Consequently several knowledge resources of inference rules were released containing the top scoring rules for each predicate Schoenmackers et al 2010 Berant et al 2011 Nakashole et al 2012", "The above mentioned methods provide a single confidence score for each rule which is based on the obtained degree of argumentvector similarities", "Thus a system that applies an inference rule to a text may estimate the validity of the rule application based on the prespecified rule score", "However the validity of an inference rule may depend on the context in which it is applied such as the context specified by the given predicates arguments", "For example ATT acquire T Mobile  ATT purchase TMobile is a validapplication of the rule X acquire Y  X purchase Y  while Children acquire skills  Chil dren purchase skills is not", "To address this issue a line of works emerged which computes a context sensitive reliability score for each rule application based on the given context", "The major trend in contextsensitive inference models utilizes latent or classbased methods for context modeling Pantel et al 2007 Szpektor et al 2008 Ritter et al 2010 Dinu and Lapata 2010b", "In particular the more recent methods Ritter et al 2010 Dinu and Lapata 2010b modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo 1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 13311340 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics cation LDA model", "Then similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context yielding a contextsensitive score for each particular rule application", "We notice at this point that while context insensitive methods represent predicates by argument vectors in the original finegrained word space contextsensitive methods represent them as vectors at the level of latent topics", "This raises the question of whether such coarsegrained topic vectors might be less informative in determining the semantic similarity between the two predicates", "To address this hypothesized caveat of prior contextsensitive rule scoring methods we propose a novel generic scheme that integrates word level and topiclevel representations", "Our scheme can be applied on top of any contextinsensitive base similarity measure for rule learning which operates at the word level such as Cosine or Lin Lin 1998", "Rather than computing a single contextinsensitive rule score we compute a distinct wordlevel similarity score for each topic in an LDA model", "Then when applying a rule in a given context these different scores are weighed together based on the specific topic distribution under the given context", "This way we calculate learning based on distributional similarity at the word level and then contextsensitive scoring for rule applications based on topiclevel similarity", "Some further discussion of related work appears in Section 6", "21 Contextinsensitive Rule Learning", "A predicate inference rule LHS  RHS such as X acquire Y  X purchase Y specifies a directional inference relation between two predicates", "Each rule side consists of a lexical predicate and two variable slots for its arguments1 Different representations have been used to specify predicates and their argument slots such as word lemma sequences regular expressions and dependency parse fragments", "A rule can be applied when its LHS matches a predicate with a pair of arguments in a text allowing us to infer its RHS with the corresponding instantiations for the argument variables", "For example given the text ATT acquires TMobile the above rule infers ATT purchases TMobile", "The DIRT algorithm Lin and Pantel 2001 follows the distributional similarity paradigm to learn predicate inference rules", "For each predicate DIRT represents each of its argument slots by an argument vector", "We denote the two vectors of the X and Y slots of a predicate pred by vx ed similarity over vectors in the original word space while biasing them towards the given context via a topic model", "In order to promote replicability and equalterm comparison with our results we based our experiments on publicly available datasets both for unsupervised learning of the evaluated models and for testing them over a random sample of rule applications", "We apply our twolevel scheme over three stateoftheart contextinsensitive similarity measures", "The evaluation compares performances both with the original contextinsensitive measures and with recent LDAbased context sensitive methods showing consistent and robust advantages of our scheme", "Finally we release a contextsensitive rule resource comprising over and vy  respectively", "Each entry of a vector v corresponds to a particular word or term w that instantiated the argument slot in a learning corpus with a value vw  P M I pred w with PMI standing for pointwise mutual information", "To learn inference rules DIRT considers in principle each pair of binary predicates that occurred in the corpus for a candidate ruleLHS  RHS", "Then DIRT computes a reliabil ity score for the rule by combining the measured similarities between the corresponding argument vectors of the two rule sides", "Concretely denoting by l and r the predicates appearing in the two rule sides DIRTs reliability score is defined as follows 2000 frequent verbs and one million rules", "scoreDIRTLHS  RHS  r 1 simvx vx  simvy  vy "]}, "P13-1138": {"title": ["A Statistical NLG Framework for Aggregated Planning and Realization"], "abstract": ["We present a hybrid natural language generation NLG system that consolidates macro and micro planning and surface realization tasks into one statistical learning process", "Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain", "First we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis", "Each sentence is then organized into semantically similar groups representing a domain specific concept by kmeans clustering", "After this semiautomatic processing human review of cluster assignments a number of corpuslevel statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus", "At generation time a set of input data the collection of semantically organized templates and the model weights are used to select optimal templates", "Our system is evaluated with automatic nonexpert crowdsourced and expert evaluation metrics", "We also introduce a novel automatic metric  syntactic variability  that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents", "The metrics for generated weather and biography texts fall within acceptable ranges", "In sum we argue that our statistical approach to NLG reduces the need for complicated knowledgebased architectures and readily adapts to different domains with reduced development time", "Ravi Kondadadi is now affiliated with Nuance Communications Inc"], "inroduction": ["NLG is the process of generating naturalsounding text from nonlinguistic inputs", "A typical NLG system contains three main components 1 Document Macro Planning  deciding what content should be realized in the output and how it should be structured 2 Sentence Micro planning  generating a detailed sentence specification and selecting appropriate referring expressions and 3 Surface Realization  generating the final text after applying morphological modifications based on syntactic rules see eg Bateman and Zock 2003 Reiter and Dale 2000 and McKeown 1985", "However document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure", "If the document planning stage fails the communicative goal of the generated text will not be met even if the other two stages are perfect", "While most traditional systems simplify development by using a pipelined approach where 13 are executed in a sequence this can result in errors at one stage propagating to successive stages see eg Robin and McKeown 1996", "We propose a hybrid framework that combines 13 by converting data to text in one single process", "Most NLG systems fall into two broad categories knowledgebased and statistical", "Knowledgebased systems heavily depend on having domain expertise to come up with handcrafted rules at each stage of a pipeline", "Although knowledgebased systems can produce high quality text they are 1 very expensive to build involving a lot of discussion with the end users of the system for the document planning stage alone 2 have limited linguistic coverage as it is time consuming to capture linguistic variation and 3 one has to start from scratch for each new domain because the developed components cannot be reused", "1406 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 14061415 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics Statistical systems on the other hand are fairly inexpensive more adaptable and rely on having historical data for the given domain", "Coverage is likely to be high if more historical data is available", "The main disadvantage with statistical systems is that they are more prone to errors and the output text may not be coherent as there are less constraints on the generated text", "Our framework is a hybrid of statistical and templatebased systems", "Many knowledgebased systems use templates to generate text", "A template structure contains gaps that are filled to generate the output", "The idea is to create a lot of templates from the historical data and select the right template based on some constraints", "To the best of our knowledge this is the first hybrid statisticaltemplatebased system that combines all three stages of NLG", "Experiments with different variants of our system for biography and weather subject matter domains demonstrate that our system generates reasonable texts", "Also in addition to the standard metrics used to evaluate NLG systems eg BLEU NIST etc we present a unique text evaluation metric called syntactic variability to measure the linguistic variation of generated texts", "This metric applies to the document collection level and is based on computing the number of unique template sequences among all the generated texts", "A higher number indicates the texts are more variable and natural sounding whereas a lower number shows they are more redundant", "We argue that this metric is useful for evaluating templatebased systems and for any type of text generation for domains where linguistic variability is favored eg the user is expected to go through more than one document in the same session", "The main contributions of this paper are 1 A statistical NLG system that combines document and sentence planning and surface realization into one single process and 2 A new metric  syntactic variability  is proposed to measure the syntactic and morphological variability of the generated texts", "We believe this is the first work to propose an automatic metric to measure linguistic variability of generated texts in NLG", "Section 2 provides an overview of related work on NLG", "We present our main system in Section 3", "The system is evaluated and discussed in Section 4", "Finally we conclude in Section 5 and point out", "future directions of research"]}, "P13-1147": {"title": ["Embedding Semantic Similarity in Tree Kernels for Domain Adaptation"], "abstract": ["Relation Extraction RE is the task of extracting semantic relationships between entities in text", "Recent studies on relation extraction are mostly supervised", "The clear drawback of supervised methods is the need of training data labeled data is expensive to obtain and there is often a mismatch between the training data and the data the system will be applied to", "This is the problem of domain adaptation", "In this paper we propose to combine i term generalization approaches such as word clustering and latent semantic analysis LSA and ii structured kernels to improve the adaptability of relation extractors to new text genresdomains", "The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation"], "inroduction": ["Relation extraction is the task of extracting semantic relationships between entities in text eg to detect an employment relationship between the person Larry Page and the company Google in the following text snippet Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21 2012", "Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve stateoftheart accuracy Zelenko et al 2002 Culotta and Sorensen 2004  The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento Povo Italy during the design of the models experiments and writing of the paper", "Zhang et al 2005 Zhou et al 2005 Zhang et al 2006 Bunescu 2007 Nguyen et al 2009 Chan and Roth 2010 Sun et al 2011", "However the clear drawback of supervised methods is the need of training data which can slow down the delivery of commercial applications in new domains labeled data is expensive to obtain and there is often a mismatch between the training data and the data the system will be applied to", "Approaches that can cope with domain changes are essential", "This is the problem of domain adaptation DA or transfer learning TL", "Technically domain adaptation addresses the problem of learning when the assumption of independent and identically distributed iid samples is violated", "Domain adaptation has been studied extensively during the last couple of years for various NLP tasks eg two shared tasks have been organized on domain adaptation for dependency parsing Nivre et al 2007 Petrov and McDonald 2012", "Results were mixed thus it is still a very active research area", "However to the best of our knowledge there is almost no work on adapting relation extraction RE systems to new domains1 There are some prior studies on the related tasks of multitask transfer learning Xu et al 2008 Jiang 2009 and distant supervision Mintz et al 2009 which are clearly related but different the former is the problem of how to transfer knowledge from old to new relation types while distant supervision tries to learn new relations from unlabeled text by exploiting weaksupervision in the form of a knowledge resource eg Freebase", "We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project but it is not clear what data was used", "httptinyurlcom bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 14981507 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics data distribution", "Weak supervision is a promising approach to improve a relation extraction system especially to increase its coverage in terms of types of relations covered", "In this paper we examine the related issue of changes in the underlying data distribution while keeping the relations fixed", "Even a weakly supervised system is expected to perform well when applied to any kind of text other domaingenre thus ideally we believe that combining domain adaptation with weak supervision is the way to go in the future", "This study is a first step towards this", "We focus on unsupervised domain adaptation ie no labeled target data", "Moreover we consider a particular domain adaptation setting single system DA ie learning a single system able to cope with different but related domains", "Most studies on DA so far have focused on building a specialized system for every specific target domain eg Blitzer et al", "2006", "In contrast the goal here is to build a single system that can robustly handle several domains which is in line with the setup of the recent shared task on parsing the web Petrov and McDonald 2012", "Participants were asked to build a single system that can robustly parse all domains reviews weblogs answers emails newsgroups rather than to build several domainspecific systems", "We consider this as a shift in what was considered domain adaptation in the past adapt from source to a specific target and what can be considered a somewhat different recent view of DA that became widespread since 20112012", "The latter assumes that the target domains isare not really known in advance", "In this setup the domain adaptation problem boils US are terms indicating an employment relation between a person and a location", "Rather than only matching the surface string of words lexical similarity enables soft matches between similar words in convolution tree kernels", "In the empirical evaluation on Automatic Content Extraction ACE data we evaluate the impact of convolution tree kernels embedding lexical semantic similarities", "The latter is derived in two ways with a Brown word clustering Brown et al 1992 and b Latent Semantic Analysis LSA", "We first show that our system aligns well with the state of the art on the ACE 2004 benchmark", "Then we test our RE system on the ACE 2005 data which exploits kernels structures and similarities for domain adaptation", "The results show that combining the huge space of tree fragments generalized at the lexical level provides an effective model for adapting RE systems to new domains"]}, "P13-1150": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 15271536 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics Unsupervised ConsonantVowel Prediction over Hundreds of Languages YoungBum Kim and Benjamin Snyder University of WisconsinMadison ybkimbsnydercswiscedu Abstract In this paper we present a solution to one aspect of the decipherment task the prediction of consonants and vowels for an unknown language and alphabet Adopting a classical Bayesian perspective we performs posterior inference over hundreds of languages leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters We achieve average accuracy in the unsupervised consonantvowel prediction task of 99 across 503 languages We further show that our methodology can be used to predict more finegrained phonetic distinctions On a threeway classification task between vowels nasals and nonnasal consonants our model yields unsupervised accuracy of 89 across the same set of languages 1 Introduction Over the past centuries dozens of lost languages have been deciphered through the painstaking work of scholars often after decades of slow progress and dead ends However several important writing systems and languages remain unde ciphered to this day In this paper we present a successful solution to one aspect of the decipherment puzzle automatically identifying basic phonetic properties of letters in an unknown alphabetic writing system Our key idea is to use knowledge of the phonetic regularities encoded in known language vocabularies to automatically build a universal probabilistic model to successfully decode new languages Our approach adopts a classical Bayesian perspective We assume that each language has an unobserved set of parameters explaining its observed vocabulary We further assume that each languagespecific set of parameters was itself drawn from an unobserved common prior shared across a cluster of typologically related languages In turn each cluster derives its parameters from a universal prior common to all language groups This approach allows us to mix together data from languages with various levels of observations and perform joint posterior inference over unobserved variables of interest At the bottom layer see Figure 1 our model assumes a languagespecific data generat ing HMM over words in the language vocabulary Each word is modeled as an emitted sequence of characters depending on a corresponding Markov sequence of phonetic tags Since individual letters are highly constrained in their range of phonetic values we make the assumption of one tagperobservationtype eg a single letter is constrained to be always a consonant or always a vowel across all words in a language Going one layer up we posit that the languagespecific HMM parameters are themselves drawn from informative nonsymmetric distributions representing a typologically coherent language grouping By applying the model to a mix of languages with observed and unobserved phonetic sequences the clusterlevel distributions can be inferred and help guide prediction for unknown languages and alphabets We apply this approach to two small decipher ment tasks"], "abstract": ["In this paper we present a solution to one aspect of the decipherment task the prediction of consonants and vowels for an unknown language and alphabet", "Adopting a classical Bayesian perspective we performs posterior inference over hundreds of languages leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typo logically coherent language clusters", "We achieve average accuracy in the unsupervised consonantvowel prediction task of 99 across 503 languages", "We further show that our methodology can be used to predict more finegrained phonetic distinctions", "On a threeway classification task between vowels nasals and non nasal consonants our model yields unsupervised accuracy of 89 across the same set of languages"], "inroduction": ["Over the past centuries dozens of lost languages have been deciphered through the painstaking work of scholars often after decades of slow progress and dead ends", "However several important writing systems and languages remain unde ciphered to this day", "In this paper we present a successful solution to one aspect of the decipherment puzzle automatically identifying basic phonetic properties of letters in an unknown alphabetic writing system", "Our key idea is to use knowledge of the phonetic regularities encoded in known language vocabularies to automatically build a universal probabilistic model to successfully decode new languages", "Our approach adopts a classical Bayesian perspective", "We assume that each language has an unobserved set of parameters explaining its observed vocabulary", "We further assume that each languagespecific set of parameters was itself drawn from an unobserved common prior shared across a cluster of typologically related languages", "In turn each cluster derives its parameters from a universal prior common to all language groups", "This approach allows us to mix together data from languages with various levels of observations and perform joint posterior inference over unobserved variables of interest", "At the bottom layer see Figure 1 our model assumes a languagespecific data generating HMM over words in the language vocabulary", "Each word is modeled as an emitted sequence of characters depending on a corresponding Markov sequence of phonetic tags", "Since individual letters are highly constrained in their range of phonetic values we make the assumption of onetagper observationtype eg a single letter is constrained to be always a consonant or always a vowel across all words in a language", "Going one layer up we posit that the language specific HMM parameters are themselves drawn from informative nonsymmetric distributions representing a typologically coherent language grouping", "By applying the model to a mix of languages with observed and unobserved phonetic sequences the clusterlevel distributions can be inferred and help guide prediction for unknown languages and alphabets", "We apply this approach to two small decipher ment tasks 1", "predicting whether individual characters in an unknown alphabet and language represent vowels or consonants and 2", "predicting whether individual characters in an unknown alphabet and language represent vowels nasals or nonnasal consonants", "For both tasks our approach yields considerable 1527 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 15271536 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics success", "We experiment with a data set consisting of vocabularies of 503 languages from around the world written in a mix of Latin Cyrillic and Greek alphabets", "In turn for each language we consider it and its alphabet unobserved  we hide the graphic and phonetic properties of the symbols  while treating the vocabularies of the remaining languages as fully observed with phonetic tags on each of the letters", "On average over these 503 leaveonelanguage out scenarios our model predicts consonantvowel distinctions with 99 accuracy", "In the more challenging task of vowelnasalnonnasal prediction our model achieves average accuracy over 89"]}, "P13-2002": {"title": ["Exact Maximum Inference for the Fertility Hidden Markov Model"], "abstract": ["The notion of fertility in word alignment the number of words emitted by a single state is useful but difficult to model", "Initial attempts at modeling fertility used heuristic search methods", "Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation", "Yet in practice we also need the single best alignment which is difficult to find using Gibbs", "Building on recent advances in dual decomposition this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM", "Finding the best alignment appears important as this model leads to a substantial improvement in alignment quality"], "inroduction": ["Wordbased translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs", "These word alignments are a crucial training component in most machine translation systems", "Furthermore they are useful in other NLP applications such as entailment identification", "The simplest models may use lexical information alone", "The seminal Model 1 Brown et al 1993 has proved very powerful performing nearly as well as more complicated models in some phrasal systems Koehn et al 2003", "With minor improvements to initialization Moore 2004 which may be important Toutanova and Galley 2011 it can be quite competitive", "Subsequent IBM models include more detailed information about context", "Models 2 and 3 incorporate a positional model based on the absolute position of the word Models 4 and 5 use a relative position model instead an English word tends to align to a French word that is nearby the French word aligned to the previous English word", "Models 3 4 and 5 all incorporate a notion of fertility the number of French words that align to any English word", "Although these latter models covered a broad range of phenomena estimation techniques and MAP inference were challenging", "The authors originally recommended heuristic procedures based on local search for both", "Such methods work reasonably well but can be computationally inefficient and have few guarantees", "Thus many researchers have switched to the HMM model Vogel et al 1996 and variants with more parameters He 2007", "This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference though the objective function is not concave local maxima are a concern", "Modeling fertility is challenging in the HMM framework as it violates the Markov assumption", "Where the HMM jump model considers only the prior state fertility requires looking across the whole state space", "Therefore the standard forwardbackward and Viterbi algorithms do not apply", "Recent work Zhao and Gildea 2010 described an extension to the HMM with a fertility model using MCMC techniques for parameter estimation", "However they do not have a efficient means of MAP inference which is necessary in many applications such as machine translation", "This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition", "The resulting model leads to substantial improvements in alignment quality", "7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 711 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics"]}, "P13-2015": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 8186"], "abstract": ["Most coreference resolvers rely heavily on string matching syntactic properties and semantic attributes of words but they lack the ability to make decisions based on individual words", "In this paper we explore the benefits of lexicalized features in the setting of domainspecific coreference resolution", "We show that adding lexicalized features to offtheshelf coreference resolvers yields significant performance gains on four domainspecific data sets and with two types of coreference resolution architectures"], "inroduction": ["Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics such as the ACE ACE03 2003 ACE04 2004 ACE05 2005 and OntoNotes Pradhan et al 2007 data sets", "Many NLP applications however involve text analysis for specialized domains such as clinical medicine Gooch and Roudsari 2012 Glinos 2011 legal text analysis BouayadAgha et al 2009 and biological literature BatistaNavarro and Ananiadou 2011 Castan o et al 2002", "Learningbased coreference resolvers can be easily retrained for a specialized domain given annotated training texts for that domain", "However we found that retraining an offtheshelf coreference resolver with domain specific texts showed little benefit", "This surprising result led us to question the nature of the feature sets used by noun phrase NP coreference resolvers", "Nearly all of the features employed by recent systems fall into three categories string match and word overlap syntactic properties eg appositives predicate nominals parse features etc and semantic matching eg gender agreement WordNet similarity named entity classes etc", "Conspicuously absent from most systems are lexical features that allow the classifier to consider the specific words when making a coreference decision", "A few researchers have experimented with lexical features but they achieved mixed results in evaluations on broadcoverage corpora Bengston and Roth 2008 Bjo rkelund and Nugues 2011 Rahman and Ng 2011a", "We hypothesized that lexicalized features can have a more substantial impact in domainspecific settings", "Lexical features can capture domain specific knowledge and subtle semantic distinctions that may be important within a domain", "For example based on the resolutions found in domainspecific training sets our lexicalized features captured the knowledge that tomcat can be coreferent with plane UAW can be coreferent with union and anthrax can be coreferent with diagnosis", "Capturing these types of domainspecific information is often impossible using only generalpurpose resources", "For example WordNet defines tomcat only as an animal does not contain an entry for UAW and categorizes anthrax and diagnosis very differently1 In this paper we evaluate the impact of lexicalized features on 4 domains management succession MUC6 data vehicle launches MUC7 data disease outbreaks ProMed texts and terrorism MUC4 data", "We incorporate lexical ized feature sets into two different coreference architectures Reconcile Stoyanov et al 2010 a pairwise coreference classifier and Sieve Raghunathan et al 2010 a rulebased system", "Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures"]}, "P13-2036": {"title": ["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 201205 Sofia Bulgaria August 49 2013 c  2013 Association for Computational Linguistics Enriching Entity Translation Discovery using Selective Temporality Gaewon You Youngrok Cha Jinhan Kim and Seungwon Hwang Pohang University of Science and Technology Republic of Korea gwyou line0930 wlsgks08 swhwangpostechedu Abstract This paper studies named entity trans lation and proposes  selective temporality as a new feature as using temporal features may be harmful for translating atemporal entities Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 61 1 Introduction Named entity translation discovery aims at mapping entity names for people locations etc in source language into their corresponding names in target language As many new named entities ap pear every day in newspapers and web sites their translations are nontrivial yet essential Early efforts of named entity translation have focused on using phonetic feature called PH to estimate a phonetic similarity between two names Knight and Graehl 1998 Li et al 2004 Virga and Khudanpur 2003 In contrast some approaches have focused on using context feature called CX which compares surrounding words of entities Fung and Yee 1998 Diab and Finch 2000 Laroche and Langlais 2010 Recently holistic approaches combining such similarities have been studied Shao and Ng 2004 You et al 2010 Kim et al 2011 Shao and Ng 2004 rank translation candidates using PH and CX independently and return results with the highest average rank You et al 2010 compute initial translation scores using PH and iteratively update the scores using relationship feature called R Kim et al 2011 boost You s approach by additionally leveraging CX More recent approaches consider temporal feature called T of entities in two corpora Klementiev and Roth 2006 Tao et al 2006 Sproat et 0 5 10 15 20 25 30 35 40 45 500 50 100 150 200 250 300 Week Freq uen cy"], "abstract": ["This paper studies named entity translation and proposes selective temporal ity as a new feature as using temporal features may be harmful for translating atemporal entities", "Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 61"], "inroduction": ["300 250 200 150 100 50 0 80 60 40 20 0 English Chinese 0 5 10 15 20 25 30 35 40 45 50 Week a Temporal entity Usain Bolt English Chinese Named entity translation discovery aims at mapping entity names for people locations etc in source language into their corresponding names in target language", "As many new named entities appear every day in newspapers and web sites their translations are nontrivial yet essential", "Early efforts of named entity translation have focused on using phonetic feature called PH to estimate a phonetic similarity between two names Knight and Graehl 1998 Li et al 2004 Virga and Khudanpur 2003", "In contrast some approaches have focused on using context feature called CX which compares surrounding words of entities Fung and Yee 1998 Diab and Finch 2000 Laroche and Langlais 2010", "Recently holistic approaches combining such similarities have been studied Shao and Ng 2004 You et al 2010 Kim et al 2011", "Shao and Ng 2004 rank translation candidates using PH and CX independently and return results with the highest average rank", "You et al 2010 compute initial translation scores using PH and iteratively update the scores using relationship feature called R", "Kim et al 2011 boost Yous approach by additionally leveraging CX", "More recent approaches consider temporal feature called T of entities in two corpora Klementiev and Roth 2006 Tao et al 2006 Sproat et 0 5 10 15 20 25 30 35 40 45 50 Week b Atemporal entity Hillary Clinton Figure 1 Illustration on temporality al 2006 Kim et al 2012", "T is computed using frequency vectors for entities and combined with PH Klementiev and Roth 2006 Tao et al 2006", "Sproat et al 2006 extend Taos approach by iteratively updating overall similarities using R", "Kim et al 2012 holistically combine all the features PH CX T and R However T used in previous approaches is a good feature only if temporal behaviors are symmetric across corpora", "In contrast Figure 1 illustrates asymmetry by showing the frequencies of Usain Bolt a Jamaican sprinter and Hillary Clinton an American politician in comparable news articles during the year 2008", "The former is mostly mentioned in the context of some temporal events eg Beijing Olympics while the latter is not", "In such case as Hillary Clinton is a famous female leader she may be associated with other Chinese female leaders in Chinese corpus while such association is rarely observed in English corpus which causes asymmetry", "That is Hillary Clinton is atemporal as Figure 1b shows such that using such dissimilarity against deciding this pair as a correct translation would be harmful", "In clear contrast for Usain Bolt similarity of temporal dis 201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 201205 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics tributions in Figure 1a is a good feature for concluding this pair as a correct one", "To overcome such problems we propose a new notion of selective temporality called this fea 23 Step 3 Reinforcement", "We reinforce R0 by leveraging R and obtain a converged matrix R using the following model ture ST to distinguish from T to automaticallydistinguish temporal and atemporal entities", "To Rt1 0   1    t uv ward this goal we design a classifier to distinguish ij  Rij 2k uvk Bt ij temporal entities from atemporal entities based on which we align temporal projections of entity graphs for the temporal ones and the entire entity graphs for the atemporal ones", "We also propose a method to identify the optimal window size for temporal entities", "We validate this selective use of temporal features boosts the accuracy by 61"]}, "P13-2122": {"title": ["Incremental TopicBased Translation Model Adaptation for"], "abstract": ["We describe a translation model adaptation approach for conversational spoken language translation CSLT which encourages the use of contextually appropriate translation options from relevant training conversations", "Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations which is used to bias translation choices towards the current context", "A significant novelty of our adaptation technique is its incremental nature we continuously update the topic distribution on the evolving test conversation as new utterances become available", "Thus our approach is wellsuited to the causal constraint of spoken conversations", "On an EnglishtoIraqi CSLT task the proposed approach gives significant improvements over a baseline system as measured by BLEU TER and NIST", "Interestingly the incremental approach outperforms a nonincremental oracle that has upfront knowledge of the whole conversation"], "inroduction": ["Conversational spoken language translation CSLT systems facilitate communication between subjects who do not speak the same language", "Current systems are typically used to achieve a specific task eg vehicle checkpoint search medical diagnosis etc", "These taskdriven Disclaimer This paper is based upon work supported by the DARPA BOLT program", "The views expressed here are those of the authors and do not reflect the official policy or position of the Department of Defense or the US Government", "Distribution Statement A Approved for Public Release Distribution Unlimited conversations typically revolve around a set of central topics which may not be evident at the beginning of the interaction", "As the conversation progresses however the gradual accumulation of contextual information can be used to infer the topics of discussion and to deploy contextually appropriate translation phrase pairs", "For example the word drugs will predominantly translate into Spanish as medicamentos medicines in a medical scenario whereas the translation drogas illegal drugs will predominate in a law enforcement scenario", "Most CSLT systems do not take highlevel global context into account and instead translate each utterance in isolation", "This often results in contextually inappropriate translations and is particularly problematic in conversational speech which usually exhibits short spontaneous and often ambiguous utterances", "In this paper we describe a novel topicbased adaptation technique for phrasebased statistical machine translation SMT of spoken conversations", "We begin by building a monolingual latent Dirichlet allocation LDA topic model on the training conversations each conversation corresponds to a document in the LDA paradigm", "At runtime this model is used to infer a topic distribution over the evolving test conversation up to and including the current utterance", "Translation phrase pairs that originate in training conversations whose topic distribution is similar to that of the current conversation are given preference through a single similarity feature which augments the standard phrasebased SMT loglinear model", "The topic distribution for the test conversation is updated incrementally for each new utterance as the available history grows", "With this approach we demonstrate significant improvements over a baseline phrasebased SMT system as measured by BLEU TER and NIST scores on an EnglishtoIraqi CSLT task", "697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 697701 Sofia Bulgaria August 49 2013", "Qc 2013 Association for Computational Linguistics"]}, "P160500_w06": {"title": ["A Hybrid Method for Entity Hyponymy Acquisition"], "abstract": ["Extracting entity hyponymy in Chinese complex sentences can be a highly difficult process", "This paper proposes a novel hybrid approach that combines parsing with supervised learning and semisupervised learning", "First conditional random fields CRF model is employed to obtain the candidate domain named entity", "Pattern matching is then used to acquire candidate hyponymy", "Next predicate and symbol features syntactic analysis and semantic roles are introduced into the CRF features template to identify the hyponymy entity pairs", "Finally analysis of both the parallel relationship of entities among sentences and entity pairs in simple sentences is conducted to obtain the hyponymy entity pairs in Chinese complex sentences", "The experimental results show that the proposed method reduces the manual work required for CRF markers and has an improved overall performance in comparison with the baseline methods", "Keywords Chinese complex sentences hyponymy entity identification CRF pattern matching syntactic analysis DOI 103103S0146411616050035"], "inroduction": ["The automatic extraction of hyponymy between entities in a specific field is often a difficult step to accomplish when developing ontology knowledge bases or during construction of knowledge graphs which is regarded as a crucial part for developing highlevel natural language applications", "Many algorithms have been developed to achieve hyponymy automatically including dictionarybased pattern based and statistical machine learning", "The dictionarybased method used to acquire hyponymy usually depends on field words synonyms or approximate synonyms in the manmade dictionary", "For instance Nakaya 1 obtained English domain concepts hyponymy with WordNet 2", "Moreover Li 3 et al used the geographical names dictionary for acquisition of toponym ontology concept relations in Chinese", "However the coverage of WordNet or HowNet 4 is still limited in the general field and they are weak in the coverage of domain specific terms and named entities", "Patternbased methods mainly use linguistics and natural language processing technologies which consider patterns as rules summarized according to some inherent regular language", "For example in this Chinese sentence    The scenic spots of Lijiang contains Lijiang old town Yulong jokul and Lugu lake etc here Lijiang Lijiang old town Yulong jokul Lugu lake are entities Ei is used to represent the entity i i  1 2  and makes E1  Lijiang E2  Lijiang old town E3  Yulong jokul E4  Lugu lake the pattern E1 contains E2  E3 and E4 can be obtained where E1 is the hypernym of E2 E3 and E4 in this context", "Several researchers have studied the subject in regards to the English language", "Hearst 5 raised 6 lexicalsyntactic patterns and avoided the use of the precoding method of knowledge and automatic recognition hyponymy from large text corpus Tuan 6 et al put forward to build a term classification hierarchy model based on the combination of pattern matching and statistical methods Bansal 7 et al used an automatic acquisition pattern and a belief propagation algorithm to obtain the high precision hypernym of arbitrary noun phrases", "In addition 1 The article is published in the original", "369 to the English language various studies have been conducted on Chinese", "For example Wu 8 et al established a universal grammar pattern library for the hyponymy relationship according to the partwhole relationship of the six different kinds of forms In view of the ISA pattern Tang 9 et al demonstrated hyponym concepts using syntactic parsing results Tian 10 et al proposed a combination of a bootstrap ping method and a Chinese doubly anchored pattern of the hypernym", "The patternbased method is highly accurate for specific sentences but compared to English Chinese language has the more complicated sentence structure and semantic so it is difficult to use this technique in generalization", "The statistical machine learning method is mainly based on corpus linguistic knowledge and the statistical language model and it uses the machinelearning algorithm to obtain the concept hyponymy relationship", "In the field of English Fan 11 et al considered the task as a matrix factorization problem to extract the relation", "For Chinese Xia 12 et al proposed a method of graph clustering to extract the hypo nymy Fu 13 et al joined a variety of features into SVM support vector machine based on the RBF radial basis function kernel", "Statistical machine learning is considered the main method for hyponymy extraction due to its advantages of reducing manual annotation and extracting information automatically", "The three methods described above generally extract entity hyponymy relations in a simple sentence", "For complex sentences it is necessary to cooperate syntactic patterns with entity recognition as well as other approaches to improve the overall performances especially for the Chinese", "In addition patterns should be extracted using the bootstrapping method", "The current paper proceeds as follows Section 2 describes the methods and the framework Section 3 discusses the process of hyponymy acquisition Section 4 reports the results Section 5 presents the conclusions"]}, "P170300_w06": {"title": ["Tracing Linguistic Relations in Winning  and Losing Sides of Explicit Opposing Groups"], "abstract": ["Linguistic relations in oral conversations present how opinions are constructed and developed in a restricted time", "The relations bond ideas arguments thoughts and feelings re shape them during a speech and finally build knowledge out of all information provided in the conversation", "Speakers share a common interest to discuss", "It is expected that each speakers reply includes duplicated forms of words from previous speakers", "However linguistic adaptation is observed and evolves in a more complex path than just transferring slightly modified versions of common concepts", "A conversation aiming a benefit at the end shows an emergent cooperation inducing the adaptation", "Not only cooperation but also competition drives the adaptation or an opposite scenario and one can capture the dynamic process by tracking how the concepts are linguistically linked", "To uncover salient complex dynamic events in verbal communications we attempt to discover selforganized linguistic relations hidden in a conversation with explicitly stated winners and losers", "We examine open access data of the United States Supreme Court", "Our understanding is crucial in big data research to guide how transition states in opinion mining and decisionmaking should be modeled and how this required knowledge to guide the model should be pinpointed by filtering large amount of data"], "inroduction": ["Traditionally in computational linguistics it is essential to integrate models and algorithms with fundamental laws of language", "Widely applied hierarchical dependency trees and parsing in natural language processing NLP follow existing grammatical relations", "Nowadays while algorithms and models reach higher levels and available data becomes bigger not enough linguistic laws are uncovered and can have a chance to meet with developed techniques", "Language processing in data science mainly considers evaluated data as single source in terms of language", "There are approaches such as crossmedia topic analysis retrieving information referring various data platforms including websites blogs and mobile phones and multimodal analysis Poria et al 2016 Poria et al 2017a Poria et al 2017b combining text data with images videos and audio however they only gather all available channels and do not address the richness of language", "On the other hand language itself has many dimensions language of a text written by a single author is different than language used in a dialogue or that of a group speech eg trialogue discussions", "Therefore it is emergent that current conventional NLP should meet with the revolutionary philosophy of linguistics Chomsky 1975 and establish new hidden laws applicable in data science the human mind easily knows and applies by birth but hardly formulates to understand the underlying structure", "One of the remarkable perspectives to dig into natural linguistic laws is provided by social and behavior sciences adaptation in language during communication as a result of changes in opinions and decisions", "Opinions and decisions are personal in individual level however they are flexible while facing public opinions and decisions", "Linguistic adaptation is twofold", "In one part collective voice unifies opinions and decisions in a complex process ideas are biased and consequently people start acting similarly talking similarly and so writing similarly", "Twitter conversations DanescuNiculescuMizil Gamon and Dumais 2011 Purohit et al 2013 and popular memes Myers and Leskovec 2012 Coscia 2013 prove this similarity in social media", "In the other part when people have a welldefined goal at the end they tend to reshape their arguments", "In the presence of distinct winning and losing sides and social hierarchy people at lower status show both cooperation through that at the higher status and competition among each other", "Therefore a verbal discussion in such explicitly opposing groups host linguistic adaptation investigated in social exchange theory Willer 1999 Thye Willer and Markovsky 2006", "While information and emotions are the fundamental elements of human knowledge commonsense knowledge is the fundamental element for gluing society Cambria et al 2009 Cambria et al 2016", "Commonsense is implicit semantic and affective information humans continuously tap on for decisionmaking communication and reasoning in general Cambria and Hussain 2015 Rajagopal et al 2013 Poria et al 2013 Tran Cambria and Hussain 2016", "Effective speeches and public talks use commonsense efficiently to drive opinions and change decisions in large scales Drath and Palus 1994", "The resultant unified collective motion is extremely interesting in social groups BorgeHolthoefer et al 2011 GonzalezBailon et al 2011", "Opinions and decisions are personal in individual level", "However as observed they are quite flexible facing with a collective decision", "Complex knowledge extraction process in micro state suddenly becomes less valuable and group decision gains Conover et al 2011", "We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge eg commonsense and so richness of opinions kept in each individual is relatively unimportant", "We can further argue that commonsense drives an adaptation in extracting knowledge", "To measure commonsense for a particular situation is hard however adaptations can be easily captured in Twitter conversations DanescuNiculescu Mizil Gamon and Dumais 2011 Purohit et al 2013 in memes Myers and Leskovec 2012 Coscia 2013 and face toface discussions DanescuNiculescuMizil et al 2012", "In this paper our main concerns are firstly to construct discussion groups including agents having different social powers and serving opposite aims", "Secondly we investigate how we can track the progress of opinions together with their influences on decisions in oral conversations", "We claim that linguistic relations Poria et al 2015 preserve all rich phenomena shortly discussed above including collective voice reshaping arguments and so adaptation", "To analyze adaptation induced by both cooperation and competition we consider court conversations they are held in clearly stated winner and loser groups with distinct hierarchy in decision making due to the presence of Justices and lawyers", "To this end we evaluate the open access data of the United States Supreme Court Hawes Lin and Resnik 2009 Hawes 2009 DanescuNiculescuMizil et al 2012 prepare conversation groups with different adaptation levels implement a suitable algorithm to extract linguistic relations in these group conversations and finally provide a comparison between the groups and the discovered linguistic relations", "The rest of the paper is organized as follows the first section presents the dataset we consider and designed conversation groups out of the data the second section describes our algorithm in detail the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic relations finally we provide experimental results and conclude the paper", "Supreme Court Data We borrow the textual data of the conversations in the United States Supreme Court preprocessed by Hawes Lin and Resnik 2009 Hawes 2009 and enriched by DanescuNiculescuMizil et al 2012 including the final votes of Justices", "Both the original data and the most updated version used here are publicly available DanescuNiculescu Mizil et al 2012", "The data gathers oral speeches before the Supreme Court and hosts 50389 conversational exchanges among Justices and lawyers", "Distinct hierarchy between Justices high power and lawyers low power impose lawyers to tune their arguments under the perspective and understandings of Justices and as a result speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs conjunctions and pronouns", "Tracking initial utterances the sides present a unique and personal speaking but after a while in the communication word selections their forms and frequencies mirror each others language preference", "The linguistic coordination is systematically quantified by DanescuNiculescuMizil et al 2012 and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups Willer 1999 Thye Willer and Markovsky 2006 Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech", "Moreover lawyers show even more cooperation to unfavorable Justices than favorable ones", "Here we enrich the comparison including the identity of winners and losers in lawsuits", "The data provides whether the petitioner or the respondent is the winner at the end of each lawsuit", "In addition the speaker of each utterance is labeled as their position eg Justice or lawyer", "Furthermore Justices votes and the side of lawyers are tagged with the utterances", "Table 1 identifies all roles carried by Justices and lawyers", "For Justices both the vote middle and whom to speak last are given", "Lawyers are allowed to speak only when Justices address their side", "ID Roles of Justices J  and Lawyers l 1 J  Vote Petitioner  Speak to Petitioners l"]}, "P22324_w09": {"title": ["Finding Paraphrase Facts"], "abstract": ["We propose a method to acquire paraphrases from the Web in accordance with a given sentence", "For example consider an input sentence Lemon is a high vitamin c fruit", "Its paraphrases are expressions or sentences that convey the same meaning but are dierent syntactically such as Lemons are rich in vitamin c or Lemons contain a lot of vitamin c", "We aim at nding sentencelevel paraphrases from the noisy Web instead of domainspecic corpora", "By observing search results of paraphrases users are able to estimate the likelihood of the sentence as a fact", "We evaluate the proposed method on ve distinct semantic relations", "Experiments show our average precision is 605  compared to TEASE method with average precision of 4415 ", "Besides we can acquire 3 paraphrases more than TEASE method per input", "Keywords Paraphrase acquisition  Coordinate relationship  Web mining  Mutual reinforcement"], "inroduction": ["Nowadays it is intuitive to utilize the Web as a huge encyclopedia and trust information on the Web", "However those information is not always correct or true", "For example it has been reported that information on the Wikipedia which is regarded as the biggest online encyclopedia is not so credible 9", "Therefore it is necessary to understand risks of Web information and distinguish facts from it", "We assume information which is often mentioned by people on the Web is more likely to be correct or true", "Consequently such information is regarded as fact with a high possibility", "On the contrary we assume information which is rarely mentioned by people on the Web is more likely to be incorrect or untrue consequently unlikely to be fact", "Based on the assumption a naive way to estimate the likelihood of a sentence as a fact is to observe its hit count on the Web", "However it always fails since the expression of a userinput sentence may be rarely used on the Web", "Suppose a user wants to know whether lemon is a high vitamin c fruit or not", "He thinks of a sentence like Lemon is a high vitamin c fruit and use it as a query to search on the Web", "Neither Google1 nor 1 httpwwwgooglecom", "Oc Springer International Publishing Switzerland 2015 A Liu et al", "Eds DASFAA 2015 Workshops LNCS 9052 pp", "135151 2015", "DOI 1010079783319223247 12 Bing2 return any matches for this query at the time of writing the document", "However if it is rewritten as Lemons are rich in vitamin c or Lemons contain a lot of vitamin c adequate number of Web pages can be obtained", "Hence the user can infer that lemon is a high vitamin c fruit", "In this paper we aim at nding sentencelevel paraphrases from the noisy Web instead of domainspecic corpora", "By observing search results of paraphrases especially frequentlyused ones users are able to estimate the likelihood of an input sentence as a fact", "Paraphrases are linguistic expressions that restate the same meaning using dierent variations", "In the most extreme case they may not be even similar in wording", "It has been shown that paraphrases are useful in many applications", "For example paraphrases can help detect fragments of text that convey the same meaning across documents and this can improve the precision of multidocument summarization 6 17", "In the eld of machine translation 8 15 16 show that augmenting the training data with paraphrases generated by pivoting through other languages can alleviate the vocabulary coverage problem", "In information extraction 7 10 26 present approaches incorporating paraphrases to extract semantic relations among entities", "In information retrieval paraphrases have been used for query expansion 2 20 25", "A large proportion of previous work extract and generate paraphrases based on parallel corpora 3 5 or comparable corpora 4 21 23", "However there are limitations in using those corpora", "For example the quality of obtained paraphrases strongly depends on the quality of the corpus a highquality corpus can cost a great deal of manpower and time to construct", "Moreover it may be hard to cover all possible genres", "For example 23 uses a corpus consisted of newswire articles written by six dierent news agencies", "Entity tuples that describe or are members of the same relationships may be dened as coordinate tuples to each other", "For example guavasvitamin c and tomatoespotassium are coordinate tuples since there is a highConcentration relation between guavas and vitamin c so is between tomatoes and potassium", "We think it is not easy to nd all variations of paraphrases by just one entity tuple and such variations exist in expressions of its coordinate ones", "For example given the sentence Guavas are rich in vitamin c it might be di cult to nd part of its paraphrases such as Guavas are considered a high vitamin c fruit since it is seldom used by the entity tuple guavasvitamin c", "However such paraphrases can be acquired from the expressions of its coordinate entity tuples ie the former paraphrase can be easily found via tomatoespotassium", "Thus we can capture more paraphrases by mining the expressions of coordinate entity tuples", "The distributional hypothesis attributed to Harris 12 has been the basis for Statistical Semantics", "It states that words that occur in the same contexts tend to have similar meanings", "Moreover its extension that if two phrases or two text units occur in similar contexts then they may be interchangeable has been extensively tested", "Our idea is based on the extended hypothesis if two templates share more common coordinate entity tuples then they may be 2 httpwwwbingcom", "paraphrase templates if two entity tuples share more common paraphrase templates then they may be coordinate entity tuples", "Thus paraphrase templates and coordinate tuples are in a mutually reinforcing relationship and this relationship can be used to nd more paraphrase templates or coordinate tuples", "We assume a sentence is mapped to a template and an entity tuple", "Thus given a sentence query it can be separated into a template and a corresponding entity tuple", "The proposed method rst extracts templates that connect that entity tuple and entity tuples mentioned by that template", "Several lters and limitations are added to eliminate partial inappropriate templates and entity tuples", "A mutually reinforcing approach is proposed to simultaneously identify dierent templates that convey the same meaning with the given template and entity tuples which hold the same relation with the given entity tuple", "Finally paraphrase queries can be generated by substituting the given entity tuple into discovered paraphrase templates", "Our contributions can be summarized as follows", "First we propose a method for detecting sentencelevel paraphrases and our method does not require deep natural language processing such as dependency parsing", "Second paraphrases are not limited to wordlevel or phraselevel", "Given a sentence query our method outputs its paraphrases at the sentence level", "Third instead of using highquality input data restricted to a particular genre our method can employe the Web as its data source", "The remainder of the paper is organized as follows", "In Sect", "2 we discuss some related work", "Section 3 shows some preliminaries and Sect", "4 describes our basic idea", "In Sect", "5 we illustrate the method to acquire paraphrases from the Web by a given sentence", "We evaluate the proposed paraphrase acquisition method using ve semantic relations in Sect", "6", "Finally Sect", "7 concludes the paper and gives an outline of our future work"]}, "P283_w09": {"title": ["DEIM Forum 2015 A33"], "abstract": ["We propose a method to acquire paraphrases from the Web in accordance with a given sentence query", "For example consider the query Guavas are rich in vitamin c", "Its paraphrases are expressions or sentences that convey the same meaning but are dierent syntactically such as Guavas are well known for their high concentration of vitamin c or Guavas are very high in vitamin c", "We aim at improving the poor performance of querying the Web by long queries especially sentence queries since they often result in failure", "By issuing paraphrase queries to the Web users are able to gather more search results about the given sentence query", "Key words paraphrase acquisition coordinate relationship Web mining mutual reinforcement"], "inroduction": ["Nowadays search engines such as Google 1and Bing 2 have become the major gateways to the huge amount of information on the Web since they enable users to obtain useful information by issuing queries based on their information needs", "It has been reported that a large fraction of search engine queries only consist of two or three keywords on average", "One possible reason for such queries could be that long queries especially sentence queries often result in failure since they may not match any Web pages", "Consequently users are enforced to think of a few keywords that are likely to be associated with their information needs", "However it may be dicult to refine a complicated information need into two or three keywords", "There have been studies to narrow the gap between short queries and information needs behind them", "We concentrate on a dierent aspect of the problem where long queries especially sentence queries are treated as well represented users information need in more detail", "A conceivable reason why long queries especially sentence queries always fail in retrieving useful information is that the expressions of such queries may be rarely used on the Web and as a result very few Web pages are returned", "For example users may want to find more information about pectin in apples and think of a sentence query like apples pop a powerful pectin punch", "None of the two aforementioned search engines return any matches for such query at  1httpwwwgooglecom  2httpwwwbingcom the time of writing the document", "However if the query is rewritten as apples contain a lot of pectin or apples are rich in pectin adequate number of Web pages with detailed information can be obtained", "In this paper we aim to improve the poor performance of sentence queries by expanding them with their paraphrases especially frequentlyused ones", "Paraphrases are linguistic expressions that restate the same meaning using dierent variations", "In the most extreme case they may not be even similar in wording", "It has been shown that paraphrases are useful in many applications", "For example paraphrases can help detect fragments of text that convey the same meaning across documents and this can improve the precision of multidocument summarization 6 16", "In the field of machine translation 8 14 15 show that augmenting the training data with paraphrases generated by pivoting through other languages can alleviate the vocabulary coverage problem", "In information extraction 25 9 7 present approaches incorporating paraphrases to extract semantic relations among entities", "In information retrieval paraphrases have been used for query expansion 19 2 24", "A large proportion of previous work extract and generate paraphrases based on parallel corpora 5 3 or comparable corpora 4 20 22", "However there are limitations in using those corpora", "For example the quality of obtained paraphrases strongly depends on the quality of the corpus a highquality corpus can cost a great deal of manpower and time to construct", "Moreover it may be hard to cover all possible genres", "For example 22 uses a corpus consisted of newswire articles written by six dierent news agencies", "Entity tuples that describe or are members of the same relationships may be defined as coordinate tuples to each other", "For example guavasvitamin c and tomatoespotassium are coordinate tuples since there is a high Concentration relation  3 between guavas and vitamin c so is between tomatoes and potassium", "We think it is not easy to find all variations of paraphrases by just one entity tuple and such variations exist in expressions of its coordinate ones", "For example given the sentence Guavas are rich in vitamin c it might be dicult to find part of its paraphrases such as Guavas are considered a high vitamin c fruit since it is seldom used by the entity tuple guavasvitamin c", "However such paraphrases can be acquired from the expressions of its coordinate entity tuples ie the former paraphrase can be easily found via tomatoespotassium", "Thus we can capture more paraphrases by mining the expressions of coordinate entity tuples", "The distributional hypothesis attributed to Harris 11 has been the basis for Statistical Semantics", "It states that words that occur in the same contexts tend to have similar meanings", "Moreover its extension that if two phrases or two text units occur in similar contexts then they may be interchangeable has been extensively tested", "Our idea is based on the extended hypothesis if two templates share more common coordinate entity tuples then they may be paraphrase templates if two entity tuples share more common paraphrase templates then they may be coordinate entity tuples", "Thus paraphrase templates and coordinate tuples are in a mutually reinforcing relationship and this relationship can be used to find more paraphrase templates or coordinate tuples", "We assume a sentence is mapped to a template and an entity tuple", "Thus given a sentence query it can be separated into a template and a corresponding entity tuple", "The proposed method first extracts templates that connect that entity tuple and entity tuples mentioned by that template", "Several filters and limitations are added to eliminate partial inappropriate templates and entity tuples", "A mutually reinforcing approach is proposed to simultaneously identify dierent templates that convey the same meaning with the given template and entity tuples which hold the same relation with the given entity tuple", "Finally paraphrase queries can be generated by substituting the given entity tuple into discovered paraphrase templates", "Our contributions can be summarized as follows", "First we propose a method for detecting sentencelevel paraphrases and our method does not require deep natural language processing such as dependency parsing", "Second paraphrases are  3We define this as a food contains a high amount of a certain nutrient", "not limited to wordlevel or phraselevel", "Given a sentence query our method outputs its paraphrases at the sentence level", "Third instead of using highquality input data restricted to a particular genre our method can employe the Web as its data source", "The remainder of the paper is organized as follows", "In Section 2 we discuss some related work", "Section 3", "shows some preliminaries", "In Section 4 we illustrate the method to acquire paraphrases from the Web by a given sentence query", "We evaluate the proposed paraphrase acquisition method using five semantic relations in Section 5", "Finally Section 6", "concludes the paper and gives an outline of our future work"]}, "P39_p07": {"title": ["Efficient Extraction of Oraclebest Translations from Hypergraphs"], "abstract": ["Hypergraphs are used in several syntax inspired methods of machine translation to compactly encode exponentially many trans lation hypotheses", "The hypotheses closest to given reference translations therefore cannot be found via brute force particularly for pop ular measures of closeness such as BLEU", "We develop a dynamic program for extracting the so called oraclebest hypothesis from a hyper graph by viewing it as the problem of finding the most likely hypothesis under an ngram language model trained from only the refer ence translations", "We further identify and re move massive redundancies in the dynamic program state due to the sparsity of ngrams present in the reference translations resulting in a very efficient program", "We present run time statistics for this program and demon strate successful application of the hypothe ses thus found as the targets for discriminative training of translation system components"], "inroduction": ["A hypergraph as demonstrated by Huang and Chi ang 2007 is a compact datastructure that can en code an exponential number of hypotheses gener ated by a regular phrasebased machine translation MT system eg Koehn et al", "2003 or a syntax based MT system eg Chiang 2007", "While the hypergraph represents a very large set of transla tions it is quite possible that some desired transla tions eg the reference translations are not con tained in the hypergraph due to pruning or inherent deficiency of the translation model", "In this case one is often required to find the translations in the hy pergraph that are most similar to the desired transla tions with similarity computed via some automatic metric such as BLEU Papineni et al 2002", "Such maximally similar translations will be called oracle best translations and the process of extracting them oracle extraction", "Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph and the exponentially large number of hypotheses makes a bruteforce linear search intractable", "Therefore efficient algorithms that can exploit the structure of the hypergraph are required", "We present an efficient oracle extraction algo rithm which involves two key ideas", "Firstly we view the oracle extraction as a bottomup model scoring process on a hypergraph where the model is trained on the reference translations", "This is sim ilar to the algorithm proposed for a lattice by Dreyer et al", "2007", "Their algorithm however requires maintaining a separate dynamic programming state for each distinguished sequence of state words and the number of such sequences can be huge mak ing the search very slow", "Secondly therefore we present a novel lookahead technique called equiv alent oraclestate maintenance to merge multiple states that are equivalent for similarity computation", "Our experiments show that the equivalent oracle state maintenance technique significantly speeds up more than 40 times the oracle extraction", "Efficient oracle extraction has at least three im portant applications in machine translation", "Discriminative Training In discriminative train ing the objective is to tune the model parameters eg weights of a perceptron model or conditional random field such that the reference translations are preferred over competitors", "However the reference translations may not be reachable by the translation system in which case the oraclebest hypotheses should be substituted in training", "9 Proceedings of NAACL HLT 2009 Short Papers pages 912 Boulder Colorado June 2009", "2009 Association for Computational Linguistics System Combination In a typical system combi nation task eg Rosti et al", "2007 each compo nent system produces a set of translations which are then grafted to form a confusion network", "The confusion network is then rescored often employ ing additional language models to select the fi nal translation", "When measuring the goodness of a hypothesis in the confusion network one requires its score under each component system", "However some translations in the confusion network may not be reachable by some component systems in which case a systems score for the most similar reachable translation serves as a good approximation", "Multisource Translation In a multisource translation task Och and Ney 2001 the input is given in multiple source languages", "This leads to a situation analogous to system combination except that each component translation system now corresponds to a specific source language"]}, "P41718_w06": {"title": ["Construction of a Russian Paraphrase Corpus"], "abstract": ["This paper presents a crowdsourcing project on the creation of a publicly available corpus of sentential paraphrases for Russian", "Collected from the news headlines such corpus could be applied for information extraction and text summarization", "We collect news headlines from dierent agencies in real time paraphrase candidates are extracted from the headlines using an unsuper vised matrix similarity metric", "We provide userfriendly online interface for crowdsourced annotation which is available at paraphraserru", "There are 5181 annotated sentence pairs at the moment with 4758 of them included in the corpus", "The annotation process is going on and the current version of the corpus is freely available at httpparaphraserru", "Keywords Russian paraphrase corpus  Lexical similarity metric  Unsupervised paraphrase extraction  Crowdsourcing"], "inroduction": ["Our aim is to create a publicly available Russian paraphrase corpus which could be applied for information extraction IE text summarization TS and compression", "We believe that such corpus can be helpful for paraphrase identication and generation for Russian and that is why we focus on the sentential paraphrases", "Indeed a sentential corpus does not impose any specic methods of further paraphrase identication or generation on the researcher", "If such corpus is representative enough it can serve as a dataset for the experiments on the extraction of word phrase and syntactic level para phrases", "Paraphrase is restatement of a text it conveys the same meaning in another form", "Such natural language processing NLP tasks as paraphrase identication and genera tion have been shown to be helpful for IE 25 question answering 14 machine trans lation 7 TS 19 text simplication 29 etc Paraphrase identication is used to detect plagiarism 6 and to remove redundancies in TS 19 and IE 25 while paraphrase generation  to expand queries in information retrieval and question answering 14 and patterns  in IE", "Paraphrase generation is also useful for text normalization 28 and textual entailment recognition tasks 9", "As far as the denition of paraphrase is concerned it generally implies that the same message is expressed in dierent words but it does not prescribe which portion of text is replaced by paraphrasing", "Neither does it state whether common knowledge can be  Springer International Publishing Switzerland 2016 P Braslavski et al", "Eds RuSSIR 2015 CCIS 573 pp", "146157 2016", "DOI 10100797833194171898 used when judging on the similarity of the two messages", "As a consequence of this ambiguity some researchers believe that paraphrases should have absolute semantic equivalence while others allow for bidirectional textual entailment when the two messages convey roughly the same meaning", "Let us consider an example from our corpus 1 BT oe poa o  Tele2  ae ee", "VTB might sell its shares in TELE2 in the nearest weeks 2 BT aocpoa poay Tele2", "VTB announced the sale of TELE2 Although it is clear that the two sentences describe the same event the rst one has additional details indication of the time and the fact that the shares are going to be sold", "A human judger with hisher knowledge about the world might consider these sentences paraphrases", "But if we intend to teach a machine to identify semantically equivalent paraphrases a threshold for paraphrases should be higher", "On the other hand the second sentence can be considered a summarization of the rst one and therefore such types of paraphrases can be used in automatic TS", "In our research we intend to construct paraphrase corpus for IE and TS", "We believe that the former task requires semantically equivalent or precise paraphrases while the latter one demands roughly similar ones socalled loose paraphrases like those in our example", "Thus it is important for us to distinguish precise paraphrases PP and loose paraphrases LP while constructing our paraphrase corpus", "Today there are already a number of available paraphrase resources Microsoft Para phrase Corpus being the most wellknown of them 13", "A wide number of metrics for paraphrase identication for English are evaluated against this corpus", "For Russian there are no publicly available paraphrase resources known to us with the only exception of the dataset published by Ganitkevich et al as part of The Para phrase Database project 17", "The latter includes paraphrases on the word phrase and syntactic levels and each paraphrase pair is annotated with the set of count and prob abilitybased features", "Such corpus can be used for both IE and TS but it lacks infor mation on the context of paraphrases", "We believe that if such context the original sentences was provided it could improve both these NLP tasks", "That is why we aim at constructing a sentential corpus", "Thus our task is to construct a corpus with both PPs and LPs and to make it helpful for paraphrase identication and generation in IE TS and text compression tasks", "Our research is a part of an ongoing crowdsourcing project available at para phraserru with our current results available at paraphraserruscorerstat"]}, "P59105ca": {"title": [""], "abstract": ["We present a Bayesian approach to word alignment inference in IBM Models 1 and 2", "In the original approach word translation probabilities ie model parameters are estimated using the expectationmaximization EM algorithm", "In the proposed approach they are random variables with a prior and are integrated out during inference", "We use Gibbs sampling to infer the word alignment posteriors", "The inferred word alignments are compared against EM and variational Bayes VB inference in terms of their endtoend translation performance on several language pairs and types of corpora up to 15 million sentence pairs", "We show that Bayesian inference outperforms both EM and VB in the majority of test cases", "Further analysis reveals that the proposed method effectively addresses the highfertility rare word problem in EM and unaligned rare word problem in VB achieves higher agreement and vocabulary coverage rates than both and leads to smaller phrase tables", "Index TermsBayesian methods Gibbs sampling statistical machine translation SMT word alignment"], "inroduction": ["ORD alignment is a crucial early step in the training pipeline of most statistical machine translation SMT systems 1", "Whether the employed models are phrasebased or treebased they use the estimated word alignments for constraining the set of candidates in phrase or grammar rule extraction 24", "As such the coverage and the accuracy of the learned phraserule translation models are strongly correlated with those of the word alignment", "Given a sentencealigned parallel corpus the goal of the word alignment is to identify the mapping between the source and target words in parallel sentences", "Since word alignment information is usually not available during corpus generation and human annotation is costly the task of word alignment is considered as an unsupervised learning problem", "Manuscript received June 04 2012 revised November 05 2012 accepted January 02 2013", "Date of publication February 01 2013 date of current version February 25 2013", "The work of M Saraclar was supported by a TBAGEBP award", "The associate editor coordinating the review of this manuscript and approving it for publication was Dr Gokhan Tur", "C Mermer is with TBTAK BLGEM Kocaeli 41470 Turkey and also with the Department of Electrical and Electronics Engineering Boazii University Istanbul 34342 Turkey email coskunmermertubitakgovtr", "M Saralar is with the Department of Electrical and Electronics Engineering Boazii University Istanbul 34342 Turkey email muratsaraclarbounedutr", "R Sarikaya is with Microsoft Redmond WA 98052 USA email ruhisarikayamicrosoftcom", "Color versions of one or more of the figures in this paper are available online at httpieeexploreieeeorg", "Digital Object Identifier 101109TASL20132244087 Stateoftheart word alignment models such as IBM Models 5 hidden Markov model HMM 6 and the jointlytrained symmetric HMM 7 contain a large number of parameters such as word translation transition and fertility probabilities that need be estimated in addition to the desired alignment variables", "The common method of inference in such models is expectationmaximization EM 8 or an approximation to EM when exact EM is intractable", "The EM algorithm finds the value of parameters that maximizes the likelihood of the observed variables", "However with many parameters to be estimated without any prior EM tends to explain the training data by overfitting the parameters", "A welldocumented example of overfitting in EMestimated word alignments is the case of rare words where some rare words act as garbage collectors aligning to excessively many words on the other side of the sentence pair 911", "Moreover EM is generally prone to getting stuck in a local maximum of the likelihood", "Finally EM is based on the assumption that there is one fixed value of parameters that explains the data ie EM gives a point estimate", "We propose1 a Bayesian approach in which we utilize a prior distribution on the parameters", "The alignment probabilities are inferred by integrating over all possible parameter values", "We treat the word translation probabilities as multinomialdistributed random variables with a sparse Dirichlet prior", "Inference is performed via Gibbs sampling which samples the posterior alignment distribution", "We compare the EM and Bayesian alignments on the case of IBM Models 1 and 2", "The inferred alignments are evaluated in terms of endtoend translation performance on various language pairs and corpora", "The remainder of this paper is organized as follows The related literature is reviewed in Section II", "The proposed model and the inference algorithm are presented in Section III", "The experiments are described and their results are presented in Section IV", "A detailed analysis of the results and various aspects of the proposed method are provided in Section V followed by the conclusions in Section VI", "II", "RELATED WORK Problems with the standard EM estimation of IBM Model 1 were pointed out by Moore 11", "A number of heuristic changes to the estimation procedure such as smoothing the parameter estimates were shown to reduce the alignment error rate but the effects on translation performance were not reported", "Zhao and Xing 13 address the data sparsity issue using symmetric Dirichlet priors in parameter estimation and they use variational EM to find the maximum a posteriori MAP solution", "Vaswani 1Part of this work was presented at a conference 12", "155879163100  2013 IEEE et al 14 encourage sparsity in the translation model by placing an prior on the parameters and then optimize for the MAP objective", "Zhao and Gildea 15 use sampling in their proposed fertility extensions to IBM Model 1 and HMM but they do not place any prior on the parameters", "Their inference method is stochastic EM also known as Monte Carlo EM a maximumlikelihood technique in which sampling is used to approximate the expected counts in the Estep", "Even though they report substantial reductions in the alignment error rate the translation performance measured in BLEU does not improve", "Bayesian modeling and inference have recently been applied to several unsupervised learning problems in natural language processing such as partofspeech tagging 16 17 word segmentation 18 19 grammar extraction 20 and finitestate transducer training 21 as well as other tasks in SMT such as synchronous grammar induction 22 and learning phrase alignments directly 23", "Word alignment learning problem was addressed jointly with segmentation learning by Xu et al 24 Nguyen et al 25 and Chung and Gildea 26", "As in this paper they treat word translation probabilities as random variables with an associated prior distribution", "Both 24 and 25 place nonparametric priors also known as cache models on the parameters", "Similar to our work this enables integration over the prior distribution", "In 24 a Dirichlet Process prior is placed on IBM Model 1 word translation probabilities", "In 25 a PitmanYor Process prior is placed on word translation probabilities in a proposed bagofwords translation model that is similar to IBM Model 1", "Both studies utilize Gibbs sampling for inference", "However", "alignment distributions are not sampled from the true posteriors but instead are updated either by running GIZA 24 or using a localbest maximization search 25", "On the other hand a sparse Dirichlet prior on the multinomial parameters is used in 26 to prevent overfitting", "Bayesian word alignment with Dirichlet priors was also investigated in a recent study using variational Bayes VB 27", "VB is a Bayesian inference method which is sometimes preferred over Gibbs sampling due to its relatively lower computational cost and scalability", "However VB inference approximates the model by assuming independence between the hidden variables and the parameters", "To evaluate the effect of this approximation we also present and analyze the experimental results obtained using VB Sections IVC and VA", "This paper extends the initial work in 12 in several aspects 1 Extension to inference in Bayesian IBM Model 2 2 complete derivation of the Gibbs sampler for both Models 1 and 2 3 performance comparison with VB 4 improved performance for both baseline and proposed systems via alignment combination 5 reporting the average and standard deviation over 10 MERT runs for each BLEU score 6 experimental results on two to three orders of magnitude larger training sets 7 results with morphologicallysegmented corpora 8 several new metrics including AER for intrinsic and extrinsic evaluation of alignments obtained using different methods 9 analysis of the effect of sampling settings and 10 convergence behavior of both EM and Gibbs sampling", "III", "BAYESIAN INFERENCE OF WORD ALIGNMENTS We first recap the IBM Model 1 presented in 5 and establish the notation used in this paper", "Given a parallel corpus of sentence pairs let denote the th sentence in  and let denote the th  th word among a total of words in 2", "We also hypothesize an imaginary null word to account for any unaligned words in  Also let and denote the size of the respective vocabularies", "We associate with each a hidden alignment variable whose value ranges over 0 ", "The set of alignments for a sentence corpus is denoted by  The model parameters consist of a table of word translation probabilities such that  Since is conditioned on  we refer to as the source word sentence and as the target word sentence3", "The conditional distribution of the Model 1 variables given parameters is expressed by the following generative model 1 2The two unknowns and are estimated using the EM al gorithm which finds the value of that maximizes the likelihood of the observed variables and according to the model", "Once the value of is known the probability of any alignment becomes straightforward to compute", "In the following derivation of our proposed model we treat the unknown as a random variable", "Following the Bayesian approach we assume a prior distribution on and infer the distribution of by integrating over all values of  A Canonical Representation of Model 1 We first convert the tokenbased expression in 2 into a type based one as with now a random variable 3 4 where in 3 the count variable denotes the number of times the source word type is aligned to the target word type in the sentence pair  and in 4  This formulation exposes two properties of IBM Model 1 that facilitates the derivation of a Bayesian inference algorithm", "First the parametrization on is in the canonical form of an 2Keeping in mind that    and introduced later are defined with respect to the th sentence we drop the subscript for notational simplicity", "3Historically the source and target designations were based on the translation task when the word alignment direction was dictated by the noisy channel model to be the inverse of the translation direction", "Today almost all SMT systems using IBM models train alignments in both directions decoupling the alignment direction from that of translation and nullifying the justification of the early nomenclature", "exponential family distribution as the innerproduct of parameters and sufficient statistics  which implies the existence of a conjugate prior that simplifies calculation of the posterior", "Second the distribution in 4 depends on the variables  and only through a set of count variables  In other words the order of words within a sentence has no effect on the likelihood which is called exchangeability or a bag of words model", "This results in simplification of the terms when deriving the Gibbs sampler", "B Prior on Word Translation Probabilities For each source word type  by definition form the parameters of a multinomial distribution that governs the distribution of the target words aligned to  Hence the conditional distribution of the th target word ina sentence pair is defined by Since the conjugate prior of multinomial is the Dirichlet distribution we choose where  Overall are the hyperparameters of the model", "The mathematical expression for the prior is provided in 13 in the Appendix", "We can encode our prior expectations for into the model by suitably setting the values of  For example we generally expect the translation probability distribution of a given source word type to be concentrated on one or a few target word types", "Setting allocates more prior weight to such sparse distributions", "C Inference by Gibbs Sampling To infer the posterior distribution of the alignments  we use Gibbs sampling 28 a stochastic inference technique that produces random samples that converge in distribution to the desired posterior", "In general for a set of random variables  a Gibbs sampler iteratively updates the variables one at a time by sampling its value from the distribution  where the superscript denotes the exclusion of the variable being sampled", "Before applying Gibbs sampling to our model in 4 since we are only after  we integrate out the unknown using 5 The remaining set of variables is  of which only is unknown", "Starting from 5 the Gibbs sampling formula is found as the derivation steps are outlined in the Appendix TABLE I ALIGNMENT INFERENCE ALGORITHM FOR BAYESIAN IBM MODEL 1 USING GIBBS SAMPLING the current alignment link between and  We can also observe the effect of the prior where the hyperparameters act as pseudocounts added to  Table I describes the complete inference algorithm", "In Step 1 can be initialized arbitrarily", "However informed initializations eg EMestimated alignments can be used for faster convergence", "Once the Gibbs sampler is deemed to have converged after burnin iterations we collect samples of to estimate the underlying distribution  To reduce correlation between these samples a lag of iterations is introduced inbetween", "Thus the algorithm is run for a total of iterations", "The phraserule extraction step requires as its input the most probable alignment  which is also called the Viterbi alignment", "Since is a vector with a large number of elements we make the assumption that the most frequent value for the vector can be approximated by the vector consisting of the most frequent values for each element  Hence we select for each its most frequent value in the", "collected samples as the Viterbi alignment", "D Extension to IBM Model 2 IBM Model 1 assumes that all alignments are equally probable ie  In IBM Model 2 5 the alignment probability distribution for a given target word at position depends on the quadruple  This dependency is parametrized by a distortion parameter for each quadruple such that 7 Note that Model 1 is a special case of Model 2 in which the parameters are fixed at  Different variants of Model 2 have been proposed to reduce the number of parameters eg by dropping dependence on  10 or using relative distortion  where 6 also called diagonaloriented Model 2 29", "In the following we used the latter parametrization the derivation for inference in the other variants would be similar", "Bayesian inference in Model 2 can be derived in an analogous manner to Model 1", "Treating the set of distortion parameters denoted by  as a new random variable equations 2 and 4 can be adapted to Model 2 as 6 Here denotes the number of times the source word type is aligned to the target word type in  not counting 8 9 where in 9 the count variable stores the number of times a particular relative distortion occurs in  Since form the parameters of a multinomial distribution on see 7 we choose a Dirichlet prior on  where are the distortion hyperparameters", "Integrating out the parameters and results in the following Gibbs sampling formula for Bayesian IBM Model 2 10 where  A complete derivation is presented in the Appendix", "To infer the alignments under Model 2 the only change needed in Table I is the use of 10 instead of 6 in step 6", "IV", "EXPERIMENTAL RESULTS A Setup We evaluated the performance of the Bayesian word alignment via bidirectional translation experiments", "We performed the initial experiments and analyses on small data then tested the best performing baseline and proposed methods on large data", "Furthermore we performed some of the side investigations and computeintensive experiments such as those concerning the alignment combination schemes morphological segmentation convergence and the effect of sampling settings only on the smallest of the datasets Turkish English", "For Turkish English experiments we used the travel domain BTEC dataset 30 from the annual IWSLT evaluations 31 for training the CSTAR 2003 test set for tuning and the IWSLT 2004 test set for testing", "For Arabic English  we used LDC2004T18 news from years 20012004 for training subsets of the AFP portion of LDC2004T17 news from year 1998 for tuning and testing and the AFP and Xinhua subsets of the respective Gigaword corpora LDC2007T07 and LDC2007T40 for additional LM training", "We filtered out sentence pairs where either side contains more than 70 words for Arabic English", "All language models are 4gram in the travel domain experiments and 5gram in the news domain experiments with modified KneserNey smoothing 32 and interpolation", "Table II shows the statistics of the data sets used in the smalldata experiments", "For each language pair we obtained maximumlikelihood word alignments using the EM implementation of GIZA 10 and Bayesian alignments using the publicly available Gibbs sampling GS implementation 33", "As sampling settings Section IIIC we used   and for and 8000 for  We chose identical symmetric Dirichlet priors for all source words with to obtain a sparse Dirichlet prior", "After alignments were obtained in both translation directions standard phrasebased SMT systems were trained in both directions using Moses 34 SRILM 35 and TABLE II CORPUS STATISTICS FOR EACH LANGUAGE PAIR IN THE SMALLDATA EXPERIMENTS", "T TURKISH E ENGLISH A ARABIC ZMERT 36 tools", "The translations were evaluated using the singlereference BLEU 37 metric", "Alignments in both directions were symmetrized using the default heuristic in Moses growdiagfinaland", "To account for the random variability in minimum errorrate training MERT 38 we report the mean and standard deviation of 10 MERT runs for each evaluation", "We also investigated alignment combination both within and across alignment methods to obtain the best possible performance", "For this purpose we obtained three alignment samples from each inference method while trying to capture as much diversity as possible", "For EM we obtained alignments after 520 and 80 iterations denoted by EM5 EM20 and EM80 re spectively", "For GS we ran three separate chains two initialized with the EM alignments denoted by GS5 and GS80 respectively and to provide even more diversity a third initialized based on cooccurrence denoted by GSN Each target word was initially aligned to the source candidate that it cooccurred with the most number of times in the entire parallel corpus", "B Performance Comparison of EM and GS Fig", "1 compares the BLEU scores of SMT systems trained with individual EM and GSinferred alignments", "In all cases using GS alignments that are initialized with the alignments from EM leads to higher BLEU scores on average than using the EM alignments directly", "In Section VA we investigate the intrinsic differences between the EM and GSinferred alignments that lead to the improved translation performance", "Alignment combination across methods heterogeneous combination has been previously shown 39 40 to improve the translation performance over individual alignments", "Moreover alignment combination within a method homogeneous combination can also cope with random variation in GS or overfit ting in EM", "We implemented alignment combination by concatenating the individual sets of alignments meanwhile replicating the training corpus and training the SMT system otherwise the same way", "We experimented with various alignment combination schemes and found that combining the EM alignments from 5 20 and 80 iterations is in general better than the individual alignments with a similar conclusion for combining the three GS alignments described in Section IVA", "Further combination of these two combinations for a total of six alignments sometimes improved the performance even more", "So we present the results in this section using these three combination Fig", "1", "Translation performance of word alignments obtained by expectation maximization EM Gibbs sampling initialized with EM GS and variational Bayes VB EM GS VB", "Fig", "2", "Translation performance of EM Gibbs sampling and variational Bayes after applying alignment combination within and across methods EMCo GSCo EMCoGSCo VBCo and EMCoVBCo", "The same BLEU scale is used as in Fig", "1", "schemes denoted by EMCo GSCo and EMCoGSCo respectively in Fig", "2", "We observe from Fig", "2 that GSCo outperforms EMCo on average both by itself and in combination with EMCo in most cases by a significant margin", "However which scheme GSCo or EMCoGSCo is the best seems to depend on the language pair andor dataset", "C Comparison with Variational Bayes Using the publicly available software 41 we experimented with variational Bayes VB inference using similar alignment combination schemes combination of three VBinferred alignments after 5 20 and 80 Model 1 iterations and further combination of it with the three EMinferred alignments above denoted by VBCo and EMCoVBCo respectively", "The translation performance of the individual VB alignments in Fig", "1 shows that compared to EM VB achieves higher Fig", "3", "Results for the morphologicallysegmented TurkishEnglish corpus", "All BLEU scores are computed at the word level", "BLEU scores in but lower scores in  On the other hand GS outperforms VB in all cases but one in Fig", "1", "As for the performance after alignment combination Fig", "2 shows that for all translation directions GSCo leads to higher average BLEU scores compared to VBCo both with and without further combination with EMCo", "The performance of VBCo relative to EMCo is similar to the case for individual alignments better in  worse in ", "However EMCoVBCo outperforms or performs as good as EM in all cases demonstrating that Bayesian word alignment can be beneficial even with a fast yet approximate inference method", "To explain the particularly low performance of VB in Arabic English we inspected the alignments inferred by EM GS and VB", "We found that while VB with sparse Dirichlet prior avoids excessive alignment fertilities it leaves many rare source words unaligned", "For example the percentage of unaligned source singletons for EM5 GS5 and VB5 in the English Arabic Arabic English alignments are 27 16 and 69 44 34 and 71 respectively", "We believe the higher rate of unaligned singletons can lead to poorer training set coverage and lower translation performance Section VA", "D Experiments With Morphologically Segmented Corpus Morphological preprocessing is a common practice in modern SMT systems dealing with morphologically unmatched language pairs", "Thus as a side investigation we also experimented with morphological segmentation in the corpus to see its effect on the performance of our proposed method morphological segmentation is also applied in the largedata experiments presented in Section IVE", "We used the morphological analyzer by Oflazer 42 to segment the Turkish words into lexical morphemes", "As a result the vocabulary size decreased to 56k from 18k cf", "Table II with 24k of them singletons", "The outofvocabulary rate in the Turkish tuning and test sets decreased from 52 and 61 to 09 and 08 respectively", "The BLEU scores were still computed at the word level in the case of English Turkish translation by joining the morphemes in the output", "The results in Fig", "3 show that the advantage of GS over EM still holds in the morphologicallysegmented condition in both translation directions both individually and with combination", "In addition comparing the BLEU scores with those in Figs", "1 TABLE III CORPUS STATISTICS FOR EACH LANGUAGE PAIR IN THE LARGEDATA EXPERIMENTS", "A ARABIC E ENGLISH C CZECH G GERMAN and 2 confirms the previous studies that applying morphological segmentation improves the translation performance significantly especially in the morphologically poorer direction ie Turkish English", "E Experiments on Larger Datasets The scalability of the alignment inference methods was also tested on publicly available large datasets Table III", "We used the 8million sentence MultiUN corpus 43 for Arabic English translation experiments", "As is common in most stateoftheart systems for this language pair we performed morphological segmentation on the Arabic side for the best performance we used the MADATOKAN tool 44", "Note that after morphological segmentation Arabic no longer exhibits the vocabulary characteristics of a morpho logicallyrich language Table III", "We set aside the last 100k sentences of the corpus and randomly extracted the tuning and test sets from this subset", "The English side of the parallel corpus was used for language model trainingWe used the WMT 2012 45 datasets for Czech Eng lish and German English translation experiments", "The training data consisted of the Europarl news commentary and the 15million sentence CzEng 10 46 corpora while the training data consisted of only the Europarl and news commentary corpora", "WMT 2011 and 2012 news testsets were used for tuning and testing respectively", "The WMT 2012 monolingual news corpora covering years 20072011 were used for language model training", "In all largedata experiments sentences longer than 70 words were excluded from translation model training", "Gibbs sampling settings of B M L1000 100 1 were used", "All language models were 4gram", "To obtain the best possible baseline we also utilized techniques that we had previously observed to improve performance on similar corpora such as lattice sampling 47 and search in random directions 48 during MERT and minimum Bayes risk decoding 49", "All other experimental settings eg 10 MERT runs etc were identical to the smalldata experiments Section IVATo conform with the majority of previous research and evalu ations in these language pairs we trained SMT systems in both directions for the WMT 2012 language pairs and in the Arabic English direction for the MultiUN task", "For the two largest datasets  and  we also experimented with 1million sentence versions for faster development experiments and to provide an intermediate data size setting", "The results are presented in Figs", "46", "For translation to English Gibbs sampling improves over EM for all five corpora the Fig", "4", "Arabic English BLEU and TER scores of various alignment methods EMCo GSCo EMCoGSCo and VBCo", "Fig", "5", "Czech English BLEU scores of various alignment methods EMCo GSCo EMCoGSCo and VBCo", "Fig", "6", "German English BLEU scores of various alignment methods EMCo GSCo EMCoGSCo and VBCo", "largest improvement achieved by GSCoEMCo in 05 to 07 BLEU mean difference and by GSCo in and 03 to 05 BLUE mean difference", "However for translation from English  and  we do not observe a consistent improvement over EM", "For the 1million sentence task we also report the translation error rates TERs 50 bottom row of Fig", "4", "Except for the comparison between GSCo and EMCoGSCo in the 1Msentence setting in all possible pairwise comparisons between the alignment methods in both corpus settings Fig", "7", "Arabic English BLEU scores of various alignment combination schemes in the 1Msentence translation task", "TABLE IV BLEU SCORES OF IBM MODEL 2 ALIGNMENT INFERENCE METHODS ON THE 1MSENTENCE ARABIC ENGLISH TRANSLATION the method with the higher mean BLEU score also has the lower mean TER score4", "In addition we compared the performance of some of the many possible alignment combination schemes Fig", "7", "Not surprisingly combination with EMCo helps both GSCo and VBCo and the relative ranking of the latter two does not change after combination with EMCo", "Furthermore combination of GSCoVBCo improves the performance slightly over EMCoGSCo", "F Bayesian Model 2 Results We tested the IBM Model 2 Gibbs sampling algorithm on the 1Msentence subset of the ArabicEnglish MultiUN corpus", "Unlike the case of translation parameters  there is no clear language and domainindependent knowledge of how the distortion parameters the distribution of  should look like", "Therefore we assumed that all distortion distributions are a priori equally probable which corresponds to setting the distortion hyperparameters for all  We also collapsed the counts for distortions larger in magnitude than 5 resulting in 11 total distortion count variables  as done in 7", "We compared the translation performance of the EM and GSinferred Model 2 alignments", "Both methods are initialized with the same EM5 alignments ie 5 iterations of Model 1 EM", "Model 2 EM is run for 5 iterations", "Model 2 GS is estimated with  and  The results are shown in Table IV", "Bayesian inference improves the mean BLUE score by 02 BLEU", "Further improvement could be possible by alignment combination within and across methods as done in Section IVB", "V ANALYSIS AND DISCUSSION A Analysis of Inferred Alignments In order to explain the BLEU score improvements achieved by the Bayesian alignment approach and to characterize the dif 4BLEU was used as the error metric for optimization in MERT", "Fig", "8", "Distribution of alignment fertilities for source language tokens", "ferences between the alignments obtained by various methods we analyzed the alignments in Fig", "1 using several intrinsic and extrinsic evaluation metrics", "As representative alignments from each method we selected EM5 VB5 and GS5", "1 Fertility Distributions Fertility of a source word is defined as the number of target words aligned to it", "In general we expect the fertility values close to the word token ratio between the languages to be the most frequent and high fertility values to be rare", "Fig", "8 shows the fertility distributions in alignments obtained from different methods", "We can observe the garbage collecting effect in the long tails of the EMestimated alignments", "For example in EnglishArabic Model 1 alignment using EM 12 of the English source tokens are aligned with nine or more Arabic target words corresponding to 223k total occurrences or about 04 occurrence per sentence", "In all alignment tasks both Bayesian methods result in fewer highfertility alignments compared to EM", "Among Bayesian inference techniques GS is more effective than VB in avoiding high fertilities", "2 Alignment Dictionary Size Reducing the number of unique alignment pairs has been proposed as an objective for word alignment 51 52 it was observed during manual alignment experiments that humans try to find the alignment with the most compact alignment dictionary a vocabulary of unique sourcetarget word pairs as possible", "Fig", "9a shows that both GS and VB explain the training data using a significantly smaller alignmentpair vocabulary compared to EM", "3 Singleton Fertilities The average alignment fertility of source singletons was proposed as an intrinsic evaluation metric in 40", "We expect lower values to correlate with better alignments", "However a value of zero could be achieved by leaving all singletons unaligned which is clearly not desirable", "Therefore we refine the definition of this metric to calculate the average over aligned singletons only", "The minimum value thus attainable is one", "Fig", "9b shows that both Bayesian methods significantly reduce singleton fertilities", "The average fertility of aligned singletons by itself is not sufficient to accurately assess an alignment since unaligned singletons are not represented", "Hence we also report the percentage of unaligned singletons in Fig", "9c", "GS has the lowest unaligned singleton rate among Model 1 inference methods", "An interesting Fig", "9", "Intrinsic and extrinsic evaluation of alignments in the small data experiments", "a Alignment dictionary size normalized by the average of source and target vocabulary sizes", "b Average alignment fertility of aligned singletons", "c Percentage of unaligned singletons", "d Number of symmetric alignments normalized by the average of source and target tokens", "e Percentage of training set vocabulary covered by singleword phrases in the phrase table", "f Decodetime rate of input words that are in the training vocabulary but without a translation in the phrase table", "g Phrase table size normalized by the average of source and target tokens", "observation is that while EMestimated alignments suffer from rare words being assigned high fertilities Fig", "9b VB suffers from a high percentage of the rare words eg about 70 of singletons in  being left unaligned resulting in lower translation performance Section IVC", "Our analysis agrees with the findings of Guzman et al 53 that unaligned words in an alignment results in lowerquality phrase tables", "4 Alignment Points in Agreement Since the IBM alignment models are onesourcetomanytarget switching the source and target languages usually result in a different set of alignment links or points in an alignment matrix", "The intersection of the two sets consists of highprecision alignment points where both alignment models agree 7", "Since the number of alignment points in each direction is constant equal to the number of target words increasing precision at the expense of recall by predicting fewer alignment points is not applicable in these models", "Therefore higher agreement rate implies not only higher precision but higher recall as well", "Fig", "9d shows that GS has the highest alignment agreement rate among the alignment methods for both language pairs", "5 Training Set Vocabulary Coverage by Phrase Table We can also evaluate the inferred alignments extrinsically eg by evaluating the SMT systems trained using those alignments", "A desirable feature in a SMT system is to have as high vocabulary coverage as possible", "This metric is highly sensitive to the performance of an alignment algorithm on infrequent words since they represent the majority of the vocabulary of a corpus see Table II", "Fig", "9e shows that alignment by GS leads to the best vocabulary coverage in all four alignment tasks", "Note that word types that appear in the phrase table only as part of larger phrases are excluded from this metric since such words are practically outofvocabulary OOV except only in those specific contexts", "TABLE V ALIGNMENT ERROR RATE  OF THE UNIDIRECTIONAL AND SYMMETRIZED CZECHENGLISH ALIGNMENTS Poor training set vocabulary coverage results in some nonOOV words being treated by the system as OOV either dropping them from the output or leaving them untranslated", "Such pseudoOOV words further degrade the translation performance in addition to the OOV words", "Fig", "9f shows that GS alignments lead to the lowest rate of pseudoOOV words", "6 Phrase Table Size In most machine translation applications having a small model size is valuable eg to reduce the memory requirement or the startupaccess time", "Alignment methods can affect the induced phrase table sizes", "Fig", "9g compares the number of phrase pairs in the SMT systems trained by different alignment methods", "In the task where model size is of more concern compared to the smaller task GS results in significantly smaller phrase tables", "This result is particularly remarkable since it means that a system using GSinferred alignments achieves more vocabulary coverage Section VA5 and higher BLEU scores Section IVB with a smaller model size", "Thanks to a larger intersection during alignment symmetrization Fig", "9d GSbased phrase tables contain a higher number of singleword phrase pairs Fig", "9e", "Moreover fewer unaligned words after symmetrization lead to fewer poorquality long phrase pairs 53", "7 Alignment Error Rate Table V shows the alignment error rates AERs 10 obtained in the alignment tasks Fig", "10", "BLEU scores obtained by different sampling settings Section IIIC", "Averages and standard deviations are over 8 separate Gibbs chains", "a and ", "b ", "a Changing B b changing M and L using a publicly available 515sentence manuallyaligned reference set 54", "The Bayesian methods achieve better AERs than EM in both alignment directions denoted by EC and CE", "Contrary to the ranking of the methods according to BLEU Fig", "5 VB achieves the best AER which also holds true after symmetrization denoted by Sym", "Furthermore the symmetrized GS5 alignment has the worst AER in the 1Msentence experiment", "These discrepancies support earlier findings by several others that AER is generally not a good predictor of BLEU performance 55", "As a final remark in Table V EM5 enjoys a larger amount of reduction in AER via symmetrization compared to GS5 which suggests the possibility that the default alignment symmetrization heuristic in Moses growdiagfinaland has been finetuned for the default EMbased alignments and thus other symmetrizationphrase extraction methods might work better for the GS and VBbased alignments", "For example Bayesian alignment inference could be complemented with a probabilistic model of phrase extraction eg 23 which is left as a future work", "B Effect of Sampling Settings We investigated the effect of changing the sampling settings   and Section IIIC on GSN alignments", "To account for the variability due to the randomness of the sampling process we present in Fig", "10 the mean and the standard deviation of BLEU scores over eight separate chains with different random seeds", "At each B value shown eight separate SMT systems were trained", "These eight runs each comprise a separate MERT run thus error bars in Fig", "10 also include the variation due to MERT", "Fig", "10a shows the effect of changing with and  In this experiment the sampler converges after roughly a few thousand iterations", "Comparing the BLEU scores in Fig", "10a to those of the three EMinitialized samplers in Fig", "1 where  for the same language pair suggests that running more iterations of Gibbs sampling can compensate for poor initializations or equivalently initializing with EM alignments can provide a head start in the convergence of the Gibbs chain", "Fig", "10b compares the effect of different readout schemes", "The settings of both 10001 and 10010 collect samples over the same 1000sample interval", "We can deduce from Fig", "11", "BLEU scores of alignments estimated at different iterations", "Left EM middle samples from the Gibbs chain right GS viterbi estimates with   A separate SMT system is trained at each shown data point on the plots", "Note the difference in xaxis scales", "their comparison in Fig", "10b that including or discarding the intermediate samples does not make a significant difference", "On the other hand comparing the settings 1001 and 10001 confirms our intuition that increasing the number of samples leads to more reliable smaller variance estimates of the Viterbi alignments", "C Convergence and Variance Between Iterations Fig", "11 compares the change in BLEU scores as iterations progress during both EM and GS", "Each dot in the graphs correspond to a separate SMT system trained and optimized from the alignment estimated at that iteration", "In the figure there are two main sources of BLEU score variation between the iterations updated alignments at each iteration and randomness due to MERTComparing the BLEU scores of sample and Viterbi align ments obtained by GS we observe smaller variance and higher average BLEU scores using Viterbi alignments", "Compared to EM GS achieves higher average BLEU scores albeit with a larger amount of variation between iterations due to the random nature of sampling", "To reduce the variation a larger value of Section VB andor a combination of alignments at different iterations can be used", "D Computational Complexity The computational complexity of the Gibbs sampling algorithm in Table I is linear in the number of sentences and roughly quadratic in the average number of words per sentence", "Running Gibbs sampling Model 1 on the largest of our datasets the 154Msentence CzechEnglish corpus takes on average 33 seconds per iteration steps 37 in Table I using 24 threads on a 347GHz Intel Xeon X56905 In the case of Model 2 the average time per Gibbs sampling iteration increases to 48 seconds", "For comparison a Model 1 EM iteration on the same hardware 5Our multithreaded implementation is actually an approximation of Gibbs sampling where the counts and are not updated until the end of an iteration", "Similar approximations have been done in scaling Gibbs sampling to large datasets using multiple parallel processors eg in 56", "All largedata experiments reported in Sections IVE and IVF have been performed using this multithreaded implementation", "and number of threads using MGIZA 57 takes 326 seconds on average excluding preprocessing and initializations6", "VI", "CONCLUSION Similarly from Section IIID 14 We developed a Gibbs samplingbased word alignment inference method for Bayesian IBM Models 1 and 2 and showed that it compares favorably to EM estimation in terms of translation BLEU scores", "We observe the largest improvement when data is sparse eg in the cases of smaller corpora andor more morphological complexity", "The proposed method successfully overcomes the wellknown garbage collection problem of rare words in EMestimated current models and learns a compact sparse word translation distribution with more training vocabulary coverage", "We also found Gibbs sampling to perform better than variational Bayes inference which leaves a substantially high portion of source singletons unaligned", "Additionally we utilized alignment combination techniques to further improve the performance and robustness", "Future research avenues include estimation of the hyperparameters from dataauxiliary sources and utilization of the proposed algorithm in either initialization or inference of more advanced alignment models", "APPENDIX DERIVATION OF THE GIBBS SAMPLING FORMULA In this section we describe the derivation of the Gibbs sampler for IBM Model 2 given in 10", "Since IBM Model 1 is a special case of Model 2 where is fixed Section IIID the derivation of the sampler for Model 1 given in 6 would follow exactly the same steps except that there would be no prior and the related terms", "A The Dirichlet Priors We choose a simple prior for the parameters where each has an independent7 Dirichlet prior with hyperparameters Section IIIB We further define the priors for the translation and distortion parameters to be independent so that  B The Complete Distribution Since we are only interested in inferring  we integrate out the unknowns and in 9 using 13 and 14 15 16 17 As a result of choosing conjugate priors the integrands with respect to and in 17 can be recognized to be in the same form as the priors ie Dirichlet distributions with new sets of Since the integral of a probability distribution is equal to 1 we obtain the closedform expression 18 where and 11 12 C Gibbs Sampler Derivation Given the complete distribution in 18 the Gibbs sampling formula Section IIIC can be derived as Hence the complete prior for is given by 19 13 6In the case of Model 2 for which multithreading is not implemented in MGIZA an EM iteration took 1960 seconds on average", "7While the prior knowledge about could have been possibly expressed as a more refined correlated distribution we show that a simple independent prior is also successful in biasing the parameters away from flat distributions", "20 21 22 23 24 25 26 where 20 follows since is independent of  in 21 we used 18 in 23 we used 12 and grouped similar factors in 25 each fraction is simplified using the property of the gamma function  and in 26 the proportionality comes from the omission of the last term in 25 which is constant for all values of "]}, "P846406_w06": {"title": ["Learning by Reading An Experiment in Text Analysis"], "abstract": ["It has long been a dream to build computer systems that learn automatically by reading text", "This dream is generally considered infeasible but some surprising developments in the US over the past three years have led to the funding of several shortterm investigations into whether and how much the best current practices in Natural Language Processing and Knowledge Representation and Reasoning when combined actually enable this dream", "This paper very briefly describes one of these efforts the Learning by Reading project at ISI which has converted a high school textbook of Chemistry into very shallow logical form and is investigating which semantic features can plausibly be added to support the kinds of inference required for answering standard high school text questions"], "inroduction": ["From almost the beginnings of Artificial Intelligence it was clear that automated systems require knowledge to reason intelligently and that for multipurpose widedomain robust reasoning the amount required is nontrivial", "Experience especially with expert systems during the 1970s illustrated just how hard it is to acquire enough of the right knowledge and how difficult it is to formalize that knowledge in ways suitable for supporting reasoning", "Naturally then the dream arose to enable systems to read text and learn by themselves", "But this dream has never been realized", "In fact as research in Knowledge Representation and Reasoning KRR and Natural Language Processing NLP progressed the two areas diverged to the point where today they are more or less entirely separate with unrelated conferences journals and research paradigms", "A few years ago three research groups funded by Vulcan Inc participated in an audacious experiment called Project Halo to manually convert the information contained in one chapter of a high school textbook on Chemistry into knowledge representation statements and then to have the knowledge representation system take a standard high school Advanced Placement AP exam", "Surprisingly two of the three systems passed albeit at a relatively low level of performance", "The project engendered wide interest see Friedland et al 2003", "Over the past year DARPA has funded five groups in the US to conduct pilot studies that investigate the feasibility of building fully Learning by Reading LbR systems", "The largest Project Mbius is a consortium of some 20 researchers from numerous institutions", "Its goal Petr Sojka Ivan Kopec ek and Karel Pala Eds TSD 2006 LNAI 4188 pp", "312 2006", "Oc SpringerVerlag Berlin Heidelberg 2006 is to design a general framework for LbR systems in the future and to advise DARPA on the wisdom of funding a new program in this area", "Typical questions include How feasible is fully automated LbR", "What are the different phasescomponentssteps of LbR", "What are the current levels of capability of the component technologies and where are the major bottlenecks and failure points", "What kind of research would best further the dream of LbR", "What sorts of results could one expect after five years", "The remaining four projects proceed independently but report back to Mbius", "All are smaller 9month efforts and each focuses on one or more specific aspects of the general LbR problem  A project jointly at Boeing and SRI led by Peter Clark of Boeing focuses on the mismatch between English sentences and their equivalent knowledge representations propositions with the methodology of building manually the representations for a carefully selected extract of 5 pages from the Chemistry textbook", " A project at Northwestern University led by Ken Forbus concentrates on the processes of selfguided inference that occurs after new information is read", "Called introspection or rumination these processes work in parallel with the reading and serve as a source of expectations questions and background checking", "This project focuses on a few selected sentences from the Chemistry textbook and the thoughts that may arise from them", " A project at CYC Corp led by Michael Witbrock addresses the problem of learning the meaning of new unknown words in context", "Starting with the knowledge already inside their very large ontology and reasoning system Cyc researchers develop methods to apply inferences in order to build up likely interpretations of a sentence and from these hypothesize the meaning of the unknown word", " The fourth project is the subject of this paper", "Located at ISI we are investigating how much can be done by combining traditional and statistical NLP methods and what kinds of KRR are absolutely required at which points in the process", "We have parsed the whole Chemistry textbook have developed methods to convert the parses into shallow logical form and are investigating the what types of semantics should be added to support the reasoning required for question answering", "In this paper we briefly outline the architecture and general aspects of ISIs LbR project which will finish in August namely about three months after the time of writing this paper"]}, "P87-94": {"title": ["International Journal of Advanced Intelligence"], "abstract": ["We present an approximative IBM Model 4 for word alignment", "Dierent with the most widelyused word aligner GIZA which implements all the 5 IBM models and HMM model in the framework of Expectation Maximum EM we adopt a full Bayesian inference which integrates over all possible parameter values rather than estimating a single parameter value", "Empirical results show promising improvements in alignment quality as well as in BLEU score for the translation performance over baselines", "Keywords  Bayesian inference Word alignment Statistical machine translation"], "inroduction": ["Word alignment can be dened as a procedure for detecting the corresponding words in a bilingual sentence pair", "One of the notorious criticisms of word alignment is the inconsistence between the word alignment model to the phrase based translation model", "In this paper we have no intention to avoid mentioning this inherent weakness of word alignment but we would say as far as we know word alignment is a fundamental component for most of the SMT systems", "Phrase or the other higher level translation knowledge is extracted based on the word alignment which is called twostage approach", "And even for approaches of socalled direct phrase alignment they can rarely abandon word alignment thoroughly", "Because of the computation complexity of phrase alignment word alignment is usually used to constrain the inference1 DeNero proposes a relative pure joint phrase model but still uses the word alignment as initialization and smoothing which shows the least dependency on word alignment2 Neubig uses Bayesian methods and Inversion Transduction Grammar for joint phrase alignment3 and the base distribution for the Dirichlet Process 5 prior is constructed by the word alignment model", "Therefore word alignment is well worth concern", "Our hope is to induce a better word alignment by 111 NojiHigashi Kusatsu Shiga Japan", "87 88 Z Li H Ikeda utilizing the stateoftheart learning technology and establish a better baseline for the word level alignment models", "Bayesian inference the approach we adopt in this paper has been broadly applied to various learning of latent structure", "Goldwater points out that two theoretical factors contribute to the superiority of Bayesian inference7 First integrating over parameter values leads to greater robustness in decision", "One of the problems that trouble EM algorithm is overtting", "Moore discusses details of how a Garbage collector is generated11 He also suggests a number of heuristic solutions but Bayesian inference can oer a more principled solution", "The second factor is that the integration permits the use of priors favoring sparse distributions which proved to be more consistent with nature of natural language", "Another practical advantage is that the implementation can be much easier than EM12 In the following sections we will have a review for IBM Model 4 in Section 2 and reformulate it into a simpler and Bayesian form in Section 3", "Section 4 gives the Bayesian inference and Section 5 reports results of experiment", "Section 6 compares related research and Section 7 concludes"]}, "P96-1010": {"title": [""], "abstract": ["This paper addresses the problem of cor recting spelling errors that result in valid though unintended words such as peace and piece or quiet and quite and also the problem of correcting particular word usage errors such as amount and num ber or among and between", "Such cor rections require contextual information and are not handled by conventional spelling programs such as Unix spell", "First we introduce a method called Trigrams that uses partofspeech trigrams to encode the context", "This method uses a small num ber of parameters compared to previous methods based on word trigrams", "How ever it is effectively unable to distinguish among words that have the same part of speech", "For this case an alternative featurebased method called Bayes per forms better but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints", "A hybrid method called Tribayes is then in troduced that combines the best of the pre vious two methods", "The improvement in performance of Tribayes over its compo nents is verified experimentally", "Tribayes is also compared with the grammar checker in Microsoft Word and is found to have sub stantially higher performance"], "inroduction": ["Spelling correction has become a very common tech nology and is often not perceived as a problem where progress can be made", "However conventional spelling checkers such as Unix spell are concerned only with spelling errors that result in words that cannot be found in a word list of a given language", "One analysis has shown that up to 15 of spelling errors that result from elementary typographical er rors character insertion deletion or transposition yield another valid word in the language Peterson 1986", "These errors remain undetected by tradi tional spelling checkers", "In addition to typographical errors words that can be easily confused with each other for instance the homophones peace and piece also remain undetected", "Recent studies of actual ob served spelling errors have estimated that overall errors resulting in valid words account for anywhere from 25 to over 50 of the errors depending on the application Kukich 1992", "We will use the term contextsensitive spelling cor rection to refer to the task of fixing spelling errors that result in valid words such as I  Can I have a peace of cake", "where peace was typed when piece was intended", "The task will be cast as one of lexical disambigua tion we are given a predefined collection of confu sion sets such as peace piece than then etc which circumscribe the space of spelling errors to look for", "A confusion set means that each word in the set could mistakenly be typed when another word in the set was intended", "The task is to predict given an occurrence of a word in one of the confusion sets which word in the set was actually intended", "Previous work on contextsensitive spelling cor rection and related lexical disambiguation tasks has its limitations", "Wordtrigram methods Mays Dam erau and Mercer 1991 require an extremely large body of text to train the wordtrigram model even with extensive training sets the problem of sparse data is often acute", "In addition huge wordtrigram tables need to be available at run time", "More over word trigrams are ineffective at capturing long distance properties such as discourse topic and tense", "Featurebased approaches such as Bayesian clas sifiers Gale Church and Yarowsky 1993 deci sion lists Yarowsky 1994 and Bayesian hybrids Golding 1995 have had varying degrees of suc cess for the problem of contextsensitive spelling correction", "However we report experiments that show that these methods are of limited effective ness for cases such as their there theyre and than then where the predominant distinction to be made among the words is syntactic", "Confusion set Train Test Most freq", "Base their there theyre 3265 850 than then 2096 514 its its 1364 366 your youre 750 187 begin being 559 146 passed past 307 74 quiet quite 264 66 weather whether 239 61 accept except 173 50 lead led 173 49 cite sight site 115 34 principal principle 147 34 raise rise 98 39 affect effect 178 49 peace piece 203 50 country county 268 62 amount number 460 123 among between 764 186 their 568 than 634 its 913 your 893 being 932 past 689 quite 833 whether 869 except 700 led 469 sight 647 principle 588 nse 641 effect 918 peace 440 country 919 number 715 between 715 Table 1 Performance of the baseline method for 18 confusion sets", "Train and Test give the number of occurrences of any word in the confusion set in the training and test corpora", "Most freq is the word in the confusion set that occurred most often in the training corpus", "Base is the percentage of correct predictions of the baseline system on the test corpus", "In this paper we first introduce a method called Trigrams that uses partofspeech trigrams to en code the context", "This method greatly reduces the number of parameters compared to known methods which are based on word trigrams", "This method also has the advantage that training can be done once and for all and quite manageably for all con fusion sets new confusion sets can be added later without any additional training", "This feature makes Trigrams a very easily expandable system", "Empirical evaluation of the trigram method demonstrates that it performs well when the words to be discriminated have different parts of speech but poorly when they have the same part of speech", "In the latter case it is reduced to simply guessing whichever word in the confusion set is the most com mon representative of its partofspeech class", "We consider an alternative method Bayes a Bayesian hybrid method Golding 1995 for the case where the words have the same part of speech", "We confirm experimentally that Bayes and Trigrams have complementary performance Trigrams being better when the words in the confusion set have dif ferent parts of speech and Bayes being better when they have the same part of speech", "We introduce a hybrid method Tribayes that exploits this com plementarity by invoking each method when it is strongest", "Tribayes achieves the best accuracy of the methods under consideration in all situations", "To evaluate the performance of Tribayes with re spect to an external standard we compare it to the grammar checker in Microsoft Word", "Tribayes is found to have substantially higher performance", "This paper is organized as follows first we present the methodology used in the experiments", "We then discuss the methods mentioned above interleaved with experimental results", "The comparison with Mi crosoft Word is then presented", "The final section concludes"]}, "P98-1046": {"title": [""], "abstract": [], "inroduction": ["In this paper we specifically address questions of polysemy with respect to verbs and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases", "We see verb classes as the key to making gen eralizations about regular extensions of mean ing", "Current approaches to English classifica tion Levin classes and WordNet have limita tions in their applicability that impede their utility as general classification schemes", "We present a refinement of Levin classes intersec tive sets which are a more finegrained clas sification and have more coherent sets of syn tactic frames and associated semantic compo nents", "We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig inal Levin classes", "We also have begun to ex amine related classes in Portuguese and find that these verbs demonstrate similarly coherent syntactic and semantic properties"]}, "P98-1081": {"title": ["Improving Data Driven Wordclass Tagging"], "abstract": [], "inroduction": ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system", "We do this by means of an experiment involving the task of morphosyntactic wordclass tagging", "Four wellknown tagger generators Hidden Markov Model MemoryBased Transformation Rules and Maximum Entropy are trained on the same corpus data", "After comparison their outputs are combined using several voting strategies and second stage classifiers", "All combination taggers outperform their best component with the best combination showing a 191 lower error rate than the best individual tagger"]}, "P98-2138": {"title": [""], "abstract": ["For languages that have no explicit word bound ary such as Thai Chinese and Japanese cor recting words in text is harder than in English because of additional ambiguities in locating er ror words", "The traditional method handles this by hypothesizing that every substrings in the input sentence could be error words and trying to correct all of them", "In this paper we pro pose the idea of reducing the scope of spelling correction by focusing only on dubious areas in the input sentence", "Boundaries of these dubious areas could be obtained approximately by ap plying word segmentation algorithm and finding word sequences with low probability", "To gener ate the candidate correction words we used a modified edit distance which reflects the charac teristic of Thai OCR errors", "Finally a partof speech trigram model and Winnow algorithm are combined to determine the most probable correction"], "inroduction": ["Optical character recognition OCR is useful in a wide range of applications such as office automation and information retrieval system", "However OCR in Thailand is still not widely used partly because existing Thai OCRs are not quite satisfactory in terms of accuracy", "Re cently several research projects have focused on spelling correction for many types of errors in cluding those from OCR Kukich 1992", "Nev ertheless the strategy is slightly different from language to language since the characteristic of each language is different", "Two characteristics of Thai which make the task of error correction different from those of other languages are 1 there is no explicit word boundary and 2 characters are written in three levels ie the middle the upper and the lower levels", "In order to solve the prob lem of OCR error correction the first task is usually to detect error strings in the input sen tence", "For languages that have explicit word boundary such as English in which each word is separated from the others by white spaces this task is comparatively simple", "If the tok enized string is not found in the dictionary it could be an error string or an unknown word", "However for the languages that have no ex plicit word boundary such as Chinese Japanese and Thai this task is much more complicated", "Even without errors from OCR it is difficult to determine word boundary in these languages", "The situation gets worse when noises are intro duced in the text", "The existing approach for correcting the spelling error in the languages that have no word boundary assumes that all substrings in input sentence are error strings and then tries to correct them Nagata 1996", "This is computationally expensive since a large portion of the input sentence is correct", "The other characteristic of Thai writing system is that we have many levels for placing Thai char acters and several characters can occupy more than one level", "These characters are easily con nected to other characters in the upper or lower level", "These connected characters cause diffi culties in the process of character segmentation which then cause errors in Thai OCR", "Other than the above problems specific to Thai realword error is another source of er rors that is difficult to correct", "Several previous works on spelling correction demonstrated that Figure 1 No explicit word delimiter in Thai upper level topHne middle level baseline lower level featurebased approaches are very effective for solving this problem", "In this paper a hybrid method for Thai OCR error correction is proposed", "The method com bines the partofspeech POS trigram model with a featurebased model", "First the POS tri gram model is employed to correct nonword as well as realword errors", "In this step the num ber of nonword errors are mostly reduced but some realword errors still remain because the POS trigram model cannot capture some use ful features in discriminating candidate words", "A featurebased approach using Winnow algo rithm is then applied to correct the remaining errors", "In order to overcome the expensive com putation cost of the existing approach we pro pose the idea of reducing the scope of correc tion by using word segmentation algorithm to find the approximate error strings from the in put sentence", "Though the word segmentation algorithm cannot give the accurate boundary of an error string many of them can give clues of unknown strings which may be error strings", "We can use this information to reduce the scope of correction from entire sentence to a more mir row scope", "Next to capture the characteristic of Thai OCR errors we have defined the modi fied edit distance and use it to enumerate plau sible candidates which deviate from the word in question within kedit distance"]}, "P9852_p00": {"title": ["A TwoLevel Morphological Analyser  for the Indonesian Language"], "abstract": ["This paper presents our efforts at developing an Indonesian morphological analyser that pro vides a detailed analysis of the rich affixation process 1 We model Indonesian morphology us ing a twolevel morphology approach decom posing the process into a set of morphotactic and morphophonemic rules", "These rules are modelled as a network of finite state transduc ers and implemented using xfst and lexc", "Our approach is able to handle reduplication a nonconcatenative morphological process"], "inroduction": ["Morphology is the study of the way that words are built up from smaller units called morphemes the minimal meaningbearing units in a language Ju rafsky 2000", "For example the English word kind consists of a single morpheme the root word kind whilst the word players consists of three mor phemes play er and s", "The morphemes kind and play can stand alone as words while affixes er and s must appear bound to another morpheme", "By applying a set of morphological rules we can produce not just morphemes but also other in formation relating to the words for example the grammatical category of the whole word as well as the subcategorisation frame of the word if it is a verb", "This process is called morphological analysis", "In this respect the value of a morphological ana lyser would be twofold from a theoretical linguis tic viewpoint it is a very useful tool for linguistic modelling and for testing certain analyses", "On the 1 This research is part of a collaborative research project", "funded by ARC Discovery Grant DP0877595", "other hand from a practical viewpoint it supports many applications eg information retrieval search engines and machine translation among others", "There is currently some interest in developing morphological tools for the Indonesian language", "In previous work Siregar 1995 and Adriani et al", "2007 discuss the development of Indonesian stemmers that recover a root from an affixed word implemented procedurally", "However stemmers are of limited use as they do not provide any more lin guistic information beyond the stem", "Hartono 2002 presents an initial version of a morphologi cal analyser developed with PCKIMMO but un fortunately it does not handle reduplication a key aspect of Indonesian morphology", "The morpho logical analyser that we are developing and that we describe here is designed to be able to handle the rich semantic lexical and grammatical information associated with words and word formation in NLP applications", "In Section 2 we first discuss Indonesian mor phology followed by a brief explanation of two level morphology in Section 3", "Sections 4 and 5 present our work in applying twolevel morphol ogy for the Indonesian language", "Finally Section 6 presents the results of some evaluations we carried out on our developed analyser"]}, "P99-1051": {"title": ["Acquiring Lexical Generalizations from Corpora"], "abstract": ["This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data", "We automatically acquire alternating verbs from large balanced corpora by using partial parsing methods and taxonomic information and discuss how corpus data can be used to quantify lin guistic generalizations", "We estimate the productiv ity of an alternation and the typicality of its mem bers using type and token frequencies"], "inroduction": ["Diathesis alternations are changes in the realization of the argument structure of a verb that are some times accompanied by changes in meaning Levin 1993", "The phenomenon in English is illustrated in I2 below", "The objective of this paper is to investigate the ex tent to which diathesis alternations are empirically attested in corpus data", "Using the dative and bene factive alternations as a test case we attempt to de termine a if some alternations are more frequent than others b if alternating verbs have frame pref erences and c what the representative members of an alternation are", "In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs under going the dative and benefactive alternations", "We assess the acquired frames using a filtering method presented in section 3", "The results are detailed in section 4", "Sections 5 and 6 discuss how the derived type and token frequencies can be used to estimate how productive an alternation is for a given verb se mantic class and how typical its members are", "Fi nally section 7 offers some discussion on future I a b", "2 a b John offers shares to his employe es", "John offers his empl oyees share s Leave a note for her", "Leave her a note", "work and section 8 conclusi ve remarks  2 Me thod 21 T he parser Example  1 illustrates the dative alternation which is characterized by an alternation between the prepositional frame V NPl to NP2 and the double object frame V NPl NP2", "The benefactive alterna tion cf", "2 is structurally similar to the dative the difference being that it involves the preposition for rather than to", "Levin 1993 assumes that the syntactic realiza tion of a verbs arguments is directly correlated with its meaning cf", "also Pinker 1989 for a similar pro posal", "Thus one would expect verbs that undergo the same alternations to form a semantically co herent class", "Levins study on diathesis alternations has influenced recent work on word sense disam biguation Dorr and Jones 1996 machine transla tion Dang et al 1998 and automatic lexical ac quisition McCarthy and Korhonen 1998 Schulte im Walde 1998", "The partofspeech tagged version of the British Na tional Corpus BNC a 100 million word collec tion of written and spoken British English Burnard 1995 was used to acquire the frames characteris tic of the dative and benefactive alternations", "Sur face syntactic structure was identified using Gsearch Keller et al 1999 a tool which allows the search of arbitrary POStagged corpora for shallow syntac tic patterns based on a userspecified contextfree grammar and a syntactic query", "It achieves this by combining a leftcomer parser with a regular ex pression matcher", "Depending on the grammar specification ie re cursive or not Gsearch can be used as a full context free parser or a chunk parser", "Depending on the syn tactic query Gsearch can parse full sentences iden tify syntactic relations eg verbobject adjective noun or even single words eg all indefinite pro nouns in the corpus", "Gsearch outputs all corpus sentences containing substrings that match a given syntactic query", "Given two possible parses that begin at the same point in the sentence the parser chooses the longest match", "If there are two possible parses that can be produced for the same substring only one parse is returned", "This means that if the number of ambiguous rules in the grammar is large the correctness of the parsed output is not guaranteed", "22 Acquisition", "We used Gsearch to extract tokens matching the patterns V NPl NP2 VP NPl to NP2 and V NPl for NP2 by specifying a chunk grammar for recognizing the verbal complex and NPs", "POStags were retained in the parsers output which was post processed to remove adverbials and interjections", "Examples of the parsers output are given in 3", "Although there are cases where Gsearch produces the right parse cf", "3a the parser wrongly iden tifies as instances of the double object frame to kens containing compounds cf", "3b bare relative clauses cf", "3c and NPs in apposition cf", "3d", "Sometimes the parser attaches prepositional phrases to the wrong site cf", "3e and cannot distinguish between arguments and adjuncts cf", "3f or be tween different types of adjuncts eg temporal cf", "3f versus benefactive cf", "3g", "Erroneous output also arises from tagging mistakes", "3 a The police driver v shot NP Jamie NP a look of enquiry which he missed", "b Some also v offer NPa free bus NP ser vice to encourage customers who do not have their own transport", "c A Jaffna schoolboy v shows NP a draw ing NP he made of helicopters strafing his home town", " d For the latter catalogue Barr v chose NP the Surrealist writer NP Georges Hugnet to write a historical essay", "e It v controlled NP access pp to NP the vault", "PPs cf", "3e using Hindle and Roaths 1993 lex ical association score cf", "section 24", "Finally we recognized benefactive PPs cf", "3g by exploiting the WordNet taxonomy cf", "section 25", "23 Guessing the double object frame", "We developed a process which assesses whether the syntactic patterns called cues below derived from the corpus are instances of the double object frame", "Linguistic Heuristics", "We applied several heuris tics to the parsers output which determined whether corpus tokens were instances of the double object frame", "The Reject heuristics below identified er roneous matches cf", "3bd whereas the Accept heuristics identified true instances of the double ob ject frame cf", "3a", "1", "Reject if cue contains at least two proper", "names adjacent to each other eg killed Henry Phipps"]}, "P99-1061": {"title": ["A Bag of Useful Techniques for Efficient and Robust Parsing"], "abstract": ["This paper describes new and improved techniques which help a unificationbased parser to process input efficiently and robustly", "In combination these methods result in a speedup in parsing time of more than an order of magnitude", "The methods are correct in the sense that none of them rule out legal rule applications"], "inroduction": ["This paper describes several generallyapplicable techniques which help a unification based parser to process input efficiently and robustly", "As well as presenting a number of new methods we also report significant improvements we have made to existing techniques", "The methods preserve correctness in the sense they do not rule out legal rule applications", "In particular none of the techniques involve statistical or approximate processing", "We also claim that these methods are independent of the concrete parser and neutral with respect to a given unificationbased grammar theoryformalism", "How can we gain reasonable efficiency in parsing when using large integrated grammars with several thousands of huge lexicon entries", "Our belief is that there is no single method which achieves this goal alone", "Instead we have to develop and use a set of cheap filters which are correct in the above sense", "As we indicate in section 10 combining these methods leads to a speedup in parsing time and reduction of space consumption of more than an order of magnitude when applied to a mature well engineered unificationbased parsing system", "We have implemented our methods as extensions to a HPSG grammar development environment Uszkoreit et al 1994 which employs a sophisticated typed feature formalism Krieger and Schifer 1994 Krieger and Schifer 1995 and an advanced agendabased bottomup chart parser Kiefer and Scherf 1996", "A specialized runtime version of this system is currently used in VERBMOBIL as the primary deep analysis component", "I In the next three sections we report on transformations we have applied to the knowledge base grammarlexicon and on modifications in the core formalism unifier type system", "In Section 58 we describe how a given parser can be extended to filter out possible rule applications efficiently before performing expensive unification", "Section 9 shows how to compute best partial analyses in order to gain a certain level of robustness", "Finally we present empirical results to demonstrate the efficiency gains and speculate on extensions we intend to work on in the near future", "Within the different sections we refer to three corpora we have used to measure the effects of our methods", "The reference corpora for English German and Japanese consist of 12005000 samples"]}, "PE2006_p00": {"title": ["FiniteState Registered  Automata"], "abstract": ["We introduce finitestate registered automata FSRAs a new computational device within the framework of finitestate technology specifically tailored for implementing nonconcatenative morphological processes", "This model extends and augments existing finitestate techniques which are presently not optimized for describing this kind of phenomena", "We first define the model and discuss its mathematical and computational properties", "Then we provide an extended regular language whose expressions denote FSRAs", "Finally we exemplify the utility of the model by providing several examples of complex morphological and phonological phenomena which are elegantly implemented with FSRAs"], "inroduction": ["Finitestate FS technology has been considered adequate for describing the morphological processes of the worlds languages since the pioneering works of Koskenniemi 1983 and Kaplan and Kay 1994", "Several toolboxes provide extended regular expression description languages and compilers of the expressions to finitestate automata FSAs and transducers FSTs Karttunen et al 1996 Mohri 1996 van Noord and Gerdemann 2001a", "While FS approaches to most natural languages have generally been very successful it is widely recognized that they are less suitable for nonconcatenative phenomena in particular FS techniques are assumed not to be able to efficiently account for the nonconcatenative word formation processes that Semitic languages exhibit Lavie et al 1988", "While much of the inflectional morphology of Semitic languages can be rather straightforwardly described using concatenation as the primary operation the main word formation process in such languages is inherently nonconcatenative", "The standard account describes words in Semitic languages as combinations of two morphemes a root and a pattern1 The root consists of consonants only by default three although longer roots are known", "The pattern is a combination of vowels and possibly consonants too with slots into which the root consonants can be inserted", "Words are created by interdigitating roots into patterns The first consonant of the root is inserted into the first consonantal slot of the pattern the second root consonant fills the second slot and the third fills the last slot", "After the root combines with the pattern some  Department of Computer Science University of Haifa 31905 Haifa Israel", "Email yaelccshaifaacil", " Department of Computer Science University of Haifa 31905 Haifa Israel", "Email shulycshaifaacil", "1 An additional morpheme vocalization is used to abstract the pattern further for the present purposes", "this distinction is irrelevant", "Submission received 17 August 2004 revised submission received 15 June 2005 accepted for publication 26 September 2005", " 2006 Association for Computational Linguistics Figure 1 Nave FSA with duplicated paths", "morphophonological alternations take place which may be nontrivial but are mostly concatenative", "The major problem that we tackle in this work is mediumdistance dependencies whereby some elements that are related to each other in some deeplevel representation eg the consonants of the root are separated on the surface", "While these phenomena do not lie outside the descriptive power of FS systems navely implementing them in existing finitestate calculi is either impossible or at best results in large networks that are inefficient to process as the following examples demonstrate", "Example 1 We begin with a simplified problem namely accounting for circumfixes", "Consider three Hebrew patterns haDDaDa hitDaDaDut and miDDaD where the empty boxes indicate the slots in the patterns into which the consonants of the roots are inserted", "Hebrew orthography2 dictates that these patterns be written hDDDa htDDDut and mDDD respectively ie the consonants are inserted into the D slots as one unit ie the patterns can be viewed as circumfixes", "An automaton that accepts all the possible combinations of threeconsonant stems and these three circumfixes is illustrated in Figure 13 Given r stems and p circumfixes the number of its states is 2r  2p  2 ie increases linearly with the number of stems and circumfixes", "The number of arcs in this automaton is 3rp  2p ie also Orp", "Evidently the three basic different paths that result from the three circumfixes have the same body which encodes the stems", "An attempt to avoid the duplication of paths is represented by the automaton of Figure 2 which accepts the language denoted by the regular expression ht  h  mrootut  a  E", "The number of states here is 2r  4 ie is independent of the number of circumfixes", "The number of arcs is 3r  2p that is Or  p and thus the complexity of the number of arcs is also reduced", "Obviously however such an automaton overgenerates by accepting also invalid words such as mDDDut", "In other words it ignores the dependencies which hold between prefixes and suffixes of the same circumfix", "Since finitestate devices have no 3 This is an oversimplified example in practice the process of combining roots with patterns is highly", "idiosyncratic like other derivational morphological processes", "50 Figure 2 Overgenerating FSA", "memory save for the states there is no simple and spaceefficient way to account for such dependencies", "Example 2 Consider now a representation of Hebrew where all vowels are explicit eg the pattern hitDaDeD", "Consider also the roots rgz bl and gbr The consonants of a given root are inserted into the D slots to obtain bases such as hitragez hitbael and hitgaber", "The finite state automaton of Figure 3 is the minimized automaton accepting the language it has fifteen states", "If the number of three letter roots is r then a general automaton accepting the combinations of the roots with this pattern will have 4r  3 states and 5r  1 arcs", "Notice the duplicated arcs which stem from copying the pattern in the different paths", "Example 3 Another nonconcatenative process is reduplication The process in which a morpheme or part of it is duplicated", "Full reduplication is used as a pluralization process in Malay and Indonesian partial reduplication is found in Chamorro to indicate intensivity", "It can also be found in Hebrew as a diminutive formation of nouns and adjectives kel eb kl ab la b a pa n p an pa n z a q a n zq an qa n  a x o r x ar xa r dog pu pp y ra bb it bu nn y b e a r d go at ee bl a c k da rk qa ta n litt le q ta n ta n ti n y Let  be a finite alphabet", "The language L  ww  w   is known to be transregular therefore no finitestate automaton accepts it", "However the language Ln  ww  w   w  n for some constant n is regular", "Recognizing Ln is a finite approximation of the general problem of recognizing L The length of the words in natural languages can in most cases be bounded by some n  N hence the amount of reduplication in natural languages is practically limited", "Therefore the descriptive power of Ln is sufficient for the amount of reduplication in natural languages by Figure 3 FSA for the pattern hitDaDeD", "51 constructing Ln for a small number of different ns", "An automaton that accepts Ln can be constructed by listing a path for each accepted string since  and n are finite the number of words in Ln is finite", "The main drawback of such an automaton is thegrowth in its size as  and n increase The number of strings in Ln is  n  Thus finite state techniques can account for limited reduplication but the resulting networks are spaceinefficient", "As a final nonlinguistic motivating example consider the problem of nbit incrementation introduced by Kornai 1996", "Example 4 The goal of this example is to construct a transducer over   0 1 whose input is a 32 bit binary number and whose output is the result of adding 1 to the input", "A transducer that performs addition by 1 on binary numbers has only 5 states and 12 arcs4 but this transducer is neither sequential nor sequentiable", "The problem is that since the input is scanned left to right but the carry moves right to left the output of the first bit has to be delayed possibly even until the last input bit is scanned", "Thus for an nbit binary incrementor 2n disjunctions have to be considered and therefore a minimized transducer has to assign a separate state to each combination of bits resulting in 2n states and a similar number of transitions", "In this work we propose a novel FS model which facilitates the expression of mediumdistance dependencies such as interdigitation and reduplication in an efficient way", "Our main motivation is theoretical ie reducing the complexity of the number of states and arcs in the networks we show that these theoretical contributions result in practical improvements", "In Section 3 we define the model formally show that it is equivalent to FSAs and define many closure properties directly5 We then Section 4 define a regular expression language for denoting FSRAs", "In Section 5 we provide dedicated regular expression operators for some nonconcatenative phenomena and exemplify the usefulness of the model by efficiently accounting for the motivating examples", "In Section 6 we extend FSRAs to transducers", "The model is evaluated through an actual implementation in Section 7", "We conclude with suggestions for future research"]}, "PEAAI_n09": {"title": ["Engineering Applications of  Articial Intelligence 51 2016 5057"], "abstract": ["The present work describes a classication schema for irony detection in Greek political tweets", "Our hypothesis states that humorous political tweets could predict actual election results", "The irony detection concept is based on subjective perceptions so only relying on human annotator driven labor might not be the best route", "The proposed approach relies on limited labeled training data thus a semisupervised approach is followed where collectivelearning algorithms take both labeled and unlabeled data into consideration", "We compare the semi supervised results with the supervised ones from a previous research of ours", "The hypothesis is evaluated via a correlation study between the irony that a party receives on Twitter its respective actual election results during the Greek parliamentary elections of May 2012 and the difference between these results and the ones of the preceding elections of 2009"], "inroduction": ["Irony as a paralinguistic element is used to guratively express a concept with a semantic meaning that is very different from its actual initial purpose", "It is a challenging eld for computational linguistics and natural language processing due to the high ambiguity and the difculty to detect it objectively", "Language use is vigorous and creative there is no predened consensual agreement on how to recognize an ironic expression due to the high subjectivity involved", "In the last decade irony expression has been thriving on social networks and particularly Twitter because of the 140 characters restraint on the status updates being a perfect t for good old one liners", "As a public social medium users realize that their writings may be read and reproduced by potentially everyone gaining popularity and followers", "But this publicity contrary to Facebooks real name policy often has no direct consequences to their everyday lives since the majority participate anonymously using an avatar and a nickname", "This nocensorship state contributes to the freedom of expressing personal thoughts on tough taboo unpopular or controversial issues part of which contains the political satire", "n Corresponding author", "Tel  30 6946354612", "D Spathis", "Email addresses p11charioniogr B Charalampakis p11spationiogr sdimitriscsdauthgr D Spathis p11kousioniogr E Kouslis kermanioniogr K Kermanidis", "Political satire is a signicant part of comedy which specializes in drawing entertainment from politics", "Most of the times it aims just to please", "By nature it does not offer a constructive view by itself when it is used as part of criticism it tends to simply pinpoint the unexpected or different", "The high topicality of Twitter combined with the ephemerality of political news forms a state which is described asecho chamber a groupthinking effect on virtually enclosed spaces ampli ed by repetition Colleoni et al 2014", "As a result the occasional user might write something political just to jump on the bandwagon without an initial conscious aim to criticize", "Adding to that politics is a topic that almost everybody is familiar with and makes more sense from the engagement and attention side to write about Obama instead of an obscure book you just read", "Studies focus on the simultaneous usage of Twitter and the TV on circumstances like a political debate where metatalk tweets reveal critical scrutiny of the agenda or the debate about the debate Kalsnes et al 2014", "In the rapidly changing web there is a plethora of available text especially from social networks which is unlabeled raw or unprocessed", "Adding to the traditional supervised methods there are quite a few techniques that enable us to take these huge unstructured data into account", "An insight from our previous work was the subjectivity involved during the tagging of a text as ironic", "Three of our authors who took up the tedious task of annotation could not agree on what should be considered as ironic or not", "As a result there cannot be a gold standard corpus of ironic tweets", "This was our main motivation to explore semisupervised techniques since they take httpdxdoiorg101016jengappai201601007 09521976 2016 Elsevier Ltd All rights reserved", "into account both train and test data", "To be specic the technique we chose is collective classication a type of semisupervised learning that presents an interesting method for optimizing the classication of partiallylabeled data", "Considering the above our empirical study tries to detect irony on a corpus of Greek political tweets by training a classier using appropriate linguistic features some of which are proposed for the rst time herein for irony detection", "Our goal is to nd a relation between the ironic tweets that refer to the political parties and leaders in Greece in the preelection period of May 2012 and their actual election results", "We compare the semisupervised results with the supervised ones from a previous research of ours", "Regarding the novelty of our study this is a rst exploration on the eld of irony detection with semisupervised learning and an application in politics", "The remainder of this paper is organized as follows In Section 2 we present the related literature on the topics of irony detection Twitter sentiment analysis and political expression", "The next Sections 3 and 4 are dedicated to data preprocessing and its representation schema through the set of linguistic features that affect irony detection", "The Section 5 describes the training procedure the evaluation of the algorithms performance and their test procedure on a large unlabeled dataset", "An overview of the study limitations future research prospects and a summary of the empirical study are described in Section 6"]}, "PMTS_n09": {"title": ["Domain Adaptation in Statistical Machine Translation of UserForum Data"], "abstract": ["This paper reports experiments on adapting components of a Statistical Machine Translation SMT system for the task of translating online usergenerated forum data from Symantec", "Such data is monolingual and differs from available bitext MT training resources in a number of important respects", "For this reason adaptation techniques are important to achieve optimal results", "We investigate the use of mixture modelling to adapt our models for this specic task", "Individual models created from different indomain and outofdomain data sources are combined using linear and loglinear weighting methods for the different components of an SMT system", "The results show a more profound effect of language model adaptation over translation model adaptation with respect to translation quality", "Surprisingly linear combination outperforms loglinear combination of the models", "The best adapted systems provide a statistically signicant improvement of 178 absolute BLEU points 685 relative and 273 absolute BLEU points 805 relative over the baseline system for EnglishGerman and EnglishFrench respectively"], "inroduction": ["In recent years Statistical Machine Translation SMT technology has been used in many online applications concentrating on professionally edited enterprise quality online content", "At the same time very little research has gone into adapting Work done while at CNGL School of Computing DCUSMT technology to the translation of user generated content on the web", "While translation of online chats Flournoy and CallisonBurch 2000 has received some attention there is surprisingly little work on translation of online user forum data despite growing interest in the area Flournoy and Rueppel 2010", "In this paper we describe our efforts in building a system to address this particular application area", "Our experiments are conducted on data collected from online forums on Symantec Security tools and services1 For a multinational company like Symantec the primary motivation behind translation of user forum data is to enable access across language barriers to information in the forums", "Forum posts are rich in information about issues and problems with tools and services provided by the company and often provide solutions to problems even before traditional customercare help lines are even aware of them", "The major challenge in developing MT systems for user forum data concerns the lack of proper parallel training material", "Forum data is monolingual and hence cannot be used directly to train SMT systems", "We use parallel training data in the form of Symantec Enterprise Translation Memories TMs from different product and service domains to train the SMT models", "As an auxiliary source we also used portions of the Europarl dataset2 Koehn 2005 selected according to their similarity with the forum data Section 32 to supplement the TM based training data", "Symantec TM data being a part of enterprise documentation is professionally 1 httpcommunitynortoncom 2 httpwwwstatmtorgeuroparl edited and by and large conforms to the Symantec controlled language guidelines and is signicantly different in nature from the user forum data which is loosely moderated and does not use controlled language at all", "In contrast Europarl data is out ofdomain with respect to the forum data", "The differences between available training and test datasets necessitate the use of adaptation techniques for optimal translation", "We use mixture model adaptation Foster and Kuhn 2007 creating individual models from different sources of data and combining them using different weights", "Monolingual forum posts were used for language modelling along with the target side of the TM training data", "A system trained only on the Symantec TM and forum data serves as the baseline system", "All our experiments are conducted on the EnglishGerman En De and EnglishFrench EnFr language pairs with a special emphasis on translation from English", "For the sake of completeness however we report translation scores for both directions here", "Apart from using models created from concatenation of indomain Symantec TM and outof domain Europarl datasets we used linear and log linear combination frameworks to combine individual models", "Both translation models and language models were separately combined using the two methods and the effect of the adaptation was measured on the translation output using established automatic evaluation metrics", "Our experiments reveal that for the current task in terms of translation quality language model adaptation is more effective than translation model adaptation and linear combination performs slightly better than the loglinear setting", "The remainder of this paper is organized as follows Section 2 briey describes related work relevant to the context", "Section 3 reports the tools and algorithms used along with a description of the datasets used", "Section 4 focuses on the mixture modelling experiments and how weights are learnt in different settings", "Section 5 presents the experiments and analysis of results followed by conclusions and future work in Section 6"]}, "PS15684_w09": {"title": ["Paraphrase Extraction using fuzzy hierarchical clustering"], "abstract": ["Paraphrase Extraction involves the discovery of equivalent text segments from large corpora and nds application in tasks such as multidocument summarization and document clustering", "Semantic similarity identication is a challenging problem which is further compounded by the large size of the corpus", "In this paper a two stage approach which involves clustering followed by Paraphrase Recognition has been proposed for extraction of sentencelevel paraphrases from text collections", "In order to handle the ambiguity and inherent variability of natural language a fuzzy hierarchical clustering approach which combines agglomeration based on verbs and division on nouns has been used", "Sentences within each resultant cluster are then processed by a machinelearning based Paraphrase Recognizer to discover the paraphrases", "The twostage approach has been applied on the Microsoft Research Paraphrase Corpus and a subset of the Microsoft Research Video Description Corpus", "The performance has been evaluated against an existing kmeans clustering approach as well as cosinesimilarity technique and Fuzzy CMeans clustering and the two stage system has consistently demonstrated better performance", "2015 Elsevier BV All rights reserved"], "inroduction": ["Vast amounts of natural language text are available on the web as well as in largescale repositories much of which is redundant", "The task of Paraphrase Extraction focuses on identication of text units which convey the same meaning", "The detection of similar text is complicated due to the rich variability of natural languages", "The large scale of the corpora is another factor which poses hurdles in Paraphrase Extraction", "Performing oneonone matching of sentences is practically ruled out even if it is assumed that a suitable Paraphrase Recognition system is available", "Therefore efcient techniques are required to identify possibly similar candidates from large scale corpora and then subject them to further processing to detect exact matches", "An effective Paraphrase Extraction system will benet various Natural Language Processing applications such as multidocument summarization plagiarism detection question answering and document clustering", "The signicant aspect of this work is that a novel twolevel fuzzy clustering technique has been proposed for sentencelevel Paraphrase Extraction", "As similar sentences tend to describe the same or similar actions Fuzzy Agglomerative Clustering based on verbs  Corresponding author", "Tel 91 9443760000", "Email addresses ctrpsggmailco A Chitra anupriya rajkumaryahoocoin A Rajkumar", "1 Tel 91 9843222273", "is performed initially", "Divisive clustering is then applied to identify subgroups of sentences which center on the same nouns", "A Support Vector Machine SVM based Paraphrase Recognizer is then used to identify the paraphrases within each cluster", "The performance of the Paraphrase Extraction system has been evaluated using the Microsoft Research Paraphrase Corpus MSRPC and a subset of the Microsoft Research Video Description Corpus MSRVDC", "The outline of the paper is as follows Section 2 contains an overview of previous work related to Paraphrase Extraction and Hierarchical Clustering", "Section 3 describes the methodology adopted for extracting paraphrases using a fuzzy hierarchical clustering approach and machine learning based Paraphrase Recognizer", "Section 4 presents the results of experiments conducted using two different corpora", "Possible directions for future work and applications are discussed in Section 5 which concludes the paper"]}, "PSMPT_n09": {"title": ["Supertags as Source Language Context in Hierarchical PhraseBased SMT"], "abstract": ["Statistical machine translation SMT models have recently begun to include source context modeling under the assumption that the proper lexical choice of the translation for an ambiguous word can be determined from the context in which it appears", "Various types of lexical and syntactic features have been explored as effective source context to improve phrase selection in SMT", "In the present work we introduce lexicosyntactic descriptions in the form of supertags as sourceside context features in the stateoftheart hierarchical phrasebased SMT HPB model", "These features enable us to exploit source similarity in addition to target similarity as modelled by the language model", "In our experiments two kinds of supertags are employed those from lexicalized treeadjoining grammar LTAG and combinatory categorial grammar CCG", "We use a memorybased classification framework that enables the efficient estimation of these features", "Despite the differences between the two supertagging approaches they give similar improvements", "We evaluate the performance of our approach on an EnglishtoDutch translation task and report statistically significant improvements of 448 and 63 BLEU scores in translation quality when adding CCG and LTAG supertags respectively as contextinformed features"], "inroduction": ["The stateoftheart hierarchical phrasebased SMT model Chiang 2007 uses the bilingual phrase pairs of phrasebased SMT PBSMT Koehn et al 2003 as a starting point to learn hierarchial rules using probabilistic synchronous contextfree grammar PSCFG", "The decoding process in the hierarchical phrasebased SMT HPB model is based on bottomup chart parsing Chiang 2007", "This chart parsing decoder also known as Hiero does not require explicit syntactic representation on either side of the phrases in rules", "Stateoftheart SMT models Koehn et al 2003 Chiang 2007 can be viewed as loglinear combinations of features Och and Ney 2002 that usually comprise translational features and the language model", "The translational features typically involved in these models express dependencies between the source and target phrases but not dependencies between the phrases in the source language themselves ie they do not take into account the contexts of those phrases", "Word sense disambiguation WSD a task intricately related to MT typically employs rich context sensitive features to determine contextually the most likely sense of a polysemous word", "Inspired by these contextrich WSD techniques researchers have tried to integrate various contextual knowledge sources into stateoftheart SMT models", "In recent years source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account", "These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information positionspecific neighbouring words Gimenez and Marquez 2007 Stroppa et al 2007 shallow and deep syntactic features Gimpel and Smith 2008 full sentential context Carpuat and Wu 2007 lexical syntactic descriptions in the form of supertags Haque et al 2009a and grammatical dependency relations Haque et al 2009b", "A limitation that Hiero Chiang 2007 shares with the PBSMT model Koehn et al 2003 is that it does not take into account the contexts in which the sourcesides of the rules appear", "In other words it can be argued that rule selection in Hiero is suboptimally modelled", "So far a small number of studies have made use of sourcelanguage context for improving rule selection in Hiero", "Positionspecific neighbouring words and their partofspeech POS prove to be effective source contexts in the HPB model He et al 2008", "In a study involving PBSMT Haque et al", "2009b showed that the translations of ambiguous words are also influenced by more distant words in the sentence", "Syntactic contexts that capture longdistance dependencies between words in a sentence can be a useful means to disambiguate among translations", "Accordingly integration of such syntactic contexts could lead to improved translation quality in PBSMT", "For instance Haque et al", "2009a showed that supertags are more powerful source contexts than neighbouring words and partofspeech tags to disambiguate a source phrase in PBSMT", "Inspired by Haque et al 2009a in the present work we extend the stateoftheart Hiero system by adopting its lexical entries with the robust and efficient supertagging approaches", "Grammars in these approaches consist of a syntactically rich lexicon and a small set of combinatory operators", "These combinatory rules combine syntactically rich lexical entries together to form parse trees", "Supertaggers assign a syntactic structure an elementary tree or a lexical category to each word in a sentence", "These syntactic structures supertag provide rich and complex linguistic information that describe the POS tag of a word its subcategorisation information and the hierarchy of phrase categories in which the word appears", "The remainder of the paper is organized as follows", "In Section 2 we discuss related work", "Section 3 provides a brief overview of HPB", "In Section 4 we describe the contextinformed features contained in our baseline HPB model", "In Section 5 we de scribe our memorybased classification approach", "Section 6 describes experimental setups", "Section 7 presents the results obtained and offers a brief qualitative analysis", "In Section 8 we formulate our conclusions and offer some avenues for further work"]}, "PSS07_w06": {"title": ["Moving from Textual Relations to Ontologized Relations"], "abstract": ["There has been recent research in openended information extraction from text that finds relational triples of the form arg1 relation phrase arg2 where the relation phrase is a text string that expresses a relation between two arbitrary noun phrases", "While such a relational triple is a good first step much further work is required to turn such a textual relation into a logical form that supports inferencing", "The strings from arg1 and arg2 must be normalized disambiguated and mapped to a formal taxonomy", "The relation phrase must likewise be normalized and mapped to a clearly defined logical relation", "Some relation phrases can be mapped to a set of predefined relations such as Part0f and Causes", "We focus instead on arbitrary relation phrases that are discovered from text", "For this we need to automatically merge synonymous relations and discover metaproperties such as entail ment", "Ultimately we want the coverage of a bottomup approach together with the rich set of axioms associated with a topdown approach", "We have begun exploratory work in ontologizing the output of TextRunner an open information extraction system that finds arbitrary relational triples from text", "Our test domain is 25 million Web pages on health and nutrition which yields relational triples such as orange contains vitamin C and fruits are rich in antioxidants", "We automatically disambiguate the strings arg1 and arg2 mapping them to WordNet synsets", "We also learn entailments between normalized relation strings eg be rich in entails contain", "This enhanced ontology enables reasoning about relationships that are not seen in the corpus but can be inferred by inheritance and entailment", "Further we define ontologybased relationships between the extracted triples themselves and experimentally show that these can be used in significantly improving probability estimation for the triples"], "inroduction": ["A dream from the early days of Artificial Intelligence is to build a system that can read autonomously with deep understanding", "In the 1970s and 1980s natural language processing NLP research tackled this problem directly but in a way that was too brittle to apply to unrestricted text", "While we may dismiss this body of research as toy systems that required carefully selected input text they genuinely performed deep reasoning about those texts", "Since the 1990s the mainstream of NLP research shifted to robust information extraction IE systems with shallow reasoning often statistically based", "Such IE systems can handle unseen text that the earlier NLP systems could not handle but have the more modest goal of scanning for a limited number of predefined relations and ignoring everything else", "Current IE systems have hardly any mechanisms to make inferences beyond what is explicitly stated in the text", "However the time may be ripe to combine the best of both eras and build robust NLP systems that support deep reasoning", "Consider textual relations with instances in the form arg1 relation phrase arg2 that have text strings for the arguments and the relation phrase", "The argument strings must be normalized and mapped to a formal taxonomy to enable inheritancebased reasoning", "Similarly the relation string must be normalized and mapped to an axiomatized set of relations where possible", "Learning entailments between relations can enable further inferencing", "The normalized instances of relations must be converted into probabilistic logical statements with clearly defined semantics", "Suppose we have an instance X relation Y where X and Y are mapped to nonleaf nodes of a WordNetlike taxonomy", "Does this mean that for all instances of X there exists an instance of Y where the relationship holds or some other combination of universal and existential quantifiers", "The ontologized instances should be in a format suitable for an inference engine that can make plausible inferences", "We have taken some early steps in this research direction but the bulk of these ideas still need to be tested and validated", "The remainder of this paper is organized as follows", "Section 2 gives a brief overview of NLP research from the 1970s up to the present", "Section 3 presents a case study of the steps needed to transform textual relations into ontologized relations", "Section 4 gives some preliminary results where we use relationships between the ontologized relations for better confidence estimation of these relations", "We conclude in Section 5 with some general discussion"]}, "PTASL_n09": {"title": ["SyntaxBased Translation With Bilingually"], "abstract": ["Syntaxbased models can significantly improve the translation performance due to their grammatical modeling on one or both language sides", "However the translation rules such as the nonlexical rule   in stringtotree models do not consider any lexicalized information on the source or target side", "The rule is so generalized that any subtree rooted at VP can substitute for the nonterminal  Because rules containing nonterminals are frequently used when generating the targetside tree structures there is a risk that rules of this type will potentially be severely misused in decoding due to a lack of lexicalization guidance", "In this article inspired by lexicalized PCFG which is widely used in monolingual parsing we propose to upgrade the STSG synchronous tree substitution grammarsbased syntax translation model with bilingually lexicalized STSG", "Using the stringtotree translation model as a case study we present generative and discriminative models to integrate lexicalized STSG into the translation model", "Both small and largescale experiments on ChinesetoEnglish translation demonstrate that the proposed lexicalized STSG can provide superior rule selection in decoding and substantially improve the translation quality", "Index TermsBilingually lexicalized synchronous tree substitution grammars discriminative model generative model syntax based statistical machine translation"], "inroduction": ["N RECENT years interest in the syntaxbased translation models has flourished and these models have led to encouraging progress in the improvement of translation quality 2 4 8 9 13 15 17 21 29 35 38 39", "According to 7 and 14 translations using syntaxbased models can be cast as a parsing problem", "Depending on whether the input is a string or a parse tree we divide these models into two categories treebased parsing models and stringbased parsing models", "Manuscript received September 07 2012 revised December 13 2012 and March 11 2013 accepted March 17 2013", "Date of publication March 28 2013 date of current version April 25 2013", "This work was supported by the Natural Science Foundation of Chinaunder Grant 60975053 and the HiTech Research and Development Program 863 Program of Chinaunder Grants 2011AA01A207 and 2012AA011102", "The associate editor coordinating the review of this manuscript and approving it for publication was Dr Gokhan Tur", "The authors are with the National Laboratory of Pattern Recognition Institute of Automation Chinese Academy of Sciences", "Beijing 100190 China email jjzhangnlpriaaccn", "Color versions of one or more of the figures in this paper are available online at httpieeexploreieeeorg", "Digital Object Identifier 101109TASL20132255283 Treebased parsing models include treetostring models 10 15 and treetotree models 7 13 35", "Both of these two types of models are based on synchronous tree substitution grammars STSG", "Given the syntactic tree of a source sentence the treebased parsing algorithm traverses each node in a topdown manner and identifies all of the translation rules that match the local subtree rooted at the node", "Using the matched translation rules the algorithm either constructs a target tree treetotree or directly generates the best target string treetostring", "Stringbased parsing models include stringtostring models 2 and stringtotree models 8 16 22", "Chiangs stringtostring model the hierarchical phrasebased model is constructed based on a degraded synchronous contextfree grammar SCFG without any linguistic information", "The stringtotree model is a stateoftheart syntaxbased translation model designed to explicitly model the target grammar", "This translation model is typical of the STSGbased models 5 8 24", "We will use the model to illustrate our ideas throughout this paper", "In stringtotree translation models an STSG rule consists of two righthand sides known as the sourcehand and target hand sides", "Traditionally both sides must follow tree substitution grammar TSG rules", "However in stringtotree translation rules the target side follows a TSG rule while the source side follows a CFG rule", "Because CFG rules can be understood as simplified TSG rules following Xiao et al 24 we use the terms STSG rule and STSG to denote the translation rules and grammar respectively in the stringtotree model", "In this model the translation problem is similar to a monolingual parsing problem", "Fig", "1 provides a comparison between monolingual parsing and stringtotree translation", "Both methods convert a string into a tree structure using grammar rules", "The difference is that monolingual parsing applies PCFG rules to the conversion of an English string into an English parse tree whereas the stringtotree translation model parses the Chinese string using the sourceside of the STSG rules and synchronously generates an English tree using the targetside of the STSG rules", "In the monolingual parsing community the PCFG model is often criticized for its lack of lexicalization1 in expressing syntactic preferences for lexical words especially when constructing the highlevel tree nodes", "To overcome this deficiency lexicalized 1Here lexicalization means considering lexicon information when building the tree nodes", "155879163100  2013 IEEE Fig", "1", "a An example in which an English sentence is parsed into a tree structure with 12 PCFG rules b an instance in which a Chinese sentence both Chinese characters and Chinese Pinyin are provided and note that we will use Chinese Pinyin throughout the paper is converted into an English tree using 6 STSG rules", "The symbol to the upper right of a node indicates that this node is constructed using rule  PCFG models such as the Collins Model 13 6 have been proposed and achieve the stateoftheart monolingual parsing performance", "Similar to the monolingual PCFG parsing models the STSG model used in the stringtotree translation also faces the problem of lacking lexicalization", "For example rule in Fig", "1b specifies that if the substring preceding the Chinese word can be translated into an English prepositional phrase  then we can use this rule to translate the Chinese string into the English verb phrase were killed  Note that it is correct to use this rule only when the Chinese word that is translated into the English preposition indicates the passive voice  in Fig", "1b for instance", "Otherwise it is incorrect to apply this rule", "For instance in the string  the Chinese substring preceding can be trans superior rule selection for constructing the target trees and can considerably improve the translation quality", "The main contributions of our paper are as follows 1 In the STSGbased stringtotree translation model we propose to incorporate both source and targetside lexicalized information into the STSG rules to alleviate the overgeneralization problem of STSG and enable the model to choose appropriate translation rules during decoding", "2 To evaluate bilingually lexicalized STSG more thoroughly we not only propose a generative model using the adapted Collins Model 1 but also introduce a discriminative model that can incorporate arbitrary lexicalized features", "The remainder of this article is organized as follows", "Section II describes the related work and Section III provides a brief overview of STSGbased stringtotree translation models and explains in detail why bilingually lexicalized parsing models are needed", "In Section IV we elaborate on the lated into an English prepositional phr  but the proposed bilingually lexicalized parsing models in which both Chinese word indicates the active voice", "Without considering lexicalization such as which lexical word generates the English preposition rule is used and the following poor translation may result the police were killed in the morning gunmen", "This detailed example illustrates the importance of lexicalized information in STSG rule selection in the stringtotree translation", "Inspired by the idea of lexicalized PCFG we focus on introducing bilingually lexicalized STSG into the stringtotree translation models", "For this purpose we propose generative as well as discriminative models", "Experiments on both small and largescale ChinesetoEnglish translation demonstrate that our proposed bilingually lexicalized STSG can provide generative and discriminative models are introduced", "Extensive experiments and their analysis are presented in Section V Finally we conclude in Section VI", "II", "RELATED WORK To our knowledge very few researchers investigated bilingually lexicalized STSG parsing models in the context of statistical machine translation", "Many researchers have focused on the contribution of the lexicalized monolingual parsing models from two directions", "One approach applies the lexicalized monolingual parsing model to syntaxbased language modeling 1 18 20 24", "Charniak et al 1 first attempted to improve the grammaticality of the output of a stringtotree system 28 using a lexicalized monolingual PCFG parsing model", "Och et al 18 concentrated on phrasebased models", "Both of these studies are based on the Penn Treebank trained PCFG parsing model and used the model for reranking", "However they found that the lexicalized monolingual parsing model cannot improve the BLEU score", "Post and Gildea 20 demonstrated that tree substitution grammars TSGs may be a better choice than contextfree grammars for language modeling", "Xiao et al 24 studied syntaxbased language modeling using the lexicalized monolingual parsing model with TSG", "The model parameters were directly learned from the automatically parsed target trees", "They reported that their approach can improve the translation quality of the stringtotree model 8", "The other approach employs the lexicalized monolingual parsing model in parsing the source string in the joint parsing and decoding stages 14", "This approach aims to improve the treetostring translation model by constructing parse trees on the source side with the help of the monolingual parsing model and generating translations on the target side simultaneously", "In contrast to these previous studies our approach does not use the parsing model as a language model and the model parameter learning process does not depend on the Penn Treebank", "Instead we propose a bilingually lexicalized STSG parsing model and used the model to distinguish good rules from poor ones in the decoding stage", "The model parameters can be learned easily from the extracted STSG translation rules", "In addition to the generative models we have also investigated a discriminative model", "The algorithm using the lexicalized monolingual TSG as a language model is just a special case of our generative model", "Many researchers are also concerned with the overgeneralization problem of STSG rules", "For example Chiang et al 3 designed many targetside syntax features to improve the stringtotree translation", "Rather than using a rule lexicalization model binary features such as rule overlap features badrewrite features and insertion features are employed in the algorithm", "They argue that certain nonterminals are more reliable than others for the rule overlap feature and they created a binary feature for each root nonterminal of a specified rule", "Regarding the badrewrite feature they observed that the nonterminals in certain nonlexical rules are associated only with specific lexical words and binary features were created to penalize the use of these rules", "The features proposed by Chiang et al are also languagedependent based on a thorough analysis of the tuning set", "In contrast to Chiang et al we argue that in any language pair the newly generated nonterminal in the target parse tree depends strongly on the underlying source and targetside lexical strings", "We therefore propose the languageindependent generative and discriminative lexicalized STSG models and regard these models as additional strong features to be integrated into the loglinear model", "Moreover to establish a strong baseline we also include the discount feature used by Chiang et al 3 in our baseline stringtotree model", "Other related work focuses on grammar or rule lexicalization", "An example of grammar lexicalization is the proposal of Zhang and Gildea 31 32 to improve the word alignment using lexicalized inversion transduction grammar", "In rule lexicalization the syntaxbased translation is inherently lexical ized using dependency structures 21 22 25 27", "In this paper we concentrate on the phrase structurebased stringto tree translation models that construct a phrase structure tree on the target side during decoding", "Using our proposed bilingually lexicalized parsing models we not only retain the merits of the phrase structure information in the stringtotree model but also enrich it with source and targetside lexicalized knowledge", "Other approaches consider lexical information during the rule extraction in syntaxbased translation models", "However these approaches are not focused on using the rule lexicalization model to distinguish between competing rules", "For example Wu et al 23 considered function words in foresttostring rule extraction to alleviate the errors in word alignment between sourceside words and targetside function words", "III", "THE STRINGTOTREE TRANSLATION MODEL The stringtotree translation model aims to render the target output more grammatical by explicitly constructing a parse tree on the target side", "The model accepts a source string as an input searches through all of the possible target trees and finally identifies the tree with the highest score", "This process is performed by recursively applying STSG rules that are extracted and pa rameterized using a wordaligned target sideparsed parallel training corpus", "A STSG Translation Rules Galley et al 9 proposed the GHKM algorithm for extracting minimal STSG rules from a triple  where is the sourcelanguage sentence is a targetlanguage parse tree whose yield is the translation of  and is the set of word alignments between and  The minimal stringtotree rules are extracted in three steps 1 frontier set computation 2 fragmentation and 3 extraction", "The frontier set FS is the set of frontier nodes that meet the following alignment constraint", "The target phrase dominated by the frontier node and its corresponding source phrase must be consistent with the word alignment", "The bold italic nodes in the English parse tree in Fig", "1b2 are all frontiers", "Given FS the graph composed of the triple is divided into several fragments", "Each fragment takes only nodes in FS as the root and leaf nodes", "Each fragment forms an STSG translation rule", "These rules are extracted through a depthfirst traversal of  for each frontier visited a rule is extracted using the fragmentation rooted at this frontier", "The extracted rules are known as minimal rules 9", "For example in Table I are minimal rules", "To improve the rule coverage SPMT models 16 can be employed to obtain the phrasal rules that are not covered by the GHKM rules", "In addition the minimal rules that share adjacent tree fragments can be connected together to form composed rules 8", "In Table I is a rule composed by combining and  The extracted STSG rules are associated with multiple probabilities such as the phrasallike translation probabilities and 2To some extent we abuse the example in Fig", "1b in that we use it as both a translation example and a training example", "TABLE I MINIMAL AND COMPOSED RULES EXTRACTED FROM FIG", "1b the rootnormalized translation probability", "The rootnormal ized probability is similar to the rule probability of the PCFG in monolingual parsing", "In this sense the stringto tree translation already employs a PCFGlike model", "B Decoding as Parsing Using the extracted STSG translation rules we now elaborate on the decoding process in the stringtotree model based on the example in Fig", "1b", "The decoding process is usually formalized as a deductive system that performs a bottomup CKYstyle parsing algorithm 2 8 14", "Fig", "2 illustrates the deductive steps in detail", "First axiom rules  and are employed to deduce oneword translations", "Then inference rules  and are applied to deduce twoword threeword and fourword translations", "We use inference rule for the following analysis", "The deductive step can be formalized as follows 1 where in which the subscript denotes the sourceside index is deduced using axiom rule  and is deduced using inference rule  Here and denote the score and the partial translation respectively", "Nodes NP and VP in rule are substituted by newly deduced structures and the resulting score for is calculated as follows 2 Here includes the increased language model score and the scores of the rulerelated submodels 3 where denotes the phrasallike scores and is the PCFGlike parsing model score which is indicative of whether the generated target tree is wellformed", "For the example rule  is computed as follows the model weight is not explicitly given 4 Fig", "2", "Illustration of stringtotree decoding", "From the translation process illustrated in detail in Fig", "2 we can see that when constructing the targetside lower tree nodes both source and targetside lexicalized information is used in the application of the axiom rules such as  and  and these axiom rules are therefore reliable", "However when the target tree is extended from lower to higher levels the inference rules containing nonterminals are increasingly utilized and these rules are less reliable as the inside generalized nonterminals are not informed by the lexical evidence", "For example rule does not consider any lexicalized information and the PCFGlike score cannot capture the knowledge which represents the pref erence to the source and targetside lexicalized information", "Rules of this type generally suffer from a lack of reliability", "Based on our analysis the stringtotree translation model requires rich lexicalized information for reliable application of the STSG rules during decoding", "Inspired by Collins lexical ized PCFG parsing models we therefore propose to enhance stringtotree translation models with bilingually lexicalized STSG", "IV", "DECODING WITH BILINGUALLY LEXICALIZED STSG PARSING MODELS In this section we first review the necessary background on bilingually lexicalized STSG parsing models including lexical ized STSG rule extraction and rule conversion", "We then propose two parameter estimation methods a generative model and a discriminative model", "Finally we demonstrate how to apply lexicalized STSG parsing models in decoding", "A Lexicalized STSG Rule Extraction Informally we define a lexicalized STSG rule as an STSG rule in which each nonterminal is associated with source and targetside lexicalized informationA question arises regarding the type of lexicalized informa tion that can be used", "For the targetside lexicalized training Fig", "3", "Lexicalized training example POS of target headword is not explicitly given", "parse trees the intuitive idea is to apply the headword information3 that is used in Collins lexicalized PCFG parsing models", "Before rule extraction we first use the head finding rules to annotate every interior node of the targetside parse tree with the nodes headword and its partofspeech", "For the sourceside lexicalized information we here use a heuristic and associate each node with the source word that is aligned to the target headword of the node", "However we may often encounter manytoone alignment situations when handling words in the source", "In this case we heuristically choose the source word that is aligned to the target headword of the node with the highest probability4", "Fig", "3 shows the lexicalized version of the English parse tree in Fig", "1bAfter annotating the targetside tree nodes with lexicalized in formation from both sides the extraction process for the lexical ized STSG rules is the same as that for traditional STSG rules except that the nonterminals in the lexicalized STSG rules are associated with lexical words", "For example the lexicalized version5 of rule in Table I is as follows B Rule Conversion for Parameterization Post and Gildea 20 and Xiao et al 24 adopted a 3step preprocessing approach to transform the training example target tree source string and word alignment to an equivalent structure from which we can extract SCFG rules", "This is an effective approach however the headword identification process is affected by the deletion of many necessary interior nodes", "Alternatively we can focus directly on the STSG rules", "From the decoding process shown in detail in Fig", "2 we can see that the interior nonterminals such as VBD VP and VBN in  contribute nothing to the generation of the target translation", "Therefore all of nonterminals of this type can be discarded before decoding", "In practice if a POS tag is among the useless nodes we still keep it for calculating the Collins Model rule probability", "All interior nonterminals except for the POS tags are therefore removed in each lexicalized STSG rule", "For example the following lexicalized rule is obtained for  The height of this STSG rule is one and the rule is equivalent to an SCFG rule", "However most of STSG rules in stringtotree models have a height greater than one on the target side see eg rules and in Table I", "In our training data we found that approximately 867 of the STSG rules have heights of two or more", "Because the Collins lexicalized parsing model is based on PCFG rules with heights of one we must develop an effective method of converting the STSG rules into equivalent SCFG rules of height one to apply the wellstudied lexicalized PCFG parsing models", "3DeNeefe and Knight 37 also considered headword in machine translation however their treeadjoining grammar is more complicated", "4Based on our manual analysis of 1000 randomly chosen pairs of targetside headwords and aligned sourceside words we find that the fraction of correctly aligned sourceside words is 718", "In addition to this heuristic we can also apply other methods such as pairwise mutual information and likelihood ratios to choose the sourceside lexical words", "We leave this for our future work", "5We omit targetside POS tags in the example rule for simplicity", "Using rules of this kind we can estimate and predict the Collins lexicalized PCFG model probabilities", "As Collins model is used only to model the generation of the monolingual grammar rules we must perform an additional conversion for the parameterization", "After attaching the sourceside words which are aligned to the target headwords to the targetside nonterminals the rules sourceside will be useless during parameter estimation and thus we can discard the rules sourceside for parameterization", "In the example above rule will becomes C A Generative Model We first apply Collins Model 1 6 to model the generation of the simplified lexicalized STSG translation rules", "Collins Model 1 breaks down the generation of a lexicalized grammar rule into a sequence of substeps", "More formally the generation process for the rule is described as follows TABLE II BACKOFF LEVEL FOR EACH SUBMODEL IN BILINGUALLY LEXICALIZED STSG BACKOFF LEVEL 1 Generate a head constituent label with probability  where is the label of the parent node and is its headword", "2 Generate all of the left and right modifiers with probabilities and  Here or  is the th left or right modifier and or  denotes the headword of or ", "The POS tag of is denoted by  and is the distance between the modifier and the head", "and  When incorporating Collins Model 1 into our bilingually lexicalized STSG parsing model the headword  and become the target headword and its aligned source word", "All of the parameters described above can be estimated using the maximum likelihood method based on the extracted and converted lexicalized STSG rules in contrast to using Treebank for parameterization in the monolingual parsing", "Given a trained model the probability of a rule can be formalized as follows 5 For example the generative proba bility of the rule can be calculated as follows the POS tag and distance are omitted for simplicity but in practice they are included for model training and prediction Note that the parsing model including only the targetside lexicalized information is just a backoff of our bilingually lexical ized STSG parsing model", "Traditionally researchers eg 1 20 24 have considered the targetside lexicalized parsing model as an augmented target language model in which source side lexicalized information cannot be used", "Here we see that the traditional method is a special case of our generative model", "In our experiments we compare the performance of the monolingual lexicalized STSG parsing model incorporating only the targetside lexicalized information with the bilingually lexical ized STSG parsing model to figure out whether the lexicalized information from both sides is more helpful", "D A Discriminative Model From formula 5 we can see that the generative model utilizes only the headword lexicalized information", "A natural question is whether we can incorporate other lexicalized information in addition to the headwordIn addition to the generative model which models the genera tion process of a STSG translation rule we can also consider that the usage preference of the STSG translation rules in decoding can be determined by the lexicalized string pair target English string which is spanned by the rules root node and its aligned source Chinese string ", "We can therefore employ a discriminative model for modeling the conditional distribution This model can utilize far more lexicalized information in addition to the headword in the generative model", "To simplify the calculation of the conditional probability we assume that the nonterminal nodes are conditionally independent of one another and that each nonterminal node in a rule is determined only by the target string spanned by this node and the aligned source string", "Moreover we ignore the structure of the rule eg the order of the nonterminal nodes", "The calculation of the conditional probability can then be formalized as follows 7 6 It is straightforward to show that this generative lexicalized model is highly specific and tends to suffer from data sparseness", "As suggested by Collins 6 for lexicalized PCFG we therefore design a backoff smoothing mechanism for each submodel of the proposed bilingually lexicalized STSG in Table II", "First we consider all of the conditions and then we exclude the condition based on the sourceside head information", "Levels 3 and 4 are similar to the lexicalized PCFG used in monolingual parsing", "where denotes the target string dominated only by the nonterminal node  and is the aligned source string corresponding to  Using formula 7 we can decompose the probability of a rule given a string pair into products of probability of each nonterminal in the rule given its spanned substring pair", "Accordingly we must model the conditional distribution of a nonterminal given a string pair eg ", "For example when applying the rule in Fig", "2  and  For   and with the bilingual strings", "Our task is to predict the tag", "The problem now becomes a traditional classification problem in which the classes include all of the constituent labels in the Treebank", "There are many sophisticated models designed for this problem such as SVM neural networks and maximum entropy models", "In this article we formulate the probability using a maximum entropy model 8 where is a binary feature and is its feature weight", "Any informative feature deduced from the string pair can be used", "Xiong et al 26 and Zollmann and Vogel 36 indicate that the boundary words the leftmost and rightmost words of a string are good substitutes for the entire string in phrase reordering and constituent construction", "Moreover the success of Collins parsing models shows that the headword is also an informative feature", "Inspired by their works we therefore define The training examples for the maximum entropy model can easily be extracted prior to the STSG rule extraction process", "For each frontier node recognized in the target parse tree we can derive an example of the form nodetag targetstring sourcestring from which the six representative features mentioned above can be extracted", "Consider thenode in Fig", "3 as an example", "The extracted training in stance is  and the corresponding lexicalized bilingual features are  In our largescale ex", "periment we totally extract 27382983 training examples in which there are 849952 headwords", "We utilized the toolkit of Zhang 34 to train the maximum entropy model and set the Gaussian prior to to avoid overtraining", "We measured the accuracy of the classifier using a heldout data and we get an accuracy of 768 using the six features mentioned above in the largescale experiment", "E Applying Lexicalized STSG in the Decoding Stage Based on the discussion above we know how to extract the lexicalized rules and estimate their lexicalized probabilities during the training stage", "A remaining question is how to apply the lexicalized STSG during decoding", "It is important to note that the lexicalized information eg headword boundary words is fully connected to the rules when training the lexicalized STSG models however the matching rules used during decoding should not be encoded using such lexicalized information", "If the rules used in decoding are fully annotated by both source and targetside lexicalized information then the resulting rule search space will be extremely sparseFor example assumes that we di rectly apply a rule such as  in the Fig", "4", "Examples of rules used during decoding", "rule matching during decoding", "This example rule can be used respectively", "Clearly this type of usage is not realistic", "A more appropriate method is to employ the original STSG translation rules of the stringtotree model in the rule matching while generating the bilingually lexicalized information for the rules dynamically during decoding", "For this purpose we have designed a simple algorithm which is illustrated through examples in Fig", "4", "For the purely lexicalized rules axiom rules such as rules a and b we retain the targetside headwords and aligned sourceside words of the root node of the rule note that the headwords are not combined with the root as matching units", "For other rules such as the nonlexical rule c we can simply record the position of origin of the headword information means that the headword of originates from  and if is replaced by a then by  passes to the node ", "In this way the headword information of a rule can be generated dynamically in a bottomup manner during decoding", "For the boundary words used in the discriminative model we can easily obtain them as we know which sourceside span is currently being handled and also know the targetside partial translation after applying an STSG rule", "From the bilingually lexicalized information generated for a given STSG rule we can calculate the lexicalized STSG probability for this rule using a generative model or discriminative model", "This lexicalized STSG probability serves as a strong feature similar to the ngram language model that can guide the decoder to choose the appropriate rules regarding the source and targetside lexicalized information", "V EXPERIMENTS In our experiments we incorporate the lexicalized STSG parsing models into the standard loglinear stringtotree translation model 8 16 and test the effectiveness of our proposed models", "We first describe the experimental setup and then we present the experimental results on both small and largescale ChinesetoEnglish translation data sets", "For each of these data sets we wish to determine whether the generative or discriminative model can provide greater improvement in the translation quality and whether the bilingually lexicalized STSG parsing model outperforms the monolingual lexicalized STSG parsing model", "In addition to the translation results we study the influence of the lexicalized STSG parsing models on the rule usage distribution in decoding in detail and investigate the types of rules that are most essential to the lexicalized STSG parsing TABLE III EXPERIMENTAL RESULTS FOR THE VARIOUS TRANSLATION SYSTEMS ON TWO TEST SETS", "BOLD FIGURES INDICATE THAT THE PERFORMANCE IS SIGNIFICANTLY BETTER THAN S2T  THE VALUES IN THE LAST COLUMN INDICATE THE AVERAGE IMPROVEMENTS OVER JOSHUA AND S2T models", "We also discuss the relationship between the lexical ized STSG parsing models and the composed rules which are believed to encode a large portion of the lexicalization", "Finally we analyze the decoding efficiency of the proposed models and present several translation examples", "A Experimental SetUp Our first dataset is the small FBIS bilingual corpus6 consisting of 236 K sentence pairs", "We employed GIZA and the grow diagfinaland balance strategy to generate the final symmetric word alignments", "We parsed the English side using the Berkeley parser 19", "Given the targetside parsed wordaligned bilingual corpus we applied the rule extraction algorithm described in Section III to extract the stringtotree STSG translation rules", "In addition to the GHKM minimal rules 9 we also extracted rules based on SPMT Model 1 16 with sourceside phrases up to length  We also extracted composed rules 8 by combining two or three adjacent minimal rules", "Following 30 we binarized the rules to facilitate the language model integration", "We trained a 5gram language model using the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus", "We used the 2003 NIST MT ChinesetoEnglish test set as the development set and the 2004 and 2005 NIST test sets as our test set", "The final translation quality is evaluated in terms of caseinsensitive BLEU4 metric with shortest length penalty and the statistical significance test is performed using the pairwise resampling approach 11", "B Experimental Results on the SmallScale Data First we provide brief descriptions of the translation systems used in our experiment", "Joshua Joshua is a freely available decoder for hierarchical phrasebased SMT 2 12", "We employ Joshua as a baseline with in the best outputs set to be 500 and other settings left at their default values", "S2T S2T is our inhouse stringtotree translation system which is reimplemented following 8 9 and 16", "S2TGenTgtLex This stringtotree translation system integrates the generative lexicalized STSG parsing model with the target lexicalized information", "6The LDC category is LDC2003E14", "S2TGenSrcTgtLex This system serves the same function as S2TGenTgtLex except that it utilizes both the source and target lexicalized information", "S2TDisSrcTgtLex This stringtotree system integrates the discriminative model with additional bilingual lexicalized features the boundary words of the constituents in addition to the headword", "Table III shows the results", "The linguistically syntaxbased stringtotree system substantially outperforms the hierarchical phrasebased system Joshua and achieves average gains of 271 BLEU points7 demonstrating that the stringtotree translation model is quite powerful", "Based on the strong stringtotree model both the generative and discriminative parsing models improve the translation quality", "Specifically the generative parsing model including the bilingual lexicalized information can significantly outperforms the stringtotree baseline model by an average of 074 BLEU points", "In contrast the improvement obtained by the generative parsing model considering only the target lexicalized information is less promising 042 BLEU points", "These results show that not only the targetside lexicalized information but also the sourceside lexicalized information is usefulFor the discriminative model incorporating additional lexi calized features last line in Table III the baseline stringto tree model is improved by an average of 070 BLEU points", "However even using additional lexicalized features eg the boundary words of a constituent it cannot substantially outperform the generative model", "The reason for this result is most likely twofold", "On the one hand in contrast to the generative model the discriminative model does not consider the structure of the rule eg the order of the nonterminal nodes on the rules righthand side", "On the other hand the parameters are determined using a relatively small data set and the consensus in the machine learning community is that more data are required for the parameter training in discriminative models", "C Experimental Results on the LargeScale Data If a proposed model is useful then it should also be effective on largescale data sets", "In this section we report our experimental results on the large data set", "The largescale Chineseto English training data set8 includes 209 M sentence pairs from the LDC", "The translation rules are extracted using the same settings as the smallscale experiment except that we obtain the composed rules by combining at most two minimal rules to avoid too many specific rules", "The 5gram language model is trained on the Xinhua portion of the English Gigaword corpus plus the target portion of the bilingual data", "Table IV shows the results", "First we observe that compared with the small data set the improvements over MT04 and MT05 are quite different on the larger data set", "Using the system S2T as an example the improvement over MT04 is 267 BLEU points 3640 vs 3373 and the improvement over MT05 is 428 BLEU points 3453 vs 3025", "We have analyzed the", "7The average gain is computed by averaging the improvements obtained on MT04 and MT05", "For example  8LDC category numbers are LDC2000T50 LDC2003E14 LDC2003E07 LDC2004T07 LDC2005T06 LDC2002L27 LDC2005T10 and LDC2005T34", "TABLE IV EXPERIMENTAL RESULTS FOR DIFFERENT TRANSLATION SYSTEMS FOR LARGESCALE DATA SET", "AND MEAN THAT THE PERFORMANCE IS SIGNIFICANTLY BETTER THAN S2T WITH AND RESPECTIVELY data and find that this result is due primarily to the vocabulary coverage", "On the smallscale data set the vocabulary coverage is 87 and 85 for MT04 and MT05 respectively whereas on the largescale data set the vocabulary coverage is 93 for both MT04 and MT05", "The vocabulary coverage improvement is larger for MT05Similar to the smallscale experiment the baseline stringto tree model S2T markedly outperforms the hierarchical phrase based system Joshua and both the generative and discriminative lexicalized STSG parsing models outperform S2T", "The generative parsing model with bilingual lexicalized information S2TGenSrcTgtLex still produce results superior to those of the generative model with only targetside lexicalized information S2TGenTgtLex", "At the same time it significantly outperforms S2T with an average gain of 058 BLEU points on two test sets", "We also observe that the performance improvement for the generative model S2TGenSrcTgtLex is smaller for the largescale data set than for the smallscale data set 058 vs 074", "Our analysis hints that this result may owe to the fact that the generative model utilizes only the headword information", "When the data set becomes large each headword becomes associated with more nonterminals rendering the prediction from headword to nonterminal more ambiguous", "We find that in the smallscale data set each headword corresponds to 213 non terminals on average whereas each headword has 244 corresponding nonterminals on average in the largescale data set", "The discriminative parsing model S2TDisSrcTgtLex demonstrates its power in the largescale experiment exhibiting the best performance among all of the translation systems", "In the largescale experiment S2TDisSrcTgtLex outperforms the generative model S2TGenSrcTgtLex on both test sets by an average of 031 BLEU points and achieves a significant improvement 089 BLEU points on average over the baseline S2T", "These results demonstrate the strength of the discriminative model the larger training data set yields superior results", "For the largescale data set we have also conducted an additional experiment for discriminative model", "We divided the six features into targetside parts and sourceside parts and tested only target features target headword and boundary words to determine whether a similar improvement is achieved", "We find that the discriminative model with only target features exhibits superior performance compared with the baseline but cannot outperform the discriminative model TABLE V DISTRIBUTION OF DIFFERENT KINDS OF RULES USED IN DECODING LEX NLEX AND MIX DENOTE RESPECTIVELY PURELY LEXICALIZED RULES NONLEXICAL RULES AND TERMINALNONTERMINAL MIXED RULES with all six features", "The BLEU scores on MT04 and MT05 are 3701 and 3508 respectively", "To see the performance when the test genre differs we split MT08 into two genres newswire first 691 sentences and we blog", "S2T gets BLEU score 2947 and 2178 respectively on newswire and weblog while our discriminative model S2TDis", "SrcTgtLex obtains 3043 and 2226 on newswire and weblog", "The improvement on newswire is much larger than that on we blog", "We think this is because that our translation rules are extracted mainly on news data", "D Rule Usage Distribution and Which Rules Need Bilingually Lexicalized STSG the Most", "The translation quality improvement in both the small and largescale experiments demonstrates the effectiveness of our proposed bilingually lexicalized STSG", "Readers may wonder how the bilingually lexicalized STSG influences the rule usage distribution in decoding and which rules make the most use of the lexicalized information", "We therefore analyze the rule usage in detail in this section", "Based on the terminals and nonterminals in a rule we divide the stringtotree rules into three categories 1 purely lexicalized rules such as  2 nonlexical rules such as  3 mixed rules in which terminals and nonterminals are mixed such as  We provide statistics regarding the rule usage of the three types of rules for each system in the largescale experiments in Table V From the statistics we can see that for each system the mixed rules are used most followed by the purely lexicalized rules", "In contrast the nonlexical rules are not used frequently they account for only 95 of the usage at most", "This result is reasonable as the nonlexical rules are usually applied in the construction of the highlevel tree structure which requires fewer rules", "The lowerlevel tree structure is typically constructed using lexicalized and mixed rules when the stringtotree model generates the target parse tree", "When we compare the bilingually lexicalized stringtotree models with the baseline Table V shows that the nonlexical rules are used even less often from highest 95 of the time in the baseline to 32 of the time in bilingually lexicalized models while the mixed rules are applied more frequently", "These results imply that given the augmented lexicalized rule probabilities the bilingually lexicalized models prefer the rules containing lexicalized words when applying rules in decoding", "TABLE VI TRANSLATION RESULTS WHEN WE DO NOT PERFORM BILINGUALLYLEXICALIZED PARSING MODELS ON THE PURELY LEXICALIZED RULES  DENOTES THE PERFORMANCE DECLINE COMPARED WITH ORIGINAL RESULTS IN TABLE III TABLE VII TRANSLATION RESULTS WHEN WE ONLY PERFORM BILINGUALLYLEXICALIZED PARSING MODELS ON THE NONLEXICAL RULES  DENOTES THE PERFORMANCE IMPROVEMENT COMPARED WITH THE BASELINE SYSTEM S2T Another question arises regarding the type of rules that make the most use of the bilingually lexicalized model", "Intuitively the nonlexical and mixed rules both contain nonterminals that are generalized and require lexicalization to distinguish good rules from poor ones while the purely lexicalized rules are fully lexicalized from the outset", "In principle the purely lexicalized rules therefore do not require any bilingually lexicalized model", "To investigate this question we repeat the largescale experiments with the bilingually lexicalized models used only for the nonlexical and mixed rules", "Table VI shows the translation results", "It is interesting that the results in Table VI are in line with our intuition the decrease in the performance is small for every system when we do not apply bilingually lexicalized parsing models to the purely lexicalized rules the largest drop of 022 BLEU points is not significant", "We can conclude that the non lexical rules and mixed rules need the bilingually lexicalized models most", "We conduct an additional experiment to test the performance when the bilingually lexicalized parsing models are applied only to the nonlexical rules", "Table VII shows the results on the largescale data", "The generative lexicalized parsing models S2TGenTgtLex and S2TGenSrcTgtLex improve the translation quality only slightly compared with the baseline S2T the largest improvement is 032 BLEU points", "The discriminative lexicalized parsing model achieves larger gains compared with the baseline and the largest improvement is 043 BLEU points", "Note that the improvement is somewhat", "promising because the nonlexical rules comprise only a small fraction of the rules used in decoding not more than 85 as shown in Table V", "The experimental results demonstrate that the bilingually lexicalized parsing models are effective in rendering the nonlexical rules less ambiguous during the rule selection in the decoding process", "TABLE VIII PERCENTAGE OF COMPOSED RULES USED DURING DECODING", "THE NUMBER IN PARENTHESES DENOTES THE PROPORTION OF NONLEXICAL AND MIXED RULES AMONG THE APPLIED COMPOSED RULES When we compare the numbers in Table VII with those in Table VI we find a modest gap the largest gain is 046 BLEU points 3683 vs 3729", "This result indicates that the bilingually lexicalized parsing models also contribute substantially to the mixed rulesIn conclusion both the nonlexical and mixed rules make sub stantial use of the bilingually lexicalized parsing models", "E Relationship With the Composed Rules The STSG rules in the stringtotree models can also be divided into minimal rules and composed rules", "It may be argued that composed rules already encode substantially more lexical ization compared with minimal rules and the use of the composed rules therefore weakens the need for bilingually lexical ized models in stringtotree translation", "We argue that the composed rules are not a competitor but rather a complement to the bilingually lexicalized models", "This relationship is reflected in the following two points On the one hand they play different roles during decoding", "The composed rules are designed to improve the rule coverage and encode more contexts 8 while our bilingually lexicalized models aim to distinguish effective rules from poor ones by dynamically associating the inside nonterminals with source and targetside lexicalized information during decoding", "On the other hand as long as the composed rules contain nonterminals on their righthand sides bilingually lexicalized models can help to distinguish them from other rules in principle", "We therefore examine the decoding process to determine how many composed rules are used and how many of the utilized composed rules contain nonterminals", "In the largescale experiment the composed rules comprise approximately 20 of all the used rules utilized and most of the applied composed rules are nonlexical and mixed ones that contain nonterminals", "Table VIII shows the statistics", "Based on the analysis in the previous section this result indicates that the bilingually lexical ized models can be useful for the composed rules", "That is the composed rules need the bilingually lexicalized models", "F Decoding Efficiency of Bilingually Lexicalized STSG Models The incorporation of lexicalized STSG is disadvantageous in terms of the decoding speed because headwords and boundary words are dynamically acquired and multiple probabilities must be calculated", "Table IX lists the average decoding time for each system on the two test sets in the largescale experiments", "All of the experiments are conducted using the same hardware GHz Fig", "5", "Two real translation examples in the second example is an unknown word", "TABLE IX AVERAGE DECODING TIME IN WORDS PER SECOND GB memory", "The stringtotree translation is relatively slow because the decoding search space is large and the system searches all of the possible target tree structures", "The incorporation of the lexicalized STSG models further reduces the decoding speed", "The generative model with bilingually lexicalized information S2TGenSrcTgtLex is the slowest this model decreases the decoding speed by 559 compared with the baseline", "This result most likely owes to the fact that the model must calculate more backoff probabilities when computing the lexicalized rule probability", "In contrast the discriminative model S2TDisSrcTgtLex exhibits slightly superior performance and slows down the baseline by only 294", "Based on the translation results and decoding speed the discriminative model is more efficient and more effective compared with the generative models", "G Translation Examples To facilitate a better intuition to the ability of our proposed bilingually lexicalized parsing models against the baseline we present two actual translation outputs produced by the baseline stringtotree system S2T and the discriminative bilingually lexicalized system S2TDisSrcTgtLex in the largescale experiments in Fig", "5", "The discriminative model is chosen because it exhibits the best performance in all of the bilingually lexical ized modelsIn the first example injured and were injured are two pos sible translations of the single Chinese word  However only the translation were injured can produce a wellformed target tree structure because the Chinese word in this sentence indicates a passive meaning", "When we investigate the optimal derivation path for these two systems we find that the baseline model transformed the first three Chinese words  into an English prepositional phrase PP", "The next two words  are transformed into a verb phrase VP", "The rule is then used to translate the first five words into a larger verb phrase injured in the earthquake in bam the passive word were is neglected", "The baseline model produced this error because it constructed the highlevel tree structure without considering the source and targetside lexicalized information", "In contrast using the bilingually lexicalized information in a discriminative fashion the system S2TDisSrcTgtLex properly transformed the last six words into the English verb phrase about 30000 people were injured and obtained the correct translation", "In the second example the Chinese word hacutee is in fact a preposition and the Chinese string is a prepositional phrase", "Without the sourceside syntactic information it is reasonable to translate the Chinese word into the English conjunction and", "However when the following context is taken into account we have sufficient information to translate the Chinese word into an English preposition", "Because the boundary lexicalization eg the words and  is not considered when the baseline model constructs the highlevel tree structures the baseline system translated the Chinese string as a part of a noun phrase", "Fortunately our proposed system S2TDisSrcTgtLex yielded a correct translation as it considers the headword and boundary words on both the source and targetsides throughout the tree construction process", "VI", "CONCLUSION AND FUTURE WORK In this paper we presented a novel bilingually lexicalized STSG parsing model to achieve improved translation rule selection in STSGbased stringtotree decoding", "We first proposed a generative model for incorporating bilingual lexicalized information eg headwords", "We then proposed a simple dis criminative model to incorporate additional bilingual lexical ized features", "Our experimental results demonstrate that our approach can significantly improve the translation quality on both smallscale and largescale data sets", "The results also imply that both the target and sourceside lexicalized information is essential in the improvement of the translation quality", "Furthermore the bilingually lexicalized parsing models positively influence the rule distribution in decoding favoring rules with lexical words as constraints and we find that the rules containing nonterminals on their righthand side need the bilingually lexicalized STSG parsing model the most", "In future work we intend to study the discriminative model in greater detail in order to explore its potential", "For instance we will explore additional informative features and investigate whether nonlexicalized parsing models used in the monolingual parsing community can inform our modeling strategies", "In addition we plan to investigate the use of bilingually lexical ized STSG in treetotree translation models which also lack lexicalization information in the construction of the highlevel targetside tree structures"]}, "PbaneaCSL": {"title": ["Computer Speech and Language 28 2014 719"], "abstract": ["Recent research on English word sense subjectivity has shown that the subjective aspect of an entity is a characteristic that is better delineated at the sense level instead of the traditional word level", "In this paper we seek to explore whether senses aligned across languages exhibit this trait consistently and if this is the case we investigate how this property can be leveraged in an automatic fashion", "We first conduct a manual annotation study to gauge whether the subjectivity trait of a sense can be robustly transferred across language boundaries", "An automatic framework is then introduced that is able to predict subjectivity labeling for unseen senses using either crosslingual or multilingual training enhanced with bootstrapping", "We show that the multilingual model consistently outperforms the crosslingual one with an accuracy of over 73 across all iterations", " 2013 Elsevier Ltd All rights reserved", "Keywords Sentiment and text classification Multilingual subjectivity analysis Sense level subjectivity"], "inroduction": ["Sentiment and subjectivity analysis seeks to automatically identify opinions beliefs speculations emotions sentiments and other private states in natural text Wiebe et al 2005", "Quirk et al", "1985 define a private state as a state that does not lend itself to an objective external validation or in other words a person may be observed to assert that God exists but not to believe that God exists", "Belief is in this sense private p 1181", "In the field of natural language processing researchers have used the term subjectivity analysis to denote identifying private states in text namely separating objective from subjective instances while sentiment or polarity analysis further refines the subjective text into positive negative or neutral", "Sentiment and subjectivity analysis has stemmed into a prolific area of research mainly due to the fact that numerous text processing applications stand to gain from incorporating sentiment dimensions into their models including automatic expressive texttospeech synthesis Alm et al 1990 tracking sentiment timelines in online forums and news Balog et al 2006 Lloyd et al 2005 and mining opinions from product reviews Hu and Liu 2004", "In many natural language processing tasks subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data", "Research that benefited from this additional layering ranges from question  This paper has been recommended for acceptance by Prof RK Moore", " Corresponding author", "Tel 1 940 369 7630", "Email addresses carmenbaneagmailcom C Banea radacsuntedu R Mihalcea wiebecspittedu J Wiebe", "08852308  see front matter  2013 Elsevier Ltd All rights reserved", "httpdxdoiorg101016jcsl201303002 answering Yu and Hatzivassiloglou 2003 to conversation summarization Carenini et al 2008 text semantic analysis Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 and lexical substitution Su and Markert 2010", "In experiments carried out on English Wiebe and Mihalcea 2006 have shown that the most robust subjectivity delineation occurs at sense and not at word level", "Following this more finegrained perspective Esuli and Sebastiani 2006 and Andreevskaia and Bergler 2006 have proposed methods to embed senselevel automatic sentiment annotations objectiveneutral negative and positive over the English WordNet structure Miller 1995 using its relationships synonymy antonymy meronymy etc", "On the other hand noticing the scarcity of hand crafted senselevel subjectivitypolarity lexica Markert and Su 2008 have explored ways to infer them from data annotated at either the word or sentence level", "Senselevel subjectivity and crosslingual subjectivity and sentiment analysis have received considerable attentions in recent years yet our paper explores the area that lies at the intersection of these two topics", "To our knowledge this area has not been formally investigated and while the techniques may be similar to those applied in sentiment and subjectivity analysis at the sentence or the review level our work explores the more difficult task of senselevel subjectivity which also involves deep semantic aspects of the language", "The manual annotation study we performed for this task crosslingual senselevel subjectivity annotations as well as the methods we proposed crosslingual and multilingual learning using dictionaries in multiple languages are novel to our knowledge", "This work seeks to answer the following questions", "First for word senses aligned across languages is their subjectivity content consistent or in other words does a subjective sense in language A map to a subjective sense in language B and similarly for an objective sense", "Second can we employ a multilingual framework that can automatically discover new subjectiveobjective senses starting with a limited amount of annotated data", "We seek to answer the first question by conducting a manual annotation study in Section 2", "For the second question we propose two models see Section 3 one crosslingual and one multilingual which are able to simultaneously use information extracted from several languages when making subjectivity senselevel predictions"]}, "Pcoling_D10": {"title": ["Fast CollocationBased Bayesian HMM Word Alignment"], "abstract": ["We present a new Bayesian HMM word alignment model for statistical machine translation", "The model is a mixture of an alignment model and a language model", "The alignment component is a Bayesian extension of the standard HMM", "The language model component is responsible for the generation of words needed for source fluency reasons from source language context", "This allows for untranslatable source words to remain unaligned and at the same time avoids the introduction of artificial NULL words which introduces unusually long alignment jumps", "Existing Bayesian word alignment models are unpractically slow because they consider each target position when resampling a given alignment link", "The sampling complexity therefore grows linearly in the target sentence length", "In order to make our model useful in practice we devise an auxiliary variable Gibbs sampler that allows us to resample alignment links in constant time independently of the target sentence length", "This leads to considerable speed improvements", "Experimental results show that our model performs as well as existing word alignment toolkits in terms of resulting BLEU score"], "inroduction": ["Word alignment is one of the basic problems in statistical machine translation SMT", "The IBM models were originally devised for translation by Brown et al", "1993", "Later when SMT started to employ entire phrases instead of single words Koehn et al 2003 the IBM models were repurposed as word alignment models", "The alignments they produce guide the phrase extraction heuristics that are used in many modern SMT systems", "There are several extensions of the classical IBM models that try to weaken their independence assumptions", "Notably Vogel et al", "1996 introduced Markovian dependencies between individual alignment links", "Those links were treated as independent events in IBM models 1 2 and 3", "The model of Vogel et al", "1996 can be viewed as a Hidden Markov Model HMM in which the hidden Markov Chain induces a probability distribution over latent alignment links", "Besides weakening the independence assumptions of the simpler IBM models the HMM alignment model has the additional benefit of being tractable", "This means that expectations under the HMM aligner can be computed exactly using the forwardbackward algorithm", "These expectations are then used in the BaumWelch algorithm Baum et al 1970 to compute parameter updates for the model", "Crucially the BaumWelch algorithm is a special case of the EM algorithm and thus guaranteed to never decrease the models likelihood at each parameter update Dempster et al 1977", "Tractability and convergence at least to a local optimum are clear advantages of the HMM aligner over IBM models 3 to 5 which are all intractable", "In practice hillclimbing heuristics are employed to approximate expectations in these more complex models", "Unfortunately all convergence guarantees of the learning algorithm are lost this way", "A major problem for the HMM aligner is the handling of NULL words1 The NULL word is a special This work is licensed under a Creative Commons Attribution 40 International Licence", "Licence details http creativecommonsorglicensesby40 1 The original work of Vogel et al", "1996 did not use NULL words and instead aligned all source words", "Later work by Och and Ney 2003 and Liang et al", "2006 has shown that using NULL words to allow for unaligned words improves performance", "lexical item that is hypothesised to stand at the beginning of every target English sentence", "Since in word alignment the source French side is generated given the target side the NULL word is used to generate source words that do not have lexical translations on the target side", "Such untranslatable words are often idiosyncratic to the source language", "Examples are prepositions such as de in French", "The translation of the English orange juice is jus d orange", "In that case the clitic preposition de would be generated from the NULL word on the English side", "For the HMM alignment model the NULL word is troublesome since it stands in the 0th English position and thus induces unusually long jumps which have to be captured by the jump distribution of the HMM", "In this work we present a Bayesian HMM aligner that does not make use of artificial NULL words", "Instead untranslatable source words are generated from the words preceding them", "This way our proposed model only needs to account for lexically motivated alignment links", "Moreover since our model is a hierarchical Bayesian model we can bias it towards inducing sparse lexical distributions", "This in turn leads to significantly better translation distributions Mermer and Saraclar 2011", "Moreover we can even perform inference on the models hyperparameters freeing us of having to choose arbitrary prior distributions", "Existing Bayesian word aligners are often too slow to be useful in practice", "We overcome this problem by designing an auxiliary variable Gibbs sampler that reduces sampling complexity by an order of magnitude", "We also provide a formal proof that this sampler works correctly", "We provide several detailed experiments which show that our model performs on par with or better than standardly used alignment toolkits in terms of BLEU score", "Notation Throughout this paper we will denote random variables RVs by upper case Roman letters", "If we want to express the probability of a specific outcome for the RV X we write P X  x", "If we want to leave the value of X underspecified we simply write P x", "We abbreviate a sequence of RVs X1 to Xn as X n and a sequence of outcomes x1 to xn as xn", "We also distinguish notationally between 1 1 probability mass functions pmfs and probability density functions pdfs", "We denote pmfs by P  and pdfs by p", "Finally we use vF and vE to denote the French source and English target vocabulary sizes"]}, "Pjournal": {"title": ["Mach Translat 2011 25239285"], "abstract": ["The translation features typically used in PhraseBased Statistical Machine Translation PBSMT model dependencies between the source and target phrases but not among the phrases in the source language themselves", "A swathe of research has demonstrated that integrating source context modelling directly into loglinear PBSMT can positively influence the weighting and selection of target phrases and thus improve translation quality", "In this contribution we present a revised extended account of our previous work on using a range of contextual features including lexical features of neighbouring words supertags and dependency information", "We add a number of novel aspects including the use of semantic roles as new contextual features in PBSMT adding new language pairs and examining the scalability of our research to larger amounts of training data", "While our results are mixed across feature selections classifier hyperparameters language pairs and learning curves we observe that including contextual features of the source sentence in general produces improvements", "The most significant improvements involve the integration of longdistance contextual features such as dependency relations in combination with partofspeech tags in DutchtoEnglish subtitle translation the combination of dependency parse and semantic role information in EnglishtoDutch parliamentary debate translation or supertag features in EnglishtoChinese translation", "R Haque  S K Naskar  A Way CNGL School of Computing Dublin City University Dublin 9 Ireland A van den Bosch B ILK Research Group Tilburg center for Cognition and Communication Tilburg University Tilburg The Netherlands email AntalvdnBoschuvtnl 123 Keywords Statistical machine translation  Phrasebased statistical machine translation  Syntax in machine translation  Translation modelling  Word alignment  Memorybased classification"], "inroduction": ["In loglinear phrasebased statistical machine translation PBSMT Koehn et al 2003 the probability Pek  fk  of a target phrase ek given a source phrase fk is modelled as a loglinear combination of features which typically consist of a finite set of translation features and a language model Och and Ney 2002", "The translation features normally used in such models express dependencies between the source and target phrases but not among phrases or tokens in the source language themselves", "Stroppa et al", "2007 observed that incorporating sourcelanguage context using neighbouring words and partofspeech tags had the potential to improve translation quality", "This has led to a whole tranche of research of which we provide an overview in Sect", "3 which has shown that integrating source context modelling into PBSMT can positively influence the weighting and selection of target phrases and thus improve translation quality", "Approaches to include sourcelanguage context to help select more appropriate target phrases have partly been inspired by methods used in wordsense disambiguation WSD where rich contextual features are employed to determine the most likely sense of a polysemous word given that context", "These contextual features may include lexical features of words appearing in the immediate context Gimnez and Mrquez 2007 Stroppa et al 2007 shallow and deep syntactic features of the sentential context Gimpel and Smith 2008 and full sentential context Carpuat and Wu 2007", "Studies in which syntactic features are employed have made use of partofspeech taggers Stroppa et al 2007 supertaggers Haque et al 2009a and shallow and deep syntactic parsers Gimpel and Smith 2008 Haque et al 2009b", "In prior work we have shown that exploring local sentential context information in the form of both supertags Haque et al 2009a and syntactic dependencies Haque et al 2009b can be successfully integrated into a PBSMT model", "Here we provide a revised extended account of this previous research", "We add a number of novel aspects including using semantic roles as new contextual features in PBSMT adding new language pairs and examining the scalability of our research to larger amounts of training data", "Our results allow us to conclude that incorporating sourcelanguage contextual features benefits a range of different language pairs both with English as source language translating to Dutch Chinese Japanese Hindi Spanish and Czech and target language from Dutch on different types of data such as news articles and commentary parliamentary debates patents and subtitles according to a range of automatic evaluation measures", "The remainder of this contribution is organized as follows", "Section 2 provides some motivation for work in this direction and related work is discussed in Sect", "3", "Section 4 provides a brief overview of PBSMT which acts as the baseline throughout the study", "In Sect", "5 we describe the range of contextinformed features we add to the baseline PBSMT model", "Section 6 describes the memorybased classification approach and how we integrated the output of the memorybased classifier into a stateoftheart PBSMT system", "In Sect", "7 we present the results obtained", "We formulate our conclusions in Sect", "8 and offer some avenues for further work"]}, "Pmendt_w11": {"title": ["Universit Franois Rabelais  Tours antenne de Blois"], "abstract": ["None"], "inroduction": ["Current technologies are making increasingly vast amounts of information available to common internet users", "Though the availability of data is ever growing finding information relevant to a specific need is not always straightforward", "Information Retrieval techniques deal with this problem and continue to receive a lot of attention in todays datadriven world", "Information Retrieval IR and Natural Language Processing NLP are deeply related because most search engines are based on a document corpus written in natural language text", "Traditionally Information Retrieval systems have been aimed at shallow levels of parsing of input queries and most studies in NLP for Information Retrieval have been directed towards parsing the English language", "However there is an increasing need to expand the scope of this type of systems due to more demanding users seeking precise answers and to the growing nature of nonEnglish online content", "This paper discusses two main problems with nontraditional IR systems multilingual information retrieval and understanding the logical meaning of queries to find more precise answers", "We analyze the challenges posed by both issues and the techniques that are being applied to solve them", "The paper is organized in four main parts", "Section two will discuss linguistic features of languages that pose challenges for information retrieval", "In particular we analyze some features that are not present in the English language but that are specific to two languages which were studied in contrast to English namely Arabic and Russian", "Section three will discuss the issue of resource availability to support NLP and the study of languages", "In particular we discuss resources from the three languages we cover in this paper", "Section four presents the different language processing levels used by IR systems highlighting the challenges of each level", "We discuss how some of these difficulties have been overcome and which additional difficulties are still open or arise when considering different languages", "Finally we discuss two new trends in the retrieval of information open domain information extraction and question answering", "These techniques require deep parsing strategies to correctly interpret the logical meaning of natural language in order to either convert natural language into a structured logical form or into a query which can be executed on a database of structured information"]}, "Pmert_n09": {"title": ["Regularized Minimum Error Rate Training"], "abstract": ["Minimum Error Rate Training MERT remains one of the preferred methods for tuning linear parameters in machine translation systems yet it faces significant issues", "First MERT is an unregularized learner and is therefore prone to overfitting", "Second it is commonly used on a noisy nonconvex loss function that becomes more difficult to optimize as the number of parameters increases", "To address these issues we study the addition of a regularization term to the MERT objective function", "Since standard regularizers such as 2 are inapplicable to MERT due to the scale invariance of its objective function we turn to two regularizers0 and a modification of 2  and present methods for efficiently integrating them during search", "To improve search in large parameter spaces we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERTs exact line searches", "Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO a learner often used with large feature sets"], "inroduction": ["Minimum Error Rate Training emerged a decade ago Och 2003 as a superior training method for small numbers of linear model parameters of machine translation systems improving over prior work using maximum likelihood criteria Och and Ney 2002", "This technique quickly rose to prominence becoming standard in many research and commercial MT systems", "Variants operating over lattices Macherey et al 2008 or hypergraphs Kumar et al 2009 were subsequently developed with the benefit of reducing the approximation error from nbest lists", "The primary advantages of MERT are twofold", "It directly optimizes the evaluation metric under consideration eg BLEU instead of some surrogate loss", "Secondly it offers a globally optimal line search", "Unfortunately there are several potential difficulties in scaling MERT to larger numbers of features due to its nonconvex loss function and its lack of regularization", "These challenges have prompted some researchers to move away from MERT in favor of linearly decomposable approximations of the evaluation metric Chiang et al 2009 Hopkins and May 2011 Cherry and Foster 2012 which correspond to easier optimization problems and which naturally incorporate regularization", "In particular recent work Chiang et al 2009 has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a marginbased approximation", "On simulated datasets Hopkins and May 2011 found that conventional MERT struggles to find reasonable parameter vectors where a smooth loss function based on Pairwise Ranking Optimization PRO performs much better on real data this PRO method appears at least as good as MERT on small feature sets and also scales better as the number of features increases", "In this paper we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search", "The idea of adding a regularization term to the MERT objective function can be perplexing at first because the most common regularizers such as 1 and 2 are not directly applicable to MERT", "Indeed these regularizers are scale sensitive while the MERT objective function is not scaling the weight vector neither changes the predictions of the linear model nor affects the error count", "Hence MERT can hedge any regularization penalty by maximally scaling down linear model weights", "The first contribution of this paper is to analyze various forms of regularization that are not susceptible to this scaling problem", "We analyze and experiment with 0 a form of regularization that is scale insensitive", "We also present new parameterizations of 2 regularization where we apply 2 regularization to scalesenstive linear transforms of the original linear model", "In addition we introduce efficient methods set of reference translations rS  r1   ", "rS ", "This yields the following optimization problem of incorporating regularization in Och 2003s exact line searches", "For all of these regularizers our methods let us find the true optimum of the regularized w  arg min w S  Ers efs w  s1 S M objective function along the line", "arg min   Ers e sm e sm  efs w Finally we address the issue of searching in a highdimensional space by using the gradient of expected BLEU Smith and Eisner 2006 to find better w where s1 m1 1 search directions for our line searches", "This direction finder addresses one of the serious concerns raised by Hopkins and May 2011 MERT widely failed to reach the optimum of a synthetic linear objective function", "In replicating Hopkins and Mays experiments we confirm that existing search algorithms for MERTincluding coordinate ascent Powells algorithm Powell 1964 and random direction sets Cer et al 2008perform poorly in this experimental condition", "However when using our gradientbased direction finder MERT has no problem finding the true optimum even in a 1000dimensional space", "Our results suggest that the combination of a regularized objective function and a gradientinformed line search algorithm enables MERT to scale well with a large number of features", "Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO Hopkins and May 2011 a parameter tuning method known to be effective with large feature sets"]}, "Pmessage": {"title": ["Message composition based on concepts and goals"], "abstract": ["The goal of this paper is to deal with a problem hardly ever addressed in natural language generation conceptual input", "In order to be able to express something one needs to have something to express to begin with ideas concepts and thoughts", "The question is how to access thoughts and build their representation in form of messages", "What are the building blocks", "How to organize and index them in order to allow for quick and intuitive access later on", "It is generally believed that ideas precede expressions", "Indeed meanings imprecise as they may be tend to precede their expression in language", "Yet message creation is hardly ever a onestep process", "Conceptual inputs are generally abstract and underspecified which implies that they need to get refined later on possibly even during the expression phase", "In this paper we investigate interactive sentence generation the focus being on conceptual input a major component of language generation", "Our views will be illustrated via three systems ILLICO a system for analyzinggenerating sentences and guiding their composition SPB a multilingual phrasebook with on the fly generated audio output and Drill Tutor DT an exercise generator", "While ILLICO is a messageunderstanding system with a messagecompletion functionality SPB and DT are messagespecification systems", "The user works quite early with fairly complete structures sentences or patterns making basically only local changes replacing words in the case of SPB and choosing them to instantiate pattern variables in the case of DT", "M Zock 18  P Sabatier  L Jakubiec Laboratoire dInformatique Fondamentale de Marseille CNRS  AixMarseille Universit 163 Avenue de Luminy Case 901 13288 Marseille Cedex 9 France email michaelzocklifunivmrsfr Keywords Conceptual input  Message composition  Message completion  Interactive natural language generation"], "inroduction": ["The goal of this paper is to investigate a problem hardly ever addressed in the literature of natural language generation conceptual input", "Being one of the major sources of a generator this is clearly an important element especially if input is to be given by a human user", "Unfortunately little is known about issues such as  Nature of the input and compositional rules words and sentences are linguistic means for expressing ideas uttered to achieve a communicative nonlinguistic goal", "If we can agree that the inputs are messages and goals we still need to clarify the nature of the building blocks concepts primitives the rules of combining them conceptual grammar and the means to present them at the interface level icons semantic networks ordinary words or words having the status of concepts or primitives in terlingua or any combination of them hybrid approach  Specificity of input inputs are rarely the specific words used in a concrete sentence", "Depending on the situation messages are encoded at various levels of abstraction semantic networks syntactic trees", "While messages may be fully specified in the case of spontaneous discourse dialogue they certainly are much less specific at the early stages of text production in particular if the message is very long challenging our shortterm memory capacities Miller 1956", "The question is what are the core message elements typically present in a message1 and how close are they to language  Moment of the input obviously input precedes output", "Yet things can be quite subtle", "Not everything is necessarily fully specified or definitely encoded before initiating expression", "During delivery speaking or writing we may change our thoughts", "Realizing that our output does not fit our goal well we may alter the initial message to a greater or lesser extent adding or deleting details as a result of afterthoughts", "In sum conceptual input need not be fully specified prior to expression it may also occur during expression or afterwards feedback corrections language feeding back on thought  Representation message elements and their combination can be represented in various ways categorially or visually graphs trees menus", "Storage and representation are relevant both for the longterm memory and for the working memory intermediate structure", "For example words can be stored permanently in a dictionary or temporarily in a graph representing the message", "Graphs are useful mnemonic devices as long as the message they represent is not too complex", "They allow us to see and remember what he has been encoded or built so far helping us to realize at the same time what is still lacking  Planning unit atomistic concepts vs holistic messages patterns the planning units can be of various sizes", "We tend to consider words as such units but words can be broken down into elements meaning form sound just as sentences can be divided into smaller units phrases words", "Yet sentences and their underlying conceptual structure patterns can also be considered as tokens or units in which case they dont need to be synthesized", "Just like words in a dictionary they only need to be accessed", "In other words they behave like lexicalized phrases  Building strategies messages can be produced in various ways step by step ie incrementally or holistically in which case they are just retrieved", "Processing can also be performed hierarchically from top to bottom a general idea is fleshed out with details or from the bottom to the top words are integrated into a larger structure", "Input can be given at once oneshot process or in various steps with backtrack corrections refinements or not", "Finally processing can be done serially ie left to right or via a mixed approach", "Natural language generation is typically driven by input messages ie meanings and goals the usual starting points", "We present here our views on how to help people provide this kind of input", "Our views will be illustrated via 1 A rough first guess would be the concepts underlying the major syntactic categories nouns verbs adjectives adverbs entities actions processes qualities", "three systems ILLICO a system that was built in the 90s and two more recent systems which are still in the development phase and have not been tested yet by users SPB a multilingual speaking phrasebook and DT a webbased exercise generator called Drill Tutor", "While all these systems have been created for assisting the text producer their design or blueprints have been guided by different principles and goals", "One function of ILLICO is to guide the user to compose a message", "In order to do so it checks both completeness and wellformedness of the message according to the context", "The goal of the other two systems is language learning", "With SPB we pursue two goals helping users survive in a new culture or language by providing them with translation equivalences and helping them discover the underlying principles for building such sentences", "The goal of DT is to help people become fluent in a new language", "All three systems address the issue of message planning but they do so by starting from different assumptions and inputs words concepts or goals", "While ILLICO is a message and sentencecompletion system SPB and DT are messagespecification systems"]}, "Pmorph_p00": {"title": ["Computing with Realizational Morphology"], "abstract": ["The theory of realizational morphology presented by Stump in his inuential book Inflectional Morphology 2001 describes the derivation of inected surface forms from underlying lexical forms by means of ordered blocks of realization rules", "The theory presents a rich formalism for expressing generalizations about phenomena commonly found in the morphological systems of natural languages", "This paper demonstrates that in spite of the apparent complexity of Stumps formalism the system as a whole is no more powerful than a collection of regular relations", "Consequently a Stumpstyle description of the morphology of a particular language such as Lingala or Bulgarian can be compiled into a nitestate transducer that maps the underlying lexical representations directly into the corresponding surface forms or forms and vice versa yielding a single lexical transducer", "For illustration we will present an explicit nitestate implementation of an analysis of Lingala based on Stumps description and other sources"], "inroduction": ["Morphology is a domain of linguistics that studies the formation of words", "It is traditional to distinguish between surface forms and their analyses called lemmas", "The lemma for a surface form such as the English word bigger typically consists of the traditional dictionary citation form of the word together with terms that convey the morphological properties of the particular form", "For example the lemma for bigger might be represented as bigAdjComp to indicate that bigger is the comparative form of the adjective big", "Alternatively the morphological properties might be encoded in terms of attributevalue pairs CatAdj DegrComp", "There are two challenges in modeling naturallanguage morphology 1", "Morphotactics", "Words are typically composed of smaller units stems and axes that mustbe combined in a certain order", "Most languages build words by concatena tion but some languages also exhibit nonconcatenative processes such as interdigitation and reduplication 3"]}, "Ponto_w06": {"title": ["Acquiring Ontological Relationships from Wikipedia Using RMRS"], "abstract": ["We investigate the extraction of ontologies from biological text using a semantic representation derived from a robust parser", "The use of a semantic representation avoids the problems that traditional patternbased approaches have with complex syntactic constructions and longdistance dependencies", "The discovery of taxonomic relationships is explored in a corpus consisting of 12200 animalrelated articles from the online encyclopaedia Wikipedia", "The semantic representation used is Robust Minimal Recursion Semantics RMRS", "Initial experiments show good results in systematising extraction across a variety of hyponymic constructions", "Key words ontologies ontology extraction Wikipedia semantics"], "inroduction": ["Ontology extraction has traditionally relied on pattern matching algorithms", "Hearst 5 introduced hyponymic extraction using lexicosyntactic patterns", "In the Hearst algorithm the system looks for instances of certain expressions in the text for example X is a Y or X such as Y and Z and infers the relations X isa Y and X isa Z", "Such systems are usually based on regular expressions over text or POStagged text and sentences containing apposition bracketing longdistance dependencies or uncommon structures have to be coded explicitly", "An example of such a sentence extracted from the Wikipedia encyclopaedia is The Firemouth Cichlid is one of the typical and most commonly seen in pet stores of the Cichlasomatype South American cichlids", "Here obtaining the relationship Firemouth cichlid isa cichlid involves the identification of the hyponym and hypernym as the first and last noun phrases of a lengthy sentence", "This suggests that traditional methods might be improved by using deeper syntactic and semantic analysis", "The work presented here investigates the use of a semantic model to address some of these issues", "Robust Minimal Recursion Semantics RMRS 4 provides argumentbased representation of sentences", "The theoretical idea is that in the problematic sentence above an RMRS output would contain the predicate associated with the identity copula be with a first argument corresponding to the term Firemouth cichlid and a second argument corresponding to cichlids regardless of the word order modification and so on", "Thus having obtained the RMRS representation of a given corpus it would be possible to extract ontological relationships from a semantic structure that abstracts over those morphological and syntactic details that do not affect the ontological relationship", "As pointed out by Pennacchiotti and Pantel 6 most ontology extraction systems so far have focused on generalised isa or partof relationships", "Our work involves extracting general hyponymic relations with RMRS and applying a filter to the results to obtain biological taxonomic relationships", "The corpus was gathered by extracting 12200 animal articles from the Wikipedia online encyclopaedia httpwwwwikipediaorg providing a semiedited setting where the added robustness of semantics might prove its usefulness", "The next section of this paper gives an overview of relevant prior work", "It is followed by the description of an extraction system based on RMRS using hard wired rules", "Results are discussed in the light of four different evaluation methods covering both manual and automatic recall and precision", "A brief overview is then given of a further system still under development the aim of which is to automate the pattern extraction process", "The conclusion presents different avenues for future work"]}, "Pproc2014_n09": {"title": ["EffectWordNet Senselevel Lexicon Acquisition  for Opinion Inference"], "abstract": ["Recently work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities effect events", "This paper addresses methods for creating a lexicon of such events to support such work on opinion inference", "Due to significant sense ambiguity our goal is to develop a senselevel rather than wordlevel lexicon", "To maximize the effectiveness of different types of information we combine a graphbased method using WordNet1 relations and a standard classifier using gloss information", "A hybrid between the two gives the best results", "Further we provide evidence that the model is an effective way to guide manual annotation to find effect senses that are not in the seed set"], "inroduction": ["Opinion mining or sentiment analysis identifies positive or negative opinions in many kinds of texts such as reviews blogs and news articles", "It has been exploited in many application areas such as review mining election analysis and information extraction", "While most previous research focusses on explicit opinion expressions recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities Deng et al 2013 Deng and Wiebe 2014", "We call such events effect events2 Deng and Wiebe 2014 show how sentiments toward one 1 WordNet 30 httpwordnetprincetonedu", "2 While the term goodForbadFor is used in previous papers Deng et al 2013 Deng and Wiebe 2014 Deng et al 2014 we have since decided that effect is a better term", "entity may be propagated to other entities via opinion inference rules", "They give the following example 1 The bill would curb skyrocketing health care costs", "The writer expresses an explicit negative sentiment by skyrocketing toward the object health care costs", "The event curb has a negative effect on costs since they are reduced", "We can reason that the writer is positive toward the event because it has a negative effect on costs toward which the writer is negative", "From there we can reason that the writer is positive toward the bill since it is the agent of the positive event", "Deng and Wiebe 2014 show that such inferences may be exploited to significantly improve explicit sentiment analysis systems", "However to achieve its results the system developed by Deng and Wiebe 2014 requires that all instances of effect events in the corpus be manually provided as input", "For the system to be fully automatic it needs to be able to recognize effect events automatically", "This paper addresses methods for creating lexicons of such events to support such work on opinion inference", "We have discovered that there is significant sense ambiguity meaning that words often have mixtures of senses among the classes effect effect and Null", "Thus we develop a senselevel rather than wordlevel lexicon", "One of our goals is to investigate whether the effect property tends to be shared among semanticallyrelated senses and another is to use a method that applies to all word senses not just to the senses of words in a given wordlevel lexicon", "Thus we build a graphbased model in which each node is a WordNet sense and edges represent semantic WordNet relations between senses", "In addition we hypothesized that glosses also contain useful information", "Thus we develop a supervised gloss classifier and define a hybrid model which gives the best overall performance", "Finally because all WordNet verb senses are incorporated into the model we investigate the ability of the method to identify unlabeled senses that are likely to be effect senses", "We find that by iteratively labeling the topweighted unlabeled senses and rerunning the model it may be used as an effective method for guiding annotation efforts"]}, "Pproc9_w11": {"title": ["Determining Compositionality of Word Expressions"], "abstract": ["This research focuses on determining seman tic compositionality of word expressions us ing word space models WSMs", "We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs corpora preprocess ing techniques methods for determining com positionality and evaluation testbeds", "We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components", "The vec tors were obtained by Latent Semantic Analy sis LSA applied to the ukWaC corpus", "Our results outperform those of all the participants in the Distributional Semantics and Composi tionality DISCO 2011 shared task"], "inroduction": ["A word expression is semantically compositional if its meaning can be understood from the literal meaning of its components", "Therefore semanti cally compositional expressions involve eg small island or hot water on the other hand seman tically noncompositional expressions are eg red tape or kick the bucket", "The notion of compositionality is closely related to idiomacy the higher the compositionality the lower the idiomacy and vice versa Sag et al 2002 Baldwin and Kim 2010", "Noncompositional expressions are often referred to as Multiword Expressions MWEs", "Baldwin and Kim 2010 differentiate the following subtypes of Pavel Pecina Charles University in Prague Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Prague Czech Republic pecinaufalmffcunicz compositionality lexical syntactic semantic prag matic and statistical", "This paper is concerned with semantic compositionality", "Compositionality as a feature of word expressions is not discrete", "Instead expressions populate a con tinuum between two extremes idioms and free word combinations McCarthy et al 2003 Bannard et al 2003 Katz 2006 Fazly 2007 Baldwin and Kim 2010 Biemann and Giesbrecht 2011", "Typical ex amples of expressions between the two extremes are zebra crossing or blind alley", "Our research in compositionality is motivated by the hypothesis that a special treatment of se mantically noncompositional expressions can im prove results in various Natural Language Process ing NPL tasks as shown for example by Acosta et al", "2011 who utilized MWEs in Information Re trieval IR", "Besides that there are other NLP ap plications that can benefit from knowing the degree of compositionality of expressions such as machine translation Carpuat and Diab 2010 lexicography Church and Hanks 1990 word sense disambigua tion Finlayson and Kulkarni 2011 partofspeech POS tagging and parsing Seretan 2008 as listed in Rarnisch 2012", "The main goal of this paper is to present an anal ysis of previous approaches using WSMs for de termining the semantic compositionality of expres sions", "The analysis can be found in Section 2", "A special attention is paid to the evaluation of the pro posed models that is described in Section 3", "Section 4 presents our first intuitive experimental setup and results ofLSA applied to the DISCO 2011 task", "Sec tion 5 concludes the paper", "42 Proceedings of the 9th Workshop on Multiword Expressions MWE 2013 pages 4250 Atlanta Georgia 1314 June 2013", "2013 Association for Computational Linguistics"]}, "Pproc_d09": {"title": ["Reordering by Parsing"], "abstract": ["We present a new discriminative reordeting model for statistical machine translation", "The model employs a standard datadriven depen dency parser to predict reordetings based on syntactic information", "This is made possi ble through the introduction of a reorderiog structure which is a word alignment structure where the target word order is transposed onto the source sentence as a path", "The approach is iotegrated io a phrasebased system", "Exper iments show a large iocrease io long distance reorderiogs", "Both automatic and human evalu ations show substantial increases io translation quality on an English to German task"], "inroduction": ["Handling word order differences between languages is one of the main challenges of statistical machine translation SMT today", "These differences are of ten most natorally handled at a syntactic level since they pertaio to entire syntactic constituents", "We present a syntactically motivated discrimioa tive reordering model", "The model exploits a reorder ing structure which is a word alignment where the target sentence is unknown", "This structure allows us to treat the reordering problem as a dependency parsing problem", "We use a standard datadriven de pendency parser to predict reorderings instead of de pendencies", "This is integrated into a phrasebased SMT PSMT framework Koehn et al 2003"]}, "Pproc_d10": {"title": ["Exact Maximum Inference for the Fertility Hidden Markov Model"], "abstract": ["The notion of fertility in word alignment the number of words emitted by a sin gle state is useful but difficult to model", "Initial attempts at modeling fertility used heuristic search methods", "Recent ap proaches instead use more principled ap proximate inference techniques such as Gibbs sampling for parameter estimation", "Yet in practice we also need the single best alignment which is difficult to find us ing Gibbs", "Building on recent advances in dual decomposition this paper introduces an exact algorithm for finding the sin gle best alignment with a fertility HMM", "Finding the best alignment appears impor tant as this model leads to a substantial improvement in alignment quality"], "inroduction": ["Wordbased translation models intended to model the translation process have found new uses iden tifying word correspondences in sentence pairs", "These word alignments are a crucial training com ponent in most machine translation systems", "Fur thermore they are useful in other NLP applica tions such as entailment identification", "The simplest models may use lexical infor mation alone", "The seminal Model 1 Brown et al 1993 has proved very powerful per forming nearly as well as more complicated models in some phrasal systems Koehn et al 2003", "With minor improvements to initializa tion Moore 2004 which may be important Toutanova and Galley 2011 it can be quite competitive", "Subsequent IBM models include more detailed information about context", "Models 2 and 3 incorporate a positional model based on the absolute position of the word Models 4 and 5 use a relative position model instead an English word tends to align to a French word that is nearby the French word aligned to the previous English word", "Models 3 4 and 5 all incorporate a no tion offertility the number of French words that align to any English word", "Although these latter models covered a broad range of phenomena estimation techniques and MAP inference were challenging", "The au thors originally recommended heuristic proce dures based on local search for both", "Such meth ods work reasonably well but can be computation ally inefficient and have few guarantees", "Thus many researchers have switched to the HMM model Vogel et al 1996 and variants with more parameters He 2007", "This captures the posi tional information in the IBM models in a frame work that admits exact parameter estimation infer ence though the objective function is not concave local maxima are a concern", "Modeling fertility is challenging in the HMM framework as it violates the Markov assump tion", "Where the HMM jump model considers only the prior state fertility requires looking across the whole state space", "Therefore the standard forwardbackward and Viterbi algorithms do not apply", "Recent work Zhao and Gildea 2010 de scribed an extension to the HMM with a fertility model using MCMC techniques for parameter es timation", "However they do not have a efficient means of MAP inference which is necessary in many applications such as machine translation", "This paper introduces a method for exact MAP inference with the fertility HMM using dual de composition", "The resulting model leads to sub stantial improvements in alignment quality", "7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics pages 711 Sofia Bulgaria August 49 2013", "2013 Association for Computational Linguistics"]}, "Pproc_w09": {"title": ["Monolingual Distributional Similarity for TexttoText Generation"], "abstract": ["Previous work on paraphrase extraction and application has relied on either parallel datasets or on distributional similarity met tics over large text corpora", "Our approach combines these two orthogonal sources of in formation and directly integrates them into our paraphrasing systems loglinear model", "We compare different distributional similar ity featuresets and show significant improve ments in grarnmaticality and meaning reten tion on the example texttotext generation task of sentence compression achieving state oftheart quality"], "inroduction": ["A wide variety of applications in natural language processing can be cast in terms of texttotext gen eration", "Given input in the form of natural lan guage a texttotext generation system produces natural language output that is subject to a set of constraints", "Compression systems for instance pro duce shorter sentences", "Paraphrases ie differ ing textual realizations of the same meaning are a crucial components of texttotext generation sys tems and have been successfully applied to tasks such as multidocument summarization Barzilay et a 1999 Barzilay 2003 query expansion Arr ick and Tipirneni 1999 Riezler eta 2007 ques tion answering McKeown 1979 Ravichandran and Hovy 2002 sentence compression Cohn and La pata 2008 Zhao et a 2009 and simplification Wubben et a 2012", "Paraphrase collections for texttotext generation have been extracted from a variety of different corpora", "Several approaches rely on bilingual para", "lei data Bannard and CallisonBurch 2005 Zhao et a 2008 CallisonBurch 2008 Ganitkevitch et a 2011 while others leverage distributional meth ods on monolingual text corpora Lin and Pantel 2001 Bhagat and Ravichandran 2008", "So far how ever ouly preliminary studies have been undertaken to combine the information from these two sources Chan eta 2011", "In this paper we describe an extension of Gan itkevitch et a", "2011s bilingual databased ap proach", "We augment the bilinguallysourced para phrases using features based on monolingual distri butional similarity", "More specifically  We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features esti mated from bilingual data", " We define distributional similarity for para phrase patterns that contain constituentlevel gaps eg simone JJ instance of NP a JJ case of NP", "This generalizes over distributional similarity for contiguous phrases", " We compare different types of monolingual distributional information and show that they can be used to achieve siguificant improve ments in grammaticality", " Finally we compare our method to several strong baselines on the texttotext generation task of sentence compression", "Our method shows stateoftheart results beating a purely bilingually sourced paraphrasing system", "256 First Joint Conference on Lexical and Computational Semantics SEM pages 256264 Montreal Canada June 78 2012", "2012 Association for Computational Linguistics their IJIInsithe term would", " t  ihre langfrisligen Ple wOrden", "ne seine lang en e aufzuQeben", "   ", " without giving up his longlenn plans Figure 1Pivotbased paraphrase extraction for con tiguous phrasesTwo phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another"]}, "Psem_p07": {"title": ["Mach Translat"], "abstract": ["This paper describes a new evaluation metric TERPlus TERp for automatic evaluation of machine translation MT", "TERp is an extension of Translation Edit Rate TER", "It builds on the success of TER as an evaluation metric and alignment tool and addresses several of its weaknesses through the use of paraphrases stemming synonyms as well as edit costs that can be automatically optimized to correlate better with various types of human judgments", "We present a correlation study comparing TERp to BLEU METEOR and TER and illustrate that TERp can better evaluate translation adequacy", "Keywords Machine translation evaluation  Paraphrasing  Alignment M G Snover B  N Madnani  B Dorr Laboratory for Computational Linguistics and Information Processing Institute for Advanced Computer Studies University of Maryland College Park MD USA email snoverumiacsumdedu N Madnani email nmadnaniumiacsumdedu B Dorr email bonnieumiacsumdedu R Schwartz BBN Technologies Cambridge MA USA email schwartzbbncom"], "inroduction": ["TERPlus or TERp1 Snover et al 2009 is an automatic evaluation metric for machine translation MT that scores a translation the hypothesis of a foreign language text the source against a translation of the source text that was created by a human translator which we refer to as a reference translation", "The set of possible correct translations is very large possibly infinite and any one reference translation represents a single point in that space", "Frequently multiple reference translations typically 4are provided to give broader sampling of the space of correct translations", "Automatic MT evaluation metrics compare the hypothesis against this set of reference translations and assign a score to the similarity such that a better score is given when the hypothesis is more similar to the references", "TERp follows this methodology and builds upon an already existing evaluation metric Translation Error Rate TER Snover et al 2006", "In addition to assigning a score to a hypothesis TER provides an alignment between the hypothesis and the reference enabling it to be useful beyond general translation evaluation", "While TER has been shown to correlate well with translation quality it has several flaws it only considers exact matches when measuring the similarity of the hypothesis and the reference and it can only compute this measure of similarity against a single reference", "The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems Rosti et al 2007", "TERp does not utilize this methodology and instead addresses the exact matching flaw of TER", "In addition to aligning words in the hypothesis and reference if they are exact matches TERp uses stemming and synonymy to allow matches between words", "It also uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference", "These phrase substitutions are generated by considering possible paraphrases of the reference words", "Matching using stems and synonyms Banerjee and Lavie 2005 as well as using paraphrases Zhou et al 2006 Kauchak and Barzilay 2006 have been shown to be beneficial for automatic MT evaluation", "Paraphrases have been shown to be additionally useful in expanding the number of references used for evaluation Madnani et al 2008 although they are not used in this fashion within TERp", "The use of synonymy stemming and paraphrases allows TERp to better cope with the limited number of reference translations provided", "TERp was one of the top metrics submitted to the NIST MetricsMATR 2008 challenge Przybocki et al 2008 having the highest average rank over all the test conditions Snover et al 2009", "We first discuss the original TER metric in Sect", "2", "In Sect", "3 we present the details of our various enhancements to TER", "We then briefly review the alignment capability of TERp along with some examples in Sect", "4", "Finally in Sect", "5 we show the results of optimizing TERp for human judgments of adequacy and compare with other established evaluation metrics followed by an analysis of the relative benefits of each of the new features of TERp in Sect", "6", "1 TERp is named after the nicknameterpof the University of Maryland College Park Mascot the", "diamondback terrapin"]}, "Pstat_p00": {"title": ["FiniteState Morphological Analysis and Generation of Arabic at Xerox"], "abstract": ["This paper describes a finitestate morphological analyzer of Modern Standard Arabic words that can be tested on the Internet", "An overview of the system is provided including the history the finitestate technology and the dictionary coverage", "This research system is scheduled for testing and commercial development in 2001"], "inroduction": ["In 1996 the Xerox Research Centre Europe produced a morphological analyzer for Modern Standard Arabic henceforth Arabic Beesley 1996", "In 1997 a Javaapplet interface was added to allow testing on the Internet using standard Arabic orthography", "The analyzergenerator is based on dictionaries from an earlier project at ALPNET Beesley 1990 Buckwalter 1990 but the system was extensively redesigned and rebuilt using Xerox finitestate technology Beesley and Karttunen 2001", "The system analyzes orthographical words that may include full partial or no diacritics and if diacritics are present they automatically constrain the ambiguity of the output", "A fully voweled spelling and a terse English gloss are also returned with each analysis", "The system is intended to serve as a pedagogical aid a comprehensionassistance tool and as a component in larger naturallanguageprocessing systems", "morphological analyses Morphological Analyzer word Figure 1 A Generic Morphological Analyzer as a Black Box"]}, "Q13-1015": {"title": ["Combined Distributional and Logical Semantics"], "abstract": ["We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics", "Distributional models have been successful in modelling the meanings of content words but logical semantics is necessary to adequately represent many function words", "We follow formal semantics in mapping language to logical representations but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate argument structure", "Our clustering algorithm is highly scalable allowing us to run on corpora the size of Gigaword", "Different senses of a word are disambiguated based on their induced types", "We outperform a variety of existing approaches on a widecoverage question answering task and demonstrate the ability to make complex multisentence inferences involving quantifiers on the FraCaS suite"], "inroduction": ["Mapping natural language to meaning representations is a central challenge of NLP", "There has been much recent progress in unsupervised distributional semantics in which the meaning of a word is induced based on its usage in large corpora", "This approach is useful for a range of key applications including question answering and relation extraction Lin and Pantel 2001 Poon and Domingos 2009 Yao et al 2011", "Because such a semantics can be automically induced it escapes the limitation of depending on relations from handbuilt training data knowledge bases or ontologies which have proved of limited use in capturing the huge variety of meanings that can be expressed in language", "However distributional semantics has largely developed in isolation from the formal semantics literature", "Whilst distributional semantics has been effective in modelling the meanings of content words such as nouns and verbs it is less clear that it can be applied to the meanings of function words", "Semantic operators such as determiners negation conjunctions modals tense mood aspect and plurals are ubiquitous in natural language and are crucial for high performance on many practical applications but current distributional models struggle to capture even simple examples", "Conversely computational models of formal semantics have shown low recall on practical applications stemming from their reliance on ontologies such as WordNet Miller 1995 to model the meanings of content words Bobrow et al 2007 Bos and Markert 2005", "For example consider what is needed to answer a question like Did Google buy YouTube", "from the following sentences 1", "Google purchased YouTube"]}, "Q13-1024-parscit130908": {"title": [""], "abstract": ["Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language", "It has been verified to be a useful constraint for word alignment", "However previous work either treats this as a hard constraint or uses it as a feature in discriminative models which is ineffective for largescale tasks", "In this paper we take dependency cohesion as a soft constraint and integrate it into a generative model for largescale word alignment experiments", "We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner", "Experiments on largescale ChineseEnglish translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality"], "inroduction": ["Word alignment is the task of identifying word correspondences between parallel sentence pairs", "Word alignment has become a vital component of statistical machine translation SMT systems since it is required by almost all stateoftheart SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules Koehn et al 2007 Galley et al 2004", "During the past two decades generative word alignment models such as the IBM Models Brown et al 1993 and the HMM model Vogel et al 1996 have been widely used primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA toolkit Och and Ney 2003", "However the word alignment quality of generative models is still far from satisfactory for SMT systems", "In recent years discriminative alignment models incorporating linguistically motivated features have become increasingly popular Moore 2005 Taskar et al 2005 Riesa and Marcu 2010 Saers et al 2010 Riesa et al 2011", "These models are usually trained with manually annotated parallel data", "However when moving to a new language pair large amount of handaligned data are usually unavailable and expensive to create", "A more practical way to improve largescale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner Wu 1997 Yamada and Knight 2001 Lopez and Resnik 2005 DeNero and Klein 2007 Pauls et al 2010", "In this paper we take dependency cohesion Fox 2002 into account which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation", "Instead of treating dependency cohesion as a hard constraint Lin and Cherry 2003 or using it as a feature in discriminative models Cherry and Lin 2006b we treat dependency cohesion as a distortion constraint and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates", "We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner", "Experiments on a largescale ChineseEnglish translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality", "The remainder of this paper is organized as follows Section 2 introduces dependency cohesion 291 Transactions of the Association for Computational Linguistics 1 2013 291300", "Action Editor Chris CallisonBurch", "Submitted 52013 Published 72013", "c2013 Association for Computational Linguistics", "constraint for word alignment", "Section 3 presents our generative model for word alignment using dependency cohesion constraint", "Section 4 describes algorithms for parameter estimation", "We discuss and analyze the experiments in Section 5", "Section 6 gives the related work", "Finally we conclude this paper and mention future work in Section 7"]}, "S07-1032": {"title": ["GPLSI Word Coarsegrained Disambiguation aided by Basic Level"], "abstract": ["We present a corpusbased supervised learning system for coarsegrained sense disambiguation", "In addition to usual features for training in word sense disambiguation our system also uses Base Level Concepts automatically obtained from WordNet", "Base Level Concepts are some synsets that generalize a hyponymy subhierarchy and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated", "Our experiments proved that using this type of features results on a significant improvement of precision", "Our system has achieved almost 08 F1 fifth place in the coarsegrained English allwords task using a very simple set of features plus Base Level Concepts annotation"], "inroduction": ["The GPLSI system in SemEvals task 7 coarse grained English allwords consists of a corpus based supervisedlearning method which uses local context information", "The system uses Base Level Concepts BLC Rosch 1977 as features", "In short BLC are synsets of WordNet WN Fell baum 1998 that are representative of a certain hyponymy subhierarchy", "The synsets that are selected to be BLC must accomplish certain conditions that will be explained in next section", "BLC This paper has been supported by the European Union under the project QALLME FP6 IST033860 and the Spanish Government under the project TextMess TIN200615265 C0601 and KNOW TIN200615049C0301 are slightly different from Base Concepts of EuroWordNet1 EWN Vossen et al 1998 Balkanet2 or Meaning Project3 because of the selection criteria but also because our method is capable to define them automatically", "This type of features helps our system to achieve 079550 F1 over the FirstSense baseline 078889 while only four systems outperformed ours being the F1 of the best one 083208", "WordNet has been widely criticised for being a sense repository that often offers too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broadcoverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Possibly building classbased classifiers would allow to avoid the data sparseness problem of the wordbased approach", "Thus some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldo van 2001 Agirre et al 2003 and on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 Ciaramita and Altun 2006", "However most of the later approaches used the original Lexico graphical Files of WN more recently called Super 1 httpwwwillcuvanlEuroWordNet 2 httpwwwceidupatrasgrBalkanet 3 httpwwwlsiupces nlpmeaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations SemEval2007 pages 157160 Prague June 2007", "Qc 2007 Association for Computational Linguistics senses as very coarsegrained sense distinctions", "However not so much attention has been paid on learning classbased classifiers from other available sensegroupings such as WordNet Domains Magnini and Cavaglia 2000 SUMO labels Niles and Pease 2001 EuroWordNet Base Concepts or Top Concept Ontology labels Atserias et al 2004", "Obviously these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD", "Possibly their combination could improve the overall results since they offer different semantic perspectives of the data", "Furthermore to our knowledge to date no comparative evaluation have been performed exploring different sensegroupings", "This paper is organized as follows", "In section 2 we present a method for deriving fully automatically a number of Base Level Concepts from any WN version", "Section 3 shows the details of the whole system and finally in section 4 some concluding remarks are provided"]}, "S0885": {"title": ["Available online at wwwsciencedirectcom"], "abstract": ["Transcriptbased topic segmentation of TV programs faces several difficulties arising from transcription errors from the presence of potentially short segments and from the limited number of word repetitions to enforce lexical cohesion ie lexical relations that"], "inroduction": ["based on generalized probabilities with a unigram language model", "On the one hand confidence measures and semantic relations are considered as additional sources of information", "On the other hand language model interpolation techniques are investigated for better language model estimation", "Experimental topic segmentation results are presented on two corpora with distinct characteristics composed respectively of broadcast news and reports on current affairs", "Significant improvements are obtained on both corpora demonstrating the effectiveness of the extended lexical cohesion measure for spoken TV contents as well as its genericity over different programs", " 2011 Elsevier Ltd All rights reserved", "Keywords Topic segmentation Lexical cohesion Confidence measures Semantic relations Language model interpolation TV broadcasts 1", "Introduction", "Structuring video feeds has become a requirement and is a highly challenging issue", "Indeed with the proliferation of videosharing websites and the increasing number of television channels the quantity of video that users can access has become so important that it is necessary to develop methods to structure this material and enable users to navigate", "A crucial structuring stage is the segmentation of shows into topically homogeneous segments Wactlar et al 1996 Allan et al 1998", "Since videos available are of different kinds movies news talk shows etc and in order to avoid the use of several domain specific methods such structuring approaches must necessarily be generic enough to treat various types of video", "To this end topic segmentation of informative content TV shows can rely on the speech pronounced in  This paper has been recommended for acceptance by INTERSPEECH guest editors", " Corresponding author", "Email addresses camilleguinaudeauinriafr C Guinaudeau guillaumegravieririsafr G Gravier pascalesebillotirisafr P Sbillot", "08852308  see front matter  2011 Elsevier Ltd All rights reserved", "doi101016jcsl201106002 the programs as it is not dependent on the kind of document", "In this case the segmentation is based on the analysis of the distribution of words within the speech a topic change being detected when the vocabulary changes significantly", "With the improvement of automatic speech recognition ASR systems in recent years Ostendorf et al 2008 topic segmentation of spoken documents can now be performed on automatic transcripts of the speech material", "However most of the work in this direction directly apply methods developed for textual topic segmentation to automatic transcripts of spoken language without taking into account the specifics of TV programs transcripts transcription errors short topic segments", "These methods are often based on the notion of lexical cohesion which corresponds to the lexical relations that exist within a text mainly enforced by reiterations ie repetitions of the same words Mulbregt et al 1999 Utiyama and Isahara 2001 Malioutov and Barzilay 2006", "Alternately discourse markers obtained from a preliminary expert or learning process can also be used to identify topic boundaries Beeferman et al 1999 Christensen et al 2005", "Christensen et al", "2005 have established that transcription errors have little effect on the performance of a supervised segmentation algorithm using discourse markers", "However we have observed a large gap of performance between manual and automatic transcripts in previous work on topic segmentation of radio broadcasts using an unsupervised approach based on lexical cohesion Huet et al 2008", "This difference is mostly due to the specifics of the material on which we focus ie TV shows", "Indeed automatic transcripts of TV shows have certain peculiarities that are detrimental to topic segmentation and in general to natural language processing", "Firstly the error rate of the ASR system used even if it remains reasonable for news can be as high as 70 for challenging programs such as talk shows or debates", "Moreover TV programs are composed of topic segments that can be very short and contain few repetitions of vocabulary particularly in news where journalists make use of synonyms to avoid reiterations", "In our corpus we have measured that a word occurs on average 18 times in a topically coherent segment in broadcast news and 20 times in reports on current affairs for more details cf", "Section 43", "In order to overcome difficulties related to transcription errors some studies have suggested to add features specific to spoken documents to the sole concept of lexical cohesion", "For example Amaral and Trancoso 2003 exploits speaker detection to locate the anchor speaker in news program relating anchor speaker occurrences with new reports and hence with topic changes", "In Stolcke et al", "1999 prosody is used in addition to automatic transcription", "However such clues are seldom used in practice because their automatic extraction is difficult", "Moreover they imply domain and genre specific knowledge and are therefore highly dependent on the type of document", "The aim of this paper is to propose a segmentation method able to deal with peculiarities of professional videos transcription errors possibly short segments and limited number of repetitions while remaining generic enough", "This method is based on the criterion of lexical cohesion which is not dependent on a type of document since it is mainly enforced by word repetitions", "However lexical cohesion is not efficient when the number of reiterations is lowie when synonyms are used or topic segments are very shortand is sensitive to transcription errors two characteristics of our video material", "We propose several extensions to a measure of lexical cohesion based on generalized probabilities using a unigram language model in order to make this criterion more robust to spoken content", "On the one hand the measure of the lexical cohesion is modified by an original technique that incorporates two sources of additional information semantic relations highlighting the semantic proximity between words and confidence measures", "On the other hand we propose to use language model interpolation techniques so as to provide better estimates of the lexical cohesion on short segments", "The paper is organized as follows we first present the topic segmentation method based on lexical cohesion developed for the segmentation of textual documents and used as a baseline in this work", "In Section 3 extensions of the probabilistic lexical cohesion measure to improve robustness to TV program specifics are described", "The experimental setup is presented in Section 4", "Finally experimental results are extensively discussed in Section 5 before the presentation of future work"]}, "S10-1090": {"title": ["GPLSIIXA Using Semantic Classes to Acquire  Monosemous Training"], "abstract": ["This paper summarizes our participation in task 17 of SemEval2 Allwords WSD on a specific domain using a supervised classbased Word Sense Disambiguation system", "Basically we use Support Vector Machines SVM as learning algorithm and a set of simple features to build three different models", "Each model considers a different training corpus SemCor SC examples from monosemous words extracted automatically from background data BG and both SC and BG SCBG", "Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire classbased annotated examples from the domain text", "We use the classbased examples gathered from the domain corpus to adapt our traditional system trained on SemCor", "The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words obtaining results above the first sense baseline and the fifth best position in the competition rank"], "inroduction": ["As empirically demonstrated by the last SensEval and SemEval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed", "In fact supervised wordbased WSD systems are very dependent of the corpora used for training and testing the system Escudero et al 2000", "One possible reason could be the use of inappropriate level of abstraction", "where each class corresponds to a particular synset of the word", "But WordNet WN has been widely criticized for being a sense repository that often provides too finegrained sense distinctions for higher level applications like Machine Translation or Question  Answering", "In fact WSD at this level of granularity has resisted all attempts of inferring robust broadcoverage models", "It seems that many wordsense distinctions are too subtle to be captured by automatic systems with the current small volumes of wordsense annotated examples", "Thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of WN Hearst and Schu tze 1993 Peters et al 1998 Mihalcea and Moldovan 2001 Agirre and LopezDeLaCalle 2003 Navigli 2006 and Snow et al 2007", "That is they provide methods for grouping senses of the same word thus producing coarser word sense groupings for better disambiguation", "In contrast some research have been focused on using predefined sets of sensegroupings for learning classbased classifiers for WSD Segond et al 1997 Ciaramita and Johnson 2003 Villarejo et al 2005 Curran 2005 Kohomban and Lee 2005 and Ciaramita and Altun 2006", "That is grouping senses of different words into the same explicit and comprehensive semantic class", "Most of the later approaches used the original Lexico graphical Files of WN more recently called SuperSenses as very coarsegrained sense distinctions", "We suspect that selecting the appropriate level of abstraction could be on between both levels", "Thus we use the semantic classes modeled by the Basic Level Concepts1 BLC Izquierdo et al 2007", "Our previous research using BLC empirically demonstrated that this automatically derived Most supervised systems simply model each polysemous word as a classification problem 1 httpadimensiehueswebBLC 402 Proceedings of the 5th International Workshop on Semantic Evaluation ACL 2010 pages 402406 Uppsala Sweden 1516 July 2010", "Qc 2010 Association for Computational Linguistics set of meanings groups senses into an adequate level of abstraction in order to perform classbased Word Sense Disambiguation WSD Izquierdo et al 2009", "Now we also show that classbased WSD allows to successfully incorporate monosemous examples from the domain text", "In fact 3 and A has 2 so D is the first maximum", "A 2 the robustness of our classbased WSD approach is shown by our system that just uses the Sem Cor examples SC", "It performs without any kind B C D 2 3 BLC of domain adaptation as the Most Frequent Sense MFS baseline", "This paper describes our participation in SemEval2010 Task 17 Agirre et al 2010", "In section 2 semantic classes used and selection algorithm used to obtain them automatically from WordNet are described", "In section 3 the technique employed to extract monosemous examples from background data is described", "Section 4 explains the general approach of our system and the experiments designed and finally in section 5 the results and some analysis are shown"]}, "S12-1011": {"title": ["Learning  Semantics and Selectional Preference of AdjectiveNoun  Pairs"], "abstract": ["We investigate the semantic relationship between a noun and its adjectival modifiers", "We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers and adjectivenoun selectional preference", "Through a combination of novel and existing evaluations we test the degree to which adjectivenoun relationships can be cat egorised", "We analyse the effect of lexical context on these relationships and the efficacy of the latent semantic representation for disambiguating word meaning"], "inroduction": ["Developing models of the meanings of words and phrases is a key challenge for computational linguistics", "Distributed representations are useful in capturing such meaning for individual words Sato et al 2008 Maas and Ng 2010 Curran 2005", "However finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic Mitchell and Lapata 2008 Baroni and Zamparelli 2010 Grefenstette and Sadrzadeh 2011", "It is in this area that our paper makes its contribution", "The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques", "However such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference", "We propose a probabilistic model of the semantic tion of noun and adjective semantics together with their compositional probabilities", "We employ this formulation to give a dual view of nounmodifier semantics the induced latent variables provide an explicit account of selectional preference while the marginal distributions of the latent variables for each word implicitly produce a distributed representation", "Most related work on selectional preference uses classbased probabilities to approximate sparse individual probabilities", "Relevant papers include O Seaghdha 2010 who evaluates several topic models adapted to learning selectional preference using cooccurence and Baroni and Zamparelli 2010 who represent nouns as vectors and adjectives as matrices thus treating them as functions over noun meaning", "Again inference is achieved using cooccurrence and dimensionality reduction"]}, "S12-1023": {"title": ["Regular polysemy A distributional model"], "abstract": ["Many types of polysemy are not word specific but are instances of general sense alternations such as ANIMALFOOD", "Despite their pervasiveness regular alternations have been mostly ignored in empirical computational semantics", "This paper presents a a general framework which grounds sense alternations in corpus data generalizes them above individual words and allows the prediction of alternations for new words and b a concrete unsupervised implementation of the framework the Centroid Attribute Model", "We evaluate this model against a set of 2400 ambiguous words and demonstrate that it outperforms two baselines"], "inroduction": ["One of the biggest challenges in computational semantics is the fact that many words are polysemous", "For instance lamb can refer to an animal as in The lamb squeezed through the gap or to a food item as in Sue had lamb for lunch", "Polysemy is pervasive in human language and is a problem in almost all applications of NLP ranging from Machine Translation as word senses can translate differently to Textual Entailment as most lexical entailments are sensespecific", "The field has thus devoted a large amount of effort to the representation and modeling of word senses", "The arguably most prominent effort is Word Sense Disambiguation WSD Navigli 2009 an invitro task whose goal is to identify which of a set of predefined senses is the one used in a given context", "In work on WSD and other tasks related to pol ysemy such as word sense induction sense alternations are treated as wordspecific", "As a result a model for the meaning of lamb that accounts for the relation between the animal and food senses cannot predict that the same relation holds between instances of chicken or salmon in the same type of contexts", "A large number of studies in linguistics and cognitive science show evidence that there are regulari ties in the way words vary in their meaning Apresjan 1974 Lakoff and Johnson 1980 Copestake and Briscoe 1995 Pustejovsky 1995 Gentner et al 2001 Murphy 2002 due to general analogical processes such as regular polysemy metonymy and metaphor", "Most work in theoretical linguistics has focused on regular systematic or logical polysemy which accounts for alternations like ANIMALFOOD", "Sense alternations also arise from metaphorical use of words as dark in dark glassdark mood and also from metonymy when for instance using the name of a place for a representative as in Germany signed the treatise", "Disregarding this evidence is empirically inadequate and leads to the wellknown lexical bottleneck of current word sense models which have serious problems in achieving high coverage Navigli 2009", "We believe that empirical computational semantics could profit from a model of polysemy1 which a is applicable across individual words and thus capable of capturing general patterns and generalizing to new 1 Our work is mostly inspired in research on regular polysemy", "However given the fuzzy nature of regularity in meaning variation we extend the focus of our attention to include other types of analogical sense construction processes", "151 First Joint Conference on Lexical and Computational Semantics SEM pages 151160 Montreal Canada June 78 2012", "Qc 2012 Association for Computational Linguistics words and b is induced in an unsupervised fashion from corpus data", "This is a longterm goal with many unsolved subproblems", "The current paper presents two contributions towards this goal", "First since we are working on a relatively unexplored area we introduce a formal framework that can encompass different approaches Section 2", "Second we implement a concrete instantiation of this framework the unsupervised Centroid Attribute Model Section 3 and evaluate it on a new task namely to detect which of a set of words in stantiate a given type of polysemy Sections 4 and 5", "We finish with some conclusions and future work Section 7"]}, "S12-1040": {"title": ["UGroningen  Negation detection with Discourse Representation Structures"], "abstract": ["We use the NLP toolchain that is used to construct the Groningen Meaning Bank to address the task of detecting negation cue and scope as defined in the shared task Resolving the Scope and Focus of Negation", "This toolchain applies the CC tools for parsing using the formalism of Combinatory Categorial Grammar and applies Boxer to produce semantic representations in the form of Discourse Representation Structures DRSs", "For negation cue detection the DRSs are converted to flat nonrecursive structures called Discourse Representation Graphs DRGs", "DRGs simplify cue detection by means of edge labels representing relations", "Scope detection is done by gathering the tokens that occur within the scope of a negated DRS", "The result is a system that is fairly reliable for cue detection and scope detection", "Furthermore it provides a fairly robust algorithm for detecting the negated event or property within the scope"], "inroduction": ["Nothing is more home to semantics than the phenomenon of negation", "In classical theories of meaning all states of affairs are divided in two truth values and negation plays a central role to determine which truth value is at stake for a given sentence", "Negation lies at the heart of deductive inference of which consistency checking searching for contradictions in texts is a prime example in natural language understanding", "It shouldnt therefore come as a surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics", "In this paper we present and evaluate a system that transforms texts into logical formulas  using the CC tools and Boxer Bos 2008  in the context of the shared task on recognising negation in English texts Morante and Blanco 2012", "We will first sketch the background and the basics of the formalism that we employ in our analysis of negation Section 2", "In Section 3 we explain how we detect negation cues and scope", "Finally in Section 4 we present the results obtained in the shared task and we discuss them in Section 5"]}, "S13-1002": {"title": ["11"], "abstract": ["We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks MLNs", "We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives", "We also show that distributional phrase similarity used as textual inference rules created on the fly improves its performance"], "inroduction": ["Tasks in natural language semantics are very diverse and pose different requirements on the underlying formalism for representing meaning", "Some tasks require a detailed representation of the structure of complex sentences", "Some tasks require the ability to recognize nearparaphrases or degrees of similarity between sentences", "Some tasks require logical inference either exact or approximate", "Often it is necessary to handle ambiguity and vagueness in meaning", "Finally we frequently want to be able to learn relevant knowledge automatically from corpus data", "There is no single representation for natural language meaning at this time that fulfills all requirements", "But there are representations that meet some of the criteria", "Logicbased representations Montague 1970 Kamp and Reyle 1993 provide an expressive and flexible formalism to express even complex propositions and they come with standardized inference mechanisms", "Distributional mod simhamster gerbil  w gerbil hamster x hamsterx  gerbilx  f w Figure 1 Turning distributional similarity into a weighted inference rule els Turney and Pantel 2010 use contextual similarity to predict semantic similarity of words and phrases Landauer and Dumais 1997 Mitchell and Lapata 2010 and to model polysemy Schu tze 1998 Erk and Pado  2008 Thater et al 2010", "This suggests that distributional models and logic based representations of natural language meaning are complementary in their strengths Grefenstette and Sadrzadeh 2011 Garrette et al 2011 which encourages developing new techniques to combine them", "Garrette et al", "2011 2013 propose a framework for combining logic and distributional models in which logical form is the primary meaning representation", "Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form as illustrated in Figure 1", "Finally Markov Logic Networks Richardson and Domingos 2006 MLNs are used to perform weighted inference on the resulting knowledge base", "However they only employed singleword distributional similarity rules and only evaluated on a small 11 Second Joint Conference on Lexical and Computational Semantics SEM Volume 1 Proceedings of the Main Conference and the Shared Task pages 1121 Atlanta Georgia June 1314 2013", "Qc 2013 Association for Computational Linguistics set of short handcrafted test sentences", "In this paper we extend Garrette et als approach and adapt it to handle two existing semantic tasks recognizing textual entailment RTE and semantic textual similarity STS", "We show how this single semantic framework using probabilistic logical form in Markov logic can be adapted to support both of these important tasks", "This is possible because MLNs constitute a flexible programming language based on probabilistic logic Domingos and Lowd 2009 that can be easily adapted to support multiple types of linguistically useful inference", "At the word and short phrase level our approach model entailment through distributional similarity Figure 1", "If X and Y occur in similar contexts we assume that they describe similar entities and thus there is some degree of entailment between them", "At the sentence level however we hold that a stricter logicbased view of entailment is beneficial and we even model sentence similarity in STS as entail ment", "There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data", "First we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words", "In comparison to existing methods for creating textual inference rules Lin and Pantel 2001b Szpek tor and Dagan 2008 these rules are computed on the fly as needed rather than precompiled", "Second we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors", "We replace deterministic conjunction by an average combiner which encodes causal independence Natarajan et al 2010", "We show that our framework is able to handle both sentence similarity STS and textual entailment RTE by making some simple adaptations to the MLN when switching between tasks", "The framework achieves reasonable results on both tasks", "On STS we obtain a correlation of r  066 with full logic r  073 in a system with weakened variable binding and r  085 in an ensemble model", "On RTE1 we obtain an accuracy of 057", "We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS", "Al though other approaches could be adapted to handle both RTE and STS we do not know of any other methods that have been explicitly tested on both problems"]}, "W00-0202": {"title": ["Representation of Actions as an Interlingua"], "abstract": ["We present a Parameterized Action Representation PAR that provides a conceptual representation ofdifferent types of actions used to animate virtual human agents in a simulated 3D environment", "These actions involve changes of state changes of location kinematic and exertion of force dynamic", "PARSare hierarchical parameterized structures that facilitate both visual and verbal expressions", "In order to support the animation of the actions PARShave to make explicit many details that are often underspecified in the language", "This detailed level ofrepresentation also provides a suitable pivot representation for generation in other natural languages ie a form of interlingua", "We show examples of how certain divergences in machine translation can be solved by our approach focusing specifically on how verbframed and satelliteframed languages can use our representation"], "inroduction": ["In this paper we describe a Parameterized Ac tion Representation PAR Badler et al 1999that provides a conceptual representation of differ ent types of actions used to animate virtual humanagents in a simulated 3D environment", "These ac tions involve changes of state changes of location kinematic and exertion of force dynamic", "PARSare hierarchical parameterized structures that fa cilitate both visual and verbal expressions Badler et al 2000", "In order to support the animation ofthe actions PARS have to make explicit many de tails that are often underspecified in the language", "This detailed level of representation is well suitedfor an interlingua for machine translation applications since the animations of actions  and therefore the PARS that control them  will be equivalent for the same actions described in different lan guages", "These representations can be incorporated into a system which uses PARbased animations asa workbench for creating accurate conceptual representations which can map to seeral different lan guages as well as produce faithful animations", "The verb classes we are currently considering in this light involve explicit physical actions such asthose expressed in the motion verb class and contact verb class Levin 1993", "Since we are employ ing PAR as an interlingual representation we willshow examples of how it can handle certain diver gences in machine translation focusing specifically on how verbframed and satelliteframed languages Talmy 1991 can yield equivalent actions in this representation"]}, "W00-0701": {"title": ["In  Proceedings  of CoNLL2000 and  LLL2000 pages 16  Lisbon  Portugal 2000"], "abstract": ["This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing", "It shows that these ap proaches can all be explained using a single dis tribution free inductive principle related to the pac model of learning", "Furthermore they all make predictions using the same simple knowl edge representation  a linear representation over a common feature space", "This is signifi cant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be ex tended to learn from more structured knowl edge intensive examples as part of a learning centered approach to higher level natural lan guage inferences"], "inroduction": ["Many important natural language inferences can be viewed as problems of resolving phonetic syntactic semantics or pragmatics ambiguities based on properties of the surrounding context", "It is generally accepted that a learning compo nent must have a central role in resolving these context sensitive ambiguities and a significant amount of work has been devoted in the last few years to developing learning methods for these tasks with considerable success", "Yet our un derstanding of when and why learning works in this domain and how it can be used to support increasingly higher level tasks is still lacking", "This article summarizes work on developing a learning theory account for the major learning approaches used in NL", "While the major statistics based methods used in NLP are typically developed with a  This research is supported by NSF grants IIS9801638 SBR9873450 and IIS9984168", "Bayesian view in mind the Bayesian principle cannot directly explain the success and robust ness of these methods since their probabilistic assumptions typically do not hold in the data", "Instead we provide this explanation using a sin gle distribution free inductive principle related to the pac model of learning", "We describe the unified learning framework and show that in addition to explaining the success and robust ness of the statistics based methods it also ap plies to other machine learning methods such as rule based and memory based methods", "An important component of the view devel oped is the observation that most methods use the same simple knowledge representation", "This is a linear representation over a new feature space  a transformation of the original instance space to a higher dimensional and more expres sive space", "Methods vary mostly algorithmicly in ways they derive weights for features in this space", "This is significant both to explaining the generalization properties of these methods and to developing an understanding for how and when can these methods be extended to learn from more structured knowledge intensive ex amples perhaps hierarchically", "These issues are briefly discussed and we emphasize the impor tance of studying knowledge representation and inference in developing a learning centered ap proach to NL inferences"]}, "W00-0733": {"title": ["In Proceedings of CoNLL2000 and LLL2000 pages 151153 Lisbon Portugal 2000"], "abstract": ["None"], "inroduction": ["We will apply a systeminternal combination of memorybased learning classifiers to the CoNLL2000 shared task finding base chunks", "Apart from testing different combination meth ods we will also examine if dividing the chunk ing process in a boundary recognition phase and a type identification phase would aid perfor mance"]}, "W01-0502": {"title": ["A Sequential Model for MultiClass Classification"], "abstract": ["Many classification problems require decisions among a large number of competing classes", "These tasks however are not handled well by general purpose learning methods and are usually addressed in an adhoc fashion", "We suggest a general approach  a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining with high probability the presence of the true outcome in the candidates set", "Some theoretical and computational properties of the model are discussed and we argue that these are important in NLPlike domains", "The advantages of the model are illustrated in an experiment in part ofspeech tagging"], "inroduction": ["A large number of important natural language inferences can be viewed as problems of resolving ambiguity either semantic or syntactic based on properties of the surrounding context", "These in turn can all be viewed as classification problems in which the goal is to select a class label from among a collection of candidates", "Examples include partof speech tagging wordsense disambiguation accent restoration word choice selection in machine translation contextsensitive spelling correction word selection in speech recognition and identifying discourse markers", "Machine learning methods have become the most popular technique in a variety of classification problems of these sort and have shown significant success", "A partial list consists of Bayesian classifiers Gale et al 1993 decision lists Yarowsky 1994 Bayesian hybrids Golding 1995 HMMs Charniak 1993 inductive logic methods Zelle and Mooney 1996 memoryThis research is supported by NSF grants IIS9801638 IIS 0085836 and SBR987345", "based methods Zavrel et al 1997 linear classifiers Roth 1998 Roth 1999 and transformation based learning Brill 1995", "In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large  all words in words selection problems all possible tags in tagging problems etc Since general purpose learning algorithms do not handle these multiclass classification problems well see below most of the studies do not address the whole problem rather a small set of candidates typically two is first selected and the classifier is trained to choose among these", "While this approach is important in that it allows the research community to develop better learning methods and evaluate them in a range of applications it is important to realize that an important stage is missing", "This could be significant when the classification methods are to be embedded as part of a higher level NLP tasks such as machine translation or information extraction where the small set of candidates the classifier can handle may not be fixed and could be hard to determine", "In this work we develop a general approach to the study of multiclass classifiers", "We suggest a sequential learning model that utilizes almost general purpose classifiers to sequentially restrict the number of competing classes while maintaining with high probability the presence of the true outcome in the candidate set", "In our paradigm the sought after classifier has to choose a single class label or a small set of labels from among a large set of labels", "It works by sequentially applying simpler classifiers each of which outputs a probability distribution over the candidate labels", "These distributions are multiplied and thresholded resulting in that each classifier in the sequence needs to deal with a significantly smaller number of the candidate labels than the previous classifier", "The classifiers in the sequence are selected to be simple in the sense that they typically work only on part of the feature space where the decomposition of feature space is done so as to achieve statistical independence", "Simple classifier are used since they are more likely to be accurate they are chosen so that with high probability whp they have one sided error and therefore the presence of the true label in the candidate set is maintained", "The order of the sequence is determined so as to maximize the rate of decreasing the size of the candidate labels set", "Beyond increased accuracy on multiclass classification problems  our scheme improves the computation time of these problems several orders of magnitude relative to other standard schemes", "In this work we describe the approach discuss an experiment done in the context of partofspeech pos tagging and provide some theoretical justifications to the approach", "Sec", "2 provides some background on approaches to multiclass classification in machine learning and in NLP", "In Sec", "3 we describe the sequential model proposed here and in Sec", "4 we describe an experiment the exhibits some of its advantages", "Some theoretical justifications are outlined in Sec", "5"]}, "W01-0505": {"title": ["ffflfi ffffiffi"], "abstract": ["The problem of finding lexical alignments for givensentence pairs is computationally expensive", "Furthermore it is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of wordforword correspondences is lowThis paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments", "We describe how the features improve the performance of the lexical alignment model", "The structural features providethe information for the correspondences of partsofspeech POS sequences which are useful in translation", "Based on maximum entropy ME concept the structural features are incrementally selected which are later embedded in the lexical alignment modelIt turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge"], "inroduction": ["Aligned bitexts are useful for the derivation ofbilingual lexical resources which are used for ma chine translation and cross languages informationretrieval", "Thus a lot of approaches have been sug gested to find sets of corresponding word tokens Brown et al 1993 Berger et al 1996 Melamed1997 phrase Shin et al 1996 noun phrase Ku piec 1993 and collocation Smadja et al 1996 in a bitext", "Some works have used lexical association measures for finding word correspondences Gale and Church1991 Fung and Church 1994", "However the associ ation measures can be misled in cases where a word in a source language frequently cooccurs with more than one word in a target language or in cases of indirect associationapos Melamed 1997", "In other works iterative parameter reestimation aposSuppose that Uk and vk are indeed mutual translation and Uk and Ukki often cooccur in text", "Then vk and 241 will also cooccur more than expected by chance  which is represented as indirect association", "techniques based on IBM model 15 2 have been employed Brown et al 1993", "They were usually incorporated in the EM algorithm Brown et al 1993 Kupiec 1993 Tillmann and Ney 2000 Och et al 2000", "However we are often faced with some difficulties as follows when the IBM modelbased approaches are directly applied to the alignment especially on bitext involving a less closely related language pair", "1", "It needs excessive iteration time for parame", "ter estimation and high decoding complexityThus most systems assumed onetoone correspondence to reduce computational complexity", "However word sequences are not trans lated literally word for word", "For example incases of collocations compound nouns and ambiguous words with different meaning depen dent on the context they require phraselevel correspondences"]}, "W01-1404": {"title": ["Approximating ContextFree by Rational Transduction for ExampleBased MT MarkJan Nederhof ATT LabsResearch 180 Park Avenue Florham Park NJ 07932 and Alfa Informatica RUG PO Box 716 NL9700 AS Groningen The Netherlands Abstract Existing studies show that a weighted contextfree transduction of reasonable quality can be effectively learned from examples This paper investigates the approximation of such transduction by means of weighted rational transduction The advantage is increased processing speed which benefits realtime applications involving spoken language 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation Some of these studies have con centrated on finitestate or extended finitestate machinery such as Vilar and others 1999 others have chosen models closer to context free grammars and contextfree transduction such as Alshawi et al 2000 Watanabe et al 2000 Yamamoto and Matsumoto 2000 and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others 1990 and Tillmann and Ney 2000 In this paper we will investigate both contextfree and finitestate models The basis for our study is context free transduction since that is a powerful model of translation which can in many cases adequately describe the changes of word"], "abstract": ["Existing studies show that a weighted contextfree transduction of reasonable quality can be effectively learned from examples", "This paper investigates the approximation of such transduction by means of weighted rational transduction", "The advantage is increased processing speed which benefits real time applications involving spoken language"], "inroduction": ["Several studies have investigated automatic or partly automatic learning of transductions for machine translation", "Some of these studies have concentrated on finitestate or extended finitestate machinery such as Vilar and others 1999 others have chosen models closer to contextfree grammars and contextfree transduction such as Alshawi et al 2000 Watanabe et al 2000 Yamamoto and Matsumoto 2000 and yet other studies cannot be comfortably assigned to either of these two frameworks such as Brown and others 1990 and Tillmann and Ney 2000", "In this paper we will investigate both context free and finitestate models", "The basis for our study is contextfree transduction since that is a powerful model of translation which can in many cases adequately describe the changes of word The second address is the current contact address supported by the Royal Netherlands Academy of Arts and Sciences current secondary affiliation is the German Research Center for Artificial Intelligence DFKI", "order between two languages and the selection of appropriate lexical items", "Furthermore for limited domains automatic learning of weighted contextfree transductions from examples seems to be reasonably successful", "However practical algorithms for computing the most likely contextfree derivation have a cubic time complexity in terms of the length of the input string or in the case of a graph output by a speech recognizer in terms of the number of nodes in the graph", "For certain lexicalized contextfree models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter Eisner and Satta 1999", "This may pose problems especially for realtime speech systems", "Therefore we have investigated approximation of weighted contextfree transduction by means of weighted rational transduction", "The finitestate machinery for implementing the latter kind of transduction in general allows faster processing", "We can also more easily obtain robustness", "We hope the approximating model is able to preserve some of the accuracy of the contextfree model", "In the next section we discuss preliminary definitions adapted from existing literature making no more than small changes in presentation", "In Section 3 we explain how contextfree transduction grammars can be represented by ordinary contextfree grammars plus a phase of postprocessing", "The approximation is discussed in Section 4", "As shown in Section 5 we may easily process input in a robust way ensuring we always obtain output", "Section 6 discusses empirical results and we end the paper with conclusions"]}, "W01-1407": {"title": ["Toward hierarchical models for statistical machine translation of inflected languages Sonja Nie  en and Hermann Ney Lehrstuhl fur Informatik VI Computer Science Department RWTH Aachen  University of Technology D52056 Aachen Germany"], "abstract": ["In statistical machine translation correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models", "Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other", "In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the in terdependencies of the different derivatives", "We do this along two directions Usage of hierarchical lexicon models and the introduction of equivalence classes in order to ignore information not relevant for the translation task", "The improvement of the translation results is demonstrated on a GermanEnglish corpus"], "inroduction": ["The statistical approach to machine translation has become widely accepted in the last few years", "It has been successfully applied to realistic tasks in various national and international research programs", "However in many applications only small amounts of bilingual training data are available for the desired domain and language pair and it is highly desirable to avoid at least parts of the costly data collection process", "Some recent publications have dealt with the problem of translation with scarce resources", "Brown et al 1994 describe the use of dictionaries", "AlOnaizan et al 2000 report on an experiment of TetuntoEnglish translation by different groups including one using statistical machine translation", "They assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries", "Nevertheless they found that human mind is very well capable of deriving dependencies such as morphology cognates proper names spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based machine translation", "The additional information results from complex reasoning and it is not directly accessible from the full word form representation of the data", "In this paper we take a different point of view Even if full bilingual training data is scarce monolingual knowledge sources like morphological analyzers and data for training the target language model as well as conventional dictionaries one word and its translation per entry may be available and of substantial usefulness for improving the performance of statistical translation systems", "This is especially the case for highly inflected languages like German", "We address the question of how to achieve a better exploitation of the resources for training the parameters for statistical machine translation by taking into account explicit knowledge about the languages under consideration", "In our approach we introduce equivalence classes in order to ignore information not relevant to the translation process", "We furthermore suggest the use of hierarchical lexicon models", "The paper is organized as follows", "After reviewing the statistical approach to machine translation we first explain our motivation for examining the morphological characteristics of an inflected language like German", "We then describe the chosen output representation after the analysis and present our approach for exploiting the information from morphosyntactic analysis", "Experimental results on the GermanEnglish Verbmobil Source Language Text morphosyntactic Analysis Transformation f J 1 Global Search maximize Pr e I  J  e I  task are reported", "1 Pr f 1 1", "I"]}, "W02-0503-parscit130908": {"title": [""], "abstract": ["Many papers have discussed different aspects of Arabic verb morphology", "Some of them used patterns others used patterns and affixes", "But very few have discussed Arabic noun morphology particularly for nouns that are not derived from verbs", "In this paper we describe a learning system that can analyze Arabic nouns to produce their morphological information and their paradigms with respect to both gender and number using a rule base that uses suffix analysis as well as pattern analysis", "The system utilizes userfeedback to classify the noun and identify the group that it belongs to"], "inroduction": ["A morphology system is the backbone of a natural language processing system", "No application in this field can survive without a good morphology system to support it", "The Arabic language has its own features that are not found in other languages", "That is why many researchers have worked in this area", "AlFedaghi and AlAnzi 1989 present an algorithm to generate the root and the pattern of a given Arabic word", "The main concept in the algorithm is to locate the position of the rootaposs letters in the pattern and examine the letters in the same position in a given word to see whether the tri graph forms a valid Arabic root or not", "AlShalabi 1998 developed a system that removes the longest possible prefix from the word where the three letters of the root must lie somewhere in the first four or five characters of the remainder", "Then he generates some combinations and checks each one of them with all the roots in the file", "AlShalabi reduced the processing but he discussed this from point of view of verbs not nouns", "Anne Roeck and Waleed AlFares 2000 developed a clustering algorithm for Arabic words sharing the same verbal root", "They used rootbased clusters to substitute for dictionaries in indexing for information retrieval", "Beesley and Karttunen 2000 described a new technique for constructing finitestate transducers that involves reapplying a regularexpression compiler to its own output", "They implementedthe system in an algorithm called compile replace", "This technique has proved useful for handling nonconcatenate phenomena and they demonstrate it on Malay fullstem reduplication and Arabic stem interdigitations", "Most verbs in the Arabic language follow clear rules that define their morphology and generate their paradigms", "Those nouns that are not derived from roots do not seem to follow a similar set of welldefined rules", "Instead there are groups showing family resemblances", "We believe that nouns in Arabic that are not derived from roots are governed not only by phonological rules but by lexical patterns that must be identified and stored for each noun", "Like irregular verbs in English their forms are determined by history and etymology not just phonology", "Among many other examples Pinker 1999 points to the survival of past forms became for become and overcame for overcome modeled on came for come while succumb with the same sound pattern has a regular past form succumbed", "The same kinds of phenomena are especially apparent for proper nouns in Arabic derived from Indian and Persian names", "Pinker uses examples like this as well as emerging research in neurophysiology to argue for the coexistence of phonological rules and lexical storage of English verb patterns", "We believe that further work in Arabic computational linguistics requires the development of a pattern bank for nouns", "This paper describes the tool that we have built for this purpose", "While the set of patterns for common nouns in Arabic may soon be established newspapers and other dynamic sources of language will always contain new proper names so we expect our tool to be a permanent part of our system even though we may need it less often as time goes on"]}, "W02-1005": {"title": ["Proceedings of the Conference on Empirical Methods in Natural"], "abstract": ["This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classification tasks", "We present a new classification correction technique that successfully addresses the problem of underestimation of infrequent classes in the training data", "We show that the mixture models are boostingfriendly and that both Adaboost and our original correction technique can improve the results of the raw model significantly achieving state oftheart performance on several standard test sets in four languages", "With substantially different output to Nave Bayes and other statistical methods the investigated models are also shown to be effective participants in classifier combination"], "inroduction": ["The focus tasks of this paper are two related problems in lexical ambiguity resolution Word Sense Disambiguation WSD and Context Sensitive Spelling Correction CSSC", "Word Sense Disambiguation has a long history as a computational task Kelly and Stone 1975 and the field has recently supported largescale international system evaluation exercises in multiple languages SEN SEVA L1 Kilgarriff and Palmer 2000 and SEN SEVA L2 Edmonds and Cotton 2001", "General purpose Spelling Correction is also a longstanding task eg McIlroy 1982 traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest valid word in a dictionary or a morphological variant typically ignoring context", "Yet Kukich 1992 observed that about 2550 of the spelling errors found in modern documents are either contextinappropriate misuses or substitutions of valid words such as principal and principle which are not detected by traditional spelling cor rectors", "Previous work has addressed the problem of CSSC from a machine learning perspective including Bayesian and Decision List models Golding 1995 Winnow Golding and Roth 1996 and TransformationBased Learning Mangu and Brill 1997", "Generally both tasks involve the selection between a relatively small set of alternatives per keyword eg sense ids such as churchBU ILD IN G and churchIN STITU T IO N or commonly confused spellings such as quiet and quite and are dependent on local and longdistance collocational and syntactic patterns to resolve between the set of alternatives", "Thus both tasks can share a common feature space data representation and algorithm infrastructure", "We present a framework of doing so while investigating the use of mixture models in conjunction with a new errorcorrection technique as competitive alternatives to Bayesian models", "While several authors have observed the fundamental similarities between CSSC and WSD eg Berleant 1995 and Roth 1998 to our knowledge no previous comparative empirical study has tackled these two problems in a single unified framework"]}, "W02-1108": {"title": [""], "abstract": ["We propose a method for semiautomatic classification of verbs to Levin classes via the semantic network of WordNet", "The method involvesfirst classifying entire WordNet senses to semantic classes and then classifying individual verbs on the basis of their WordNet senses", "We report evaluation which shows that the method can be used to build a verb classification accurateenough for practical NLP use", "The WordNetLevin mapping produced as a byproduct may in turn be used to supplement WordNet with novel information"], "inroduction": ["Linguistic research has shown that verbs fallinto classes distinctive in terms of their syntac tic and semantic properties Jackendoff 1990 Hale and Keyser 1993 Levin 1993 Pinker 1989", "For example verbs which share the meaning component of aposmotionapos eg fly and walk tend to behave similarly also in terms of subcategorization and can thus be grouped to a linguistically coherent class", "While the correspondence between the syntax and semantics of verbs is arguably not perfect and while the whole notion of a verb class is somewhat elusiveapos verb classifications can be constructed which provide a generalization over a range of syntactic and semantic properties of verbs", "Such classifications are particularly useful from a practical NLP point of view", "They can be used as a means of reducing redundancy inthe lexicon and for filling gaps in lexical knowl edge", "To a certain extent they enable inferring 1For example as most verbs can be characterized byseveral meaning components there is potential for crossclassification", "Therefore different equally viable classifi cation schemes can be constructedthe semantics of a word on the basis of its syn tactic behaviour and the syntax of a word on the basis of its semantic behaviour", "Verb classifications have in fact been usedto support various NLP tasks including ma chine translation language generation Dorr 1997 document classification Klavans and Kan 1998 lexicography Sanfilippo 1994 andlexical acquisition such as word sense disambiguation Dorr and Jones 1996 and subcate gorization acquisition Korhonen 2002b", "The verb classification employed most widely in NLP is Levinaposs taxonomy of verbs and their classes Levin 1993", "Levin classes are based onthe ability of a verb to occur in specific diathe sis alternations ie specific pairs of syntacticframes which are assumed to be meaning re tentive", "The classification covers a substantial number of diathesis alternations occurring in English", "It is not however exhaustive", "More work is required on extending and refining it until a comprehensive resource can be obtained suitable for largescale NLP use Dorr and Jones 1996 Dorr 1997 Korhonen 2002bPerhaps the most challenging task is to ex tend the classification with new participants", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming Levin 1993 Dang et al 1998", "Fully automatic classification on the other hand is fast but suffers from low accuracy Dorr 1997 Stevenson and Merlo 1999", "In this paper we propose combining the strengths of theseapproaches", "We present a method for semi automatic semantic classification of verbs which exploits automatic techniques but also allows for some manual interventionThe method involves classifying verbs seman tically via the semantic network of WordNet Miller 1990", "Unlike Levinaposs source Wordnet is a comprehensive lexical database", "Although it classifies verbs on a purely semantic basis the syntactic regularities studied by Levin are to some extent reflected by semantic relatedness asit is represented by WordNetaposs particular struc ture Dorr 1997 Fellbaum 1999", "Our methodmakes use of this partial overlap between WordNet and Levin classes", "It involves first classify ing entire WordNet senses to semantic classesand then classifying individual verbs on the ba sis of their WordNet sensesWe use this method to classify verbs in a num ber of WordNet files and report experimental evaluation which shows that the method is fairlyaccurate and can also be used to build a clas sification suitable for practical NLP use", "The classification built using our method can alsobe used to benefit linguistic research as knowledge about novel verb and verb class associa tions can be used to test and enrich linguistic theory eg", "Levin 1993As a byproduct the method produces a map ping between WordNet senses and Levin classes", "This mapping  once comprehensive  can actas a useful supplement to WordNet which incorporates information about diathesis alterna tions and semantic verb classes", "Currently this information is absent in WordNet", "We discuss the background for our work in section 2", "In section 3 we describe the method for classifying verbs semantically via WordNet", "The details of the experimental evaluation of our method are supplied in section 4", "Section 5 concludes with directions for future work"]}, "W03-0423": {"title": ["Named Entity Recognition with a Maximum Entropy Approach"], "abstract": ["NONE"], "inroduction": ["The named entity recognition NER task involves identifying noun phrases that are names and assigning a class to each name", "This task has its origin from the Message Understanding Conferences MUC in the 1990s a series where o refers to the outcome h the history or context and Z h is a normalization function", "The features used in the maximum entropy framework are binary", "An example of a feature function is 1 if o  orgB word  PETER of conferences aimed at evaluating systems that extract information from natural language texts", "It became evi fj h o  0 otherwise dent that in order to achieve good performance in information extraction a system needs to be able to recognize names", "A separate subtask on NER was created in MUC 6 and MUC7 Chinchor 1998", "Much research has since been carried out on NER using both knowledge engineering and machine learning approaches", "At the last CoNLL in 2002 a common NER task was used to evaluate competing NER systems", "In this years CoNLL the NER task is to tag noun phrases with the following four classes person PER organization ORG location LOC and miscellaneous MISC", "This paper presents a maximum entropy approach to the NER task where NER not only made use of local context within a sentence but also made use of other occurrences of each word within the same document to extract useful features global features", "Such global features enhance the performance of NER Chieu and Ng 2002b"]}, "W03-0432": {"title": ["Named Entity Recognition Using a Characterbased Probabilistic Approach"], "abstract": ["We present a named entity recognition and classification system that uses only probabilistic characterlevel features", "Classifications by multiple orthographic tries are combined in a hidden Markov model framework to incorporate both internal and contextual evidence", "As part of the system we perform a preprocessing stage in which capitalisation is restored to sentenceinitial and allcaps words with high accuracy", "We report fvalues of 8665 and 7978 for English and 5062 and 5443 for the German datasets"], "inroduction": ["Language independent NER requires the development of a metalinguistic model that is sufficiently broad to accommodate all languages yet can be trained to exploit the specific features of the target language", "Our aim in this paper is to investigate the combination of a character level model orthographic tries with a sentencelevel hidden Markov model", "The local model uses affix information from a word and its surrounds to classify each word independently and relies on the sentencelevel model to determine a correct state sequence", "Capitalisation is an oftenused discriminator for NER but can be misleading in sentenceinitial or allcaps text", "We choose to use a model that makes no assumptions about the capitalisation scheme or indeed the character set of the target language", "We solve the problem of misleading case in a novel way by removing the effects of sentenceinitial or allcaps capitalisation", "This results in a simpler language model and easier recognition of named entities while remaining strongly language independent"]}, "W03-0910": {"title": ["Deriving VerbMeaning Clusters from Syntactic Structure"], "abstract": ["This paper presents a methodology for using the argument structure of sentences as encoded by the PropBank project to develop clusters of verbs with similar meaning and usage These clusters can be favorably compared to the classes developed by the VerbNet project The most interesting cases are those where the clustering methodology suggests new members for VerbNet classes which will then be associated with the semantic predicates for that class"], "inroduction": ["There can be no doubt that meaning is dependent upon a great number of factors The diculty in higherlevel comprehension in Natural Language Processing is due to the identification of these factors as well as the successful implementation of methods for handling them Early generative models of syntax failed to answer the meaning question because of their assumption that semantics was a natural outcome of structure thus neglecting the other factors involved More recent works such as those found in WordNet Miller 1985 Fellbaum 1998 and its descendents fall short for the inverse reason they focus solely on lexical meaning while ignoring structure altogether Another factor in the meaning game is the relationship between individual lexical items of the same function How are all determiners alike for example and how are they individually different", "Are there subclasses within the class of determiners and how do these subclasses differ from each other", "Human beings are supreme patternmatchers and assign things to categories almost to a fault The categories into which semantically loaded items are groupedmust therefore be one of the factors leading to higher level comprehension and knowledge of what these categories are and how they are built can help solve the greater meaning question"]}, "W04-0705": {"title": ["Applying Coreference to Improve Name Recognition"], "abstract": ["We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese", "We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger", "For names with low confidence we show how these names can be filtered using coreference features to improve accuracy", "In addition we present rules which use coreference information to correct some name tagging errors", "Finally we show how these gains can be magnified by clustering documents and using crossdocument coreference in these clusters", "These combined methods yield an absolute improvement of about 31 in tagger F score"], "inroduction": ["The problem of name recognition and classification has been intensively studied since1995 when it was introduced as part of the MUC 6 Evaluation Grishman and Sundheim 1996", "A wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Bikel et al 1997 Maximum Entropy methods Borthwick et al 1998 Chieu and Ng 2002 Decision Trees Sekine et al 1998 Conditional Random Fields McCallum and Li 2003 Classbased Language Model Sun et al 2002 Agentbased Approach Ye et al 2002 and Support Vector Machines", "However the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ", "In particular most of these methods classify an instance of a name based on the information about that instance alone and very local context of that instance  typically one or 1 The best results reported for Chinese named entity recognition on the MET2 test corpus are 092 to 095 Fmeasure for the different name types Ye et al 2002", "two words preceding and following the name", "If a name has not been seen before and appears in a relatively uninformative context it becomes very hard to classify", "We propose to use more global information to improve the performance of name recognition", "Some name taggers have incorporated a name cache or similar mechanism which makes use of names previously recognized in the document", "In our approach we perform coreference analysis and then use detailed evidence from other phrases in the document which are coreferential with this name in order to disambiguate the name", "This allows us to perform a richer set of corrections than with a name cache", "We then go one step further and process similar documents containing instances of the same name and combine the evidence from these additional instances", "At each step we are able to demonstrate a small but consistent improvement in named entity recognition", "The rest of the paper is organized as follows", "Section 2 briefly describes the baseline name tagger and coreference resolver used in this paper", "Section 3 considers methods for assessing the confidence of name tagging decisions", "Section 4 examines the distribution of name errors as a motivation for using coreference information", "Section 5 shows the coreference features we use and how they are incorporated into a statistical name filter", "Section 6 describes additional rules using coreference to improve name recognition", "Section 7 provides the flow graph of the improved system", "Section 8 reports and discusses the experimental results while Section 9 summarizes the conclusions"]}, "W04-3238": {"title": [""], "abstract": ["Logs of user queries to an internet search engine provide a large amount of implicit and explicit information about language", "In this paper we investigate their use in spelling correction of search queries a task which poses many additional challenges beyond the traditional spelling correction problem", "We present an approach that uses an iterative transformation of the input query strings into other strings that correspond to more and more likely queries according to statistics extracted from internet search query logs"], "inroduction": ["The task of general purpose spelling correction has a long history eg Damerau 1964 Rieseman and Hanson 1974 McIlroy 1982 traditionally focusing on resolving typographical errors such as insertions deletions substitutions and transpositions of letters that result in unknown words ie words not found in a trusted lexicon of the language", "Typical word processing spell checkers compute for each unknown word a small set of inlexicon alternatives to be proposed as possible corrections relying on information about inlexiconword frequencies and about the most common keyboard mistakes such as typing m instead of n and phoneticcognitive mistakes both at word level eg the use of acceptible instead of acceptable and at character level eg the misuse of f instead of ph", "Very few spell checkers attempt to detect and correct word substitution errors which refer to the use of inlexicon words in inappropriate contexts and can also be the result of both typographical mistakes such as typing coed instead of cord and cognitive mistakes eg principal and principle", "Some research efforts to tackle this problem have been made for example Heidorn et al", "1982 and Garside et al", "1987 developed systems that rely on syntactic patterns to detect substitution errors while Mays et al", "1991 employed word cooccurrence evidence from a large corpus to detect and correct such errors", "The former approaches were based on the impractical assumption that all possible syntactic uses of all words ie partofspeech are known and presented both recall and precision problems because many of the substitution errors are not syntactically anomalous and many unusual syntactic constructions do not contain errors", "The latter approach had very limited success under the assumptions that each sentence contains at most one misspelled word each misspelling is the result of a single point change insertion deletion substitution or transposition and the defect rate the relative number of errors in the text is known", "A different body of work eg Golding 1995 Golding and Roth 1996 Mangu and Brill 1997 focused on resolving a limited number of cognitive substitution errors in the framework of context sensitive spelling correction CSSC", "Although promising results were obtained 9295 accuracy the scope of this work was very limited as it only addressed known sets of commonly confused words such as peace piece", "11 Spell Checking of Search Engine Queries", "The task of webquery spelling correction addressed in this work has many similarities to traditional spelling correction but also poses additional challenges", "Both the frequency and severity of spelling errors for search queries are significantly greater than in word processing", "Roughly 1015 of the queries sent to search engines contain errors", "Typically the validity of a query cannot be decided by lexicon lookup or by checking its gram maticality", "Because web queries are very short on average less than 3 words techniques that use a multitude of features based on relatively wide context windows such as those investigated in CSSC are difficult to apply", "Rather than being well formed sentences most queries consist of one concept or an enumeration of concepts many times containing legitimate words that are not found in any traditional lexicon", "Just defining what a valid web query is represents a difficult enterprise", "We clearly cannot use only a static trusted lexicon as many new names and concepts such as aznar blog naboo nimh nsync and shrek become popular every day and it would be extremely difficult if not impossible to maintain a highcoverage lexicon", "In addition employing very large lexicons can result in more errors surfacing as word substitutions which are very difficult to detect rather than as unknown words", "One alternative investigated in this work is to exploit the continuously evolving expertise of millions of people that use web search engines as collected in search query logs seen as histograms over the queries received by a search engine", "In some sense we could say that the validity of a word can be inferred from its frequency in what people are querying for similarly to Wittgensteins 1968 observation that the meaning of a word is its use in the language", "Such an approach has its own caveats", "For example it would be er ple it can be the ratio between the number of letters two words do not have in common and the number of letters they share1 The two most used classes of distances in spelling correction are edit distances as proposed by Damerau 1964 and Levenshtein 1965 and correlation matrix distances Cherkassky et al 1974", "In our study we use a modified version of the DamerauLev enshtein edit distance as presented in Section 3", "One flaw of the preceding formulation is that it does not take into account the frequency of words in a language", "A simple solution to this problem is to compute the probability of words in the target language as maximum likelihood estimates MLE over a large corpus and reformulate the general spellingcorrection problem as follows roneous to simply extract from webquery logs all the queries whose frequencies are above a certain value and consider them valid", "Misspelled queries Given w dist w w    L  find and Pw  w L max v Ldist  wv  such that Pv  such as britny spears are much more popular than correctly spelled queries such as bayesian nets and amd processors", "Our challenge is to try to utilize query logs to learn what queries are valid and to build a model for valid query probabilities despite the fact that a large percentage of the logged queries are misspelled and there is no trivial way to determine the valid from invalid queries"]}, "W05-0612": {"title": ["Proceedings of the 9th Conference on Computational Natural Language Learning CoNLL"], "abstract": ["We propose an unsupervised Expectation Maximization approach to pronoun resolution", "The system learns from a fixed list of potential antecedents for each pronoun", "We show that unsupervised learning is possible in this context as the performance of our system is comparable to supervised methods", "Our results indicate that a probabilistic gendernumber model determined automatically from unlabeled text is a powerful feature for this task"], "inroduction": ["Coreference resolution is the process of determining which expressions in text refer to the same real world entity", "Pronoun resolution is the important yet challenging subset of coreference resolution where a system attempts to establish coreference between a pronominal anaphor such as a thirdperson pronoun like he she it or they and a preceding noun phrase called an antecedent", "In the following example a pronoun resolution system must determine the correct antecedent for the pronouns his and he 1 When the president entered the arena with his family he was serenaded by a mariachi band", "Pronoun resolution has applications across many areas of Natural Language Processing particularly in the field of information extraction", "Resolving a pronoun to a noun phrase can provide a new interpretation of a given sentence giving a Question Answering system for example more data to consider", "Our approach is a synthesis of linguistic and statistical methods", "For each pronoun a list of antecedent candidates derived from the parsed corpus is presented to the Expectation Maximization EM learner", "Special cases such as pleonastic reflexive and cataphoric pronouns are dealt with linguistically during list construction", "This allows us to train on and resolve all thirdperson pronouns in a large Question Answering corpus", "We learn lexicalized gendernumber language and antecedent probability models", "These models tied to individual words can not be learned with sufficient coverage from labeled data", "Pronouns are resolved by choosing the most likely antecedent in the candidate list according to these distributions", "The resulting resolution accuracy is comparable to supervised methods", "We gain further performance improvement by initializing EM with a gendernumber model derived from special cases in the training data", "This model is shown to perform reliably on its own", "We also demonstrate how the models learned through our unsupervised method can be used as features in a supervised pronoun resolution system"]}, "W06-0106": {"title": ["Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 4047"], "abstract": ["Coreference resolution is the process of identifying expressions that refer to the same entity", "This paper presents a clustering algorithm for unsupervised Chinese coreference resolution", "We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for phoric reference to Clinton", "  President Clinton is described as the antecedent of  he", " Clinton  President Clinton and the second  he are all mentions of the same entity that refers to former US president Bill Clinton", "12     English can be extended to Chinese", "The 3 1 proposed system exploits clustering as it has advantages over traditional classification methods such as the fact that no training data is required and it is easily extended to accommodate additional features", "We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance", "Our system is evaluated on an annotated version of the TDT3 corpus using the MUC7 scorer and obtains comparable performance", "We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resolution"], "inroduction": ["Noun phrase coreference resolution is the process of detecting noun phrases NPs in a document and determining whether the NPs refer to the same entity where an entity is defined as a construct that represents an abstract identity", "The NPs that refer to the entity are known as mentions", "Mentions can be antecedents or anaphors", "An anaphor is an expression that refers back to a previous expression in a discourse", "In Figure 1  President Clinton refersto  Clinton and is described as an ana 31  12    Clinton1 said that Washington would progressively follow through on economic aid to Korea2", "Kim DaeJung3 applauded Clinton1s speech", "He1 said President Clinton1 reiterated in the talks that he1 would provide solid support for Korea2 to shake off the economic crisis", "Figure 1 An excerpt from the text with core ferring noun phrases annotated", "English translation in italics", "NP coreference resolution is an important sub task in natural language processing NLP applications such as text summarization information extraction data mining and question answering", "This task has attracted much attention in recent years Cardie and Wagstaff 1999 Harabagiu et al 2001 Soon et al 2001 Ng and Cardie 2002 Yang et al 2004 Florian et al 2004 Zhou et al 2005 and has been included as a subtask in the MUC Message Understanding Conferences and ACE Automatic Content Extraction competitions", "Coreference resolution is a difficult task for various reasons", "Firstly a list of features can play a role to support coreference resolution such as 40 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 4047 Sydney July 2006", "Qc 2006 Association for Computational Linguistics gender agreement number agreement head noun matches semantic class positional information contextual information appositive abbreviation etc Ng and Cardie 2002 found 53 features which are useful for this problem", "However no single feature is completely reliable since there are always exceptions eg the number agree ment test returns false when     this army singular is matched against  army members plural despite the two phrases being coreferential", "Secondly identifying features automatically and accurately is hard", "Features such as semantic class come from named entity recognition NER systems and ontologies and gazetteers but they are not always accurate especially where new terms are concerned", "Thirdly coreference resolution subsumes the pronoun resolution problem which is already difficult since pronouns carry limited lexical and semantic information", "In addition to the aforementioned Chinese coreference resolution is also made more difficult due to the lack of morphological and orthographic clues", "Chinese words contain less exterior information than words in many Indoeuropean languages", "For example in English number agreement can be detected through word inflections and partofspeech POS tags but there are no simple rules in Chinese to distinguish whether a word is singular or plural", "Proper name and abbreviations are identified by capitalization in English but Chinese does not use capitalization", "Moreover written Chinese does not have word boundaries so word segmentation is a crucial problem as we cannot get the true meaning of the sentence based on characters alone", "A simple sentence can be segmented in several different ways to get different meanings", "This characteristic affects the performance of all parts and leads to irrecoverable errors", "In addition there are very few Chinese coreference data sets available for research purposes none of them freely available and as a result no easily obtainable benchmark ing dataset for training and measuring performance", "Building a reasonably large coreference corpus is a laborconsuming task", "To our knowledge there have only been two Chinese coreference systems in previously published work Florian et al", "2004 which presents a statistical framework and reports experiment results on Chinese texts and Zhou et al", "2005 which proposed a unified transformation based learning framework for Chinese entity detection and tracking", "It consists of two models the detection model locates possibly coreferring NPs and the tracking model links the coreference relations", "This paper presents research performed on Chinese noun phrase coreference resolution", "Since there are no freely available Chinese coreference resources we used an unsupervised method that partially borrows from Cardie and Wagstaffs 1999 clusteringbased technique with features that are specially designed for Chinese", "In addition we perform and present the results of experiments designed to investigate the contribution of each feature"]}, "W06-0118": {"title": ["Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 126129 Sydney July 2006 c  2006 Association for Computational Linguistics Voting between Dictionarybased and Subword Tagging Models for Chinese Word Segmentation Dong Song and Anoop Sarkar School of Computing Science Simon Fraser University Burnaby BC Canada V5A1S6 dsonganoopcssfuca Abstract This paper describes a Chinese word segmentation system that is based on majority voting among three models a forward maximum matching model a con ditional random field CRF model using maximum subword based tagging and a CRF model using minimum subwordbased tagging In addition it contains a postprocessing component to deal with inconsistencies Testing on the closed track of CityU MSRA and UPUC corpora in the third SIGHAN Chinese Word Segmentation Bakeoff the system achieves a Fscore of 0961 0953 and 0919 respectively 1 Introduction Tokenizing input text into words is the first step of any text analysis task In Chinese a sentence is written as a string of characters to which we shall refer by their traditional name of hanzi without separations between words As a result before any text analysis on Chinese word segmentation task has to be completed so that each word is  isolated by the wordboundary information Participating in the third SIGHAN Chinese Word Segmentation Bakeoff in 2006 our system is tested on the closed track of CityU MSRA and UPUC corpora The sections below provide a de tailed description of the system and our experimental results 2 System Description In our segmentation system a hybrid strategy is applied Figure 1 First forward maximum matching Chen and Liu 1992 which is a dictionarybased method is used to generate a segmentation result Also the CRF model using maximum subword based tagging Zhang et al 2006 and the CRF model using minimum subwordbased tagging both of which are statistical methods are used individually to solve the problem In the next step the solutions from these three methods are combined via the hanzilevel majority voting algorithm Then a postprocessing procedure is applied in order to to get the final output This procedure merges adjoining words to match the dictionary entries and then splits words which are inconsistent with entries in the training corpus Forward Postprocessing Majority Voting Result Input Sentence Tagging Subword based Minimum CRF with Tagging Subword based Maximum CRF with Matching Maximum Figure 1 Outline of the segmentation process"], "abstract": ["This paper describes a Chinese word segmentation system that is based on majority voting among three models a forward maximum matching model a conditional random field CRF model using maximum subwordbased tagging and a CRF model using minimum subword based tagging", "In addition it contains a postprocessing component to deal with inconsistencies", "Testing on the closed track of CityU MSRA and UPUC corpora problem", "In the next step the solutions from these three methods are combined via the hanzi level majority voting algorithm", "Then a post processing procedure is applied in order to to get the final output", "This procedure merges adjoining words to match the dictionary entries and then splits words which are inconsistent with entries in the training corpus", "Input Sentence in the third SIGHAN Chinese Word Segmentation Bakeoff the system achieves a Fscore of 0961 0953 and 0919 respectively"], "inroduction": ["Tokenizing input text into words is the first step of any text analysis task", "In Chinese a sentence is written as a string of characters to which we shall refer by their traditional name of hanzi without separations between words", "As a result before any text analysis on Chinese word segmentation task Forward Maximum Matching CRF with Maximum Subwordbased Tagging Majority Voting Postprocessing Result CRF with Minimum Subwordbased Tagging has to be completed so that each word is isolated by the wordboundary information", "Participating in the third SIGHAN Chinese Word Segmentation Bakeoff in 2006 our system is tested on the closed track of CityU MSRA and UPUC corpora", "The sections below provide a detailed description of the system and our experimental results"]}, "W06-0119": {"title": ["BMMbased Chinese Word Segmentor with Word Support Model for"], "abstract": ["This paper describes a Chinese word segmentor CWS for the third International Chinese Language Processing Bakeoff SIGHAN Bakeoff 2006", "We participate in the word segmentation task at the Microsoft Research MSR closed testing track", "Our CWS is based on backward maximum matching with word support model WSM and contextualbased Chinese unknown word identification", "From the scored results and our experimental results it shows WSM can improve our previous CWS which was reported at the SIGHAN Bakeoff 2005 about 1 of Fmeasure"], "inroduction": ["A highperformance Chinese word segmentor CWS is a critical processing stage to produce an intermediate result for later processes such as search engines text mining word spell checking texttospeech and speech recognition etc As per Lin et al 1993 Tsai et al 2003 Tsai 2005 the bottleneck for developing a high performance CWS is to comprise of high performance Chinese unknown word identification UWI", "It is because Chinese is written without any separation between words and more than 50 words of the Chinese texts in web corpus are outofvocabulary Tsai et al 2003", "In our report for the SIGHAN Bakeoff 2005 Tsai 2005 we have shown that a highly performance of 991 Fmeasure can be achieved while a BMMbased CWS using a perfect system dictionary Tsai 2005", "A perfect system dictionary means all word types of the dictionary are extracted from training and testing gold standard corpus", "Conventionally there are four approaches to develop a CWS 1 Dictionarybased approach Cheng et al 1999 especial forward and backward maximum matching Wong and Chan 1996 2 Linguistic approach based on syntaxsemantic knowledge Chen et al 2002 3 Statistical approach based on statistical language model SLM Sproat and Shih 1990 Teahan et al 2000 Gao et al 2003 and 4 Hybrid approach trying to combine the benefits of dictionarybased linguistic and statistical approaches Tsai et al 2003 Ma and Chen 2003", "In practice statistical approaches are most widely used because their effective and reasonable performance", "To develop UWI there are three approaches 1 Statistical approach researchers use common statistical features such as maximum entropy Chieu et al 2002 association strength mutual information ambiguous matching and multistatistical features for unknown word detection and extraction 2 Linguistic approach three major types of linguistic rules knowledge morphology syntax and semantics are used to identify unknown words and 3 Hybrid approach recently one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches", "Statistical approaches are simple and efficient whereas linguistic approaches are effective in identifying low frequency unknown words Chen et al 2002", "To develop WSD there are two major types of word segmentation ambiguities while there are no unknown word problems with them 1 Overlap Ambiguity OA", "Take string C1C2C3 130 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing pages 130133 Sydney July 2006", "Qc 2006 Association for Computational Linguistics comprised of three Chinese characters C1 C2 and C3 as an example", "If its segmentation can be either C1C2C3 or C1C2C3 depending on context meaning the C1C2C3 is called an overlapambiguity string OAS such as a gen eraluse and to getfor military use the symbol  indicates a word bound ary", "2 Combination Ambiguity CA", "Take string C1C2 comprised of two Chinese characters C1 and C2 as an example", "If its segmentation can be either C1C2 or C1C2 depending on context meaning the C1C2 is called a combina tion ambiguity string CAS such as just can and ability Besides the OA and CA problems the other two types of word segmentation errors are caused by unknown word problems", "They are 1 Lack of unknown word LUW it means segmentation error occurred by lack of an unknown word in the system dictionary and 2 Error identified word EIW it means segmentation error occurred by an error identified unknown words", "The goal of this paper is to report the approach and experiment results of our backward maximum matchingbased BMMbased CWS with word support model WSM for the SIGHAN Bakeoff 2006", "In Tsai 2006 WSM has been shown effectively to improve Chinese input system", "In the third Bakeoff our CWS is mainly addressed on improving its performance of OACA disambiguation by WSM", "We show that WSM is able to improve our BMMbased CWS which reported at the SIGHAN Bakeoff 2005 about 1 of Fmeasure", "The remainder of this paper is arranged as follows", "In Section 2 we present the details of our BMMbased CWS comprised of WSM", "In Section 3 we present the scored results of the CWS at the Microsoft Research closed track and give our experiment results and analysis", "Finally in Section 4 we give our conclusions and future research directions"]}, "W06-0206": {"title": ["Proceedings of the Workshop on Information Extraction Beyond The Document pages 4855"], "abstract": ["We present two semisupervised learning techniques to improve a stateoftheart multilingual name tagger", "For English and Chinese the overall system obtains 17  21 improvement in Fmeasure representing a 135  174 relative reduction in the spurious missing and incorrect tags", "We also conclude that simply relying upon large corpora is not in itself sufficient we must pay attention to unlabeled data selection too", "We describe effective measures to automatically select documents and sentences"], "inroduction": ["When applying machine learning approaches to natural language processing tasks it is time consuming and expensive to handlabel the large amounts of training data necessary for good performance", "Unlabeled data can be collected in much larger quantities", "Therefore a natural question is whether we can use unlabeled data to build a more accurate learner given the same amount of labeled data", "This problem is often referred to as semisupervised learning", "It significantly reduces the effort needed to develop a training set", "It has shown promise in improving the performance of many tasks such as name tagging Miller et al 2004 semantic class extraction Lin et al 2003 chunking Ando and Zhang 2005 coreference resolution Bean and Riloff 2004 and text classification Blum and Mitchell 1998", "However it is not clear when semisupervised learning is applied to improve a learner how the system should effectively select unlabeled data and how the size and relevance of data impact the performance", "In this paper we apply two semisupervised learning algorithms to improve a stateoftheart name tagger", "We run the baseline name tagger on a large unlabeled corpus bootstrapping and the test set selftraining and automatically generate highconfidence machinelabeled sentences as additional training data", "We then iteratively retrain the model on the increased training data", "We first investigated whether we can improve the system by simply using a lot of unlabeled data", "By dramatically increasing the size of the corpus with unlabeled data we did get a significant improvement compared to the baseline system", "But we found that adding offtopic unlabeled data sometimes makes the performance worse", "Then we tried to select relevant documents from the unlabeled data in advance and got clear further improvements", "We also obtained significant improvement by selftraining boot strapping on the test data without any additional unlabeled data", "Therefore in contrast to the claim in Banko and Brill 2001 we concluded that for some applications effective use of large unlabeled corpora demands good data selection measures", "We propose and quantify some effective measures to select documents and sentences in this paper", "The rest of this paper is structured as follows", "Section 2 briefly describes the efforts made by previous researchers to use semisupervised learning as well as the work of Banko and Brill 2001", "Section 3 presents our baseline name tag ger", "Section 4 describes the motivation for our approach while Section 5 presents the details of two semisupervised learning methods", "Section 6 presents and discusses the experimental results on both English and Chinese", "Section 7 presents our conclusions and directions for future work"]}, "W06-1624": {"title": ["A Weakly Supervised Learning Approach"], "abstract": ["In this paper we present a weakly supervised learning approach for spoken language understanding in domainspecific dialogue systems", "We model the task of spoken language understanding as a successive classification problem", "The first classifier topic classifier is used to identify the topic of an input utterance", "With the restriction of the recognized target topic the second classifier semantic classifier is trained to extract the corresponding slotvalue pairs", "It is mainly datadriven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language", "Most importantly it allows the employment of weakly supervised strategies for training the two classifiers", "We first apply the training strategy of combining active learning and selftraining Tur et al 2005 for topic classifier", "Also we propose a practical method for bootstrapping the topicdependent semantic classifiers from a small amount of labeled sentences", "Experiments have been conducted in the context of Chinese public transportation information inquiry domain", "The experimental results demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce human labeling efforts significantly"], "inroduction": ["Spoken Language Understanding SLU is one of the key components in spoken dialogue systems", "Its task is to identify the users goal and extract from the input utterance the information needed to complete the query", "Traditionally there are mainly two mainstreams in the SLU researches knowledgebased approaches which are based on robust parsing or template matching techniques Sneff 1992 Dowding et al 1993 Ward and Issar 1994 and datadriven approaches which are generally based on stochastic models Pieraccini and Levin 1993 Miller et al 1995", "Both approaches have their drawbacks however", "The former approach is costexpensive to develop since its grammar development is time consuming laboursome and requires linguistic skills", "It is also strictly domaindependent and hence difficult to be adapted to new domains", "On the other hand although addressing such drawbacks associated with knowledgebased approaches the latter approach often suffers the data sparseness problem and hence needs a fully annotated corpus in order to reliably estimate an accurate model", "More recently some new variation methods are proposed through certain trade offs such as the semiautomatically grammar learning approach Wang and Acero 2001 and Hidden Vector State HVS model He and Young 2005", "The two methods require only minimally annotated data only the semantic frames are annotated", "This paper proposes a novel weakly supervised spoken language understanding approach", "Our SLU framework mainly includes two successive classifiers topic classifier and semantic classifier", "The main advantage of the proposed approach is that it is mainly datadriven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language", "In particular the two classifiers are trained using weakly supervised strategies the former one is trained through the combination of active learning and selftraining Tur et al 2005 and the latter one 199 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 199207 Sydney July 2006", "Qc 2006 Association for Computational Linguistics is trained using a practical bootstrapping technique"]}, "W06-1634": {"title": ["Automatic Construction of Predicateargument Structure Patterns"], "abstract": ["This paper presents a method of automatically constructing information extraction patterns on predicateargument structures PASs obtained by full parsing from a smaller training corpus", "Because PASs represent generalized structures for syntactical variants patterns on PASs are expected to be more generalized than those on surface words", "In addition patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results", "In this paper we present experimental results and analyze them on how well proteinprotein interactions were extracted from MEDLINE abstracts", "The results demonstrated that our method improved accuracy compared to a machine learning approach using surface wordpartofspeech patterns"], "inroduction": ["One primitive approach to Information Extraction IE is to manually craft numerous extraction patterns for particular applications and this is presently one of the main streams of biomedical IE Blaschke and Valencia 2002 Koike et al 2003", "Although such IE attempts have demonstrated nearpractical performance the same sets of patterns cannot be applied to different kinds of information", "A realworld task requires several kinds of IE thus manually engineering extraction Current Afliation  FUJITSU LABORATORIES LTD  Faculty of Informatics Kogakuin University patterns which is tedious and timeconsuming process is not really practical", "Techniques based on machine learning Zhou et al 2005 Hao et al 2005 Bunescu and Mooney 2006 are expected to alleviate this problem in manually crafted IE", "However in most cases the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data which requires tedious amount of manual labor to annotate text", "To systematically reduce the necessary amount of training data we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has specic properties according to the information to be extracted", "The former subtask is of full parsing ie recognizing syntactic structures of sentences and the latter subtask is of constructing specic extraction patterns ie nding clue words to extract information based on the obtained syntactic structures", "We adopted full parsing from various levels of parsing because we believe that it offers the best utility to generalize sentences into normalized syntactic relations", "We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine SVM to learn a prediction model using the matching results of extraction patterns", "As an actual IE task we extracted pairs of interacting protein names from biomedical text"]}, "W06-1667": {"title": ["Unsupervised Relation Disambiguation with Order Identification"], "abstract": ["We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts", "It works by calculating eigen vectors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors", "This method can address two difficulties encoutered in Hasegawa et al", "2004s hierarchical clustering no consideration of manifold structure in data and requirement to provide cluster number by users", "Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al", "2004s hierarchical clustering method and a plain kmeans clustering method"], "inroduction": ["The task of relation extraction is to identify various semantic relations between name entities from text", "Prior work on automatic relation extraction come in three kinds supervised learning algorithms Miller et al 2000 Zelenko et al 2002 Culotta and Soresen 2004 Kambhatla 2004 Zhou et al 2005 semisupervised learning algorithms Brin 1998 Agichtein and Gravano 2000 Zhang 2004 and unsupervised learning algorithm Hasegawa et al 2004", "Among these methods supervised learning is usually more preferred when a large amount of la beled training data is available", "However it is timeconsuming and laborintensive to manually tag a large amount of training data", "Semisupervised learning methods have been put forward to minimize the corpus annotation requirement", "Most of semisupervised methods employ the bootstrapping framework which only need to predefine some initial seeds for any particular relation and then bootstrap from the seeds to acquire the relation", "However it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal number of them", "Compared with supervised and semisupervised methods Hasegawa et al", "2004s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels", "Hasegawa et al", "2004s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities", "However the drawback of hierarchical clustering is that it required providing cluster number by users", "Furthermore clustering is performed in original high dimensional space which may induce nonconvex clusters hard to identified", "This paper presents a novel application of spectral clustering technique to unsupervised relation extraction problem", "It works by calculating eigenvec tors of an adjacency graphs Laplacian to recover a submanifold of data from a high dimensional space and then performing cluster number estimation on a transformed space defined by the first few eigen vectors", "This method may help us find nonconvex clusters", "It also does not need to predefine the number of the context clusters or prespecify the similarity threshold for the clusters as Hasegawa et al 568 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 568575 Sydney July 2006", "Qc 2006 Association for Computational Linguistics 2004s method", "The rest of this paper is organized as follows", "Section 2 formulates unsupervised relation extraction and presents how to apply the spectral clustering technique to resolve the task", "Then section 3 reports experiments and results", "Finally we will give a conclusion about our work in section 4"]}, "W06-1670": {"title": ["BroadCoverage Sense Disambiguation and Information Extraction with"], "abstract": ["In this paper we approach word sense disambiguation and information extraction as a unified tagging problem", "The task consists of annotating text with the tagset defined by the 41 Wordnet super sense classes for nouns and verbs", "Since the tagset is directly related to Wordnet synsets the tagger returns partial word sense disambiguation", "Furthermore since the noun tags include the standard named entity detection classes  person location organization time etc  the tagger as a byproduct returns extended named entity information", "We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminativelytrained Hidden Markov Model", "Experimental evaluation on the main senseannotated datasets available ie Semcor and Senseval shows considerable improvements over the best known firstsense baseline"], "inroduction": ["Named entity recognition NER is the most studied information extraction IE task", "NER typically focuses on detecting instances of person location organization names and optionally instances of miscellaneous or time categories", "The scalability of statistical NER allowed researchers to apply it successfully on large collections of newswire text in several languages and biomedical literature", "Newswire NER performance in terms of Fscore is in the upper 80s Carreras et al 2002 Florian et al 2003 while BioNER accuracy ranges between the low 70s and 80s depending on the dataset used for trainingevaluation Dingare et al 2005", "One shortcoming of NER is its oversimplified onto logical model leaving instances of other potentially informative categories unidentified", "Hence the utility of named entity information is limited", "In addition instances to be detected are mainly restricted to sequences of proper nouns", "Word sense disambiguation WSD is the task of deciding the intended sense for ambiguous words in context", "With respect to NER WSD lies at the other end of the semantic tagging spectrum since the dictionary defines tens of thousand of very specific word senses including NER categories", "Wordnet Fellbaum 19981 possibly the most used resource for WSD defines word senses for verbs common and proper nouns", "Word sense disambiguation at this level of granularity is a complex task which resisted all attempts of robust broadcoverage solutions", "Many distinctions are too subtle to be captured automatically and the magnitude of the class space  several orders larger than NERs  makes it hard to approach the problem with sophisticated but scal able machine learning methods", "Lastly even if the methods would scale up there are not enough manually tagged data at the word sense level for training a model", "The performance of state of the art WSD systems on realistic evaluations is only comparable to the first sense baseline cf", "Section 53", "Notwithstanding much research the benefits of disambiguated lexical information for language processing are still mostly speculativeThis paper presents a novel approach to broad The first author is now at Yahoo", "Research", "The tag ger described in this paper is free software and can be downloaded from httpwwwloacnritciaramitahtml", "1 When referring to Wordnet throughout the paper we", "mean Wordnet version 20", "594 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing EMNLP 2006 pages 594602 Sydney July 2006", "Qc 2006 Association for Computational Linguistics NOUNS SUPERSENSE NOUNS DENOTING SUPERSENSE NOUNS DENOTING act animal artifact attribute body cognition communication event feeling food group location motive acts or actions animals manmade objects attributes of people and objects body parts cognitive processes and contents communicative processes and contents natural events feelings and emotions foods and drinks groupings of people or objects spatial position goals object quantity phenomenon plant possession process person relation shape state substance time Tops natural objects not manmade quantities and units of measure natural phenomena plants possession and transfer of possession natural processes people relations between people or things or ideas two and three dimensional shapes stable states of affairs substances time and temporal relations abstract terms for unique beginners VERBS SUPERSENSE VERBS OF SUPERSENSE VERBS OF body change cognition communication competition consumption contact creation grooming dressing and bodily care size temperature change intensifying thinking judging analyzing doubting telling asking ordering singing fighting athletic activities eating and drinking touching hitting tying digging sewing baking painting performing emotion motion perception possession social stative weather feeling walking flying swimming seeing hearing feeling buying selling owning political and social activities and events being having spatial relations raining snowing thawing thundering Table 1", "Nouns and verbs supersense labels and short description from the Wordnet documentation", "coverage information extraction and word sense disambiguation", "Our goal is to simplify the disambiguation task for both nouns and verbs to a level at which it can be approached as any other tagging problem and can be solved with state of the art methods", "As a byproduct this task includes and extends NER", "We define a tagset based on Word nets lexicographers classes or supersenses Ciaramita and Johnson 2003 cf", "Table 1", "The size of the supersense tagset allows us to adopt a structured learning approach which takes local dependencies between labels into account", "To this extent we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model HMM based on that of Collins 2002 on the manually annotated Semcor corpus Miller et al 1993", "In two experiments we evaluate the accuracy of the tagger on the Semcor corpus itself and on the English all words Senseval 3 shared task data Snyder and Palmer 2004", "The model outperforms remarkably the best known baseline the first sense heuristic  to the best of our knowledge for the first time on the most realistic all words evaluation setting", "The paper is organized as follows", "Section 2 introduces the tagset Section 3 discusses related work and Section 4 the learning model", "Section 5 reports on experimental settings and results", "In Section 6 we summarize our contribution and consider directions for further research"]}, "W06-2709": {"title": [""], "abstract": ["We present ANNIS a linguistic database that aims at facilitating the process of exploiting richly annotated language data by naive users", "We describe the role of the database in our research project and the project requirements with a special focus on aspects of multilevel annotation", "We then illustrate the usability of the database by illustrative examples", "We also address current challenges and next steps"], "inroduction": ["Until recently working with data that is annotated at multiple levels with different types of annotation required rather advanced computer skills which cannot be expected from the majority of potentially interested users", "We present ANNIS a linguistic database that aims at providing the infrastructure for supporting linguists in their work on multilevel annotations", "We describe and illustrate the current state of our work and sketch the next steps", "In sec", "2 we present the research scenario AN NIS is developed for show the role of the linguistic database therein and sketch the major requirements it aims to fulfill", "We then describe the architecture and current functionality and discuss the way difficult aspects of multidimensional annotations are treated sec", "3", "In sec", "4 we illustrate the work with the database by three exemplary approaches", "Finally we sketch our next steps"]}, "W06-2910": {"title": ["69"], "abstract": ["This paper investigates whether human associations to verbs as collected in a web experiment can help us to identify salient verb features for semantic verb classes", "Assuming that the associations model aspects of verb meaning we apply a clustering to the verbs as based on the associations and validate the resulting verb classes against standard approaches to semantic verb classes ie GermaNet and FrameNet", "Then various clusterings of the same verbs are performed on the basis of standard corpusbased types and evaluated against the associationbased clustering as well as GermaNet and FrameNet classes", "We hypothesise that the corpus based clusterings are better if the instantiations of the feature types show more overlap with the verb associations and that the associations therefore help to identify salient feature types"], "inroduction": ["There are a variety of manual semantic verb classifications major frameworks are the Levin classes Levin 1993 WordNet Fellbaum 1998 and FrameNet Fontenelle 2003", "The different frameworks depend on different instantiations of semantic similarity eg Levin relies on verb similarity referring to syntaxsemantic alternation behaviour WordNet uses synonymy and FrameNet relies on situationbased agreement as defined in Fillmores frame semantics Fillmore 1982", "As an alternative to the resourceintensive manual classifications automatic methods such as classification and clustering are applied to induce verb classes from corpus data eg", "Merlo and Stevenson 2001 Joanis and Stevenson 2003 Korhonen et al 2003 Stevenson and Joanis 2003 Schulte im Walde 2003 Fer rer 2004", "Depending on the types of verb classes to be induced the automatic approaches vary their choice of verbs and classificationclustering algorithm", "However another central parameter for the automatic induction of semantic verb classes is the selection of verb features", "Since the target classification determines the similarity and dissimilarity of the verbs the verb feature selection should model the similarity of interest", "For example Merlo and Stevenson 2001 classify 60 English verbs which alternate between an intransitive and a transitive usage and assign them to three verb classes according to the semantic role assignment in the frames their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment", "In largerscale classifications such as Korhonen et al 2003 Stevenson and Joanis 2003 Schulte im Walde 2003 which model verb classes with similarity at the syntaxsemantics interface it is not clear which features are the most salient", "The verb features need to relate to a behavioural component modelling the syntaxsemantics interplay but the set of features which potentially influence the behaviour is large ranging from structural syntactic descriptions and argument role fillers to adverbial adjuncts", "In addition it is not clear how finegrained the features should be for example how much information is covered by lowlevel window cooccurrence vs higherorder syntactic frame fillers", "69 Proceedings of the 10th Conference on Computational Natural Language Learning CoNLLX pages 6976 New York City June 2006", "Qc 2006 Association for Computational Linguistics In this paper we investigate whether human associations to verbs can help us to identify salient verb features for semantic verb classes", "We collected associations to German verbs in a web experiment and hope that these associations represent a useful basis for a theoryindependent semantic classification of the German verbs assuming that the associations model a nonrestricted set of salient verb meaning aspects", "In a preparatory step we perform an unsupervised clustering on the experiment verbs as based on the verb associations", "We validate the resulting verb classes henceforth assocclasses by demonstrating that they show considerable overlap with standard approaches to semantic verb classes ie GermaNet and FrameNet", "In the main body of this work we compare the associations underlying the assocclasses with standard corpusbased feature types We check on how many of the associations we find among the corpusbased features such as adverbs direct object nouns etc we hypothesise that the more associations are found as instantiations in a feature set the better is a clustering as based on that feature type", "We assess our hypothesis by applying various corpusbased feature types to the experiment verbs and comparing the resulting classes henceforth corpusclasses against the assocclasses", "On the basis of the comparison we intend to answer the question whether the human associations help identify salient features to induce semantic verb classes ie do the corpusbased feature types which are identified on the basis of the associations outperform previous clustering results", "By applying the feature choices to GermaNet and FrameNet we address the question whether the same types of features are salient for different types of semantic verb classes", "In what follows the paper presents the association data in Section 2 and the associationbased classes in Section 3", "In Section 4 we compare the associations with corpusbased feature types and in Section 5 we apply the insights to induce semantic verb classes"]}, "W06-3604": {"title": ["Allword prediction as the ultimate confusable disambiguation"], "abstract": ["We present a classificationbased word prediction model based on IGTR EE a decisiontree induction algorithm with favorable scaling abilities and a functional equivalence to ngram models with back off smoothing", "Through a first series of experiments in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text we demonstrate that the system exhibits loglinear increases in prediction accuracy with increasing numbers of training examples", "Trained on 30 million words of newswire text prediction accuracies range between 126 on fictional text and 422 on newswire text", "In a second series of experiments we compare allwords prediction with confusable prediction ie the same task but specialized to predicting among limited sets of words", "Con fusable prediction yields high accuracies on nine example confusable sets in all genres of text", "The confusable approach outperforms the allwordsprediction approach but with more data the difference decreases"], "inroduction": ["Word prediction is an intriguing language engineering semiproduct", "Arguably it is the archetypical prediction problem in natural language processing EvenZohar and Roth 2000", "It is usually not an engineering end in itself to predict the next word in a sequence or fill in a blankedout word in a sequence", "Yet it could be an asset in higherlevel proofing or authoring tools eg to be able to automatically discern among confusables and thereby to detect con fusable errors Golding and Roth 1999 EvenZohar and Roth 2000 Banko and Brill 2001 Huang and Powers 2001", "It could alleviate problems with low frequency and unknown words in natural language processing and information retrieval by replacing them with likely and higherfrequency alternatives that carry similar information", "And also since the task of word prediction is a direct interpretation of language modeling a word prediction system could provide useful information for to be used in speech recognition systems", "A unique aspect of the word prediction task as compared to most other tasks in natural language processing is that realworld examples abound in large amounts", "Any digitized text can be used as training material for a word prediction system capable of learning from examples and nowadays gigascale and terascale document collections are available for research purposes", "A specific type of word prediction is confus able prediction ie learn to predict among limited sets of confusable words such as totwotoo and theretheirtheyre Golding and Roth 1999 Banko and Brill 2001", "Having trained a confusable predictor on occurrences of words within a confusable set it can be applied to any new occurrence of a word from the set if its prediction based on the context deviates from the word actually present then 25 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing pages 2532 New York City New York June 2006", "Qc 2006 Association for Computational Linguistics this word might be a confusable error and the classifiers prediction might be its correction", "Confusable prediction and correction is a strong asset in proofing tools", "In this paper we generalize the word prediction task to predicting any word in context", "This is basically the task of a generic language model", "An explicit choice for the particular study on allwords prediction is to encode context only by words and not by any higherlevel linguistic nonterminals which have been investigated in related work on word prediction Wu et al 1999 EvenZohar and Roth 2000", "This choice leaves open the question how the same tasks can be learned from examples when nonterminal symbols are taken into account as well", "The choice for our algorithm a decisiontree approximation of knearestneigbor kNN based or memorybased learning is motivated by the fact that as we describe later in this paper this particular algorithm can scale up to predicting tens of thousands of words while simultaneously being able to scale up to tens of millions of examples as training material predicting words at useful rates of hundreds to thousands of words per second", "Another motivation for our choice is that our decisiontree approximation of knearest neighbor classification is functionally equivalent to backoff smoothing Zavrel and Daelemans 1997 not only does it share its performance capacities with ngram models with backoff smoothing it also shares its scaling abilities with these models while being able to handle large values of n The article is structured as follows", "In Section 2 we describe what data we selected for our experiments and we provide an overview of the experimental methodology used throughout the experiments including a description of the IGTR EE algorithm central to our study", "In Section 3 the results of the word prediction experiments are presented and the subsequent Section 4 contains the experimental results of the experiments on confusables", "We briefly relate our work to earlier work that inspired the current study in Section 5", "The results are discussed and conclusions are drawn in Section 6"]}, "W07-0722": {"title": ["Domain Adaptation in Statistical Machine Translation with Mixture"], "abstract": ["Mixture modelling is a standard technique for density estimation but its use in statistical machine translation SMT has just started to be explored", "One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset", "This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears", "In this paper we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a stateoftheart phrasebased system", "Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling"], "inroduction": ["Mixture modelling is a popular approach for density estimation in many scientific areas G J McLachlan and D Peel 2000", "One of the most interesting properties of mixture modelling is its capability to model multimodal datasets by defining soft partitions on these datasets and learning specific probability distributions for each partition that better explains the general data generation process", "Work supported by the EC FEDER and the Spanish MEC under grant TIN200615694CO201 the Consellera dEmpresa Universitat i CienciaGeneralitat Valenciana under contract GV06252 the Universidad Politecnica de Valencia with ILETA project and Ministerio de Educacio n y Ciencia", "In Machine Translation MT it is common to encounter large parallel corpora devoted to heterogeneous topics", "These topics usually define sets of topicspecific lexicons that need to be translated taking into the semantic context in which they are found", "This semantic dependency problem could be overcome by learning topicdependent translation models that capture together the semantic context and the translation process", "However there have not been until very recently that the application of mixture modelling in SMT has received increasing attention", "In Zhao and Xing 2006 three fairly sophisticated bayesian topical translation models taking IBM Model 1 as a baseline model were presented under the bilingual topic admixture model formalism", "These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence", "The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task", "In Civera and Juan 2006 a mixture extension of IBM model 2 along with a specific dynamic programming decoding algorithm were proposed", "This IBM2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semisynthetic task", "In this work we present a mixture extension of the wellknown HMM alignment model first proposed in Vogel and others 1996 and refined in Och and Ney 2003", "This model possesses appealing properties among which are worth mentioning the simplicity of the firstorder word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation pages 177180 Prague June 2007", "Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages and the efficient and exact computation of the Estep and Viterbi alignment by using a dynamicprogramming approach", "These properties have made this model suitable for extensions Toutanova et al 2002 and integration in a phrasebased model Deng and Byrne 2005 in the past", "3 Mixture of HMM alignment models", "Let us suppose that px  y has been generated using a Tcomponent mixture of HMM alignment models T px  y   pt  y px  y t t1 T   pt  y  px a  y t 6"]}, "W07-0802-parscit130908": {"title": ["Implementation of the Arabic Numerals and their Syntax in GF"], "abstract": ["The numeral system of Arabic is rich in its morphosyntactic variety yet suffers from the lack of a good computational resource that describes it in a reusable way", "This implies that applications that require the use of rules of the Arabic numeral system have to either reimplement them each time which implieswasted resources or use simplified imprecise rules that result in low quality applications", "A solution has been devised withGrammatical Framework GF to use language constructs and grammars as librariesthat can be written once and reused in various applications", "In this paper we describe our implementation of the Arabic numeralsystem as an example of a bigger implementation of a grammar library for Arabic", "We show that users can reuse our system by accessing a simple languageindependent API rule"], "inroduction": ["11 Problem", "Language technology and software localization con sume a significant share of many companies timeand work", "Translating an operating system or an application to different languages involves in the tra ditional approach translating outofcontext strings into different languages", "This requires a languageexpert for each new language and will still involve languagerelated problems because of the difficulty in translating outofcontext strings and tak ing care of morphological and syntactic variations at the same time", "We illustrate this with an example", "A mail reader application wants to display messages like You have 1 new message You have 2 new messages You have 3 new messages You have 100 new messagesIf these are to be translated into Arabic special mor phological and syntactic considerations should bemade which include inflecting message in num ber  P risalatun   2 messages  P risalatani   310 messages PrasaVa 1199 messages    P risalatan x100 messages  P risalatin So the word messages is translated into dif ferent words in Arabic depending on the numeral counting it", "Counted nouns are an extreme example of how varied case inflection can be The case of thesingular and the dual is determined by their syntac tic function nominative in the example above", "Thisis not the case for plurals which assume the geni tive case from three to ten  P is diptote thus the   to nightynine and genitive again for plurals that aremultiples of hundred", "This is not to mention noun adjective agreement which should be taken care of when translating new messages into ArabicThe aforementioned details should not be the responsibility of the application programmer and hav 1 message     marker then accusative singular from eleven 9 Proceedings of the 5th Workshop on Important Unresolved Matters pages 916 Prague Czech Republic June 2007", "c2007 Association for Computational Linguistics ing translators do this work over and over again for each application can be costly and lead to repeated work andor poor results", "12 Solution and Contributions", "We reviewed in other works Dada and Ranta 2007 an approach that addresses problems in language technology similar but not limited to the above", "We applied this approach to Arabic thus developing aresource grammar for Arabic in which we imple ment rules that cover the orthography morphologyand syntax", "In short this approach is based on de veloping libraries of natural language constructs andrules which can be used by an application programmer who is not knowledgeable in a specific language", "The core programming language is Grammatical Framework GF Ranta 2004", "The lan guage library called a resource grammar Khegai and Ranta 2004 and comprising the linguistic rules can be reused in applications through an Application Programming Interface API by programmers thatare unaware of the details of the specific natural language", "Such a programmer uses a resource gram mar assuming it will take care of morphological andsyntactic rules", "So far we have implemented significant parts of the Arabic morphology syntax ortho graphic rules and provided a sample lexicon of 300 words based on the Swadesh list Hymes 1960", "In this paper we only describe part of the work namely the numeral system of Arabic and its syntax", "In the next section we elaborate on the approach the programming language that implements it and on Resource Grammars"]}, "W07-0909": {"title": ["Proceedings of the Workshop on Language Technology for Cultural Heritage Data LaTeCH 2007 pages 6572 Prague 28 June 2007 c  2007 Association for Computational Linguistics Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation Idan Szpektor Ido Dagan Dept of Computer Science Bar Ilan University szpekticsbiuacil Alon Lavie Language Technologies Inst Carnegie Mellon University alaviecscmuedu Danny Shacham Shuly Wintner Dept of Computer Science University of Haifa shulycshaifaacil Abstract We describe a system which enhances the experience of museum visits by providing users with languagetechnology based information retrieval capabilities The system consists of a crosslingual search en gine augmented by state of the art semantic expansion technology specifically designed for the domain of the museum history and archaeology of Israel We discuss the technology incorporated in the system its adaptation to the specific domain and its contribution to cultural heritage appreciation 1 Introduction Museum visits are enriching experiences they provide stimulation to the senses and through them to the mind But the experience does not have to end when the visit ends further exploration of the artifacts and their influence on the visitor is possible after the visit either on location or elsewhere One common means of exploration is Information Retrieval IR via a Search Engine For example a museum could implement a search engine over a collection of documents relating to the topics exhibited in the museum However such document collections are usually much smaller than general collections in particular the World Wide Web Consequently phenomena inherent to natural languages may severely hamper the performance of human language technology when applied to small collections One such phenomenon is the semantic variability of natural languages the ability to express a specific meaning in many different ways For example the expression  Archaeologists found a new tomb can be expressed also by Archaeologists discovered a tomb or  A sarcophagus was dug up by Egyptian Researchers  On top of monolingual variability the same information can also be expressed in different languages Ignoring natural language variability may result in lower recall of relevant documents for a given query especially in small document collections This paper describes a system that attempts to cope with semantic variability through the use of state of the art human language technology The system provides both semantic expansion and cross lingual IR and presentation of information in the domain of archaeology and history of Israel It was specifically developed for the Hecht Museum in Haifa Israel which contains a small but unique collection of artifacts in this domain The system provides different users with different capabilities bridging over language divides it addresses semantic variation in novel ways and it thereby complements the visit to the museum with longlasting instillation of information The main component of the system is a domainspecific search engine that enables users to specify queries and retrieve information pertaining to the domain of the museum The engine is enriched by linguistic capabilities which embody an array of means for addressing semantic variation Queries are expanded using two main techniques semantic expansion based on textual entailment and crosslingual expansion based on translation of Hebrew queries to English and vice versa Retrieved documents are presented as links with associated snippets the system also translates snippets from Hebrew to English The main contribution of this work is of course the system itself which was recently demonstrated 65 successfully at the museum and which we believe could be useful to a variety of museum visitor types from children to experts For example the system provides Hebrew speakers access to English doc uments pertaining to the domain of the museum and vice versa thereby expanding the availability of multilingual material to museum visitors More generally it is an instance of adaptation of state of the art human language technology to the domain of cultural heritage appreciation demonstrating how general resources and tools are adapted to a specific domain thereby improving their accuracy and usability Finally it provides a testbed for evaluating the contribution of language technology in general as well as specific components and resources to a largescale natural language processing system 2 Background and Motivation Internet search is hampered by the complexity of natural languages The two main characteristics of this complexity are ambiguity and variability the former refers to the fact that a given text can be interpreted in more than one way the latter indicates that the same meaning can be linguistically expressed in several ways The two phenomena make simple search techniques too weak for unsophisticated users as existing search engines perform only direct keyword matching with very limited linguistic processing of the texts they retrieve Specifically IR systems that do not address the variability in languages may suffer from lower recall especially in restricted domains and small doc ument locations We next describe two prominent types of variability that we think should be addressed in IR systems"], "abstract": ["We describe a system which enhances the experience of museum visits by providing users with languagetechnologybased information retrieval capabilities", "The system consists of a crosslingual search engine augmented by state of the art semantic expansion technology specifically designed for the domain of the museum history and archaeology of Israel", "We discuss the technology incorporated in the system its adaptation to the specific domain and its contribution to cultural heritage appreciation"], "inroduction": ["Museum visits are enriching experiences they provide stimulation to the senses and through them to the mind", "But the experience does not have to end when the visit ends further exploration of the artifacts and their influence on the visitor is possible after the visit either on location or elsewhere", "One common means of exploration is Information Retrieval IR via a Search Engine", "For example a museum could implement a search engine over a collection of documents relating to the topics exhibited in the museum", "However such document collections are usually much smaller than general collections in particular the World Wide Web", "Consequently phenomena inherent to natural languages may severely hamper the performance of human language technology when applied to small collections", "One such phenomenon is the semantic variability of natural languages the ability to express a specific meaning in many different ways", "For example the expression Archae ologists found a new tomb can be expressed also by Archaeologists discovered a tomb or A sarcophagus was dug up by Egyptian Researchers", "On top of monolingual variability the same information can also be expressed in different languages", "Ignoring natural language variability may result in lower recall of relevant documents for a given query especially in small document collections", "This paper describes a system that attempts to cope with semantic variability through the use of state of the art human language technology", "The system provides both semantic expansion and cross lingual IR and presentation of information in the domain of archaeology and history of Israel", "It was specifically developed for the Hecht Museum in Haifa Israel which contains a small but unique collection of artifacts in this domain", "The system provides different users with different capabilities bridging over language divides it addresses semantic variation in novel ways and it thereby complements the visit to the museum with longlasting in stillation of information", "The main component of the system is a domain specific search engine that enables users to specify queries and retrieve information pertaining to the domain of the museum", "The engine is enriched by linguistic capabilities which embody an array of means for addressing semantic variation", "Queries are expanded using two main techniques semantic expansion based on textual entailment and crosslingual expansion based on translation of Hebrew queries to English and vice versa", "Retrieved documents are presented as links with associated snippets the system also translates snippets from Hebrew to English", "The main contribution of this work is of course the system itself which was recently demonstrated 65 Proceedings of the Workshop on Language Technology for Cultural Heritage Data LaTeCH 2007 pages 6572 Prague 28 June 2007", "Qc 2007 Association for Computational Linguistics successfully at the museum and which we believe could be useful to a variety of museum visitor types from children to experts", "For example the system provides Hebrew speakers access to English documents pertaining to the domain of the museum and vice versa thereby expanding the availability of multilingual material to museum visitors", "More generally it is an instance of adaptation of state of the art human language technology to the domain of cultural heritage appreciation demonstrating how general resources and tools are adapted to a specific domain thereby improving their accuracy and us ability", "Finally it provides a testbed for evaluating the contribution of language technology in general as well as specific components and resources to a largescale natural language processing system"]}, "W08-0329": {"title": ["Incremental Hypothesis Alignment for Building Confusion Networks with"], "abstract": ["Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation MT systems in the recent DARPA GALE and NIST Open MT evaluations", "Due to the varying word order between outputs from different MT systems the hypothesis alignment presents the biggest challenge in confusion network decoding", "This paper describes an incremental alignment method to build confusion networks based on the translation edit rate TER algorithm", "This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBNs submission to the WMT08 shared translation task"], "inroduction": ["Confusion network decoding has been applied in combining outputs from multiple machine translation systems", "The earliest approach in Bangalore et al 2001 used edit distance based multiple string alignment MSA Durbin et al 1988 to build the confusion networks", "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model Matusov et al 2006 or edit distance alignments allowing shifts Rosti et al 2007", "The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment", "The confusion networks are built around a skeleton hypothesis", "The skeleton hypothesis defines the word order of the decoding output", "Usually the 1best hypotheses from each system are considered as possible skeletons", "Using the pairwise hypothesis alignment the confusion networks are built in two steps", "First all hypotheses are aligned against the skeleton independently", "Second the confusion networks are created from the union of these alignments", "The incremental hypothesis alignment algorithm combines these two steps", "All words from the previously aligned hypotheses are available even if not present in the skeleton hypothesis when aligning the following hypotheses", "As in Rosti et al 2007 confusion networks built around all skeletons are joined into a lattice which is expanded and re scored with language models", "System weights and language model weights are tuned to optimize the quality of the decoding output on a development set", "This paper is organized as follows", "The incremental TER alignment algorithm is described in Section 2", "Experimental evaluation comparing the incremental and pairwise alignment methods are presented in Section 3 along with results on the WMT08 Europarl test sets", "Conclusions and future work are presented in Section 4"]}, "W08-0335": {"title": ["Proceedings of the Third Workshop on Statistical Machine Translation pages 216 223 Columbus Ohio USA June 2008 c  2008 Association for Computational Linguistics Improved Statistical Machine Translation by Multiple Chinese Word Segmentation Ruiqiang Zhang12 and Keiji Yasuda12 and Eiichiro Sumita12 1National Institute of Information and Communications Technology 2ATR Spoken Language Communication Research Laboratories 222 Hikaridai Science City Kyoto 6190288 Japan ruiqiangzhangkeijiyasudaeiichirosumitaatrjp Abstract Chinese word segmentation CWS is a necessary step in ChineseEnglish statistical machine translation SMT and its performance has an impact on the results of SMT However there are many settings involved in creating a CWS system such as various specifications and CWS methods This paper investigates the effect of these settings to SMT We tested dictionarybased and CRFbased approaches and found there was no significant difference between the two in the qualty of the resulting translations We also found the correlation between the CWS Fscore and SMT BLEU score was very weak This paper also proposes two methods of combining advantages of different specifications a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated We found these approaches were very effective in improving quality of translations 1 Introduction Chinese word segmentation CWS is a necessary step in ChineseEnglish statistical machine translation SMT The research on CWS independently from SMT has been conducted for decades As an evidence the CWS evaluation campaign the Sighan Bakeoff Emerson 20051 has been held four times since 2004 However works on relations between CWS and SMT are scarce Generally two factors need to be considered in constructing a CWS system The first one is the specifications for CWS ie the rules or guidelines for word segmentation and the second one is the CWS methods There are many CWS specifications used by different organizations Unfortunately these organizations do not seem to have any intention of reaching a unified specification More than five or six specifications have been used in the four Sighan Bakeoffs There is also significant disagreement on the specifications although much of their contents is the same One of the aims of this work was therefore to establish whether inconsistencies in specifications significantly affect the quality of SMT The second factor is CWS methods We grouped all of the CWS methods into two classes the class without outofvocabulary OOV recognition and the class with OOV recognition represented by the dictionarybased CWS and the CRFbased CWS respectively Outofvocabulary recognition may have twosided effects on SMT performance The CRFbased CWS that supports OOV recognition produces word segmentations with a higher Fscore but a huge number of new words recognized correctly and incorrectly that can incur data sparseness in training the SMT models On the other hand the dictionarybased approach that does not support OOV recognition produced a lower Fscore but with a relatively weak data spareness problem Which approach pro1A CWS competition organized by the ACL special interest group on Chinese 216 Table 1 Examples of disagreement in segmentation guidelines ChineseName EnglishName Time AS DENGXIAOPING GEORGE BUSH 1997YEAR 7MONTH 1DAY CITYU DENGXIAOPING GEORGEBUSH 1997 YEAR 7 MONTH 1 DAY MSR DENGXIAOPING GEORGEBUSH 1997YEAR7MONTH1DAY PKU DENG XIAOPING GEORGEBUSH 1997YEAR 7MONTH 1DAY Table 2 A second example of disagreement in segmentation guidelines Composite words Composite words AS FUJITSUCOMPANY EUROZONE CITYU FUJITSU COMPANY EUROZONE MSR FUJITSUCOMPANY EURO ZONE PKU FUJITSU COMPANY EUROZONE duces a better SMT result is our research interest in this work The performance of CWS is usually measured by the Fscore while that of SMT is measured using the BLEU score Does a CWS with a higher Fscore produce a better translation In this paper we answer this question by comparing Fscores with BLEU scores In this work we also propose approaches to make use of all the Sighan training data regardless of the specifications Two methods are proposed 1 a simple combination of all the training data and 2 implementing linear interpolation of multiple translation models Linear interpolation is widely used in language modeling for speech recognition We interpolated multiple translation models generated by the CWS schemes and found our approaches were very effective in improving the translations 2 CWS specifications and corpora from the second Sighan Bakeoff A Chinese word is composed of one or more characters There are no spaces between the words Automatic word segmentation is required for machine translation Usually a specification is needed to carry out word segmentation Unfortunately there are many different versions of specifications Different tasks give rise to different requirements and the CWS specifications must be adjusted accordingly For example shorter segmentation has been shown to be better for speech recognition A composite word numbers dates times etc is split into characters even if it is one word defined by linguists In contrast longer segmentation is preferred for named entity recognition consisting of longer character sequences such as the name of people places and organizations This work investigated four wellknown specifications created by four different organizations Academia Sinica AS City University of Hong Kong CITYU Microsoft Research Beijing MSR and Beijing University PKU These specs were used in the second Sighan Bakeoff Emerson 2005 When we compared the four specifications and the manual segmentations in the Sighan Bakeoff training data we found there were many inconsistencies among the four specifications Some examples are shown in Table 1 and 2 For instance the AS and PKU specifications are distinct in splitting both Chinese and English names We also found the MSR specification generated more composite words and grouped longer character sequences into a word Using this specification could generate tens of thousands of new words which can cause data sparseness for SMT In addition to using the four specifications we also downloaded the training and test corpora of the second Sighan Bakeoff We used each of the training corpora provided to create a CWS scheme and evaluated the performance of the schemes on our test 217 data This enabled us to examine the effect of CWS specifications on SMT We used a Chinese word segmentation tool Achilles to implement word segmentation Part of the work using this tool was described by Zhang et al 2006 The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff Moreover this tool meets our need to test the effect of the two kinds of CWS approaches for SMT We can easily train a dictionarybased and a CRFbased CWS by using this tool By turning the programs option for the CRF model on and off we can use the Achilles as a dictionarybased approach and as a CRFbased CWS In fact the dictionarybased approach is the default approach for Achilles 3 Experiments"], "abstract": ["Chinese word segmentation CWS is a necessary step in ChineseEnglish statistical machine translation SMT and its performance has an impact on the results of SMT", "However there are many settings involved in creating a CWS system such as various specifications and CWS methods", "This paper investigates the effect of these settings to SMT", "We tested dictionary based and CRFbased approaches and found there was no significant difference between the two in the qualty of the resulting translations", "We also found the correlation between the CWS Fscore and SMT BLEU score was very weak", "This paper also proposes two methods of combining advantages of different specifications a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated", "We found these approaches were very effective in improving quality of translations"], "inroduction": ["Chinese word segmentation CWS is a necessary step in ChineseEnglish statistical machine translation SMT", "The research on CWS independently from SMT has been conducted for decades", "As an evidence the CWS evaluation campaign the Sighan Bakeoff Emerson 20051 has been held four times since 2004", "However works on relations between CWS and SMT are scarce", "Generally two factors need to be considered in constructing a CWS system", "The first one is the specifications for CWS ie the rules or guidelines for word segmentation and the second one is the CWS methods", "There are many CWS specifications used by different organizations", "Unfortunately these organizations do not seem to have any intention of reaching a unified specification", "More than five or six specifications have been used in the four Sighan Bakeoffs", "There is also significant disagreement on the specifications although much of their contents is the same", "One of the aims of this work was therefore to establish whether inconsistencies in specifications significantly affect the quality of SMT", "The second factor is CWS methods", "We grouped all of the CWS methods into two classes the class without outofvocabulary OOV recognition and the class with OOV recognition represented by the dictionarybased CWS and the CRFbased CWS respectively", "Outofvocabulary recognition may have twosided effects on SMT performance", "The CRF based CWS that supports OOV recognition produces word segmentations with a higher Fscore but a huge number of new words recognized correctly and incorrectly that can incur data sparseness in training the SMT models", "On the other hand the dictionary based approach that does not support OOV recognition produced a lower Fscore but with a relatively weak data spareness problem", "Which approach pro 1 A CWS competition organized by the ACL special interest group on Chinese", "216 Proceedings of the Third Workshop on Statistical Machine Translation pages 216223 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics Table 1 Examples of disagreement in segmentation guidelines ChineseName EnglishName Time AS DENGXIAOPING GEORGE BUSH 1997YEAR 7MONTH 1DAY CITYU DENGXIAOPING GEORGEBUSH 1997 YEAR 7 MONTH 1 DAY", "MSR DENGXIAOPING GEORGEBUSH 1997YEAR7MONTH1DAY PKU DENG XIAOPING GEORGEBUSH 1997YEAR 7MONTH 1DAY Table 2 A second example of disagreement in segmentation guidelines Composite words Composite words AS FUJITSUCOMPANY EUROZONE CITYU FUJITSU COMPANY EUROZONE MSR FUJITSUCOMPANY EURO ZONE PKU FUJITSU COMPANY EUROZONE duces a better SMT result is our research interest in this work", "The performance of CWS is usually measured by the Fscore while that of SMT is measured using the BLEU score", "Does a CWS with a higher F score produce a better translation", "In this paper we answer this question by comparing Fscores with BLEU scores", "In this work we also propose approaches to make use of all the Sighan training data regardless of the specifications", "Two methods are proposed 1 a simple combination of all the training data and 2 implementing linear interpolation of multiple translation models", "Linear interpolation is widely used in language modeling for speech recognition", "We interpolated multiple translation models generated by the CWS schemes and found our approaches were very effective in improving the translations"]}, "W08-0602": {"title": ["Extracting Clinical Relationships from Patient Narratives"], "abstract": ["The Clinical EScience Framework CLEF project has built a system to extract clinically significant information from the textual component of medical records for clinical research evidencebased healthcare and genotypemeetsphenotype informatics", "One part of this system is the identification of relationships between clinically important entities in the text", "Typical approaches to relationship extraction in this domain have used full parses domainspecific grammars and large knowledge bases encoding domain knowledge", "In other areas of biomedical NLP statistical machine learning approaches are now routinely applied to relationship extraction", "We report on the novel application of these statistical techniques to clinical relationships", "We describe a supervised machine learning system trained with a corpus of oncology narratives handannotated with clinically important relationships", "Various shallow features are extracted from these texts and used to train statistical classifiers", "We compare the suitability of these features for clinical relationship extraction how extraction varies between inter and intrasentential relationships and examine the amount of training data needed to learn various relationships"], "inroduction": ["The application of Natural Language Processing NLP is widespread in biomedicine", "Typically it is applied to improve access to the everburgeoning research literature", "Increasingly biomedical researchers need to relate this literature to phenotypic data both to populations and to individual clinical subjects", "The computer applications used in biomedical research including NLP applications therefore need to support genotypemeets phenotype informatics and the move towards translational biology", "Such support will undoubtedly include linkage to the information held in individual medical records both the structured portion and the unstructured textual portion", "The Clinical EScience Framework CLEF project Rector et al 2003 is building a framework for the capture integration and presentation of this clinical information for research and evidence based health care", "The projects data resource is a repository of the full clinical records for over 20000 cancer patients from the Royal Marsden Hospital Europes largest oncology centre", "These records combine structured information clinical narratives and free text investigation reports", "CLEF uses information extraction IE technology to make information from the textual portion of the medical record available for integration with the structured record and thus available for clinical care and research", "The CLEF IE system analyses the textual records to extract entities events and the relationships between them", "These relationships give information that is often not available in the structured record", "Why was a drug given", "What were the results of a physical examination", "What problems were not present", "We have previously reported entity extraction in the CLEF IE system Roberts et al 2008b", "This paper examines relationship extraction", "Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system", "Several such systems have been described", "They generally use a syntactic parse with domain specific grammar rules", "The Linguistic String project Sager et al 1994 used a full syntactic and 10 BioNLP 2008 Current Trends in Biomedical Natural Language Processing pages 1018 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical statements", "These were mapped to a database model incorporating medical facts and the relationships between them", "MedLEE Friedman et al 1994 and more recently BioMedLEE Lussier et al 2006 used a semantic lexicon and grammar of domainspecific semantic patterns", "The patterns encode the possible relationships between entities allowing both entities and the relationships between them to be directly matched in the text", "Other systems have incorporated large scale domainspecific knowledge bases", "MEDSYNDIKATE Hahn et al 2002 employed a rich discourse model of entities and their relationships built using a dependency parse of texts and a description logic knowledge base reengineered from existing terminologies", "MENELAS Zweigenbaum et al 1995 also used a full parse a conceptual representation of the text and a large scale knowledge base", "In other applications of biomedical NLP a second paradigm has become widespread the application of statistical machine learning techniques to featurebased models of the text", "Such approaches have typically been applied to journal texts", "They have been used both for entity recognition and extraction of various relations such as proteinprotein interactions see for example Grover et al 2007", "This follows on from the success of these methods in general NLP see for example Zhou et al 2005", "Statistical machine learning has also been applied to clinical text but its use has generally been limited to entity recognition", "The Mayo Clinic text analysis system Pakhomov et al 2005 for example uses a combination of dictionary lookup and a Nave Bayes classifier to identify entities for information retrieval applications", "To the best of our knowledge statistical methods have not been previously applied to extraction of clinical relationships from text", "This paper describes experiments in the statistical machine learning of relationships from a novel text type oncology narratives", "The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE such as querying to support clinical research", "We apply Support Vector Machine SVM classifiers to learn these relationships", "The classifiers are trained and evaluated using novel data a gold standard corpus of clinical text handannotated with semantic entities and relationships", "In order to test the applicability of this method to the clinical domain we train classifiers using a number of comparatively simple text features and look at the contribution of these features to system performance", "Clinically interesting relationships may span several sentences and so we compare classifiers trained for both intra and inter sentential relationships spanning one or more sentence boundaries", "We also examine the influence of training corpus size on performance as hand annotation of training data is the major expense in supervised machine learning"]}, "W08-0703": {"title": ["A Bayesian  model of natural language  phonology"], "abstract": ["A stochastic approach to learning phonology", "The model presented captures 715 more phonologically plausible underlying forms than a simple majority solution because it prefers pure alternations", "It could be useful in cases where an approximate solution is needed or as a seed for more complex models", "A similar process could be involved in some stages of child language acquisition in particular early learning of phonotactics"], "inroduction": ["Sound changes in natural language such as stem variation in inflected forms can be described as phonological processes", "These are governed by a constraint hierarchy as in Optimality Theory OT or by a set of ordered rules", "Both rely on a single lexical representation of each morpheme ie its underlying form and contextsensitive transformations to surface forms", "Phonological changes often affect segments near morpheme boundaries but can also apply over an entire prosodic word as in vowel harmony", "It does not seem straightforward to incorporate context into a Bayesian model of phonology although a clever solution may yet be found", "A standard way of incorporating conditioning environments is to treat them as factors in a Gibbs model Liang and Klein 2007 but such models require an explicit calculation of the partition function", "Unless the rule contexts possess some kind of locality we dont know how to compute this partition function efficiently", "Some context could be captured by generating underlying phonemes from an ngram model or by annotating surface forms with neighborhood features", "However the effects of autosegmental phonology and other longrange dependencies like vowel harmony cannot be easily Bayesianized", "11 Related Work", "In the last decade finitestate approaches to phonology Gildea and Jurafsky 1996 Beesley and Karttunen 2000 have effectively brought theoretical linguistic work on rewrite rules into the computational realm", "A finitestate approximation of optimality theory Karttunen 1998 was later refined into a compact treatment of gradient constraints Gerdemann and van Noord 2000", "Recent work on Bayesian models of morphological segmentation Johnson et al 2007 could be combined with phonological rule induction Goldwater and Johnson 2004 in a variety of ways some of which will be explored in our discussion of future work", "Also the Hierarchical Bayes Compiler Daume III 2007 could be used to generate a model similar to the one presented here but less con strained1 which makes correspondingly more random less accurate predictions", "12 Dataset", "As we describe the model and its implementation in this and subsequent sections we will refer to a sam 1 Recent updates to HBC inspired by discussions with the author have addressed some of these limitations", "12 Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology pages 1219 Columbus Ohio USA June 2008", "Qc 2008 Association for Computational Linguistics ple dataset in Figure 1 consisting of a paradigm2 of verb stems and personnumber suffixes", "The head of each row or column is an underlying form which in 3rd person singular is a phonologically null segment represented as ", "In surface forms the realization of each morpheme is affected by phonological processes", "For example in the combination of tieta  vat the result is tietavat where the 3rd person plural a becomes a due to vowel harmony", "13 Bayesian Approach", "As a baseline model we select the most frequently occurring allophone as the underlying form", "Our goal is to outperform this baseline using a Bayesian model", "In other words what patterns in phonological processes can be inferred with such a statistical model", "This simple framework begins learning with the assumption that the underlying forms are faithful to the surface ie without considering markedness or phonotactics", "We model the generation of surface forms from underlying ones on the segmental character level", "Input is an inflectional paradigm with tokens of the form stemsuffix", "Morphology is limited to a single suffix no agglutination and is already identified", "Each character of an underlying stem or suffix ui  generates surface characters sij  in an entire row or column of the input", "To capture the phonology of a variety of languages with a single model we need constraints from linguistically plausible priors universal grammar", "We prefer that underlying characters be preserved in surface forms especially when there is no alternation", "It is also reasonable that there be fewer underlying forms phonemes than surface forms phones phonetic inventory to account for allo phones", "We expect to be able to capture a significant subset of phonological processes using a simple model only faithfulness constraints", "14 Pure Generators", "Our model has an advantage over the baseline in its preference for purity in underlying forms", "Each underlying segment should generate as few distinct 2 The paradigm format lends itself to analysis of word types but if supplemented with surface counts can also handle tokens", "surface segments as possible if it generates non alternating identical segments it will be less likely to generate an alternation in addition", "This means that when two segments alternate the underlying form should be the one that appears less frequently in other contexts irrespective of the majority within the alternation", "In the first stem of our Finnish verb conjugation Figure 1 we see a td alternation a case of consonant gradation as well as unalternating t", "If we isolate three of the surface forms where tieta is inflected 1st person singular and 3rd person singular and plural and consider only the dental segments in the stem of each we have two underlying segments", "Here we use question marks to indicate unknown underlying segments", " dt tt tt In this subset of the data the reasonable candidate underlying forms are t and d", "These two compete to explain the observed data surface forms", "The nature of the prior probability distribution determines whether the majority is hypothesized for each underlying form so t produces both alternating and unalternating surface segments or d is hypothesized as the source of the alternation and t remains pure", "In a Bayesian setting we impose a sparse prior over underlying forms conditioned on the surface forms they generateIf u2 is hypothesized to be t the posterior prob ability of u1 being t goes down P u1  tu2  t  P u1  t The probability of u1 being the competitor d correspondingly increases P u1  du2  t  P u1  d Even though the majority in this case would be t the favored candidate for the alternating form was d", "This happened because of how we defined the models prior in combination with the evidence that t assigned to u2 generated the sequence of t", "So selection bias prefers d as the source of an ambiguous segment leaving t to always generate itself", "A similar effect can occur if there are both unalternating ts and ds on the surface in addition to the td alternation", "The candidate t or d that is  S Figure 1 Sample dataset constructed by hand Finnish verbs with inflection for person and number", "generating fewer unalternating segments is preferred to explain the alternation", "For example if there were 1000 cases of t 500 d and 500 td we would expect the following hypotheses t  t d  d and d  t d", "This is because one of the two candidates must be responsible for both unalternating and alternating segments but we prefer to have as much purity as possible to minimize ambiguity", "With this solution we still have 1000 pure t  t and only the 500 d  d are now indistinct from d  t d", "If we had selected t as the source of the alternation there would be only 500 remaining pure d segments and 1500 ambiguous t", "Our Bayesian model should prefer the less prior over underlying form encourages sparse solutions so u  1 for all u The prior over surface form given underlying encourages identity mapping x  x so xx  1 and discourages different segments x  y so xy  1 for all x  y     ambiguous purer solution given an appropriate nc prior"]}, "W09-0441": {"title": ["Fluency Adequacy or HTER"], "abstract": ["Automatic Machine Translation MT evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance", "Different types of human judgments such as Fluency Adequacy and HTER measure varying aspects of MT performance that can be captured by automatic MT metrics", "We explore these differences through the use of a new tunable MT metric TERPlus which extends the Translation Edit Rate evaluation metric with tun able parameters and the incorporation of morphology synonymy and paraphrases", "TERPlus was shown to be one of the top metrics in NISTs Metrics MATR 2008 Challenge having the highest average rank in terms of Pearson and Spear man correlation", "Optimizing TERPlus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits demonstrating significant differences between the types of human judgments"], "inroduction": ["Since the introduction of the BLEU metric Pa pineni et al 2002 statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics", "These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality", "Numerous methods of judging MT output by humans have been used including Fluency Adequacy and more recently Humanmediated Translation Edit Rate HTER Snover et al 2006", "Fluency measures whether a translation is fluent regardless of the correct meaning while Adequacy measures whether the translation conveys the correct meaning even if the translation is not fully fluent", "Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale with their average being used as a single score of translation quality", "HTER is a more complex and semiautomatic measure in which humans do not score translations directly but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference", "This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate TER Snover et al 2006 or when used with other automatic metrics such as BLEU or METEOR Banerjee and Lavie 2005", "One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits as measured by TER between the MT output and the targeted reference creating the reference that is as close as possible to the MT output while still being adequate and fluent", "In this way only true errors in the MT output are counted", "While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy it is much more time consuming and taxing on human annotators than other types of human judgments making it difficult and expensive to use", "In addition because HTER treats all edits equally no distinction is made between serious errors errors in names or missing subjects and minor edits such as a difference in verb agreement Proceedings of the Fourth Workshop on Statistical Machine Translation  pages 259268 Athens Greece 30 March  31 March 2009", "Qc 2009 Association for Computational Linguistics or a missing determinator", "Different types of translation errors vary in importance depending on the type of human judgment being used to evaluate the translation", "For example errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent", "On the other hand deletion of content words might not lower the fluency of a translation but the adequacy would suffer", "In this paper we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric", "To study this we introduce a new evaluation metric TERPlus TERp1 that improves over the existing Translation Edit Rate TER metric Snover et al 2006 incorporating morphology synonymy and paraphrases as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments", "Section 2 summarizes the TER metric and discusses how TERp improves on it", "Correlation results with human judgments including independent results from the 2008 NIST Metrics MATR evaluation where TERp was consistently one of the top metrics are presented in Section 3 to show the utility of TERp as an evaluation metric", "The generation of paraphrases as well as the effect of varying the source of paraphrases is discussed in Section 4", "Section 5 discusses the results of tuning TERp to Fluency Adequacy and HTER and how this affects the weights of various edit types"]}, "W09-0621": {"title": ["Clustering and Matching Headlines for Automatic Paraphrase Acquisition"], "abstract": ["For developing a datadriven text rewriting algorithm for paraphrasing it is essential to have a monolingual corpus of aligned paraphrased sentences", "News article headlines are a rich source of paraphrases they tend to describe the same event in various different ways and can easily be obtained from the web", "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases one based on clustering and the other on pairwise similaritybased matching", "We show that the latter performs best on the task of aligning paraphrastic headlines"], "inroduction": ["In recent years texttotext generation has received increasing attention in the field of Natural Language Generation NLG", "In contrast to traditional concepttotext systems texttotext generation systems convert source text to target text where typically the source and target text share the same meaning to some extent", "Applications of texttotext generation include sum marization Knight and Marcu 2002 question answering Lin and Pantel 2001 and machine translation", "For texttotext generation it is important to know which words and phrases are semantically close or exchangable in which contexts", "While there are various resources available that capture such knowledge at the word level eg synset knowledge in WordNet this kind of information is much harder to get by at the phrase level", "Therefore paraphrase acquisition can be considered an important technology for producing resources for texttotext generation", "Paraphrase generation has already proven to be valuable for Question Answering Lin and Pantel 2001 Riezler et al 2007 Machine Translation CallisonBurch et al 2006 and the evaluation thereof RussoLassner et al 2006 Kauchak and Barzilay 2006 Zhou et al 2006 but also for text simplification and explanation", "In the study described in this paper we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation", "News article headlines are abundant on the web and are already grouped by news aggregators such as Google News", "These services collect multiple articles covering the same event", "Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases Dolan et al 2004 Nelken and Shieber 2006", "We use this method to collect a large amount of aligned paraphrases in an automatic fashion"]}, "W09-0802": {"title": ["The Karamel System and Semitic Languages Structured MultiTiered"], "abstract": ["Karamel is a system for finitestate morphology which is multitape and uses a typed Cartesian product to relate tapes in a structured way", "It implements statically compiled feature structures", "Its language allows the use of regular expressions and Generalized Restriction rules to define multitape transducers", "Both simultaneous and successive application of local constraints are possible", "This system is interesting for describing rich and structured morphologies such as the morphology of Semitic languages"], "inroduction": ["Karamel is a system for defining and executing multitape finitestate transducers where relationships between tapes are expressed using a tree structure", "This structure is obtained through embedded units which are used to analyze a tuple of strings recognized by the transducer", "For instance the units considered in an example may be affix form and sentence", "The system includes a language and an Integrated Development Environment", "The language uses extended regular expressions computations and contextual rules", "The environment provides a graphical interface to write and execute finitestate descriptions", "Karamel has many applications", "For Natural Language Processing it may be used for morphological analysis transliteration parsing etc This paper is dedicated to the application of Karamel to the morphological analysis of Semitic languages for which both multiple tapes and complex structures are useful", "Some descriptions of the morphology of Semitic Languages use several tiers", "For instance McCarthy 1981 uses four tiers one for prefixes one for the root one for the template consonant vowel pattern and the last one for the vocalizationSuch a multitiered description may be im plemented using a cascade of 2tape machines Beesley 1998 or using a multitape transducer where each tier is described by a tape and the surface form by an additional tape", "This is the approach of G A Kiraz for the Syriac language Kiraz 2000", "Karamel is designed for the later solution", "The multitape feature is also interesting for describing related dialects whenever a great part of the analysis may be shared", "A separate tape is dedicated to the surface form in each dialect", "The Semitic Morphology is strongly structured by the roots", "The basis of an analysis is the identification of the root", "Furthermore several layers of affixes may be distinguished around the core containing the roots paradigmatic prefixes affixes encoding the person gender and number clitics such as pronouns", "This structure is conveniently defined using Karamels units", "In the following section of the paper we present Karamels language its theoretical background and its syntax", "Section 3 describe the other aspects of the Karamel System its development environment its current state and future evolutions", "Then comes an example of Semitic morphology written in Karamel the description of Akkadian verbal flexion", "The last section compares Karamel to some other systems"]}, "W09-1007": {"title": ["Language models for contextual error detection and correction"], "abstract": ["The problem of identifying and correcting confusibles ie contextsensitive spelling errors in text is typically tackled using specifically trained machine learning classifiers", "For each different set of con fusibles a specific classifier is trained and tuned", "In this research we investigate a more generic approach to contextsensitive con fusible correction", "Instead of using specific classifiers we use one generic classifier based on a language model", "This measures the likelihood of sentences with different possible solutions of a confusible in place", "The advantage of this approach is that all confusible sets are handled by a single model", "Preliminary results show that the performance of the generic classifier approach is only slightly worse that that of the specific classifier approach"], "inroduction": ["When writing texts people often use spelling checkers to reduce the number of spelling mistakes in their texts", "Many spelling checkers concentrate on nonword errors", "These errors can be easily identified in texts because they consist of character sequences that are not part of the language", "For example in English woord is is not part of the language hence a nonword error", "A possible correction would be word  Even when a text does not contain any non word errors there is no guarantee that the text is errorfree", "There are several types of spelling errors where the words themselves are part of the language but are used incorrectly in their context", "Note that these kinds of errors are much harder to recognize as information from the context in which they occur is required to recognize and correct these errors", "In contrast nonword errors can be recognized without context", "One class of such errors called confusibles consists of words that belong to the language but are used incorrectly with respect to their local sentential context", "For example She owns to cars contains the confusible to  Note that this word is a valid token and part of the language but used incorrectly in the context", "Considering the context a correct and very likely alternative would be the word two  Confusibles are grouped together in confusible sets", "Confusible sets are sets of words that are similar and often used incorrectly in context", "Too is the third alternative in this particular confusible set", "The research presented here is part of a larger project which focusses on contextsensitive spelling mistakes in general", "Within this project all classes of contextsensitive spelling errors are tackled", "For example in addition to confusibles a class of pragmatically incorrect words where words are incorrectly used within the document wide context is considered as well", "In this article we concentrate on the problem of confusibles where the context is only as large as a sentence"]}, "W10-1704": {"title": ["Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR pages 54 59 Uppsala Sweden 1516 July 2010 c2010 Association for Computational Linguistics LIMSIs statistical translation systems for WMT  10 Alexandre Allauzen Josep M Crego Ilknur Durgar ElKahlout and Francois Yvon LIMSICNRS and Universite ParisSud 11 France BP 133 91403 Orsay Cedex FirstnameLastnamelimsifr Abstract This paper describes our Statistical Machine Translation systems for the WMT10 evaluation where LIMSI participated for two language pairs FrenchEnglish and GermanEnglish in both directions For GermanEnglish we concentrated on normalizing the German side through a proper preprocessing aimed at reducing the lexical redundancy and at splitting complex compounds For FrenchEnglish we studied two extensions of our inhouse N code decoder firstly the effect of integrating a new bilingual reordering model second the use of adaptation techniques for the translation model For both set of experiments we report the improvements obtained on the development and test data 1 Introduction LIMSI took part in the WMT 2010 evaluation campaign and developed systems for two languages pairs FrenchEnglish and GermanEnglish in both directions For GermanEnglish we focused on preprocessing issues and performed a series of experiments aimed at normalizing the German side by removing some of the lexical redundancy and by splitting compounds For this pair all the experiments were performed using the Moses decoder Koehn et al 2007 For FrenchEnglish we studied two extensions of our ngram based system first the effect of integrating a new bilingual reordering model second the use of adaptation techniques for the translation model Decoding is performed using our inhouse N code Marin o et al 2006 decoder 2 System architecture and resources In this section we describe the main characteristics of the phrasebased systems developed for this evaluation and the resources that were used to train our models As far as resources go we used all the data supplied by the 2010 evaluation organizers Based on our previous experiments De chelotte et al 2008 which have demonstrated that better normalization tools provide better BLEU scores Papineni et al 2002 we took advantage of our inhouse text processing tools for the tokenization and detokenization steps Only for German data did we used the TreeTagger Schmid 1994 tokenizer Similar to last years experiments all of our systems are built in truecase  3 GermanEnglish systems As German is morphologically more complex than English the default policy which consists in treating each word form independently from the others is plagued with data sparsity which poses a number of difficulties both at training and decoding time When aligning parallel texts at the word level German compound words typically tend to align with more than one English word this in turn tends to increase the number of possible translation counterparts for each English type and to make the corresponding alignment scores less reliable In decoding new compounds or unseen morphological variants of existing words artificially increase the number outofvocabulary OOV forms which severely hurts the overall translation quality Several researchers have proposed normalization Niessen and Ney 2004 Corstonoliver and Gamon 2004 Goldwater and McClosky 2005 and compound splitting Koehn and Knight 2003 Stymne 2008 Stymne 2009 methods Our approach here is similar yet uses different implementations we also studied the joint effect of combining both techniques"], "abstract": ["This paper describes our Statistical Machine Translation systems for the WMT10 evaluation where LIMSI participated for two language pairs FrenchEnglish and GermanEnglish in both directions", "For GermanEnglish we concentrated on normalizing the German side through a proper preprocessing aimed at reducing the lexical redundancy and at splitting complex compounds", "For FrenchEnglish we studied two extensions of our inhouse N code decoder firstly the effect of integrating a new bilingual reordering model second the use of adaptation techniques for the translation model", "For both set of experiments we report the improvements obtained on the development and test data"], "inroduction": ["LIMSI took part in the WMT 2010 evaluation campaign and developed systems for two languages pairs FrenchEnglish and GermanEnglish in both directions", "For GermanEnglish we focused on preprocessing issues and performed a series of experiments aimed at normalizing the German side by removing some of the lexical redundancy and by splitting compounds", "For this pair all the experiments were performed using the Moses decoder Koehn et al 2007", "For French English we studied two extensions of our ngram based system first the effect of integrating a new bilingual reordering model second the use of adaptation techniques for the translation model", "Decoding is performed using our inhouse N code Marin o et al 2006 decoder"]}, "W10-1727": {"title": ["Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR pages 183 188 Uppsala Sweden 1516 July 2010 c2010 Association for Computational Linguistics Vs and OOVs Two Problems for Translation between German and English Sara Stymne Maria Holmqvist Lars Ahrenberg Linko ping University Sweden sarstmarholahidaliuse Abstract In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system In training two reordering strategies were studied i reorder on the basis of the alignments from Giza and"], "abstract": ["In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system", "In training two reordering strategies were studied i reorder on the basis of the alignments from Giza and ii reorder by moving all verbs to the end of segments", "In translation outof vocabulary words were preprocessed in a knowledgelite fashion to identify a likely equivalent", "All three strategies were implemented for our EnglishGerman system submitted to the WMT10 shared task", "Combining them lead to improvements in both language directions"], "inroduction": ["We present the Liu translation system for the constrained condition of the WMT10 shared translation task between German and English in both directions", "The system is based on the 2009 Liu submission Holmqvist et al 2009 that used compound processing morphological sequence models and improved alignment by reordering", "This year we have focused on two issues translation of verbs which is problematic for translation between English and German since the verb placement is different with German verbs often being placed at the end of sentences and OOVs out ofvocabulary words which are problematic for machine translation in general", "Verb translation is targeted by trying to improve alignment which we believe is a crucial step for verb translation since verbs that are far apart are often not aligned at all", "We do this mainly by moving verbs to the end of sentences previous to alignment which we also combine with other alignments", "We transform OOVs into known words in a postprocessing step based on casing stemming and splitting of hyphenated compounds", "In addition we perform general compound splitting for German both before training and translation which also reduces the OOV rate", "All results in this article are for the development test set newstest2009 on truecased output", "We report Bleu scores Papineni et al 2002 and Meteor ranking without WordNet scores Agar wal and Lavie 2008 using percent notation", "We also used other metrics but as they gave similar results they are not reported", "For significance testing we used approximate randomization Riezler and Maxwell 2005 with p  005"]}, "W10-1730-parscit": {"title": ["Maximum Entropy Translation Model"], "abstract": ["Maximum Entropy Principle has been used successfully in various NLP tasks", "Inthis paper we propose a forward translation model consisting of a set of maximum entropy classifiers a separate classifier is trained for each sufficiently frequent sourceside lemma", "In this way the estimates of translation probabilitiescan be sensitive to a large number of features derived from the source sentence including nonlocal features features making use of sentence syntactic structureetc", "When integrated into EnglishtoCzech dependencybased translation scenario implemented in the TectoMT framework the new translation model significantly outperforms the baseline modelMLE in terms of BLEU", "The performance is further boosted in a configurationinspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the targetlanguage dependency tree model"], "inroduction": ["The principle of maximum entropy states thatgiven known constraints the probability distri bution which best represents the current state of knowledge is the one with the largest entropyMaximum entropy models based on this princi ple have been widely used in Natural Language Processing eg for tagging Ratnaparkhi 1996parsing Charniak 2000 and named entity recog nition Bender et al 2003", "Maximum entropy models have the following form where fi is a feature function Ai is its weight and Zx is the normalizing factor Zx   exp Aifix y y iIn statistical machine translation SMT trans lation model TM pts is the probability that the string t from the target language is the translation of the string s from the source language", "Typical approach in SMT is to use backward translationmodel pst according to Bayes rule and noisy channel model", "However in this paper we deal only with the forward direct model1The idea of using maximum entropy for con structing forward translation models is not new", "It naturally allows to make use of various featurespotentially important for correct choice of targetlanguage expressions", "Let us adopt a motivat ing example of such a feature from Berger et al 1996 which contains the first usage of maxenttranslation model we are aware of If house ap pears within the next three words eg the phrases in the house and in the red house then dans might be a more likely French translation of in Incorporating nonlocal features extracted fromthe source sentence into the standard noisychannel model in which only the backward trans lation model is available is not possible", "Thisdrawback of the noisychannel approach is typi cally compensated by using large targetlanguage ngram models which can  in a result  play arole similar to that of a more elaborate more context sensitive forward translation model", "How ever we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side", "1A backward translation model is used only for pruning training data in this paper", "1  pyx  Zxexp Aifix y i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR pages 201206 Uppsala Sweden 1516 July 2010", "c2010 Association for Computational Linguistics A deeper discussion on the potential advantagesof maximum entropy approach over the noisy channel approach can be found in Foster 2000and Och and Ney 2002 in which another suc cessful applications of maxent translation models are shown", "Loglinear translation models instead of MLE with rich feature sets are used also in Ittycheriah and Roukos 2007 and Gimpel andSmith 2009 the idea can be traced back to Pap ineni et al 1997", "What makes our approach different from the previously published works is that1", "we show how the maximum entropy trans lation model can be used in a dependencyframework we use deepsyntactic dependency trees as defined in the Prague Depen dency Treebank Hajic et al 2006 as the transfer layer2", "we combine the maximum entropy transla tion model with targetlanguage dependency tree model and use treemodified Viterbisearch for finding the optimal lemmas label ing of the targettree nodes", "The rest of the paper is structured as follows", "InSection 2 we give a brief overview of the translation framework TectoMT in which the experi ments are implemented", "In Section 3 we describehow our translation models are constructed", "Sec tion 4 summarizes the experimental results and Section 5 contains a summary"]}, "W10-1750": {"title": ["Documentlevel Automatic MT Evaluation based on Discourse Representations"], "abstract": ["This paper describes the joint submission of Universitat Politecnica de Catalunya and Universitat de Barcelona to the Metrics MaTr 2010 evaluation challenge in collaboration with ELDAELRA", "Our work is aimed at widening the scope of current automatic evaluation measures from sentence to document level", "Preliminary experiments based on an extension of the metrics by Gimenez and Marquez 2009 operating over discourse representations are presented"], "inroduction": ["Current automatic similarity measures for Machine Translation MT evaluation operate all without exception at the segment level", "Translations are analyzed on a segmentbysegment1 fashion ignoring the text structure", "Document and system scores are obtained using aggregate statistics over individual segments", "This strategy presents the main disadvantage of ignoring cross sententialdiscursive phenomena", "In this work we suggest widening the scope of evaluation methods", "We have defined genuine documentlevel measures which are able to exploit the structure of text to provide more informed evaluation scores", "For that purpose we take advantage of two coincidental facts", "First test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event story or topic Przybocki et al 2008 Przybocki et al 2009 CallisonBurch et al 2009", "Second we count on automatic linguistic processors which provide very detailed discourse level representations of text Curran et al 2007", "Discourse representations allow us to focus on relevant pieces of information such as the agent 1 A segment typically consists of one or two sentences", "who location where time when and theme what which may be spread all over the text", "Counting on a means of discerning the events the individuals taking part in each of them and their role is crucial to determine the semantic equivalence between a reference document and a candidate translation", "Moreover the discourse analysis of a document is not a mere concatenation of the analyses of its individual sentences", "There are some phenomena which may go beyond the scope of a sentence and can only be explained within the context of the whole document", "For instance in a newspaper article facts and entities are progressively added to the discourse and then referred to anaphorically later on", "The following extract from the development set illustrates the importance of such a phenomenon in the discourse analysis Among the current or underlying crises in the Middle East Rod Larsen mentioned the ArabIsraeli conflict and the Iranian nuclear portfolio as well as the crisis between Lebanon and Syria", "He stated All this leads us back to crucial values and opinions which render the situation prone at any moment to getting out of control more so than it was in past days", "The subject pronoun he works as an anaphoric pronoun whose antecedent is the proper noun Rod Larson", "The anaphoric relation established between these two elements can only be identified by analyzing the text as a whole thus considering the gender agreement between the third person singular masculine subject pronoun he and the masculine proper noun Rod Larson", "However if the two sentences were analyzed separately the identification of this anaphoric relation would not be feasible due to the lack of connection between the two elements", "Discourse representations allow us to trace links across sentences between the different facts and entities appearing in them", "Therefore providing an approach to the text more similar to that of a human which implies taking into account the whole text structure instead of considering each sentence separately", "The rest of the paper is organized as follows", "Section 2 describes our evaluation methods and the linguistic theory upon which they are based", "Experimental results are reported and discussed in Section 3", "Section 4 presents the metric submitted to the evaluation challenge", "Future work is outlined in Section 5", "As an additional result documentlevel metrics generated in this study have been incorporated to the IQMT package for automatic MT evaluation2 "]}, "W10-1757": {"title": ["Nbest Reranking by Multitask Learning"], "abstract": ["We propose a new framework for Nbest reranking on sparse feature sets", "The idea is to reformulate the reranking problem as a Multitask Learning problem where each Nbest list corresponds to a distinct task", "This is motivated by the observation that Nbest lists often show significant differences in feature distributions", "Training a single reranker directly on this heteroge nous data can be difficult", "Our proposed metaalgorithm solves this challenge by using multitask learning such as 12 regularization to discover common feature representations across N best lists", "This metaalgorithm is simple to implement and its modular approach allows one to plugin different learning algorithms from existing literature", "As a proof of concept we show statistically significant improvements on a machine translation system involving millions of features"], "inroduction": ["Many natural language processing applications such as machine translation MT parsing and language modeling benefit from the Nbest reranking framework Shen et al 2004 Collins and Koo 2005 Roark et al 2007", "The advantage of Nbest reranking is that it abstracts away the complexities of firstpass decoding allowing the researcher to try new features and learning algorithms with fast experimental turnover", "In the Nbest reranking scenario the training data consists of sets of hypotheses ie Nbest lists generated by a firstpass system along with their labels", "Given a new Nbest list the goal is to rerank it such that the best hypothesis appears near the top of the list", "Existing research have focused on training a single reranker directly on the entire data", "This approach is reasonable if the data is homogenous but it fails when features vary significantly across different Nbest lists", "In particular when one employs sparse feature sets one seldom finds features that are simultaneously active on multiple Nbest lists", "In this case we believe it is more advantageous to view the Nbest reranking problem as a multi task learning problem where each Nbest list corresponds to a distinct task", "Multitask learning a subfield of machine learning focuses on how to effectively train on a set of different but related datasets tasks", "Our heterogenous Nbest list data fits nicely with this assumption", "The contribution of this work is threefold 1", "We introduce the idea of viewing Nbest", "reranking as a multitask learning problem", "This view is particularly apt to any general reranking problem with sparse feature sets"]}, "W10-1761": {"title": ["Improved Translation with Source Syntax Labels"], "abstract": ["We present a new translation model that include undecorated hierarchicalstyle phrase rules decorated sourcesyntax rules and partially decorated rules", "Results show an increase in translation performance of up to 08 BLEU for GermanEnglish translation when trained on the newscommentary corpus using syntactic annotation from a source language parser", "We also experimented with annotation from shallow taggers and found this increased performance by 05 BLEU"], "inroduction": ["Hierarchical decoding is usually described as a formally syntactic model without linguistic commitments in contrast with syntactic decoding which constrains rules and production with linguistically motivated labels", "However the decoding mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar", "Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field", "Both use synchronous context free grammar SCFG formalism consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence", "The most common algorithm for decoding with SCFG is currently CKY with cube pruning works for both hierarchical and syntactic systems as implemented in Hiero Chiang 2005 Joshua Li et al 2009 and Moses Hoang et al 2009 Rewrite rules in hierarchical systems have general applicability as their nonterminals are undec orated giving hierarchical system broad coverage", "However rules may be used in inappropriate situations without the labeled constraints", "The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses", "Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage", "This paper presents work on combining hierarchical and syntax translation utilizing the high coverage of hierarchical decoding and the insights that syntactic information can bring", "We seek to balance the generality of using undeco rated nonterminals with the specificity of labeled nonterminals", "Specifically we will use syntactic labels from a source language parser to label nonterminal in production rules", "However other source span information such as chunk tags can also be used", "We investigate two methods for combining the hierarchical and syntactic approach", "In the first method syntactic translation rules are used concurrently with a hierarchical phrase rules", "Each ruleset is trained independently and used concurrently to decode sentences", "However results for this method do not improve", "The second method uses one translation model containing both hierarchical and syntactic rules", "Moreover an individual rule can contain both decorated syntactic nonterminals and undeco rated hierarchicalstyle nonterminals also the lefthandside nonterminal may or may not be decorated", "This results in a 08 improvement over the hierarchical baseline and analysis suggest that longrange ordering has been improved", "We then applied the same methods but using linguistic annotation from a chunk tagger Abney 1991 instead of a parser and obtained an improvement of 05 BLEU over the hierarchical baseline showing that gains with additional source side annotation can be obtained with simpler tools"]}, "W10-3909": {"title": ["Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX 2010 pages 6068"], "abstract": ["Semantic information is a very important factor in coreference resolution", "The combination of large corpora and deep analysis procedures has made it possible to acquire a range of semantic information and apply it to this task", "In this paper we generate two statisticallybased semantic features from a large corpus and measure their influence on pronoun coreference", "One is contextual compatibility which decides if the antecedent can be used in the anaphors context the other is role pair which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity", "We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model", "These features produce an absolute gain of 15 to 17 in resolution accuracy a 6 reduction in errors", "To understand the limitations of these features we also extract patterns from the test corpus use these patterns to train a coreference model and examine some of the cases where coreference still fails", "We also compare the performance of patterns extracted from semantic role labeling and syntax"], "inroduction": ["Coreference resolution is the task of determining whether two phrases refer to the same entity", "Coreference is critical to most NLP tasks yet even the subproblem of pronoun coreference remains very challenging", "In principle we need several types of information to identify the right antecedent", "First number and gender agreement constraints can narrow the candidate set", "If multiple candidates remain we would next use some sequence or syntactic features like position word word salience and discourse focus", "For example whether an antecedent is in subject position might be helpful because the subject is more likely to be referred to or an entity that has been referred to repeatedly is more likely to be referred to again", "However these features do not suffice to pick the correct antecedent and sometimes similar syntactic structures might have quite different coreference solutions", "For example for the following two sentences 1 The terrorist shot a 13yearold boy he was arrested after the attack", "2 The terrorist shot a 13yearold boy he was fatally wounded in the attack", "it is likely that he refers to terrorist in 1 and boy in 2", "However we cannot get the right antecedent using the features we mentioned above because the examples share the same antecedent words and syntactic structure", "People can still resolve these correctly because terrorist is more likely to be arrested than boy and because the one shooting is more likely to be arrested than the one being shot", "In such cases semantic constraints and preferences are required for correct coreference resolution", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era NLPIX 2010 pages 6068 Beijing August 2010 recent work on anaphora resolution", "Dagan and Itai 1990 Bean and Riloff 2004 Yang and Su 2007 and Ponzetto and Strube 2006 all explored this task", "However this task is difficult because it requires the acquisition of a large amount of semantic information", "Furthermore there is not universal agreement on the value of these semantic preferences for pronoun coreference", "Kehler et al", "2004 reported that such information did not produce apparent improvement in overall pronoun resolution", "In this paper we will extract semantic features from a semantic role labeling system instead of a parse tree and explore whether pronoun coreference resolution can benefit from such knowledge which is automatically extracted from a large corpus", "We studied two features the contextual compatibility feature which has been demonstrated to work at the syntactic level by previous work and the role pair feature which has not previously been applied to general domain pronoun coreference", "In addition to obtain a rough upper bound on the benefits of our approach and understand its limitations we conducted a second experiment in which the semantic knowledge is extracted from the evaluation corpus", "We will use the term mention to describe an individual referring phrase", "For most studies of coreference mentions are noun phrases and may be headed by a name a common noun or a pronoun", "We will use the term entity to refer to a set of coreferential mentions"]}, "W10-4128": {"title": ["HMM Revises Low Marginal Probability by CRF for Chinese Word Segmentation"], "abstract": ["This paper presents a Chinese word segmentation system for CIPSSIGHAN 2010 Chinese language processing task", "Firstly based on Conditional Random Field CRF model with local features and global features the characterbased tagging model is designed", "Secondly Hidden Markov Models HMM is used to revise the substrings with low marginal probability by CRF", "Finally confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers", "As is well known that characterbased approach has outstanding capability of discovering outofvocabulary OOV word but external information of word lost", "HMM makes use of word information to increase invocabulary IV recall", "We participate in the simplified Chinese word segmentation both closed and open test on all four corpora which belong to different domains", "Our system achieves better performance"], "inroduction": ["Chinese Word Segmentation CWS has witnessed a prominent progress in the first four SIGHAN Bakeoffs", "Since Xue 2003 used characterbased tagging this method has attracted more and more attention", "Some previous work Peng et al 2004 Tseng et al 2005 Low et al 2005 illustrated the effectiveness of using characters as tagging units while literatures Zhang et al 2006 Zhao and Kit 2007a Zhang and Clark 2007 focus on employing lexical words or subwords as tagging units", "Because the wordbased models can capture the wordlevel contextual information and IV knowledge", "Besides many strategies are proposed to balance the IV and OOV performance Wang et al 2008", "CRF has been widely used in sequence labeling tasks and has a good performance Laffertyet al 2001", "Zhao and Kit 2007b 2008 at tempt to integrate global information with local information to further improve CRFbased tagging method of CWS which provides a solid foundation for strengthening CRF learning with unsupervised learning outcomes", "In order to increase the accuracy of tagging using CRF we adopt the strategy which is if the marginal probability of characters is lower than a threshold the modified component based on HMM will be trigged combining the confidence measure the results will be regenerated"]}, "W10-4135": {"title": ["High OOVRecall Chinese Word Segmenter Xiaoming Xu Muhua Zhu Xiaoxu Fei and Jingbo Zhu School of Information Science and Engineering Northeastern University xuxm zhumh feixxicsneueducn zhujingbomailneueducn Abstract For the competition of Chinese word segmentation held in the first CIPSSIGHNA joint conference We applied a subwordbased word segmenter using CRFs and extended the segmenter with OOV words recognized by Accessor Variety Moreover we proposed several postprocessing rules to improve the performance Our system achieved promising OOV recall among all the participants 1 Introduction Chinese word segmentation is deemed to be a prerequisite for Chinese language processing The competition in the first CIPSSIGHAN joint con ference put the task of Chinese word segmentation in a more challengeable setting where training and test data are obtained from different domains This setting is widely known as domain adaptation For domain adaptation either a largescale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain In this word segmentation competition unfortunately only a small size of unlabeled target domain data is available Thus we focus on handling outofvocabulary OOV words For this purpose our system is based on a combina tion of subword based tagging method Zhang et al 2006 and accessor varietybased new word recognition method Feng et al 2004 In more detail we adopted and extended subwordbased method Subword list is augmented with newword list recognized by accessor variety method Feature Template Description a cn 2 1 0 1 2 unigram of characters b cncn1 2 1 0 1 bigram of characters c cn  1cncn1 1 0 1 trigram of characters d PuC0 whether punctuation e T C 1T C0T C1 type of characters Table 1 Basic Features for CRFbased Segmenter We participated in the close track of the word segmentation competition on all the four test datasets in two of which our system is ranked at the 1st position with respect to the metric of OOV"], "abstract": ["For the competition of Chinese word segmentation held in the first CIPSSIGHNA joint conference", "We applied a subword based word segmenter using CRFs and extended the segmenter with OOV words recognized by Accessor Variety", "Moreover we proposed several postprocessing rules to improve the performance", "Our system achieved promising OOV recall among all the participants"], "inroduction": ["Chinese word segmentation is deemed to be a prerequisite for Chinese language processing", "The competition in the first CIPSSIGHAN joint conference put the task of Chinese word segmentation in a more challengeable setting where training and test data are obtained from different domains", "This setting is widely known as domain adaptation", "For domain adaptation either a largescale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain", "In this word segmentation competition unfortunately only a small size of unlabeled target domain data is available", "Thus we focus on handling outofvocabulary OOV words", "For this purpose our system is based on a combination of subwordbased tagging method Zhang et al 2006 and accessor varietybased new word recognition method Feng et al 2004", "In more detail we adopted and extended subwordbased method", "Subword list is augmented with new word list recognized by accessor variety method", "Feature Template Description a cn 2 1 0 1 2 unigram of characters b cn cn1 2 1 0 1 bigram of characters c cn1 cn cn1 1 0 1 trigram of characters d Pu C0  whether punctuation e T C1T C0 T C1 type of characters Table 1 Basic Features for CRFbased Segmenter We participated in the close track of the word segmentation competition on all the four test datasets in two of which our system is ranked at the 1st position with respect to the metric of OOV recall"]}, "W10-4138": {"title": ["Term Contributed Boundary Tagging by Conditional Random Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff TianJian Jiang ShihHung Liu ChengLung Sung WenLian Hsu  Department of Computer Science National TsingHua University Department of Electrical Engineering National Taiwan University Institute of Information Science Academia Sinica tmjiangjourneyclsunghsuiissinicaedutw"], "abstract": ["This paper presents a Chinese word segmentation system submitted to the closed training evaluations of CIPSSIGHAN2010 bakeoff", "The system uses a conditional random field model with one simple feature called term contributed boundaries TCB in addition to the BI characterbased tagging approach", "TCB can be extracted from unlabeled corpora automatically and segmentation variations of different domains are expected to be reflected implicitly", "The experiment result shows that TCB does improve BI tagging domain independently about 1 of the F1 measure score"], "inroduction": ["The CIPSSIGHAN2010 bakeoff task of Chinese word segmentation is focused on cross domain texts", "The design of data set is challenging particularly", "The domainspecific training corpora remain unlabeled and two of the test corpora keep domains unknown before releasing therefore it is not easy to apply ordinary machine learning approaches especially for the closed training evaluations"]}, "W10-4223": {"title": ["Paraphrase Generation as Monolingual  Translation  Data and Evaluation"], "abstract": ["In this paper we investigate the automatic generation and evaluation of sentential paraphrases", "We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard PhraseBased Machine Translation PBMT framework", "The output of this system is compared to a word substitution baseline", "Human judges prefer the PBMT paraphrasing system over the word substitution system", "We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence"], "inroduction": ["Texttotext generation is an increasingly studied subfield in natural language processing", "In contrast with the typical natural language generation paradigm of converting concepts to text in text totext generation a source text is converted into a target text that approximates the meaning of the source text", "Texttotext generation extends to such varied tasks as summarization Knight and Marcu 2002 questionanswering Lin and Pan tel 2001 machine translation and paraphrase generation", "Sentential paraphrase generation SPG is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence but approximates its meaning", "Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input for example to generate paraphrases of questions that in their original form cannot be answered Lin and Pantel 2001 Riezler et al 2007 or to generate paraphrases of sentences that failed to translate CallisonBurch et al 2006", "Paraphrasing has also been used in the evaluation of machine translation system output RussoLassner et al 2006 Kauchak and Barzilay 2006 Zhou et al 2006", "Adding certain constraints to paraphrasing allows for additional useful applications", "When a constraint is specified that a paraphrase should be shorter than the input text paraphrasing can be used for sentence compression Knight and Marcu 2002 Barzilay and Lee 2003 as well as for text simplification for question answering or subtitle generation Daelemans et al 2004", "We regard SPG as a monolingual machine translation task where the source and target languages are the same Quirk et al 2004", "However there are two problems that have to be dealt with to make this approach work namely obtaining a sufficient amount of examples and a proper evaluation methodology", "As CallisonBurch et al", "2008 argue automatic evaluation of paraphrasing is problematic", "The essence of SPG is to generate a sentence that is structurally different from the source", "Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity while paraphrasing centers around achieving dissimilarity", "Besides the evaluation issue another problem is that for an data driven MT account of paraphrasing to work a large collection of data is required", "In this case this would have to be pairs of sentences that are paraphrases of each other", "So far paraphrasing data sets of sufficient size have been mostly lacking", "We argue that the headlines aggregated by Google News offer an attractive avenue"]}, "W11-0301": {"title": ["Proceedings of the Fifteenth Conference on Computational Natural Language Learning pages 19 Portland Oregon USA 2324 June 2011 c  2011 Association for Computational Linguistics Modeling Syntactic Context Improves Morphological Segmentation Yoong Keok Lee Aria Haghighi Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology yklee aria42 reginacsailmitedu Abstract The connection between partofspeech POS categories and morphological properties is welldocumented in linguistics but underutilized in text processing systems This pa per proposes a novel model for morphological segmentation that is driven by this connection Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic 1 1 Introduction A tight connection between morphology and syntax is welldocumented in linguistic literature In many languages morphology plays a central role in marking syntactic structure while syntactic relations help to reduce morphological ambiguity Harley and Phillips 1994 Therefore in an unsupervised linguistic setting which is rife with ambiguity modeling this connection can be particularly beneficial However existing unsupervised morphological analyzers take little advantage of this linguistic property In fact most of them operate at the vocabulary level completely ignoring sentence con text This design is not surprising a typical morphological analyzer does not have access to syntac1The source code for the work presented in this paper is available at httpgroupscsailmitedurbgcodemorphsyn tic information because morphological segmentation precedes other forms of sentence analysis In this paper we demonstrate that morphological analysis can utilize this connection without assuming access to fullfledged syntactic information In particular we focus on two aspects of the morphosyntactic connection  Morphological consistency within POS categories Words within the same syntactic category tend to select similar affixes This linguistic property significantly reduces the space of possible morphological analyses ruling out assignments that are incompatible with a syntactic category"], "abstract": ["The connection between partofspeech POS categories and morphological properties is welldocumented in linguistics but underutilized in text processing systems", "This paper proposes a novel model for morphological segmentation that is driven by this connection", "Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words", "Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic", "1"], "inroduction": ["A tight connection between morphology and syntax is welldocumented in linguistic literature", "In many languages morphology plays a central role in marking syntactic structure while syntactic relations help to reduce morphological ambiguity Harley and Phillips 1994", "Therefore in an unsupervised linguistic setting which is rife with ambiguity modeling this connection can be particularly beneficial", "However existing unsupervised morphological analyzers take little advantage of this linguistic property", "In fact most of them operate at the vocabulary level completely ignoring sentence context", "This design is not surprising a typical morphological analyzer does not have access to syntac 1 The source code for the work presented in this paper is available at httpgroupscsailmitedurbgcodemorphsyn", "tic information because morphological segmentation precedes other forms of sentence analysis", "In this paper we demonstrate that morphological analysis can utilize this connection without assuming access to fullfledged syntactic information", "In particular we focus on two aspects of the morpho syntactic connection  Morphological consistency within POS categories", "Words within the same syntactic category tend to select similar affixes", "This linguistic property significantly reduces the space of possible morphological analyses ruling out assignments that are incompatible with a syntactic category", " Morphological realization of grammatical agreement", "In many morphologically rich languages agreement between syntactic dependents is expressed via correlated morphological markers", "For instance in Semitic languages gender and number agreement between nouns and adjectives is expressed using matching suffixes", "Enforcing mutually consistent segmentations can greatly reduce ambiguity of word level analysis", "In both cases we do not assume that the relevant syntactic information is provided but instead jointly induce it as part of morphological analysis", "We capture morphosyntactic relations in a Bayesian model that grounds intraword decisions in sentencelevel context", "Like traditional unsupervised models we generate morphological structure from a latent lexicon of prefixes stems and suffixes", "1 Proceedings of the Fifteenth Conference on Computational Natural Language Learning pages 19 Portland Oregon USA 2324 June 2011", "Qc 2011 Association for Computational Linguistics In addition morphological analysis is guided by a latent variable that clusters together words with similar affixes acting as a proxy for POS tags", "Moreover a sequencelevel component further refines the analysis by correlating segmentation decisions between adjacent words that exhibit morphological agreement", "We encourage this behavior by encoding a transition distribution over adjacent words using string match cues as a proxy for grammatical agreement", "We evaluate our model on the standard Arabic treebank", "Our full model yields 862 accuracy outperforming the best published results Poon et al 2009 by 85", "We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories", "Overall our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis"]}, "W11-0311": {"title": ["Improving the Impact of Subjectivity Word Sense Disambiguation  on"], "abstract": ["Subjectivity word sense disambiguation SWSD is automatically determining which word instances in a corpus are being used with subjective senses and which are being used with objective senses", "SWSD has been shown to improve the performance of contextual opinion analysis but only on a small scale and using manually developed integration rules", "In this paper we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance by successfully gathering data annotated by nonexpert annotators", "Further by improving the method for integrating SWSD into contextual opinion analysis even greater benefits from SWSD are achieved than in previous work", "We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis"], "inroduction": ["Often methods for opinion sentiment and subjectivity analysis rely on lexicons of subjective opinioncarrying words eg Turney 2002 Whitelaw et al 2005 Riloff and Wiebe 2003 Yu and Hatzivassiloglou 2003 Kim and Hovy 2004 Bloom et al 2007 Andreevskaia and Bergler 2008 Agarwal et al 2009", "Examples of such words are the following in bold 1 He is a disease to every team he has gone to", "Converting to SMF is a headache", "The concert left me cold", "That guy is such a pain", "However even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity Su and Markert 2008 Gyamfi et al 2009", "That is many clues in these lexicons have both subjective and objective senses", "This ambiguity leads to errors in opinion and sentiment analysis because objective instances represent false hits of subjectivity clues", "For example the following sentence contains the keywords from 1 used with objective senses 2 Early symptoms of the disease include severe headaches red eyes fevers and cold chills body pain and vomiting", "Recently in Akkaya et al 2009 we introduced the task of subjectivity word sense disambiguation SWSD which is to automatically determine which word instances in a corpus are being used with subjective senses and which are being used with objective senses", "We developed a supervised system for SWSD and exploited the SWSD output to improve the performance of multiple contextual opinion analysis tasks", "Although the reported results are promising there are three obvious shortcomings", "First we were able to apply SWSD to contextual opinion analysis only on a very small scale due to a shortage of annotated data", "While the experiments show that SWSD improves contextual opinion analysis this was only on the small amount of opinionannotated data that was in the coverage of our system", "Two questions arise is it feasible to obtain greater amounts of the needed data and do SWSD performance improvements on contextual opinion analysis hold on a 87 Proceedings of the Fifteenth Conference on Computational Natural Language Learning pages 8796 Portland Oregon USA 2324 June 2011", "Qc 2011 Association for Computational Linguistics larger scale", "Second the annotations in Akkaya et al 2009 are piggybacked on SENSEVAL sense tagged data which are finegrained word sense annotations created by trained annotators", "A concern is that SWSD performance improvements on contextual opinion analysis can only be achieved using such finegrained expert annotations the availability of which is limited", "Third Akkaya et al 2009 uses manual rules to apply SWSD to contextual opinion analysis", "Although these rules have the advantage that they transparently show the effects of SWSD they are somewhat ad hoc", "Likely they are not optimal and are holding back the potential of SWSD to improve contextual opinion analysis", "To address these shortcomings in this paper we investigate 1 the feasibility of obtaining a substantial amount of annotated data 2 whether performance improvements on contextual opinion analysis can be realized on a larger scale and 3 whether those improvements can be realized with subjectivity sense tagged data that is not built on expert full inventory sense annotations", "In addition we explore better methods for applying SWSD to contextual opinion analysis"]}, "W11-0910": {"title": ["Incorporating Coercive Constructions into a Verb Lexicon"], "abstract": ["We take the first steps towards augmenting a lexical resource VerbNet with probabilistic information about coercive constructions", "We focus on CAUSED MOTION as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion which occurs productively and adds substantially to the relational semantics of a verb", "However through annotation we find that VerbNet fails to accurately capture all usages of the construction", "We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet", "We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages"], "inroduction": ["Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles eg see Mrquez et al 2008", "Underlying these recent successes are lexical resources such as PropBank Palmer et al 2005 VerbNet Kipper et al 2008 and FrameNet Baker et al 1998 Fillmore et al 2002 which encode the relational semantics of numerous lexical items especially verbs", "However because authors and speakers use verbs productively in previously unseen ways semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources cf", "Pustejovsky  Jezek 2008", "To achieve more accurate semantic analyses we must augment such resources with knowledge of the extensibility of verbs", "Central to verb extensibility is the process of semantic and syntactic coercion", "Coercion allows a verb to be used in atypical contexts that extend its relational semantics thereby enabling expression of a novel concept or simply more fluid expression of a complex concept", "For example consider a strictly intransitive action verb such as blink", "This verb may instead be used in a construction with an object as in She blinked the snow off her lashes leading to an interpretation of the verb in which the object is causally affected and changes location the CAUSED MOTION construction Goldberg 1995", "This type of constructional coercion is common in language and underlies much extensibility of verb usages", "Understanding such coercive processes thus has significant impact on how we should represent knowledge about verbs in a lexical resource", "Importantly constructional coercion is not an allor nothing process  a word must be semantically and syntactically compatible in some respects with a context in order for its use to be extended to that context but the restrictions on compatibility are not hardandfast rules Langacker 1987 Kay  Fillmore 1999 Goldberg 2006 Goldberg to appear", "Gradience of compatibility plays an important role in coercion suggesting that a probabilistic approach may be necessary for encoding knowledge of constructional coercion in a verb lexicon cf", "Lapata  Lascarides 2003", "Our hypothesis here is that due to this gradient process of productivity existing verb lexicons do not adequately capture the actual patterns of use of extensible constructions", "In this paper we focus on the CAUSED MOTION CM construction as an initial test case", "We first annotate the classes of an extensive verb lexicon VerbNet as to whether the CM construction is allowed for all some or none of the verbs in the class noting additionally whether it is a typical or coerced usage", "We find that many of the classes that allow the construction for at least some verbs do not include the CM frame in their definition indicating a significant shortcoming in the relational knowledge encoded in the lexicon", "Next we develop probabilistic measures for determining to what degree a class is likely to admit the CM construction", "We then test our measures over corpus data manually annotated for use of the CM construction", "Finally we present preliminary work on automatic techniques for calculating the proposed measures in an unsupervised way to avoid the need for expensive manual annotation", "This work forms the preliminary steps toward empirically augmenting VerbNets predictive capabilities concerning the event semantics of verbs in coercible constructions"]}, "W11-1101": {"title": ["A Combination of Topic Models with Maxmargin Learning for Relation"], "abstract": ["This paper proposes a novel application of a supervised topic model to do entity relation detection ERD", "We adapt Maximum Entropy Discriminant Latent Dirichlet Allocation MEDLDA with mixed membership for relation detection", "The ERD task is reformulated to fit into the topic modeling framework", "Our approach combines the benefits of both maximumlikelihood estimation MLE and maxmargin estimation MME and the mixed membership formulation enables the system to incorporate heterogeneous features", "We incorporate different features into the system and perform experiments on the ACE 2005 corpus", "Our approach achieves better overall performance for precision recall and Fmeasure metrics as compared to SVMbased and LLDAbased models"], "inroduction": ["Entity relation detection ERD aims at finding relations between pairs of Named Entities NEs in text", "Availability of annotated corpora NIST 2003 Doddington et al 2004 and introduction of shared tasks eg", "Farkas et al 2010 Carreras and Marquez 2005 has spurred a large amount of research in this field in recent times", "Researchers have used supervised and semisupervised approaches Hasegawa et al 2004 Mintz et al 2009 Jiang 2009 and explored rich features Kambhatla 2004 kernel design Culotta and Sorensen 2004 Zhou et al 2005 Bunescu and Mooney 2005 Qian et al 2008 and inference algorithms Chan and Roth 2011 to detect predefined relations between NEs", "In this work we explore if and how the latent semantics of the text can help in detecting entity relations", "For this we adapt the Latent Dirichlet Allocation LDA approach to solve the ERD task", "Specifically we present a ERD system based on Maximum Entropy Discriminant Latent Dirichlet Allocation MEDLDA", "MEDLDA Zhu et al 2009 is an extension of Latent Dirichlet Allocation LDA that combines capability of capturing latent semantics with the discriminative capabilities of SVM", "There are a number of challenges in employing the LDA framework for ERD", "Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA LLDA Ramage et al 2009 and supervised LDA sLDA Blei and McAuliffe 2008 are powerful generative models that capture the underlying semantics of texts", "However they have trouble discovering marginal classes and easily employing rich feature sets both of which are important for ERD", "We overcome the first drawback by employing a MEDLDA framework which integrates maximum likelihood estimation MLE and maximum margin estimation MME", "Specifically it is a combination of sLDA and support vector machines SVMs", "Further in order to employ rich and heterogeneous features we introduce a separate exponential family distribution for each feature similar to Shan et al 2009 into our MEDLDA model", "We formulate the relation detection task within the topic model framework as follows", "Pairs of NE mentions1 and the text between them is considered 1 Adopting the terminology used in the Automatic Context Extraction ACE program NIST 2003 specific NE instances are called mentions", "as minidocument", "Each minidocument has a relation type analogous to the response variable in the supervised topic model", "The topic model infers the topic relation type distribution of the mini documents", "The supervised topic model discovers a latent topic representation of the minidocuments and a response parameter distribution", "The topic representation is discovered with observed response variables during training", "During testing the topic distribution of each minidocument can form a prediction of the relation types", "We carry out experiments to measure the effectiveness of our approach and compare it to SVM based and LLDAbased models as well as to a previous work using the same corpora", "We also measure and analyze the effectiveness of incorporating different features in our model relative to other models", "Our approach exhibits better overall precision recall and Fmeasure than baseline systems", "We also find that the MEDLDAbased approach shows consistent capability for incorporation and improvement due to a variety of heterogeneous features", "The rest of the paper is organized as follows", "We describe the proposed model in Section 2 and the features that we explore in this work in Section 3", "Section 4 describes the data experiments results and analyses", "We discuss the related work in Section 5 before concluding in Section 6"]}, "W11-1215": {"title": ["Crosslingual Slot Filling from Comparable Corpora Matthew Snover Xiang Li WenPin Lin Zheng Chen Suzanne Tamang Mingmin Ge Adam Lee Qi Li Hao Li Sam Anzaroot Heng Ji Computer Science Department Queens College and Graduate Center City University of New York New York NY 11367 USA msnoverqccunyedu hengjicsqccunyedu Abstract This paper introduces a new task of crosslingual slot filling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language It is a very challenging task which suffers from both information extraction and machine translation errors In this paper we analyze the types of errors produced by five different baseline approaches and present a novel supervised rescoring based valida tion approach to incorporate global evidence from very large bilingual comparable corpora Without using any additional labeled data this new approach obtained 385 relative improvement in Precision and 867 relative improvement in Recall over several stateoftheart approaches The ultimate system outperformed monolingual slot filling pipelines built on much larger monolingual corpora 1 Introduction The slot filling task at NIST TAC Knowledge Base Population KBP track Ji et al 2010 is a relatively new and popular task with the goal of automatically building profiles of entities from large amounts of unstructured data and using these profiles to populate an existing knowledge base These profiles consist of numerous slots such as  title   parents for persons and  topemployees for organizations A variety of approaches have been proposed to address both tasks with considerable success nevertheless all of the KBP tasks so far have been limited to monolingual processing However as the shrinking fraction of the world s Web pages are written in English many slot fills can only be discovered from comparable documents in foreign languages By comparable corpora we mean texts that are about similar topics but are not in general translations of each other These corpora are naturally available for ex ample many news agencies release multilingual news articles on the same day In this paper we propose a new and more challenging crosslingual slot filling task to find information for any English query from crosslingual comparable corpora and then present its profile in English We developed complementary baseline ap proaches which combine two difficult problems information extraction IE and machine trans lation MT In this paper we conduct detailed error analysis to understand how we can exploit comparable corpora to construct more complete and accurate profiles Many correct answers extracted from our baselines will be reported multiple times in any external large collection of comparable docu ments We can thus take advantage of such information redundancy to rescore candidate answers To choose the best answers we consult large comparable corpora and corresponding IE results We prefer those answers which frequently appear together with the query in certain IE contexts including co occurring names coreference links relations and events For ex ample we prefer  South Korea instead of New York Stock Exchange  as the peremployee of  answer for  Roh Moohyun using global evidence from employment relation extraction Such global knowledge from comparable corpora 110 Proceedings of the 4th Workshop on Building and Using Comparable Corpora pages 110119 49th Annual Meeting of the Association for Computational Linguistics Portland Oregon 24 June 2011 c  2011 Association for Computational Linguistics provides substantial improvement over each individual baseline system and even stateoftheart monolingual slot filling systems Compared to previous methods of exploiting comparable corpora our approach is novel in multiple aspects because it exploits knowledge from 1 both local and global statistics 2 both languages and 3 both shallow and deep analysis 2 Related Work Sudo et al 2004 found that for a crosslingual singledocument IE task source language extraction and fact translation performed notably better than machine translation and target language extraction We observed the same results In addition we also demonstrate that these two approaches are complementary and can be used to boost each others results in a statistical rescoring model with global evidence from large comparable corpora HakkaniTur et al 2007 described a filtering mechanism using two crosslingual IE systems for improving crosslingual document retrieval Many previous validation methods for crosslingual QA such as those organized by Cross Language Evaluation Forum Vallin et al 2005 focused on local information which involves only the query and answer eg Kwork and Deng 2006 keyword translation eg Mitamura et al 2006 and surface patterns eg Soubbotin and Soubbotin 2001 Some global valida tion approaches considered information redundancy based on shallow statistics including co occurrence density score and mutual information Clarke et al 2001 Magnini et al 2001 Lee et al 2008 deeper knowledge from dependency parsing eg Shen et al 2006 or logic reasoning eg Harabagiu et al 2005 How ever all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation Several recent IE studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output Downey et al 2005 Yangarber 2006 Patwardhan and Riloff 2009 Ji and Grishman 2008 Some recent research used comparable corpora to rescore name transliterations Sproat et al 2006 Klementiev and Roth 2006 or mine new word translations Fung and Yee 1998 Rapp 1999 Shao and Ng 2004 Tao and Zhai 2005 Hassan et al 2007 Udupa et al 2009 Ji 2009 To the best of our knowledge this is the first work on mining facts from comparable corpora for answer validation in a new crosslingual entity profiling task 3 Experimental Setup"], "abstract": ["This paper introduces a new task of crosslingual slot lling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language", "It is a very challenging task which suers from both information extraction and machine translation errors", "In this paper we analyze the types of errors produced by ve dierent baseline approaches and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora", "Without using any additional labeled data this new approach obtained 385 relative improvement in Precision and 867 relative improvement in Recall over several stateoftheart approaches", "The ultimate system outperformed monolingual slot lling pipelines built on much larger monolingual corpora"], "inroduction": ["The slot lling task at NIST TAC Knowledge Base Population KBP track Ji et al 2010 is a relatively new and popular task with the goal of automatically building proles of entities from large amounts of unstructured data and using these proles to populate an existing knowledge base", "These proles consist of numerous slots such as title parents for persons and topemployees for organizations", "A variety of approaches have been proposed to address both tasks with considerable success nevertheless all of the KBP tasks so far have been limited to monolingual processing", "However as the shrinking fraction of the worlds Web pages are written in English many slot lls can only be discovered from comparable documents in foreign languages", "By comparable corpora we mean texts that are about similar topics but are not in general translations of each other", "These corpora are naturally available for example many news agencies release multilingual news articles on the same day", "In this paper we propose a new and more challenging crosslingual slot lling task to nd information for any English query from crosslingual comparable corpora and then present its prole in English", "We developed complementary baseline approaches which combine two dicult problems information extraction IE and machine translation MT", "In this paper we conduct detailed error analysis to understand how we can exploit comparable corpora to construct more complete and accurate proles", "Many correct answers extracted from our baselines will be reported multiple times in any external large collection of comparable documents", "We can thus take advantage of such information redundancy to rescore candidate answers", "To choose the best answers we consult large comparable corpora and corresponding IE results", "We prefer those answers which frequently appear together with the query in certain IE contexts including cooccurring names coreference links relations and events", "For example we prefer South Korea instead of New York Stock Exchange as the peremployee of  answer for Roh Moohyun using global evidence from employment relation extraction", "Such global knowledge from comparable corpora 110 Proceedings of the 4th Workshop on Building and Using Comparable Corpora pages 110119 49th Annual Meeting of the Association for Computational Linguistics Portland Oregon 24 June 2011", "Qc 2011 Association for Computational Linguistics provides substantial improvement over each individual baseline system and even stateofthe art monolingual slot lling systems", "Compared to previous methods of exploiting comparable corpora our approach is novel in multiple aspects because it exploits knowledge from 1 both local and global statistics 2 both languages and 3 both shallow and deep analysis"]}, "W11-1604": {"title": ["Comparing  Phrasebased and Syntaxbased Paraphrase Generation"], "abstract": ["Paraphrase generation can be regarded as machine translation where source and target language are the same", "We use the Moses statistical machine translation toolkit for paraphrasing comparing phrasebased to syntaxbased approaches", "Data is derived from a recently released large scale 21M tokens paraphrase corpus for Dutch", "Preliminary results indicate that the phrasebased approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source"], "inroduction": ["One of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms", "As the ability to deal with paraphrases holds great potential for improving the coverage of NLP systems a substantial body of research addressing recognition extraction and generation of paraphrases has emerged Androutsopoulos and Malakasiotis 2010 Madnani and Dorr 2010", "Paraphrase Generation can be regarded as a translation task in which source and target language are the same", "Both Paraphrase Generation and Machine Translation MT are instances of TextToText Generation which involves transforming one text into another obeying certain restrictions", "Here these restrictions are that the generated text must be grammatically wellformed and semanticallytranslationally equivalent to the source text", "Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree", "The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation", "One popular approach  arguably the most successful so far  is Statistical Phrase based Machine Translation PBMT which learns phrase translation rules from aligned bilingual text corpora Och et al 1999 Vogel et al 2000 Zens et al 2002 Koehn et al 2003", "Prior work has explored the use of PBMT for paraphrase generation Quirk et al 2004 Bannard and CallisonBurch 2005 Mad nani et al 2007 CallisonBurch 2008 Zhao et al 2009 Wubben et al 2010 However since many researchers believe that PBMT has reached a performance ceiling ongoing research looks into more structural approaches to statistical MT Marcu and Wong 2002 Och and Ney 2004 Khalilov and Fonollosa 2009", "Syntax based MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases presupposing syntactic structures for source target or both languages", "Syntactic information might lead to better results in the area of grammatical wellformedness and unlike phrase based MT that uses contiguous ngrams syntax enables the modeling of longdistance translation patterns", "While the verdict on whether or not this approach leads to any significant performance gain is still out a similar line of reasoning would suggest that syntaxbased paraphrasing may offer similar advantages over phrasebased paraphrasing", "Considering the fact that the success of PBMT can partly be attributed to the abundance of large parallel corpora 27 Workshop on Monolingual TextToText Generation pages 2733 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics pages 2733 Portland Oregon 24 June 2011", "Oc 2011 Association for Computational Linguistics and that sufficiently large parallel corpora are still lacking for paraphrase generation using more linguistically motivated methods might prove beneficial for paraphrase generation", "At the same time automatic syntactic analysis introduces errors in the parse trees as no syntactic parser is perfect", "Likewise automatic alignment of syntactic phrases may be prone to errors", "The main contribution of this paper is a systematic comparison between phrasebased and syntaxbased paraphrase generation using an offtheshelf statistical machine translation SMT decoder namely Moses Koehn et al 2007 and the wordalignment tool GIZA Och and Ney 2003", "Training data derives from a new large scale 21M tokens paraphrase corpus for Dutch which has been recently released", "The paper is organized as follows", "Section 2 reviews the paraphrase corpus from which provides training and test data", "Next Section 3 describes the paraphrase generation methods and the experimental setup", "Results are presented in Section 4", "In Section 5 we discuss our findings and formulate our conclusions"]}, "W11-1815": {"title": ["BioNLP 2011 Task Bacteria Biotope  The Alvis system"], "abstract": ["This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope BB task of the BioNLP 2011 shared tasks", "Bacteria geographical locations and host entities were processed by a patternbased approach and domain lexical resources", "For the extraction of environment locations we propose a framework based on semantic analysis supported by an ontology of the biotope domain", "Domainspecific rules were developed for dealing with Bacteria anaphora", "Official results show that our Alvis system achieves the best performance of participating systems"], "inroduction": ["Given a set of Web pages the information extraction goal of the Bacteria Biotope BB task is to precisely identify bacteria and their locations and to relate them", "The type of the predicted locations has to be selected among eight types", "Among them the host and hostpart locations have to be related by the partof relation", "Three teams participated in the challenge", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges Kim et al 2010 and geographical locations Zhou et al 2005", "Locations include natural environments and hosts as well as food and medical locations", "In order to deal with this heterogeneity we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope BB terminoontology", "This mapping derives the type of location terms and filters out nonlocation terms", "Large external dictionaries of host names ie NCBI taxonomy and geographical names ie Agrovoc thesaurus complete the lexical resources", "The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty", "Our Alvis system implements an anaphora resolution algorithm that takes into consideration the anaphoric distance and the position of the antecedent in the sentence", "Alvis predicts the bacteria names and their relation to the locations with the help of handmade patterns based on linguistic analysis and lexical resources", "The methods for predicting and typing locations section 2 and bacteria section 3 are first described", "Section 4 details the method for relating them", "Section 5 comments the experimental results"]}, "W11-2135": {"title": ["Proceedings of the 6th Workshop on Statistical Machine Translation pages 309 315 Edinburgh Scotland UK July 30 31 2011 c  2011 Association for Computational Linguistics LIMSI  WMT11 Alexandre Allauzen He lene BonneauMaynard HaiSon Le Aurelien Max Guillaume Wisniewski Franc ois Yvon Univ ParisSud and LIMSICNRS"], "abstract": ["This paper describes LIMSIs submissions to the Sixth Workshop on Statistical Machine Translation", "We report results for the French English and GermanEnglish shared translation tasks in both directions", "Our systems use ncode an open source Statistical Machine Translation system based on bilingual ngrams", "For the FrenchEnglish task we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data", "In particular using a simple filtering strategy helped to improve both processing time and translation quality", "To translate from English to French and German we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10gram SOUL model", "We also briefly report experiments with several alternatives to the standard nbest MERT procedure leading to a significant speedup"], "inroduction": ["This paper describes LIMSIs submissions to the Sixth Workshop on Statistical Machine Translation where LIMSI participated in the FrenchEnglish and GermanEnglish tasks in both directions", "For this evaluation we used ncode our inhouse Statistical Machine Translation SMT system which is open source and based on bilingual ngrams", "This paper is organized as follows", "Section 2 provides an overview of ncode while the data pre processing and filtering steps are described in Section 3", "Given the large amount of parallel data avail able we proposed a method to filter the French English GigaWord corpus Section 32", "As in our previous participations data cleaning and filtering constitute a nonnegligible part of our work", "This includes detecting and discarding sentences in other languages removing sentences which are also included in the provided development sets as well as parts that are repeated for the monolingual news data this can reduce the amount of data by a factor 3 or 4 depending on the language and the year normalizing the character set nonutf8 characters which are aberrant in context or in the case of the GigaWord corpus a lot of nonprintable and thus invisible control characters such as EOT end of transmission1", "For target language modeling Section 4 a standard backoff ngram model is estimated and tuned as described in Section 41", "Moreover we also introduce in Section 42 the use of the SOUL language model LM Le et al 2011 in SMT", "Based on neural networks the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption up to 10gram in this work", "Finally experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates TER measured on the provided newstest2010 dataset"]}, "W11-2139": {"title": ["The CMUARK GermanEnglish Translation System"], "abstract": ["This paper describes the GermanEnglish translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation WMT11", "We present the results of several modeling and training improvements to our core hierarchical phrasebased translation system including feature engineering to improve modeling of the derivation structure of translations better handing of OOVs and using development set translations into other languages to create additional pseudo references for training"], "inroduction": ["We describe the GermanEnglish translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation WMT11 by the ARK research group at Carnegie Mellon University1 The core translation system is a hierarchical phrasebased machine translation system Chiang 2007 that has been extended in several ways described in this paper", "Some of our innovations focus on modeling", "Since German and English word orders can diverge considerably particularly in nonmatrix clauses we focused on feature engineering to improve the modeling of longdistance relationships which are poorly captured in standard hierarchical phrase based translation models", "To do so we developed features that assess the goodness of the source 1 httpwwwarkcscmuedu language parse tree under the translation grammar rather than of a linguistic grammar", "To train the feature weights we made use of a novel twophase training algorithm that incorporates a probabilistic training objective and standard minimum error training Och 2003", "These segmentation features were supplemented with a 7gram classbased language model which more directly models longdistance relationships", "Together these features provide a modest improvement over the baseline and suggest interesting directions for future work", "While our work on parse modeling was involved and required substantial changes to the training pipeline some other modeling enhancements were quite simple for example improving how outofvocabulary words are handled", "We propose a very simple change and show that it provides a small consistent gain", "On the training side we had two improvements over our baseline system", "First we were inspired by the work of Madnani 2010 who showed that when training to optimize BLEU Papineni et al 2002 overfitting is reduced by supplementing a single humangenerated reference translation with additional computergenerated references", "We generated supplementary pseudoreferences for our development set which is translated into many languages but once by using MT output from a secondary SpanishEnglish translation system", "Second following Foster and Kuhn 2009 we used a secondary development set to select from among many optimization runs which further improved generalization", "We largely sought techniques that did not require languagespecific resources eg treebanks POS 337 Proceedings of the 6th Workshop on Statistical Machine Translation pages 337343 Edinburgh Scotland UK July 3031 2011", "Qc 2011 Association for Computational Linguistics annotations morphological analyzers", "An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German", "Aside from this no further manually annotated data was used and we suspect many of the improvements described here can be had in other language pairs", "Despite avoiding languagespecific resources and using only the training data provided by the workshop an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rulebased systems that made use of languagespecific resources CallisonBurch et al 2011"]}, "W11-2145": {"title": ["Proceedings of the 6th Workshop on Statistical Machine Translation pages 379 385 Edinburgh Scotland UK July 30 31 2011 c  2011 Association for Computational Linguistics The Karlsruhe Institute of Technology Translation Systems for the WMT 2011 Teresa Herrmann Mohammed Mediani Jan Niehues and Alex Waibel Karlsruhe Institute of Technology Karlsruhe Germany firstnamelastnamekitedu Abstract This paper describes the phrasebased SMT systems developed for our participation in the WMT11 Shared Translation Task Translations for EnglishGerman and English French were generated using a phrasebased translation system which is extended by additional models such as bilingual and finegrained POS language models POSbased reordering lattice phrase extraction and discriminative word alignment Furthermore we present a special filtering method for the EnglishFrench Giga corpus and the phrase scoring step in the training is parallelized 1 Introduction In this paper we describe our systems for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation We participated in the Shared Translation Task and submitted translations for English German and English French We use a phrasebased decoder that can use lattices as input and developed several models that extend the standard loglinear model combination of phrasebased MT These include advanced reordering models and corresponding adaptations to the phrase extraction process as well as extension to the translation and language model in form of discriminative word alignment and a bilingual language model to extend source word context For EnglishGerman language models based on finegrained partofspeech tags were used to address the difficult target language generation due to the rich morphology of German We also present a filtering method directly addressing the problems of webcrawled corpora which enabled us to make use of the FrenchEnglish Giga corpus Another novelty in our systems this year is the parallel phrase scoring method that reduces the time needed for training which is especially convenient for such big corpora as the Giga corpus 2 System Description The baseline systems for all languages use a trans lation model that is trained on EPPS and the News Commentary corpus and the phrase table is based on a GIZA word alignment The language model was trained on the monolingual parts of the same corpora by the SRILM Toolkit Stolcke 2002 It is a 4gram SRI language model using KneserNey smoothing The problem of word reordering is addressed using the POSbased reordering model as described in Section 24 The partofspeech tags for the reordering model are obtained using the TreeTagger Schmid 1994 An inhouse phrasebased decoder Vogel 2003 is used to perform translation and optimization with regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al 2005 During decoding only the top 20 translation options for every source phrase were considered"], "abstract": ["This paper describes the phrasebased SMT systems developed for our participation in the WMT11 Shared Translation Task", "Translations for EnglishGerman and EnglishFrench were generated using a phrasebased translation system which is extended by additional models such as bilingual and finegrained POS language models POSbased reordering lattice phrase extraction and discriminative word alignment", "Furthermore we present a special filtering method for the EnglishFrench Giga corpus and the phrase scoring step in the training is parallelized"], "inroduction": ["In this paper we describe our systems for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation", "We participated in the Shared Translation Task and submitted translations for EnglishGerman and EnglishFrench", "We use a phrasebased decoder that can use lattices as input and developed several models that extend the standard loglinear model combination of phrasebased MT These include advanced reordering models and corresponding adaptations to the phrase extraction process as well as extension to the translation and language model in form of discriminative word alignment and a bilingual language model to extend source word context", "For EnglishGerman language models based on finegrained partofspeech tags were used to address the difficult target language generation due to the rich morphology of German", "We also present a filtering method directly addressing the problems of webcrawled corpora which enabled us to make use of the FrenchEnglish Giga corpus", "Another novelty in our systems this year is the parallel phrase scoring method that reduces the time needed for training which is especially convenient for such big corpora as the Giga corpus"]}, "W11-2147": {"title": ["Proceedings of the 6th Workshop on Statistical Machine Translation pages 393 398 Edinburgh Scotland UK July 30 31 2011 c  2011 Association for Computational Linguistics Experiments with word alignment normalization and clause reordering for SMT between English and German Maria Holmqvist Sara Stymne and Lars Ahrenberg Department of Computer and Information Science Linko ping University Sweden firstnamelastnameliuse Abstract This paper presents the LIU system for the WMT 2011 shared task for translation be tween German and English For English German we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrasebased word alignments For German English translation we tried to make the German text more similar to the English text by normalizing German morphology and performing rulebased clause reordering of the German text This resulted in small improvements for both translation directions 1 Introduction In this paper we present the LIU system for the WMT11 shared task for translation between English and German in both directions We added a number of features that address problems for translation between German and English such as word order differences incorrect alignment of certain words such as verbs and the morphological complexity of German compared to English as well as dealing with previously unseen words In both translation directions our systems include compound processing morphological sequence models and a hierarchical reordering model For German English translation we also added morphological normalization source side reordering and processing of outofvocabulary words OOVs For EnglishGerman translation we extracted word alignments with a supervised method and combined these alignments with Giza alignments in various ways to improve the phrase table We experimented with different ways of combining the two alignments such as using heuristic symmetrization and interpolating phrase tables Results are reported on three metrics BLEU Papineni et al 2002 NIST Doddington 2002 and Meteor ranking scores Agarwal and Lavie 2008 based on truecased output 2 Baseline System This years improvements were added to the LIU baseline system Stymne et al 2010 Our baseline is a factored phrase based SMT system that uses the Moses toolkit Koehn et al 2007 for translation model training and decoding GIZA Och and Ney 2003 for word alignment SRILM Stol cke 2002 an KenLM Heafield 2011 for language modelling and minimum error rate training Och 2003 to tune model feature weights In addition the LIU baseline contains  Compound processing including compound splitting and for translation into German also compound merging  Partofspeech and morphological sequence models All models were trained on truecased data Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training We created two language models The first model is a 5gram model that we created by interpolating two language 393 models from bilingual News Commentary and Europarl with more weight on the News Commentary model The second model is a 4gram model trained on monolingual News only All models were created using entropybased pruning with 108 as the threshold Due to time constraints all tuning and evaluation were performed on half of the provided shared task data Systems were tuned on 1262 sentences from newstest2009 and all results reported in Tables 1 and 2 are based on a devtest set of 1244 sentences from newstest2010"], "abstract": ["This paper presents the LIU system for the WMT 2011 shared task for translation between German and English", "For English German we attempted to improve the translation tables with a combination of standard statistical word alignments and phrasebased word alignments", "For GermanEnglish translation we tried to make the German text more similar to the English text by normalizing German morphology and performing rulebased clause reordering of the German text", "This resulted in small improvements for both translation directions"], "inroduction": ["In this paper we present the LIU system for the WMT11 shared task for translation between English and German in both directions", "We added a number of features that address problems for translation between German and English such as word order differences incorrect alignment of certain words such as verbs and the morphological complexity of German compared to English as well as dealing with previously unseen words", "In both translation directions our systems include compound processing morphological sequence models and a hierarchical reordering model", "For GermanEnglish translation we also added morphological normalization source side reordering and processing of outofvocabulary words OOVs", "For EnglishGerman translation we extracted word alignments with a supervised method and combined these alignments with Giza alignments in various ways to improve the phrase table", "We experimented with different ways of combining the two alignments such as using heuristic symmetrization and interpolating phrase tables", "Results are reported on three metrics BLEU Pa pineni et al 2002 NIST Doddington 2002 and Meteor ranking scores Agarwal and Lavie 2008 based on truecased output"]}, "W11-2206": {"title": ["Proceedings of EMNLP 2011 Conference on Empirical Methods in Natural Language Processing pages 43 52 Edinburgh Scotland UK July 27 31 2011 c  2011 Association for Computational Linguistics Unsupervised LanguageIndependent Name Translation Mining from Wikipedia Infoboxes WenPin Lin Matthew Snover Heng Ji Computer Science Department Queens College and Graduate Center City University of New York New York NY 11367 USA danniellingmailcom msnoverqccunyedu hengjicsqccunyedu Abstract The automatic generation of entity profiles from unstructured text such as Knowledge Base Population if applied in a multilingual setting generates the need to align such profiles from multiple languages in an unsupervised manner This paper describes an unsupervised and languageindependent approach to mine name translation pairs from entity profiles using Wikipedia Infoboxes as a standin for high quality entity profile extraction Pairs are initially found using expressions that are written in languageindependent forms such as dates and numbers and new translations are then mined from these pairs The algorithm then iteratively bootstraps from these translations to learn more pairs and more translations The algorithm maintains a high precision over 95 for the majority of its iterations with a slightly lower precision of 859 and an fscore of 76 A side effect of the name mining algorithm is the unsupervised creation of a translation lexicon between the two languages with an accuracy of 64 We also duplicate three stateoftheart name translation mining methods and use two ex isting name translation gazetteers to compare with our approach Comparisons show our approach can effectively augment the results from each of these alternative methods and resources 1 Introduction A shrinking fraction of the world s web pages are written in English while about 3000 languages are endangered Krauss 2007 Therefore the ability to access information across a range of languages especially lowdensity languages is becoming increasingly important for many applications In this paper we hypothesize that in order to extend crosslingual information access to all the language pairs on the earth or at least to some lowdensity languages which are lacking fundamental linguistic resources we can start from the much more scalable task of information translation or more specifically new name translation Wikipedia as a remarkable and rich online encyclopedia with a wealth of general knowledge about varied concepts entities events and facts in the world may be utilized to address this need As of March 2011 Wikipedia contains pages from 275 languages1 but statistical machine translation MT techniques can only process a small portion of them"], "abstract": ["The automatic generation of entity profiles from unstructured text such as Knowledge Base Population if applied in a multilingual setting generates the need to align such profiles from multiple languages in an unsupervised manner", "This paper describes an unsupervised and languageindependent approach to mine name translation pairs from entity profiles using Wikipedia Infoboxes as a standin for high quality entity profile extraction", "Pairs are initially found using expressions that are written in languageindependent forms such as dates and numbers and new translations are then mined from these pairs", "The algorithm then iteratively bootstraps from these translations to learn more pairs and more translations", "The algorithm maintains a high precision over 95 for the majority of its iterations with a slightly lower precision of 859 and an fscore of 76", "A side effect of the name mining algorithm is the unsupervised creation of a translation lexicon between the two languages with an accuracy of 64", "We also duplicate three stateoftheart name translation mining methods and use two existing name translation gazetteers to compare with our approach", "Comparisons show our approach can effectively augment the results from each of these alternative methods and resources"], "inroduction": ["A shrinking fraction of the worlds web pages are written in English while about 3000 languages are endangered Krauss 2007", "Therefore the ability 43 to access information across a range of languages especially lowdensity languages is becoming increasingly important for many applications", "In this paper we hypothesize that in order to extend cross lingual information access to all the language pairs on the earth or at least to some lowdensity languages which are lacking fundamental linguistic resources we can start from the much more scalable task of information translation or more specifically new name translation", "Wikipedia as a remarkable and rich online encyclopedia with a wealth of general knowledge about varied concepts entities events and facts in the world may be utilized to address this need", "As of March 2011 Wikipedia contains pages from 275 languages1  but statistical machine translation MT techniques can only process a small portion of them eg Google translate can only translate between 59 languages", "Wikipedia infoboxes are a highly structured form of data and are composed of a set of subjectattributevalue triples that summarize or highlight the key features of the concept or subject of each article", "A large number of instance centered knowledgebases that have harvested this structured data are available", "The most wellknown are probably DBpedia Auer et al 2007 Freebase Bollacker et al 2007 and YAGO Suchanek et al 2007", "However almost all of these existing knowledge bases contain only one language", "Even for highdensity languages more than 70 of Wikipedia pages and their infobox entries do not contain crosslingual links", "1 httpmetawikimediaorgwikiListof Wikipedias Proceedings of EMNLP 2011 Conference on Empirical Methods in Natural Language Processing pages 4352 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics Recent research into Knowledge Base Population the automatic generation of profiles for named entities from unstructured text has raised the possibility of automatic infobox generation in many languages", "Crosslingual links between entities in this setting would require either expensive multilingual human annotation or automatic name pairing", "We hypothesize that overlaps in information across languages might allow automatic pairing of profiles without any preexisting translational capabilities", "Wikipedia infoboxes provide a proxy for these high quality cross lingual automatically generated profiles upon which we can explore this hypothesis", "In this paper we propose a simple and general unsupervised approach to discover name translations from knowledge bases in any language pair using Wikipedia infoboxes as a case study", "Although different languages have different writing systems a vast majority of the worlds countries and languages use similar forms for representing information such as timecalendar date number website URL and currency IBM 2010", "In fact most languages commonly follow the ISO 8601 standard2 so the formats of timedate are the same or very similar", "Therefore we take advantage of this languageindependent formatting to design a new and simple bootstrapping based name pair mining approach", "We start from languageindependent expressions in any two languages and then extract those infobox entries which share the same slot values", "The algorithm iteratively mines more name pairs by utilizing these pairs and comparing other slot values", "In this unsupervised manner we dont need to start from any name transliteration module or documentwise temporal distributions as in previous work", "We conduct experiments on English and Chinese as we have bilingual annotators available for evaluating results", "However our approach does not require any languagespecific knowledge so its generally applicable to any other language pairs", "We also compare our approach to stateoftheart name translation mining approaches", "11 Wikipedia Statistics", "A standard Wikipedia entry includes a title a document describing the entry and an infobox which 2 httpenwikipediaorgwikiISO8601 is a fixedformat table designed to be added to the top righthand corner of the article to consistently present a summary of some unifying attributes or slots about the entry", "For example in the Wikipedia entry about the singer Beyonce Knowles the infobox includes information about her birth date origin song genres occupation etc As of November 2010 there were 10355225 English Wikipedia entries and 772826 entries", "Only 272 of English Wikipedia entries have cross lingual hyperlinks referring to their corresponding Chinese entries", "Wikipedia entries are created and updated exponentially Almeida et al 2007 because of the increasing number of contributors many of whom are not multilingual speakers", "Therefore it is valuable to align the crosslingual entries by effective name mining", "12 Motivating Example", "Figure 1 A Motivating Example Figure 1 depicts a motivating example for our approach", "Based on the assumption that if two person entries had the same birth date and death date they are likely to be the same person we can find the entity pair of Michael Jackson  ", "We can get many name pairs using similar language independent clues", "Then starting from these name pairs we can iteratively get new pairs with a large portion of overlapped slots", "For example since  and The Jackson 5 share many slot values such as member and years active they are likely to be a translation pair", "Next we can use the new pair of The Jackson 5   to mine more pairs such as  and Steeltown Records"]}, "W11-2408": {"title": ["Discovering Commonsense Entailment  Rules Implicit in Sentences"], "abstract": ["Reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge including expectations about the probable results of actions and the lexical entailments for many predicates", "We describe initial work to acquire such a collection of conditional ifthen knowledge by exploiting presuppositional discourse patterns such as ones involving but yet and hoping to and abstracting the matched material into general rules"], "inroduction": ["We are interested ultimately in enabling an inference system to reason forward from facts as well as backward from goals using lexical knowledge together with world knowledge", "Creating appropriate collections of general world knowledge to support reasoning has long been a goal of researchers in Artificial Intelligence", "Efforts in information extraction eg Banko et al", "2007 have focused on learning base facts about specific entities such as that Barack Obama is president and work in knowledge extraction eg Van Durme and Schubert 2008 has found generalizations such as that a president may make a speech", "While the latter provides a basis for possibilistic forward inference Barack Obama probably makes a speech at least occasionally when its meaning is sharpened Gordon and Schubert 2010 these resources dont provide a basis for saying what we might expect to happen if for instance someone crashes their car", "That the driver in a car crash might be injured and the car damaged is a matter of common sense and as such is rarely stated directly", "However it can be found in sentences where this expectation is disconfirmed Sally crashed her car into a tree but she wasnt hurt We have been exploring the use of lexicosyntactic discourse patterns indicating disconfirmed expectations as well as peoples goals Joe apologized repeatedly hoping to be forgiven", "The resulting rules expressed at this point in natural language are a first step toward obtaining classes of general conditional knowledge typically not obtained by other methods"]}, "W11-2605": {"title": ["Proceedings of EMNLP 2011 Conference on Empirical Methods in Natural Language Processing pages 39 48 Edinburgh Scotland UK July 27 31 2011 c  2011 Association for Computational Linguistics Learning wordlevel dialectal variation as phonological replacement rules using a limited parallel corpus Mans Hulden University of Helsinki Language Technology manshuldenhelsinkifi In aki Alegria IXA taldea UPV EHU ialegriaehues Izaskun Etxeberria IXA taldea UPV EHU izaskunetxeberriaehues Montse Maritxalar IXA taldea UPV EHU montsemaritxalarehues Abstract This paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialectform text given that a computational description of the standard morphology is available The goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available NLP tools that only assume standardform input The results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature 1 Introduction In our work with the Basque language a morphological description and analyzer is available for the standard language along with other tools for processing the language Alegria et al 2002 How ever it would be convenient to be able to analyze variants and dialectal forms as well As the dialectal differences within the Basque language are largely lexical and morphophonological analyzing the dialectal forms would in effect require a separate morphological analyzer that is able to handle the unique lexical items in the dialect together with the differing affixes and phonological changes Morphological analyzers are traditionally handwritten by linguists most commonly using some variant of the popular finitestate morphology approach Beesley and Karttunen 2002 This entails having an expert model a lexicon inflectional and derivational paradigms as well as phonological alternations and then producing a morphological an alyzergenerator in the form of a finitestate trans ducer As the development of such wide coverage morphological analyzers is laborintesive the hope is that an analyzer for a variant could be automatically learned from a limited parallel standarddialect corpus given that an analyzer already exists for the standard language This is an interesting problem because a good solution to it could be applied to many other tasks as well to enhancing access to digital libraries containing diachronic and dialectal variants for example or to improving treatment of informal registers such as SMS messages and blogs etc In this paper we evaluate two methods of learning a model from a standardvariant parallel corpus that translates a given word of the dialect to its standardform equivalent Both methods are based on finitestate phonology The variant we use for experiments is Lapurdian1 a dialect of Basque spoken in the Lapurdi fr Labourd region in the Basque Country Because Basque is an agglutinative highly inflected language we believe some of the results can be extrapolated to many other languages facing sim ilar challenges One of the motivations for the current work is that there are a large number of NLP tools avail able and in development for standard Basque also called Batua a morphological analyzer a POS tagger a dependency analyzer an MT engine among 1Sometimes also called NavarroLabourdin or Labourdin 39 others Alegria et al 2011 However these tools do not work well in processing the different dialects of Basque where lexical items have a different orthographic representation owing to slight differences in phonology and morphology Here is a brief contrastive example of the kinds of differences found in the a Lapurdian dialect and standard Basque b parallel corpus2"], "abstract": ["This paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialectform text given that a computational description of the standard morphology is available", "The goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available NLP tools that only assume standardform input", "The results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature"], "inroduction": ["In our work with the Basque language a morphological description and analyzer is available for the standard language along with other tools for processing the language Alegria et al 2002", "However it would be convenient to be able to analyze variants and dialectal forms as well", "As the dialectal differences within the Basque language are largely lexical and morphophonological analyzing the dialectal forms would in effect require a separate morphological analyzer that is able to handle the unique lexical items in the dialect together with the differing affixes and phonological changes", "Morphological analyzers are traditionally handwritten by linguists most commonly using some variant of the popular finitestate morphology approach Beesley and Karttunen 2002", "This entails 39 having an expert model a lexicon inflectional and derivational paradigms as well as phonological alternations and then producing a morphological analyzergenerator in the form of a finitestate transducer", "As the development of such widecoverage morphological analyzers is laborintesive the hope is that an analyzer for a variant could be automatically learned from a limited parallel standarddialect corpus given that an analyzer already exists for the standard language", "This is an interesting problem because a good solution to it could be applied to many other tasks as well to enhancing access to digital libraries containing diachronic and dialectal variants for example or to improving treatment of informal registers such as SMS messages and blogs etc In this paper we evaluate two methods of learning a model from a standardvariant parallel corpus that translates a given word of the dialect to its standard form equivalent", "Both methods are based on finite state phonology", "The variant we use for experiments is Lapurdian1 a dialect of Basque spoken in the La purdi fr", "Labourd region in the Basque Country", "Because Basque is an agglutinative highly inflected language we believe some of the results can be extrapolated to many other languages facing similar challenges", "One of the motivations for the current work is that there are a large number of NLP tools available and in development for standard Basque also called Batua a morphological analyzer a POS tag ger a dependency analyzer an MT engine among 1 Sometimes also called NavarroLabourdin or Labourdin", "Proceedings of EMNLP 2011 Conference on Empirical Methods in Natural Language Processing pages 3948 Edinburgh Scotland UK July 2731 2011", "Qc 2011 Association for Computational Linguistics others Alegria et al 2011", "However these tools do not work well in processing the different dialects of Basque where lexical items have a different orthographic representation owing to slight differences in phonology and morphology", "Here is a brief contrastive example of the kinds of differences found in the a Lapurdian dialect and standard Basque b parallel corpus2 a Ez gero uste izan nexkatxa guziek tu egiten dautatela b Ez gero uste izan neskatxa guztiek tu egiten didatela As the example illustrates the differences are minor overallthe word order and syntax are unaffected and only a few lexical items differ", "This reflects the makeup of our parallel corpus quite well in it slightly less than 20 of the word tokens are distinct", "However even such relatively small discrepancies cause great problems in the potential reuse of current tools designed for the standard forms only", "We have experimented with two approaches that attempt to improve on a simple baseline of memorizing wordpairs in the dialect and the standard", "The first approach is based on work by Almeida et al", "2010 on contrasting orthography in Brazilian Portuguese and European Portuguese", "In this approach differences between substrings in distinct wordpairs are memorized and these transformation patterns are then applied whenever novel words are encountered in the evaluation", "To prevent over generation the output of this learning process is later subject to a morphological filter where only actual standardform outputs are retained", "The second approach is an Inductive Logic Programmingstyle ILP Muggleton and De Raedt 1994 learning algorithm where phonological transformation rules are learned from wordpairs", "The goal is to find a minimal set of transformation rules that is both necessary and sufficient to be compatible with the learning data ie the word pairs seen in the training data", "The remainder of the paper is organized as follows", "The characteristics of the corpus available to us are described in section 2", "In sections 3 4 and 5 we describe the steps and variations of the methods we have applied and how they are evaluated", "Section 6 presents the experimental results and finally 2 English translation of the example Dont think all girls spit on me we discuss the results and present possibilities for potential future work in section 7", "11 Related work", "The general problem of supervised learning of dialectal variants or morphological paradigms has been discussed in the literature with various connection to computational phonology morphology machine learning and corpusbased work", "For example Kestemont et al", "2010 presents a language independent system that can learn intralemma spelling variation", "The system is used to produce a consistent lemmatization of texts in Middle Dutch literature in a medieval corpus CorpusGysseling which contains manuscripts dated before 1300 AD", "These texts have enormous spelling variation which makes a computational analysis difficult", "Koskenniemi 1991 provides a sketch of a discovery procedure for phonological twolevel rules", "The idea is to start from a limited number of paradigms essentially pairs of inputoutput forms where the input is the surface form of a word and the output a lemmatization plus analysis", "The problem of finding phonological rules to model morphological paradigms is essentially similar to the problem presented in this paper", "An earlier paper Johnson 1984 presents a discovery procedure for learning phonological rules from data something that can be seen as a precursor to the problem dealt with by our ILP algorithm", "Mann and Yarowsky 2001 present a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages", "Bilingual lexicons within languages families are induced using probabilistic string edit distance models", "Inspired by that paper Scherrer 2007 uses a generateandfilter approach quite similar to our first method", "He compares different measures of graphemic similarity applied to the task of bilingual lexicon induction between Swiss German and Standard German", "Stochastic transducers are trained with the EM algorithm and using handmade transduction rules", "An improvement of 11 in Fscore is reported over a baseline method using Levenshtein Distance", "Se nte nc es 21 17 1  6 9 4 4 2 3 Wo rds 12 15 0 9  7 3 4 2  4 1 7 Uni qu e wo rds Sta nd ard Ba sq ue 35 53 3  0 8 0 1  1 9 2 La pu rdi an 38 30 3  2 9 2 1  2 3 9 Filt ere d pai rs 36 10 3  1 0 8 1  1 7 2 Ide nti cal pai rs 25 32 2  2 0 0 8 7 1 Dis tin ct pai rs 10 78 9 0 8 3 0 1 Table 1 Characteristics of the parallel corpus used for experiments"]}, "W12-0304": {"title": ["Google Books Ngram  Corpus used as a Grammar Checker"], "abstract": ["In this research we explore the possibility of using a large ngram corpus Google Books to derive lexical transition probabilities from the frequency of word ngrams and then use them to check and suggest corrections in a target text without the need for grammar rules", "We conduct several experiments in Spanish although our conclusions also reach other languages since the procedure is corpusdriven", "The paper reports on experiments involving different types of grammar errors which are conducted to test different grammarchecking procedures namely spotting possible errors deciding between different lexical possibilities and fillingin the blanks in a text"], "inroduction": ["This paper discusses a series of early experiments on a methodology for the detection and correction of grammatical errors based on cooccurrence statistics using an extensive corpus of ngrams Google Books compiled by Michel et al 2011", "We start from two complementary assumptions on the one hand books are published accurately that is to say they usually go through different phases of revision and correction with high standards and thus a large proportion of these texts can be used as a reference corpus for inferring the grammar rules of a language", "On the other hand we hypothesise that with a sufficiently large corpus a high percentage of the information about these rules can be extracted with word ngrams", "Thus although there are still many grammatical errors that cannot be detected with this method there is also another important group which can be identified and corrected successfully as we will see in Section 4", "Grammatical errors are the most difficult and complex type of language errors because grammar is made up of a very extensive number of rules and exceptions", "Furthermore when grammar is observed in actual texts the panorama becomes far more complicated as the number of exceptions grows and the variety and complexity of syntactical structures increase to an extent that is not predicted by theoretical studies of grammar", "Grammar errors are extremely important and the majority of them cannot be considered to be performancebased because it is the meaning of the text and therefore the success or failure of communication that is compromised", "To our knowledge no grammar book or dictionary has yet provided a solution to all the problems a person may have when he or she writes and tries to follow the grammar rules of language", "Doubts that arise during the writing process are not always clearly associated to a lexical unit or the writer is not able to detect such an association and this makes it difficult to find the solution using a reference book", "In recent years some advances have been made in the automatic detection of grammar mistakes see Section 2", "Effective rulebased methods have been reported but at the cost of a very time consuming task and with an inherent lack of flexibility", "In contrast statistical methods are easier and faster to implement as well as being more flexible and adaptable", "The experiment we will describe in the following sections is the first part of a more extensive study", "Most probably the logical step to follow in order to continue such a study will be a hybrid approach based on both 27 Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing pages 2734 Avignon France April 23 2012", "Qc 2012 Association for Computational Linguistics statistics and rules", "Hence this paper aims to contribute to the statistical approach applied to grammar checking", "The Google Books Ngram Corpus is a database of ngrams of sequences of up to 5 words and records the frequency distribution of each unit in each year from 1500 onwards", "The bulk of the corpus however starts from 1970 and that is the year we took as a starting point for the material that we used to compile our reference corpus", "The idea of using this database as a grammar checker is to analyse an input text and detect any sequence of words that cannot be found in the ngram database which only contains ngrams with frequency equal to or greater than 40 and eventually to replace a unit in the text with one that makes a frequent ngram", "More specifically we conduct four types of operations accepting a text and spotting possible errors inflecting a lemma into the appropriate form in a given context fillingin the blanks in a text and selecting from a number of options the most probable word form for a given context", "In order to evaluate the algorithm we applied it to solve exercises from a Spanish grammar book and also tested the detection of errors in a corpus of real errors made by second language learners", "The paper is organised as follows we first offer a brief description of related work and then explain our methodology for each of the experiments", "In the next section we show the evaluation of the results in comparison to the Microsoft Word grammar checker and finally we draw some conclusions and discuss lines of future work"]}, "W12-1003": {"title": ["Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage Social Sciences and Humanities pages 13 17 Avignon France 24 April 2012 c  2012 Association for Computational Linguistics BAD An assistant tool for making verses in Basque Manex Agirrezabal In aki Alegria Bertol Arrieta University of the Basque Country UPVEHU maguirrezaba008ikasleehues ialegriaehues bertolehues Mans Hulden Ikerbasque Basque Science Foundation mhuldenemailarizonaedu Abstract We present work on a versecomposition assistant for composing checking correctness of and singing traditional Basque bertsoak impromptu verses on particular themes A performing bertsolari a verse singer in the Basque Country must ad here to strict rules that dictate the format and content of the verses sung To help the aspiring bertsolari we provide a tool that includes a web interface that is able to analyze correct provide suggestions and synonyms and tentatively also sing using texttospeech synthesis verses composed by the user 1 Introduction In the Basque Country there exists a longstanding live performance tradition of improvising verses a type of ex tempore composition and singing called bertsolaritza Verses in bertsolaritza can be seen as discourses with strict rules governing the technical structure of them verses must contain a certain number of lines and each line must have a defined number of syllables certain lines have to rhyme in certain patterns and so forth In this paper we present a webbased assistant tool for constructing verses bertsoak according to the rules of bertsolaritza Garzia et al 2001 If the reader is interested in this topic we recommend watching the 2011 film Bertsolari1 2 directed by Asier Altuna 1IMDB httpwwwimdbcomtitlett2058583 2Trailer on httpvimeocom9355066 2 Relationship to earlier work There exist some prior works dealing with Basque versemaking and computer technologies such as BertsolariXa Arrieta et al 2001 which is a rhyme search tool implemented as finitestate au tomata using the twolevel morphology formalism The tool also contains other features including semantic categorization of words narrowing wordsearches to certain themes etc While BertsolariXa focuses mostly on the wordlevel the current work also includes constraints on overall verse structure in its implementation as well as a synonym search tool a melody suggestion system and possibilities for plugging in texttospeech synthesis of verses"], "abstract": ["We present work on a versecomposition assistant for composing checking correctness of and singing traditional Basque bertsoakimpromptu verses on particular themes", "A performing bertsolaria verse singer in the Basque Countrymust adhere to strict rules that dictate the format and content of the verses sung", "To help the aspiring bertsolari we provide a tool that includes a web interface that is able to analyze correct provide suggestions and synonyms and tentatively also sing using texttospeech synthesis verses composed by the user"], "inroduction": ["In the Basque Country there exists a long standing live performance tradition of improvising versesa type of ex tempore composition and singing called bertsolaritza", "Verses in bertsolaritza can be seen as discourses with strict rules governing the technical structure of them verses must contain a certain number of lines and each line must have a defined number of syllables certain lines have to rhyme in certain patterns and so forth", "In this paper we present a webbased assistant tool for constructing verses bertsoak according to the rules of bertsolaritza Garzia et al 2001", "If the reader is interested in this topic we recommend watching the 2011 film Bertsolari1 2 directed by Asier Altuna", "1 IMDB httpwwwimdbcomtitlett2058583"]}, "W12-1914": {"title": ["NAACLHLT Workshop on the Induction of Linguistic Structure pages 100 104 Montreal Canada June 38 2012 c2012 Association for Computational Linguistics Hierarchical clustering of word class distributions Grzegorz Chrupaa gchrupalalsvunisaarlandde Spoken Language Systems Saarland University Abstract We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation Then we create a hierarchical clustering of the word types we use an agglomerative clustering algorithm where the distance between clusters is defined as the JensenShannon divergence between the probability distributions over classes associated with each wordtype When assigning POS tags we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives First it is of interest to cognitive scientists who model syntactic category acquisition by children Redington et al 1998 Mintz 2003 Parisien et al 2008 Chrupa  a and Alishahi 2010 where the primary concern is match ing human performance patterns and satisfying cognitively motivated constraints such as incremental learning Second learning categories has been cast as unsupervised partofspeech tagging task recent work includes Ravi and Knight 2009 Lee et al 2010 Lamar et al 2010 Christodoulopoulos et al 2011 and primarily motivated as useful for tagging underresourced languages Finally learning categories has also been researched from the point of view of feature learning where the induced categories provide an intermediate level of representation abstracting away and generalizing over word form features in an NLP application Brown et al 1992 Miller et al 2004 Lin and Wu 2009 Turian et al 2010 Chrupala 2011 Ta ckstro  m et al 2012 The main difference from the partofspeech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold partofspeech tags Some researchers have used both approaches to evaluation"], "abstract": ["We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation", "Then we create a hierarchical clustering of the word types we use an agglomerative clustering algorithm where the distance between clusters is defined as the JensenShannon divergence between the probability distributions over classes associated with each wordtype", "When assigning POS tags we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag", "This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets"], "inroduction": ["Unsupervised induction of word categories has been approached from three broad perspectives", "First it is of interest to cognitive scientists who model syntactic category acquisition by children Redington et al 1998 Mintz 2003 Parisien et al 2008 Chrupaa and Alishahi 2010 where the primary concern is matching human performance patterns and satisfying cog nitively motivated constraints such as incremental learning", "Second learning categories has been cast as unsupervised partofspeech tagging task recent work includes Ravi and Knight 2009 Lee et al", "2010 Lamar et al", "2010 Christodoulopoulos et al", "2011 and primarily motivated as useful for tagging underresourced languages", "Finally learning categories has also been researched from the point of view of feature learning 100 where the induced categories provide an intermediate level of representation abstracting away and generalizing over word form features in an NLP application Brown et al 1992 Miller et al 2004 Lin and Wu 2009 Turian et al 2010 Chrupala 2011 Tackstro m et al 2012", "The main difference from the partofspeech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold partofspeech tags", "Some researchers have used both approaches to evaluation", "This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations", "For partofspeech tagging what is needed is a mapping from word tokens to a small set of discrete atomic labels", "For feature learning there are is no such limitation and other types of representations have been used such as lowdimensional continuous vectors learned by neural network language models as in Bengio et al", "2006 Mnih and Hinton 2009 or distributions over word classes learned using Latent Dirichlet Allocation as in Chrupala 2011", "In this paper we propose a simple method of mapping distributions over word classes to a set of discrete labels by hierarchically clustering word class distributions using JensenShannon divergence as a distance metric", "This allows us to effectively use the algorithm of Chrupala 2011 and similar ones in settings where using distributions directly is not possible or desirable", "Equivalently our approach can be seen as a generic method to convert a soft clustering to hard clustering while conserving much of the information encoded in the original soft cluster assignments", "We evaluate this method on the unsupervised partofspeech tagging task on ten datasets NAACLHLT Workshop on the Induction of Linguistic Structure pages 100104 Montreal Canada June 38 2012", "Qc 2012 Association for Computational Linguistics in nine languages as part of the shared task at the NAACLHLT 2012 Workshop on Inducing Linguistic Structure"]}, "W12-3141": {"title": ["Proceedings of the 7th Workshop on Statistical Machine Translation pages 330 337 Montreal Canada June 78 2012 c2012 Association for Computational Linguistics LIMSI  WMT  12 HaiSon Le12 Thomas Lavergne2 Alexandre Allauzen12 Marianna Apidianaki2 Li Gong12 Aure lien Max12 Artem Sokolov2 Guillaume Wisniewski12 Francois Yvon12 Univ ParisSud1 and LIMSICNRS2 rue John von Neumann 91403 Orsay cedex France firstnamelastnamelimsifr Abstract This paper describes LIMSI s submissions to the shared translation task We report results for FrenchEnglish and GermanEnglish in both directions Our submissions use ncode an open source system based on bilingual ngrams In this approach both the translation and target language models are estimated as conventional smoothed ngram models an approach we extend here by estimating the translation probabilities in a continuous space using neural networks Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions We also report preliminary experiments using an  onthefly translation model 1 Introduction This paper describes LIMSI s submissions to the shared translation task of the Seventh Workshop on Statistical Machine Translation LIMSI participated in the FrenchEnglish and GermanEnglish tasks in both directions For this evaluation we used ncode an open source inhouse Statistical Machine Translation SMT system based on bilingual ngrams1 The main novelty of this year s participation is the use in a large scale system of the continuous space translation models described in HaiSon et al 2012 These models estimate the ngram probabilities of bilingual translation units using neural networks We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated onthefly 1httpncodelimsifr by sampling relevant examples instead of considering the entire training set Finally we also describe the use in a rescoring step of several additional features based on IBM1 models and word sense disambiguation information The rest of this paper is organized as follows Section 2 provides an overview of the baseline systems built with ncode including the standard translation model TM The continuous space translation models are then described in Section 3 As in our previous participations several steps of data preprocessing cleaning and filtering are applied and their improvement took a nonnegligible part of our work These steps are summarized in Section 5 The last two sections report experimental results obtained with the  onthefly system in Section 6 and with ncode in Section 7 2 System overview ncode implements the bilingual ngram approach to SMT Casacuberta and Vidal 2004 Marin  o et al 2006 Crego and Marin  o 2006 In this framework translation is divided in two steps a source reordering step and a monotonic translation step Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words Applying these rules result in a finitestate graph of possible source reorderings which is then searched for the best possible candidate translation"], "abstract": ["This paper describes LIMSIs submissions to the shared translation task", "We report results for FrenchEnglish and GermanEnglish in both directions", "Our submissions use ncode an open source system based on bilingual ngrams", "In this approach both the translation and target language models are estimated as conventional smoothed ngram models an approach we extend here by estimating the translation probabilities in a continuous space using neural networks", "Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions", "We also report preliminary experiments using an onthefly translation model"], "inroduction": ["This paper describes LIMSIs submissions to the shared translation task of the Seventh Workshop on Statistical Machine Translation", "LIMSI participated in the FrenchEnglish and GermanEnglish tasks in both directions", "For this evaluation we used ncode an open source inhouse Statistical Machine Translation SMT system based on bilingual ngrams1", "The main novelty of this years participation is the use in a large scale system of the continuous space translation models described in HaiSon et al 2012", "These models estimate the ngram probabilities of bilingual translation units using neural networks", "We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated onthefly 1 httpncodelimsifr by sampling relevant examples instead of considering the entire training set", "Finally we also describe the use in a rescoring step of several additional features based on IBM1 models and word sense disambiguation information", "The rest of this paper is organized as follows", "Section 2 provides an overview of the baseline systems built with ncode including the standard translation model TM", "The continuous space translation models are then described in Section 3", "As in our previous participations several steps of data pre processing cleaning and filtering are applied and their improvement took a nonnegligible part of our work", "These steps are summarized in Section 5", "The last two sections report experimental results obtained with the onthefly system in Section 6 and with ncode in Section 7"]}, "W12-3144": {"title": ["Proceedings of the 7th Workshop on Statistical Machine Translation pages 349 355 Montreal Canada June 78 2012 c2012 Association for Computational Linguistics The Karlsruhe Institute of Technology Translation Systems for the WMT 2012 Jan Niehues Yuqi Zhang Mohammed Mediani Teresa Herrmann Eunah Cho and Alex Waibel Karlsruhe Institute of Technology Karlsruhe Germany firstnamelastnamekitedu Abstract This paper describes the phrasebased SMT systems developed for our participation in the WMT12 Shared Translation Task Translations for EnglishGerman and English French were generated using a phrasebased translation system which is extended by additional models such as bilingual finegrained partofspeech POS and automatic cluster language models and discriminative word lexica In addition we explicitly handle outofvocabulary OOV words in German if we have translations for other morphological forms of the same stem Furthermore we extended the POSbased reordering approach to also use information from syntactic trees 1 Introduction In this paper we describe our systems for the NAACL 2012 Seventh Workshop on Statistical Machine Translation We participated in the Shared Translation Task and submitted translations for English German and English French We use a phrasebased decoder that can use lattices as input and developed several models that extend the standard loglinear model combination of phrasebased MT In addition to the POSbased reordering model used in past years for GermanEnglish we extended it to also use rules learned using syntax trees The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier For the FrenchEnglish and EnglishFrench translation systems we also used phrase table adaptation to avoid overestimation of the probabilities of the huge but noisy Giga corpus In the GermanEnglish system we tried to learn translations for OOV words by ex ploring different morphological forms of the OOVs with the same lemma Furthermore we combined different language models in the loglinear model We used wordbased language models trained on different parts of the training corpus as well as POSbased language models using finegrained POS information and language models trained on automatic word clusters"], "abstract": ["This paper describes the phrasebased SMT systems developed for our participation in the WMT12 Shared Translation Task", "Translations for EnglishGerman and EnglishFrench were generated using a phrasebased translation system which is extended by additional models such as bilingual finegrained partofspeech POS and automatic cluster language models and discriminative word lexica", "In addition we explicitly handle outofvocabulary OOV words in German if we have translations for other morphological forms of the same stem", "Furthermore we extended the POSbased reordering approach to also use information from syntactic trees"], "inroduction": ["In this paper we describe our systems for the NAACL 2012 Seventh Workshop on Statistical Machine Translation", "We participated in the Shared Translation Task and submitted translations for EnglishGerman and EnglishFrench", "We use a phrasebased decoder that can use lattices as input and developed several models that extend the standard loglinear model combination of phrasebased MT In addition to the POSbased reordering model used in past years for GermanEnglish we extended it to also use rules learned using syntax trees", "The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier", "For the FrenchEnglish and EnglishFrench translation systems we also used phrase table adaptation to avoid overestimation of the probabilities of the huge but noisy Giga corpus", "In the GermanEnglish system we tried to learn translations for OOV words by exploring different morphological forms of the OOVs with the same lemma", "Furthermore we combined different language models in the loglinear model", "We used word based language models trained on different parts of the training corpus as well as POSbased language models using finegrained POS information and language models trained on automatic word clusters", "The paper is organized as follows The next section gives a detailed description of our systems including all the models", "The translation results for all directions are presented afterwards and we close with a conclusion"]}, "W12-3205": {"title": ["42"], "abstract": [], "inroduction": ["Research in Natural Language Processing NLP has long benefitted from the fact that text can often be treated as simply a bag of words or a bag of sentences", "But not always Position often matters  eg It is wellknown that the first one or two sentences in a news report usually comprise its best ex tractive summary", "Order often matters  eg very different events are conveyed depending on how clauses and sentences are ordered", "1 a I said the magic words and a genie appeared", "b A genie appeared and I said the magic words", "Adjacency often matters  eg attributed material may span a sequence of adjacent sentences and contrasts are visible through sentence juxtaposition", "Context always matters  eg All languages achieve economy through minimal expressions that can only convey intended meaning when understood in context", "Position order adjacency and context are intrinsic features of discourse and research on discourse processing attempts to solve the challenges posed by contextbound expressions and the discourse structures that give rise when linearized to position order and adjacency", "But challenges are not why Language Technology LT researchers should care about discourse Rather discourse can enable LT to overcome known obstacles to better performance", "Consider automated summarization and machine translation Humans regularly judge output quality in terms that include referential clarity and coherence", "Systems can only improve here by paying attention to discourse  ie to linguistic features above the level of n grams and single sentences", "In fact we predict that as soon as cheap  ie nonmanual  methods are found for reliably assessing these features  for example using proxies like those suggested in Pitler et al 2010  they will supplant or at least complement todays common metrics Bleu and Rouge that say little about what matters to human text understanding CallisonBurch et al 2006", "Consider also work on automated text simplification One way that human editors simplify text is by reexpressing a long complex sentence as a discourse sequence of simple sentences", "Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse", "Other examples of LT applications already benefitting from recognizing and applying discourselevel information include automated assessment of student essays Burstein and Chodorow2010 summarization Thione et al 2004 infor 42 Proceedings of the ACL2012 Special Workshop on Rediscovering 50 Years of Discoveries pages 4254 Jeju Republic of Korea 10 July 2012", "Qc 2012 Association for Computational Linguistics Eales et al 2008 Maslennikov and Chua 2007 and more recently statistical machine translation Foster et al 2010", "These are described in more detail in Webber et al 2012", "Our aim here then on this occasion of ACLs 50th Annual Meeting is to briefly describe the evolution of computational approaches to discourse structure reflect on where the field currently stands and what new challenges it faces in trying to deliver on its promised benefit to Language Technology"]}, "W12-3311": {"title": ["A Generic Framework for Multiword Expressions Treatment"], "abstract": ["This paper presents an open and flexible methodological framework for the automatic acquisition of multiword expressions MWEs from monolingual textual corpora", "This research is motivated by the importance of MWEs for NLP applications", "After briefly presenting the modules of the framework the paper reports extrinsic evaluation results considering two applications computeraided lexicography and statistical machine translation", "Both applications can benefit from automatic MWE acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality", "The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into these and many other applications"], "inroduction": ["Multiword expressions MWEs range over linguistic constructions such as idioms to pay an arm and a leg fixed phrases rock n roll and noun compounds dry ice", "There is no unique and widely accepted definition for the term multiword expression", "It can be an arbitrary and recurrent word combination Smadja 1993 or a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components Choueka 1988 or simply an idiosyncratic interpretation that crosses word boundaries or spaces Sag et al 2002", "MWEs lie in the fuzzy zone between lexicon and syntax thus constituting a real challenge for NLP systems", "In addition they are very pervasive occurring frequently in everyday language as well as in specialised communications", "Some common properties of MWEs are1 1 These are not binary yesno flags but values in a continuum going from flexible word combinations to prototypical fixed expressions", "SRC I paid my poor parents a visit MT Jai pay mes pauvres parents une visite REF Jai rendu visite  mes pauvres parents SRC Students pay an arm and a leg to park on campus MT Les tudiants paient un bras et une jambe pour se garer sur le campus REF Les tudiants paient les yeux de la tte pour se garer sur le campus SRC It shares the translationinvariance and homogeneity properties with the central moment MT Il partage la traductioninvariance et proprits dhomognit avec le moment central REF Il partage les proprits dinvariance par translation et dhomognit avec le moment central Table 1 Examples of SMT errors due to MWEs", " Arbitrariness sometimes valid constructions are not acceptable because people do not use them", "Smadja 1993 p 143144 illustrates this by presenting 8 different ways of referring to the Dow Jones index among which only 4 are used", " Institutionalisation MWEs are recurrent as they correspond to conventional ways of saying things", "Jackendoff 1997 estimates that they compose half of the entries of a speakers lexicon and Sag et al", "2002 point out that this may be an underestimate if we consider domainspecific MWEs", " Limited semantic variability MWEs do not undergo the same semantic compositionality rules as ordinary word combinations", "This is expressed in terms of i noncompositionality as the meaning of the whole expression often cannot be directly inferred from the meaning of the parts composing it ii nonsubstitutability as it is not possible to replace part of an MWE by a related synonymequivalent word or construction and iii no wordfor word translation", "61 Proceedings of the 2012 Student Research Workshop pages 6166 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics  Limited syntactic variability standard grammatical rules do not apply to MWEs", "This can be expressed in terms of i lexicalisation as one cannot list all MWEs in the lexicon undergeneration nor include them all in the grammar overgeneration and ii extragrammaticality as MWEs are unpredictable and seem weird for a second language learner who only knows general rules2  Heterogeneity MWEs are hard to define because they encompass a large amount of phenomena", "Thus NLP applications cannot use a unified approach and need to rely on some typology3  In this paper I adopt the definition by Calzolari et al", "2002 who define MWEs as different but related phenomena which can be described as a sequence4 of words that acts as a single unit at some level of linguistic analysis", "This generic and intentionally vague definition can be narrowed down according to the application needs", "For example for the statistical machine translation MT system5 used in the examples shown in Table 1 an MWE is any sequence of words which when not translated as a unit generates errors ungrammatical or unnatural verbal constructions sentence 1 awkward literal translations of idioms sentence 2 and problems of lexical choice and word order in specialised texts sentence 3", "These examples illustrate the importance of correctly dealing with MWEs in MT applications and more generally MWEs can speed up and help remove ambiguities in many current NLP applications for example  Lexicography Church and Hanks 1990 used a lexicographic environment as their evaluation scenario comparing manual and intuitive research with the automatic association ratio they proposed", " Word sense disambiguation MWEs tend to be less polysemous than simple words", "Finlayson and Kulkarni 2011 exemplify that the word world has 9 senses in Wordnet 16 record has 14 but world record has only 1", " POS tagging and parsing recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities Seretan 2008", " Information retrieval when MWEs like pop star are indexed as a unit the accuracy of the system improves on multiword queries Acosta et al 2011"]}, "W12-4006": {"title": ["Proceedings of the 3rd Workshop on the People s Web Meets NLP ACL 2012 pages 34 43 Jeju Republic of Korea 814 July 2012 c  2012 Association for Computational Linguistics Extracting ContextRich Entailment Rules from Wikipedia Revision History Elena Cabrio INRIA 2004 route de Lucioles BP93 06902 Sophia Antipolis France elenacabrioinriafr Bernardo Magnini FBK Via Sommarive 18 38100 PovoTrento Italy magninifbkeu Angelina Ivanova University of Oslo Gaustadalleen 23B OleJohan Dahls hus N0373 Oslo Norway angeliiifiuiono Abstract Recent work on Textual Entailment has shown a crucial role of knowledge to support entailment inferences However it has also been demonstrated that currently available entailment rules are still far from being optimal We propose a methodology for the automatic acquisition of large scale context rich entailment rules from Wikipedia revisions taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable We report on rule acquisition experiments on Wikipedia showing that it en ables the creation of an innovative ie acquired rules are not present in other available resources and good quality rule repository 1 Introduction Entailment rules have been introduced to provide pieces of knowledge that may support entailment judgments Dagan et al 2009 with some degree of confidence More specifically an entailment rule is defined Szpektor et al 2007 as a directional relation between two sides of a pattern corresponding to text fragments with variables typically phrases or parse subtrees The lefthand side LHS of the pattern entails the righthand side RHS of the same pattern under the same variable instantiation Given the TextHypothesis pair T H in Example 1 Example 1"], "abstract": ["Recent work on Textual Entailment has shown a crucial role of knowledge to support entail ment inferences", "However it has also been demonstrated that currently available entail ment rules are still far from being optimal", "We propose a methodology for the automatic acquisition of large scale contextrich entailment rules from Wikipedia revisions taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable", "We report on rule acquisition experiments on Wikipedia showing that it enables the creation of an innovative ie acquired rules are not present in other available resources and good quality rule repository"], "inroduction": ["Entailment rules have been introduced to provide pieces of knowledge that may support entailment judgments Dagan et al 2009 with some degree of confidence", "More specifically an entailment rule is defined Szpektor et al 2007 as a directional relation between two sides of a pattern corresponding to text fragments with variables typically phrases or parse subtrees", "The lefthand side LHS of the pattern entails the righthand side RHS of the same pattern under the same variable instantiation", "Given the TextHypothesis pair TH in Example 1 Example 1", "T Dr Thomas Bond established a hospital in Philadelphia for the reception and cure of poor sick persons", "H Dr Bond created a medical institution for sick people", "a directional lexical rule like 1 LHS hospital  RHS medical institution probability 08 brings to a TE system aimed at recognizing that a particular target meaning can be inferred from different text variants in several NLP application eg Question Answering or Information Extraction the knowledge that the word hospital in Text can be aligned or transformed into the word medical institution in the Hypothesis with a probability 08 that this operation preserves the entailment relation among T and H Similar considerations apply for more complex rules involving verbs as 2 LHS X establish Y  RHS X create Y probability 08 where the variables may be instantiated by any textual element with a specified syntactic relation with the verb", "Both kinds of rules are typically acquired either from structured sources eg WordNet Fellbaum 1998 or from unstructured sources according for instance to distributional properties eg DIRT Lin and Pantel 2001", "Entailment rules should typically be applied only in specific contexts defined in Szpektor et al 2007 as relevant contexts", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules eg", "Sekine 2005 CallisonBurch 2008 but most do not", "Because of a lack of an adequate representation of the linguistic context in which the 34 Proceedings of the 3rd Workshop on the Peoples Web Meets NLP ACL 2012 pages 3443 Jeju Republic of Korea 814 July 2012", "Qc 2012 Association for Computational Linguistics rules can be successfully applied their concrete use reflects this limitation", "For instance rule 2 extracted from DIRT fails if applied to The mathematician established the validity of the conjecture where the sense of establish is not a synonym of create but of prove demonstrate decreasing systems precision", "Moreover these rules often suffer from lack of directionality and from low accuracy ie the strength of association of the two sides of the rule is often weak and not well defined", "Such observations are also in line with the discussion on ablation tests carried out at the last RTE evaluation campaigns Bentivogli et al 2010", "Additional constraints specifying the variable types are therefore required to correctly instantiate them", "In this work we propose to take advantage of Collaboratively Constructed Semantic Resources CSRs namely Wikipedia to mine information useful to contextrich entailment rule acquisition", "More specifically we take advantage of material obtained through Wikipedia revisions which provides at the same time real textual variations from which we may extrapolate the relevant syntactic context and several simplifications with respect to alternative resources", "We consider TH pairs where T is a revision of a Wikipedia sentence and H is the original sentence as the revision is considered more informative then the revised sentence", "We demonstrate the feasibility of the proposed approach for the acquisition of contextrich rules from Wikipedia revision pairs focusing on two case studies ie the acquisition of entailment rules for causality and for temporal expressions", "Both phenomena are highly frequent in TE pairs and for both there are no available resources yet", "The result of our experiments consists in a repository that can be used by TE systems and that can be easily extended to entailment rules for other phenomena", "The paper is organized as follows", "Section 2 reports on previous work highlighting the specificity of our work", "Section 3 motivates and describes the general principles underlying our ac"]}, "W12-6201": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 19 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Effect of Language and Error Models on Efficiency of FiniteState SpellChecking and Correction Tommi A Pirinen University of Helsinki Department of Modern Languages FI00014 Univ of Helsinki PO box 24 tommipirinenhelsinkifi Sam Hardwick University of Helsinki Department of Modern Languages FI00014 Univ of Helsinki PO box 24 samhardwickhelsinkifi Abstract We inspect the viability of finitestate spellchecking and contextless correction of nonword errors in three languages with a large degree of morphological variety Overviewing previous work we conduct largescale tests involving three languages  English Finnish and Greenlandic  and a variety of error models and algorithms including proposed improvements of our own Special reference is made to online threeway composition of the input the error model and the language model Tests are run on realworld text acquired from freely available sources We show that the finitestate approaches discussed are sufficiently fast for highquality correction even for Greenlandic which due to its morphological complexity is a difficult task for nonfinitestate approaches 1 Introduction In most implementations of spellchecking efficiency is a limiting factor for selecting or discarding spellchecking solutions In the case of finitestate spellchecking it is known that finitestate language models can efficiently encode dictionaries of natural languages Beesley and Karttunen 2003 even for polysynthetic languages Most contem porary spellchecking and correction systems are still based on programmatic solutions eg hunspell1 and its spell relatives or at most specialised algorithms for implementing errortolerant traversal of the finitestate dictionaries Oflazer 1996 Hulden 2009a There have also been few fully 1httphunspellsfnet finitestate implementations that both detect and correct errors Schulz and Mihov 2002 Pirinen and Linde n 2010 In this paper we further evaluate the use of finitestate dictionaries with twotape finitestate automatons as a mechanism for correcting mis spellings and optimisations to the finitestate error models intending to demonstrate that purely finitestate algorithms can be made sufficiently efficient To evaluate the general usability and efficiency of finitestate spellchecking we test a number of possible implementations of such a system with three languages of typologically different morphological features2 and reference implementations for contemporary spellchecking applications English as a morphologically more isolating language with essentially a wordlist approach to spellchecking Finnish whose computational complexity has been just beyond the edge of being too hard to implement nicely in eg hunspell Pitka nen 2006 and Greenlandic a polysynthetic language which is implemented as a finitestate system using Xeroxs original finitestate morphology formalism Beesley and Karttunen 2003 As a general purpose finitestate library we use HFST3 which also contains our spell2We will not go into details regarding the morphological features of these languages We thank the anonymous reviewer for guiding us to make a rough comparison using a piece of trans lated text We observe from the translations of the Universal Declaration of Human Rights with preamble included as follows the number of wordlike tokens for English is 1746 for Finnish 1275 and for Greenlandic 1063 The count of the 15 most frequent tokens are for English 12028 for Finnish 85 10 and for Greenlandic 38 7 The average word length is 50 characters for English 78 for Finnish and 149 for Greenlandic For the complexity of computational models refer to Table 2 in this article 3httphfstsfnet 1 checking code As neither Finnish nor Greenlandic have been successfully implemented in the hunspell formalism we mainly use them to evaluate how the complexity of a language model affects the efficiency of finitestate spellchecking For a fullscale survey on the stateoftheart nonfinitestate spellchecking refer to Mitton 2009 The efficiency results are contrasted with the ex isting research on finitestate spellchecking in Hassan et al 2008 and the theoretical results on finitestate error models in Mitankin 2005 Our contribution primarily comprises the addition of morphologically complex languages with actual cyclic dictionary automata ie infinite dictionaries formed by compounding and recurring derivation and more complex structure in general compared to those of English and Arabic Our goal is to demonstrate that finitestate spelling is tractable for these complex languages to document their implications for performance and to present an algorithm for the task"], "abstract": ["We inspect the viability of finitestate spell checking and contextless correction of non word errors in three languages with a large degree of morphological variety", "Overviewing previous work we conduct largescale tests involving three languages  English Finnish and Greenlandic  and a variety of error models and algorithms including proposed improvements of our own", "Special reference is made to online threeway composition of the input the error model and the language model", "Tests are run on realworld text acquired from freely available sources", "We show that the finitestate approaches discussed are sufficiently fast for highquality correction even for Greenlandic which due to its morphological complexity is a difficult task for nonfinitestate approaches"], "inroduction": ["In most implementations of spellchecking efficiency is a limiting factor for selecting or discarding spellchecking solutions", "In the case of finite finitestate implementations that both detect and correct errors Schulz and Mihov 2002 Pirinen and Linden 2010", "In this paper we further evaluate the use of finitestate dictionaries with twotape finite state automatons as a mechanism for correcting misspellings and optimisations to the finitestate error models intending to demonstrate that purely finite state algorithms can be made sufficiently efficient", "To evaluate the general usability and efficiency of finitestate spellchecking we test a number of possible implementations of such a system with three languages of typologically different morphological features2 and reference implementations for contemporary spellchecking applications English as a morphologically more isolating language with essentially a wordlist approach to spellchecking Finnish whose computational complexity has been just beyond the edge of being too hard to implement nicely in eg", "hunspell Pitkanen 2006 and Green landic a polysynthetic language which is implemented as a finitestate system using Xeroxs original finitestate morphology formalism Beesley and Karttunen 2003", "As a general purpose finitestate library we use HFST3 which also contains our spell state spellchecking it is known that finitestate language models can efficiently encode dictionaries of natural languages Beesley and Karttunen 2003 even for polysynthetic languages", "Most contemporary spellchecking and correction systems are still based on programmatic solutions eg hunspell1 and its spell relatives or at most specialised algorithms for implementing errortolerant traversal of the finitestate dictionaries Oflazer 1996 Hulden 2009a", "There have also been few fully 1 httphunspellsfnet tures of these languages", "We thank the anonymous reviewer for guiding us to make a rough comparison using a piece of translated text", "We observe from the translations of the Universal Declaration of Human Rights with preamble included as follows the number of wordlike tokens for English is 1746 for Finnish 1275 and for Greenlandic 1063", "The count of the 15 most frequent tokens are for English 12028 for Finnish 85 10 and for Greenlandic 387", "The average word length is 50 characters for English 78 for Finnish and 149 for Greenlandic", "For the complexity of computational models refer to Table 2 in this article", "3 httphfstsfnet 1 Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 19 DonostiaSan Sebastian July 2325 2012", "Qc 2012 Association for Computational Linguistics checking code", "As neither Finnish nor Greenlandic have been successfully implemented in the hunspell formalism we mainly use them to evaluate how the complexity of a language model affects the efficiency of finitestate spellchecking", "For a fullscale survey on the stateoftheart nonfinitestate spellchecking refer to Mitton 2009", "The efficiency results are contrasted with the existing research on finitestate spellchecking in Has san et al", "2008 and the theoretical results on finite state errormodels in Mitankin 2005", "Our contribution primarily comprises the addition of morphologically complex languages with actual cyclic dictionary automata ie infinite dictionaries formed by compounding and recurring derivation and more complex structure in general compared to those of English and Arabic", "Our goal is to demonstrate that finitestate spelling is tractable for these complex the language and one automaton to map misspelt words into correct strings or the error model", "Both the language model and the error model are usually Pirinen and Linden 2010 weighted finitestate automata where the weights represent the probabilities are of a word being correctly spelled in the language model and of specific misspellings respectively", "We evaluate here the effect of both the language and error model automatons structure and complexity on the efficiency of the finitestate spelling task4 21 Language Models", "The most basic language model for a spellchecking dictionary is a list of correctly spelled word forms", "One of the easiest ways of creating such a spell checker is to collect the word forms from a reasonably large corpus of mostly correctly spelt texts", "Additionally we can count the frequency of wordslanguages to document their implications for per and use that as the likelihood P w  cw wD cw formance and to present an algorithm for the task", "We also point out that previous approaches have neglected to simultaneously constrain the error model and the dictionary with each other in online composition which affords a significant speed benefit compared to generating the two component compositions", "The rest of the paper is organised as follows", "In Section 2 we discuss the spellchecking task current nonfinitestate spellcheckers and previously used finitestate methods for spellchecking and correction and propose some possible speed optimisations for the error models", "We also investigate algorithmic limitations of finitestate approaches and ways to remedy them", "In Section 3 we present the language models error models and the testing corpora", "In Section 4 we present the comparisons of speed and quality with combinations of different language and error models and corpora for spellchecking", "In Section 5 we summarise our findings and results and outline future goals"]}, "W12-6202": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 10 19 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Practical Finite State Optimality Theory Dale Gerdemann University of Tubingen dgsfsnphilunituebingende Mans Hulden University of the Basque Country IXA Group IKERBASQUE Basque Foundation for Science mhuldenemailarizonaedu Abstract Previous work for encoding Optimality Theory grammars as finitestate transducers has included two prominent approaches the socalled  counting method where constraint violations are counted and filtered out to some set limit of approximability in a finitestate system and the  matching method where constraint violations in alternative strings are matched through violation alignment in order to remove suboptimal candidates In this pa per we extend the matching approach to show how not only markedness constraints but also faithfulness constraints and the interaction of the two types of constraints can be captured by the matching method This often produces exact and small FST representations for OT grammars which we illustrate with two practical example grammars We also provide a new proof of nonregularity of simple OT grammars 1 Introduction The possibility of representing Optimality Theory OT grammars Prince and Smolensky 1993 as computational models and finitestate transducers in particular has been widely studied since the inception of the theory itself In particular constructing an OT grammar stepbystep as the composition of a set of transducers akin to rewrite rule composition in Kaplan and Kay 1994 has offered the attractive possibility of simultaneously modeling OT parsing and generation as a natural consequence of the bidirectionality of finitestate trans ducers Two main approaches have received atten tion as practical options for implementing OT with finitestate transducers that of Karttunen 1998 and Gerdemann and van Noord 20001 Both ap 1Earlier finitestate approaches do exist see eg Ellison 1994 and Hammond 1997 proaches model constraint interaction by constructing a GENtransducer which is subsequently composed with filtering transducers that mark violations of constraints and remove suboptimal candidates candidates that have received more violation marks than the optimal candidate with the general template Grammar  Gen o MarkC1 o FilterC1  MarkCN o FilterCN In Karttunen s system auxiliary  counting trans ducers are created that first remove candidates with maximally k violation marks for some fixed k then k1 and so on until nothing can be removed without emptying the candidate set using a finitestate operation called priority union Gerdemann and van Noord 2000 present a similar system that they call a matching approach but which does not rely on fixing a maximal number of distinguishable violations k The matching method is a procedure by which we can in many cases though not always distinguish between infinitely many violations in a finitestate system something that is not possible when encoding OT by the alternative approach of counting violations In this paper our primary purpose is to both ex tend and simplify this  matching method We will include interaction of both markedness and faithfulness constraints MAX DEP and IDENT violations going beyond both Karttunen 1998 and Gerdemann and van Noord 2000 where only markedness constraints were modeled We shall also clarify the notation and markup used in the matching approach as well as present a set of generic trans ducer templates for EVAL by which modeling varying OT grammars becomes a simple matter of modifying the necessary constraint transducers and ordering them correctly in a series of compositions 10 We will first give a detailed explanation of the matching approach in section 2 our encoding notation and tools differ somewhat from that of Gerdemann and van Noord 2000 although the core techniques are essentially alike This is followed by an illustration of our encoding and method through a standard OT grammar example in section 3 In that section we also give examples of debugging OT grammars using standard finite state calculus methods In section 4 we also present an alternate en coding of an OT account of prosody in Karttunen 2006 illustrating devices where GEN is assumed to add metrical and stress markup in addition to changing inserting or deleting segments We also compare this grammar to both a nonOT grammar and an OT grammar of the same phenomenon described in Karttunen 2006 In section 5 we conclude with a brief discussion about the limitations of FSTbased OT grammars in light of the method developed in this paper as well as show a new proof of nonregularity of some very simple OT constraint systems"], "abstract": ["Previous work for encoding Optimality Theory grammars as finitestate transducers has included two prominent approaches the so called counting method where constraint violations are counted and filtered out to some set limit of approximability in a finitestate system and the matching method where constraint violations in alternative strings are matched through violation alignment in order to remove suboptimal candidates", "In this paper we extend the matching approach to show how not only markedness constraints but also faithfulness constraints and the interaction of the two types of constraints can be captured by the matching method", "This often produces exact and small FST representations for OT grammars which we illustrate with two practical example grammars", "We also provide a new proof of nonregularity of simple OT grammars"], "inroduction": ["The possibility of representing Optimality Theory OT grammars Prince and Smolensky 1993 as computational models and finitestate transducers in particular has been widely studied since the inception of the theory itself", "In particular constructing an OT grammar stepbystep as the composition of a set of transducers akin to rewrite rule composition in Kaplan and Kay 1994 has offered the attractive possibility of simultaneously modeling OT parsing and generation as a natural consequence of the bidirectionality of finitestate transducers", "Two main approaches have received attention as practical options for implementing OT with finitestate transducers that of Karttunen 1998 and Gerdemann and van Noord 20001 Both ap 1 Earlier finitestate approaches do exist see eg Ellison", "1994 and Hammond 1997", "proaches model constraint interaction by constructing a GENtransducer which is subsequently composed with filtering transducers that mark violations of constraints and remove suboptimal candidates candidates that have received more violation marks than the optimal candidate with the general template Grammar  Gen o MarkC1 o FilterC1 ", "MarkCN o FilterCN In Karttunens system auxiliary counting transducers are created that first remove candidates with maximally k violation marks for some fixed k then k  1 and so on until nothing can be removed without emptying the candidate set using a finitestate operation called priority union", "Gerdemann and van Noord 2000 present a similar system that they call a matching approach but which does not rely on fixing a maximal number of distinguishable violations k The matching method is a procedure by which we can in many cases though not always distinguish between infinitely many violations in a finitestate systemsomething that is not possible when encoding OT by the alternative approach of counting violations", "In this paper our primary purpose is to both extend and simplify this matching method", "We will include interaction of both markedness and faithfulness constraints MAX DEP and IDENT violationsgoing beyond both Karttunen 1998 and Gerdemann and van Noord 2000 where only markedness constraints were modeled", "We shall also clarify the notation and markup used in the matching approach as well as present a set of generic transducer templates for EVAL by which modeling varying OT grammars becomes a simple matter of modifying the necessary constraint transducers and ordering them correctly in a series of compositions", "We will first give a detailed explanation of the matching approach in section 2our encoding notation and tools differ somewhat from that of Gerdemann and van Noord 2000 although the core techniques are essentially alike", "This is followed by an illustration of our encoding and method through a standard OT grammar example in section 3", "In that section we also give examples of debugging OT grammars using standard finite state calculus methods", "In section 4 we also present an alternate encoding of an OT account of prosody in Karttunen 2006 illustrating devices where GEN is assumed to add metrical and stress markup in addition to changing inserting or deleting segments", "We also compare this grammar to both a nonOT grammar and an OT grammar of the same phenomenon described in Karttunen 2006", "In section 5 we conclude with a brief discussion about the limitations of FSTbased OT grammars in light of the method developed in this paper as well as show a new proof of nonregularity of some very simple OT constraint systems", "11 Notation", "All the examples discussed are implemented with the finitestate toolkit foma Hulden 2009b", "The regular expressions are also compilable with the Xerox tools Beesley and Karttunen 2003 although some of the tests of properties of finitestate transducers crucial for debugging are unavailable", "The regular expression formalism used is summarized in table 1"]}, "W12-6205": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 30 34 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Integrating Aspectually Relevant Properties of Verbs into a Morphological Analyzer for English"], "abstract": ["The integration of semantic properties into morphological analyzers can significantly enhance the performance of any tool that uses their output as input eg for derivation or for syntactic parsing", "In this paper will be presented my approach to the integration of aspectually relevant properties of verbs into a morphological analyzer for English"], "inroduction": ["Heid Radtke and Klosa 2012 have recently surveyed morphological analyzers and interactive online dictionaries for German and French", "They have established that most of them do not utilize semantic properties", "The integration of semantic properties into morphological analyzers can significantly enhance the performance of any tool that uses their output as input eg for derivation or for syntactic parsing", "In this paper will be presented my approach to the integration of aspectually relevant properties of verbs into a morphological analyzer for English", "In section 2 I will describe a prototypical finite state morphological analyzer for English that doesnt utilize semantic properties", "Some classifications of English verbs with respect to the aspectually relevant properties that they lexicalize will be outlined in section 3", "In section 4 will be presented my approach to the integration the semantic classes in the lexicon", "I will describe the modified morphological analyzer for English in section 5 and point out in section 6 the challenges that inflectionallyrich languages present to the techniques outlined in section 4", "Finally in section 7 I will draw some conclusions and outline future work on other languages"]}, "W12-6209": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 50 54 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Kleene a Free and OpenSource Language for FiniteState Programming Kenneth R Beesley SAP Labs LLC"], "abstract": ["Kleene is a highlevel programming language based on the OpenFst library for constructing and manipulating finitestate acceptors and transducers", "Users can program using regular expressions alternationrule syntax and rightlinear phrasestructure grammars and Kleene provides variables lists functions and familiar programcontrol syntax", "Kleene has been approved by SAP AG for release as free opensource code under the Apache License Version 20 and will be available by August 2012 for downloading from http wwwkleenelangorg", "The design implementation development status and future plans for the language are discussed"], "inroduction": ["Kleene1 is a finitestate programming language in the tradition of the ATT Lextools Roark and Sproat 20072 the SFSTPL language Schmid 20053 the XeroxPARC finitestate toolkit Beesley and Karttunen 20034 and FOMA Hulden 2009b5 all of which provide higherlevel programming for malisms built on top of lowlevel finitestate libraries", "Kleene itself is built on the OpenFst library 1 Kleene is named after American mathematician Stephen Cole Kleene 19091994 who investigated the properties of regular sets and invented the metalanguage of regular expressions", "2 httpwwwresearchattcomalb lextools 3 httpwwwimsunistuttgartde projektegramotronSOFTWARESFSThtml 4 httpwwwfsmbookcom 5 httpcodegooglecompfoma Allauzen et al 20076 developed by Google Labs and NYUs Courant Institute", "The design and implementation of the language were motivated by three main principles summarized as Syntax Matters Licensing Matters and Open Source Matters", "As for the syntax Kleene allows programmers to specify weighted or unweighted finitestate machines FSMs including acceptors that encode regular languages and twoprojection transducers that encode regular relationsusing regular expressions alternation rule syntax and rightlinear phrasestructure grammars", "The regularexpression operators are borrowed as far as possible from familiar Perllike and academic regular expressions and the alternation rules are based on the rewrite rules made popular by Chomsky and Halle Chomsky and Halle 1968", "Borrowing from generalpurpose programming languages Kleene also provides variables lists and functions plus nested code blocks and familiar control structures such as ifelse statements and while loops", "As for the licensing Kleene like the OpenFst library is released under the Apache License Version 20 and its other dependencies are also released under this and similar permissive licenses that allow commercial usage", "In contrast many notable finite state implementations released under the GPL and similar licenses are restricted to academic and other noncommercial use", "The Kleene code is also open source allowing users to examine correct augment and even adopt the code if the project should ever be abandoned by its original maintainers", "6 httpwwwopenfstorg 50 Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 5054 DonostiaSan Sebastian July 2325 2012", "Qc 2012 Association for Computational Linguistics It is hoped that Kleene will provide an attractive development environment for experts and students", "Preedited Kleene scripts can be run from the command line but a graphical user interface is also provided for interactive learning programming testing and drawing of FSMs", "Like comparable implementations of finitestate machines Kleene can be used to implement a variety of useful applications including spellchecking and correction phonetic modeling morphological analysis and generation and various kinds of pattern matching", "The paper continues with a brief description the Kleene language the current state of development and plans for the future"]}, "W12-6211": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 60 64 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics First approaches on Spanish medical record classification using Diagnostic Term to class transduction"], "abstract": ["This paper presents an application of finite state transducers to the domain of medicine", "The objective is to assign disease codes to each Diagnostic Term in the medical records generated by the Basque Health Hospital System", "As a starting point a set of manually coded medical records were collected in order to code new medical records on the basis of this set of positive samples", "Since the texts are written in natural language by doctors the same Diagnostic Term might show alternative forms", "Hence trying to code a new medical record by exact matching the samples in the set is not always feasible due to sparsity of data", "In an attempt to increase the coverage of the data our work centered on applying a set of finitestate transducers that helped the matching process between the positive samples and a set of new entries", "That is these transducers allowed not only exact matching but also approximate matching", "While there are related works in languages such as English this work presents the first results on automatic assignment of disease codes to medical records written in Spanish"], "inroduction": ["During the last years an exponential increase in the number of electronic documents in the medical domain has occurred", "The automatic processing of these documents allows to retrieve information helping the health professionals in their work", "There are different sort of valuable data that help to exploit medical information", "Our framework lays on the classification of Medical Records MRs according to a standard", "In our context the MRs produced in a hospital have to be classified with respect to the World Health Organizations 9th Revision of the International Classification of Diseases1 ICD9", "ICD9 is designed for the classification of morbidity and mortality information and for the indexing of hospital records by disease and procedure", "The already classified MRs are stored in a database that serves for further classification purposes", "Each MR consists of two pieces of information Diagnostic Terms DTs one or more terms that describe the diseases corresponding to the MR Bodytext a description of the patients details antecedents symptoms adverse effects methods of administration of medicines etc Even though the DTs are within a limited domain their description is not subject to a standard", "Doctors express the DTs in natural language with their own style and different degrees of precision", "Usually a given concept might be expressed by alternative DTs with variations due to modifiers abbreviations acronyms dates names misspellings or style", "This is a typical problem that arises in natural language processing due to the fact that doctors focus on the patients and not so much on the writing of the MR On account of this there is ample variability in the presentation of the DTs", "Consequently it is not a straightforward task to get the corresponding ICD codes", "That is the task is by far more complex than a standard dictionary lookup", "1 httpwwwcdcgovnchsicdicd9htm The Basque Health Hospital System is concerned with the automatization of this ICDcode assignment task", "So far the hospital processes the daily produced documents in the following sequence 1", "Automatic exact match of the DTs in a set of", "manually coded samples"]}, "W12-6212": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 65 69 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Developing an open source FST grammar for verb chain transfer in a SpanishBasque MT System Aingeru Mayor Mans Hulden Gorka Labaka Ixa Group University of the Basque Country aingeruehues mhuldenemailarizonaedu gorkalabakaehues Abstract This paper presents the current status of de velopment of a finite state transducer grammar for the verbalchain transfer module in Matxin a Rule Based Machine Translation system between Spanish and Basque Due to the distance between Spanish and Basque the verbalchain transfer is a very complex module in the overall system The grammar is compiled with foma an open source finitestate toolkit and yields a translation execution time of 2000 verb chainssecond 1 Introduction This paper presents the current status of development of an FST Finite State Transducer grammar we have developed for Matxin a Machine Translation system between Spanish and Basque Basque is a minority language isolate and it is likely that an early form of this language was already present in Western Europe before the arrival of the IndoEuropean languages Basque is a highly inflected language with free order of sentence constituents It is an agglutinative language with a rich flexional morphology Basque is also a socalled ergativeabsolutive language where the subjects of intransitive verbs appear in the absolutive case which is unmarked and where the same case is used for the direct object of a transitive verb The subject of the transitive verb that is the agent is marked differently with the ergative case in Basque by the suffix k The presence of this morpheme also triggers main and auxiliary verbal agreement Auxiliary verbs or periphrastic verbs which accompany most main verbs agree not only with the subject but also with the direct object and the indirect object if present Among European languages this polypersonal system multiple verb agreement is rare and found only in Basque some Caucasian languages and Hungarian The fact that Basque is both a morphologically rich and lessresourced language makes the use of statistical approaches for Machine Translation difficult and raises the need to develop a rulebased architecture which in the future could be combined with statistical techniques The Matxin eseu SpanishBasque MT engine is a classic transferbased system comprising three main modules analysis of the Spanish text based on FreeLing Atserias et al 2006 transfer and generation of the Basque target text In the transfer process lexical transfer is first carried out using a bilingual dictionary coded in the XML format of Apertium dictionary files dix Forcada et al 2009 and compiled using the FST library implemented in the Apertium project the lttoolbox library into a finitestate transducer that can be processed very quickly Following this structural transfer at the sentence level is performed and some information is trans ferred from some chunks1 to others while some chunks may be deleted Finally the structural trans1A chunk is a nonrecursive phrase noun phrase preposi tional phrase verbal chain etc which expresses a constituent Abney 1991 Civit 2003 In our system chunks play a crucial part in simplifying the translation process due to the fact that each module works only at a single level either inside or between chunks 65 fer at the verb chunk level is carried out The verbal chunk transfer is a very complex module because of the nature of Spanish and Basque auxiliary verb con structions and is the main subject of this paper This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen 2003 using the foma finitestate toolkit Hulden 2009 In total the system consists of 166 separate replacement rules that together perform the verb chunk translation In practice the input is given to the first transducer after which its output is passed to the second and so forth in a cascade Each rule in the system is unambiguous in its output that is for each input in a particular step along the verb chain transfer the transducers never produce multiple outputs ie the transducers in question are functional Some of the rules are joined together with composition yielding a total of 55 separate transducers In principle all the rules could be composed together into one monolithic transducer but in practice the size of the composed transducer is too large to be feasible The choice to combine some transducers while leaving others separate is largely a memorytranslation speed tradeoff 2 Spanish and Basque verb features and their translation In the following we will illustrate some of the main issues in translating Spanish verb chains to Basque Since both languages make frequent use of auxiliary verb constructions and since periphrastic verb constructions are frequent in Basque transfer rules can get quite complex in their design For example in translating the phrase Yo compro una manzana"], "abstract": ["This paper presents the current status of development of a finite state transducer grammar for the verbalchain transfer module in Matxin a Rule Based Machine Translation system between Spanish and Basque", "Due to the distance between Spanish and Basque the verbalchain transfer is a very complex module in the overall system", "The grammar is compiled with foma an opensource finite state toolkit and yields a translation execution time of 2000 verb chainssecond"], "inroduction": ["This paper presents the current status of development of an FST Finite State Transducer grammar we have developed for Matxin a Machine Translation system between Spanish and Basque", "Basque is a minority language isolate and it is likely that an early form of this language was already present in Western Europe before the arrival of the IndoEuropean languages", "Basque is a highly inflected language with free order of sentence constituents", "It is an agglutinative language with a rich flexional morphology", "Basque is also a socalled ergativeabsolutive language where the subjects of intransitive verbs appear in the absolutive case which is unmarked and where the same case is used for the direct object of a transitive verb", "The subject of the transitive verb that is the agent is marked differently with the ergative case in Basque by the suffix k", "The presence of this morpheme also triggers main and auxiliary verbal agreement", "Auxiliary verbs or periphrastic verbs which accompany most main verbs agree not only with the subject but also with the direct object and the indirect object if present", "Among European languages this polypersonal system multiple verb agreement is rare and found only in Basque some Caucasian languages and Hungarian", "The fact that Basque is both a morphologically rich and lessresourced language makes the use of statistical approaches for Machine Translation difficult and raises the need to develop a rulebased architecture which in the future could be combined with statistical techniques", "The Matxin eseu SpanishBasque MT engine is a classic transferbased system comprising three main modules analysis of the Spanish text based on FreeLing Atserias et al 2006 transfer and generation of the Basque target text", "In the transfer process lexical transfer is first carried out using a bilingual dictionary coded in the XML format of Apertium dictionary files dix Forcada et al 2009 and compiled using the FST library implemented in the Apertium project the lt toolbox library into a finitestate transducer that can be processed very quickly", "Following this structural transfer at the sentence level is performed and some information is transferred from some chunks1 to others while some chunks may be deleted", "Finally the structural trans 1 A chunk is a nonrecursive phrase noun phrase prepositional phrase verbal chain etc which expresses a constituent Abney 1991 Civit 2003", "In our system chunks play a crucial part in simplifying the translation process due to the fact that each module works only at a single level either inside or between chunks", "65 Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 6569 DonostiaSan Sebastian July 2325 2012", "Qc 2012 Association for Computational Linguistics fer at the verb chunk level is carried out", "The verbal chunk transfer is a very complex module because of the nature of Spanish and Basque auxiliary verb constructions and is the main subject of this paper", "This verb chain transfer module is implemented as a series of ordered replacement rules Beesley and Karttunen 2003 using the foma finitestate toolkit Hulden 2009", "In total the system consists of 166 separate replacement rules that together perform the verb chunk translation", "In practice the input is given to the first transducer after which its output is passed to the second and so forth in a cascade", "Each rule in the system is unambiguous in its output that is for each input in a particular step along the verb chain transfer the transducers never produce multiple outputs ie the transducers in question are functional", "Some of the rules are joined together with composition yielding a total of 55 separate transducers", "In principle all the rules could be composed together into one monolithic transducer but in practice the size of the composed transducer is too large to be feasible", "The choice to combine some transducers while leaving others separate is largely a memorytranslation speed tradeoff"]}, "W12-6213": {"title": ["Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing pages 70 74 Donostia San Sebastian July 23 25 2012 c  2012 Association for Computational Linguistics Conversion of Procedural Morphologies to FiniteState Morphologies a Case Study of Arabic Mans Hulden University of the Basque Country IXA Group IKERBASQUE Basque Foundation for Science mhuldenemailarizonaedu Younes Samih HeinrichHeineUniversita t Du sseldorf samihphiluniduesseldorfde Abstract In this paper we describe a conversion of the Buckwalter Morphological Analyzer for Arabic originally written as a Perlscript into a pure finitestate morphological analyzer Representing a morphological analyzer as a finitestate transducer FST con fers many advantages over running a procedural affixmatching algorithm Apart from ap plication speed an FST representation immediately offers various possibilities to flexibly modify a grammar In the case of Arabic this is illustrated through the addition of the ability to correctly parse partially vocalized forms without overgeneration something not possible in the original analyzer as well as to serve both as an analyzer and a generator 1 Introduction Many lexicon driven morphological analysis systems rely on a general strategy of breaking down input words into constituent parts by consulting customized lexicons and rules designed for a particular language The constraints imposed by the lexica designed are then implemented as program code that handles co occurrence restrictions and analysis of possible orthographic variants finally producing a parse of the input word Some systems designed along these lines are meant for general use such as the hunspell tool Halacsy et al 2004 which allows users to specify lexicons and constraints while others are languagedependent such as the Buckwalter Arabic Morphological Analyzer BAMA Buckwalter 2004 In this paper we examine the possibility of converting such morphological analysis tools to FSTs that perform the same task As a case study we have chosen to implement a one toone faithful conversion of the Buckwalter Arabic analyzer into a finitestate representation using the foma finite state compiler Hulden 2009b while also adding some ex tensions to the original analyzer These are useful extensions which are difficult to add to the original Perlbased analyzer because of its procedural nature but very straightforward to perform in a finitestate environment using standard design techniques There are several advantages to representing morphological analyzers as FSTs as is well noted in the literature Here in addition to documenting the conversion we shall also discuss and give examples of the flexibility extensibility and speed of application which results from using a finitestate representation of a morphology1 2 The Buckwalter Analyzer Without going into an extensive linguistic discussion we shall briefly describe the widely used Buckwalter morphological analyzer for Arabic The BAMA accepts as input Arabic words with or without vocalization and produces as output a breakdown of the affixes participating in the word the stem together with information about conjugation classes For example for the input word ktbI J BAMA returns among others LOOKUP WORD ktb SOLUTION 1 kataba katabu1 katabVERBPERFECT aPVSUFFSUBJ3MS GLOSS  write  heit verb 1The complete code and analyzer are available at httpbuckwalterfstgooglecodecom 70 Figure 1 The Buckwalter Arabic Morphological Analyzers lookup process exemplified for the word lilkitAbi"], "abstract": ["In this paper we describe a conversion of the Buckwalter Morphological Analyzer for Arabic originally written as a Perlscript into a pure finitestate morphological analyzer", "Representing a morphological analyzer as a finitestate transducer FST confers many advantages over running a procedural affixmatching algorithm", "Apart from application speed an FST representation immediately offers various possibilities to flexibly modify a grammar", "In the case of Arabic this is illustrated through the addition of the ability to correctly parse partially vocalized forms without overgeneration something not possible in the original analyzer as well as to serve both as an analyzer and a generator"], "inroduction": ["Many lexicondriven morphological analysis systems rely on a general strategy of breaking down input words into constituent parts by consulting customized lexicons and rules designed for a particular language", "The constraints imposed by the lexica designed are then implemented as program code that handles cooccurrence restrictions and analysis of possible orthographic variants finally producing a parse of the input word", "Some systems designed along these lines are meant for general use such as the hunspell tool Halacsy et al 2004 which allows users to specify lexicons and constraints while others are languagedependent such as the Buckwalter Arabic Morphological Analyzer BAMA Buckwalter 2004", "In this paper we examine the possibility of converting such morphological analysis tools to FSTs that perform the same task", "As a case study we have chosen to implement a onetoone faithful conversion of the Buckwalter Arabic analyzer into a finite state representation using the foma finite state compiler Hulden 2009b while also adding some extensions to the original analyzer", "These are useful extensions which are difficult to add to the original Perlbased analyzer because of its procedural nature but very straightforward to perform in a finitestate environment using standard design techniques", "There are several advantages to representing morphological analyzers as FSTs as is well noted in the literature", "Here in addition to documenting the conversion we shall also discuss and give examples of the flexibility extensibility and speed of application which results from using a finitestate representation of a morphology1"]}, "W13-2101": {"title": ["Aligning Formal Meaning Representations with Surface Strings for"], "abstract": ["Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of textmeaning pairs", "Even though such corpora exist nowadays or could be constructed using robust semantic parsing the simple alignment between text and meaning representation is too coarse for developing robust statistical NLG systems", "By reformatting semantic representations as graphs finegrained alignment can be obtained", "Given a precise alignment at the word level the complete surface form of a meaning representations can be deduced using a simple declarative rule"], "inroduction": ["Surface Realization is the task of producing fluent text from some kind of formal abstract representation of meaning Reiter and Dale 2000", "However while it is obvious what the output of a natural language generation component should be namely text there is little to no agreement on what its input formalism should be Evans et al 2002", "Since opendomain semantic parsers are able to produce formal semantic representations nowadays Bos 2008 Butler and Yoshimoto 2012 it would be natural to see generation as a reversed process and consider such semantic representations as input of a surface realization component", "The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently Basile and Bos 2011 Wanner et al 2012", "The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power 1999 who derives semantic networks enriched with scope information from knowledge representations for content planning", "In this paper we take a further step towards the goal of generating text from deep semantic representations and consider the issue of aligning the representations with surface strings that capture their meaning", "First we describe the basic idea of aligning semantic representations logical forms with surface strings in a formalismindependent way Section 2", "Then we apply our method to a wellknown and widelyused semantic formalism namely Discourse Representation Theory DRT first demonstrating how to represent Discourse Representation Structures DRSs as graphs Section 3 and showing that the resulting Discourse Representation Graphs DRGs are equivalent to DRSs but are more convenient to fulfill word level alignment Section 4", "Finally in Section 5 we present a method that generates partial surface strings for each discourse referent occurring in the semantic representation of a text and composes them into a complete surface form", "All in all we think this would be a first and important step in surface realization from formal semantic representations"]}, "W13-2501": {"title": ["Proceedings of the 6th Workshop on Building and Using Comparable Corpora pages 110 Sofia Bulgaria August 8 2013 c  2013 Association for Computational Linguistics Crosslingual WSD for Translation Extraction from Comparable Corpora Marianna Apidianaki LIMSICNRS Rue John Von Neumann BP 133 91403 Orsay Cedex France mariannalimsifr Nikola Ljubesic Dept of Information Sciences University of Zagreb Ivana Luc ic a 3 HR10000 Zagreb Croatia nljubesiffzghr Darja Fiser Department of Translation University of Ljubljana As kerc eva 2 SI1000 Ljubljana Slovenia darjafiserffuniljsi Abstract We propose a datadriven approach to en hance translation extraction from comparable corpora Instead of resorting to an external dictionary we translate source vector features by using a crosslingual Word Sense Disambiguation method The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature The translations found in the disambiguation output con vey the sense of the features in the source vector while the use of translation clusters permits to expand their translation with several variants As a consequence the translated vectors are less noisy and richer and allow for the extraction of higher quality lexicons compared to simpler methods 1 Introduction Largescale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for lowresourced languages and to complement and reduce the sparseness of existing resources Munteanu and Marcu 2005 Snover et al 2008 The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts Fung 1998 Rapp 1999 So in order to identify a translation correspondence between the two languages the contexts of the source word and the candidate translation have to be compared For this comparison to take place the same vector space has to be produced which means that the vectors of the one language have to be translated in the other language This generally assumes the availability of a bilingual dictionary which might however not be the case for some language pairs and domains Moreover the classic way in which a dictionary is put into use which consists in trans lating vector features by their first translation in the dictionary neglects semantics We expect that a method capable of identifying the correct sense of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons In this paper we show how source vectors can be translated into the target language by a crosslingual Word Sense Disambiguation WSD method which exploits the output of datadriven Word Sense Induction WSI Apidianaki 2009 and demonstrate how feature disambiguation en hances the quality of the translations extracted from the comparable corpus This study extends our previous work on the topic Apidianaki et al 2012 by applying the proposed methods to a comparable corpus of general language built from Wikipedia and optimizing various parameters that affect the quality of the extracted translations We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general mixeddomain corpus Our experiments are carried out on the EnglishSlovene language pair but as the methods are totally datadriven the approach can be easily applied to other languages The paper is organized as follows In the next section we present some related work on bilingual lexicon extraction from comparable corpora Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup In Section 5 we report and discuss the obtained results before concluding and presenting some directions for future work"], "abstract": ["We propose a datadriven approach to enhance translation extraction from comparable corpora", "Instead of resorting to an external dictionary we translate source vector features by using a crosslingual Word Sense Disambiguation method", "The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature", "The translations found in the disambiguation output convey the sense of the features in the source vector while the use of translation clusters permits to expand their translation with several variants", "As a consequence the translated vectors are less noisy and richer and allow for the extraction of higher quality lexicons compared to simpler methods"], "inroduction": ["Largescale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications", "Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for lowresourced languages and to complement and reduce the sparseness of existing resources Munteanu and Marcu 2005 Snover et al 2008", "The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts Fung 1998 Rapp 1999", "So in order to identify a translation correspondence between the two languages the contexts of the source word and the candidate translation have to be compared", "For this comparison to take place the same vector space has to be produced which means that the vectors of the one language have to be translated in the other language", "This generally assumes the availability of a bilingual dictionary which might however not be the case for some language pairs and domains", "Moreover the classic way in which a dictionary is put into use which consists in translating vector features by their first translation in the dictionary neglects semantics", "We expect that a method capable of identifying the correct sense of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons", "In this paper we show how source vectors can be translated into the target language by a crosslingual Word Sense Disambiguation WSD method which exploits the output of datadriven Word Sense Induction WSI Apidianaki 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus", "This study extends our previous work on the topic Apidianaki et al 2012 by applying the proposed methods to a comparable corpus of general language built from Wikipedia and optimizing various parameters that affect the quality of the extracted translations", "We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general mixeddomain corpus", "Our experiments are carried out on the EnglishSlovene language pair but as the methods are totally datadriven the approach can be easily applied to other languages", "The paper is organized as follows In the next section we present some related work on bilingual lexicon extraction from comparable corpora", "Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup", "In Section 5 we report and discuss the obtained results before concluding and presenting some directions for future work", "1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora pages 110 Sofia Bulgaria August 8 2013", "Qc 2013 Association for Computational Linguistics"]}, "W13-2512": {"title": ["Proceedings of the 6th Workshop on Building and Using Comparable Corpora pages 95104 Sofia Bulgaria August 8 2013 c  2013 Association for Computational Linguistics Using a Random Forest Classifier to recognise translations of biomedical terms across languages Georgios Kontonatsios12 Ioannis Korkontzelos12 Jun ichi Tsujii3 Sophia Ananiadou12 National Centre for Text Mining University of Manchester Manchester UK1 School of Computer Science University of Manchester Manchester UK2 Microsoft Research Asia Beijing China3 gkontonatsiosikorkontzelossananiadoucsmanacuk jtsujiimicrosoftcom Abstract We present a novel method to recognise semantic equivalents of biomedical terms in language pairs We hypothesise that biomedical term are formed by semantically similar textual units across languages Based on this hypothesis we employ a Random Forest RF classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples We apply our method on two language pairs one that uses the same character set and another with a different script EnglishFrench and EnglishChinese respectively We show that EnglishFrench pairs of terms are highly transliterated in contrast to the EnglishChinese pairs Nonetheless our method performs robustly on both cases We evaluate RF against a stateoftheart alignment method GIZA and we report a statistically significant improvement Finally we compare RF against Support Vector Machines and analyse our results 1 Introduction Given a term in a source language and term in a target language the task of this paper is to classify this pair as a translation or not We investigate the performance of the proposed classifier by applying it on a balanced classification problem ie our experimental datasets contain an equal number of positive and negative examples The proposed classification model can be used as a component of a larger system that automatically compiles bilingual dictionaries of technical terms across languages Bilingual dictionaries of terms are important resources for many Natural Language Processing NLP applications including Statistical Machine Translation SMT Feng et al 2004 Huang and Vogel 2002 Wu et al 2008 CrossLanguage Information Retrieval Ballesteros and Croft 1997 and Question Answering systems AlOnaizan and Knight 2002 Especially in the biomedical domain manually creating and more importantly updating such resources is an expensive process due to the vast amount of neologisms"], "abstract": ["We present a novel method to recognise semantic equivalents of biomedical terms in language pairs", "We hypothesise that biomedical term are formed by semantically similar textual units across languages", "Based on this hypothesis we employ a Random Forest RF classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples", "We apply our method on two language pairs one that uses the same character set and another with a different script EnglishFrench and EnglishChinese respectively", "We show that EnglishFrench pairs of terms are highly transliterated in contrast to the EnglishChinese pairs", "Nonetheless our method performs robustly on both cases", "We evaluate RF against a stateoftheart alignment method GIZA and we report a statistically significant improvement", "Finally we compare RF against Support Vector Machines and analyse our results"], "inroduction": ["Given a term in a source language and term in a target language the task of this paper is to classify this pair as a translation or not", "We investigate the performance of the proposed classifier by applying it on a balanced classification problem ie our experimental datasets contain an equal number of positive and negative examples", "The proposed classification model can be used as a component of a larger system that automatically compiles bilingual dictionaries of technical terms across languages", "Bilingual dictionaries of terms are important resources for many Natural Language Processing NLP applications including Statistical Machine Translation SMT Feng et al 2004 Huang and Vogel 2002 Wu et al 2008 Cross Language Information Retrieval Ballesteros and Croft 1997 and Question Answering systems AlOnaizan and Knight 2002", "Especially in the biomedical domain manually creating and more importantly updating such resources is an expensive process due to the vast amount of neologisms ie newly introduced terms Pustejovsky et al 2001", "The UMLS metathesaurus which is one the most popular hub of multilingual resources in the biomedical domain contains technical terms in 21 languages that are linked together using a concept identifier", "In Spanish the second most popular language in UMLS only 1644 of the 76M English terms are covered while other languages fluctuate between 00052 for Hebrew terms to 326 for Japanese terms", "Hence these lexica are far for complete and methods that semi automatically ie in a postprocessing step curators can manually remove erroneous dictionary entries discover pairs of terms across languages are needed to enrich such multilingual resources", "Our method can be applied to parallel aligned corpora where we expect approximately the same balanced classification problem", "However in comparable corpora the search space of candidate alignments is of vast size ie quadratic the the size of the input data", "To cope with this heavily unbalanced classification problem we would need to narrow down the number of negative instances before classification", "We hypothesise that there are language independent rules that apply to biomedical terms across many languages", "Often the same or similar textual units eg morphemes and suffixes are concatenated to realise the same terms in different languages", "For example Table 1 illustrates how a morpheme expressing pain ache in English is used to realise the same terms in English Chinese and French", "The realisations of the term head 95 Proceedings of the 6th Workshop on Building and Using Comparable Corpora pages 95104 Sofia Bulgaria August 8 2013", "Qc 2013 Association for Computational Linguistics English Morpheme ache Chinese Morpheme  French Morpheme mal headache  mal de tete backache  mal au dos earache  mal doreille Table 1 An example of English Chinese and French terms consisting of the same morphemes ache is expected to consist of the units for head and ache regardless of the language of realisation", "Hence knowing the translations of head and ache allows the reconstruction headache in a target language", "In our method we use a Random Forest RF classifier Breiman 2001 to learn the underlying rules according to which terms are being constructed across languages", "An RF is an ensemble of Decision Trees voting for the most popular class", "RF classifiers are popular in the biomedical domain for various tasks classification of microarray data DazUriarte and De Andres 2006 compound classification in cheminformatics Svetnik et al 2003 classification of microRNA data Jiang et al 2007 and proteinprotein interactions in Systems Biology Chen and Liu 2005", "In NLP RF classifiers have been used for Language Modelling Xu and Jelinek 2004 and semantic parsing Nielsen and Pradhan 2004", "To the best of the authors knowledge this is the first attempt to employ RF for identifying translation equivalents of biomedical terms", "We prefer RF over other traditional machine learning approaches such as Support Vector Machines SVMs for a number of reasons", "Firstly RF is able to automatically construct correlation paths from the feature space ie decision rules that correspond to the translation rules that we intend to capture", "Secondly RF is considered one of the most accurate classifier available DazUriarte and De Andres 2006 Jiang et al 2007", "Finally RF is reported to cope well with datasets where the number of features is larger than the number of observations DazUriarte and De Andres 2006", "In our dataset the number of features is almost four times more than that of the observations", "We represent pairs of terms using character gram features ie first order features", "Such shallow features have been proven effective in a number of NLP applications including Named Entity Recognition Klein et al 2003 Multilingual Named Entity Transliteration Klementiev and Roth 2006 Freitag and Khadivi 2007 and predicting authorship Stamatatos 2006", "In addition by selecting character ngrams instead of word ngrams one avoids to segment words in Chinese which has been proven to be a challenging topic Sproat and Emerson 2003", "We evaluate our proposed method on two datasets of biomedical terms EnglishFrench and EnglishChinese that contain equal numbers of positive and negative instances", "RF achieves higher classification performance than baseline methods", "To boost SVMs performance further we used a second order feature space to represent the data", "It consists of pairs of character grams that cooccur in translation pairs", "In the second order feature space the performance of SVMs improved significantly", "The rest of the paper is structured as follows", "In Section 2 we present previous approaches in identifying translation equivalents of terms or named entities", "In Section 3 we define the classification problem we formulate the RF classifier and we discuss the first and second order feature space that we use to represent pairs of terms", "In Section 4 we show that RF achieves superior classification performance", "In Section 5 we overview our method and we discuss how it can be used to compile largescale bilingual dictionaries of terms from comparable corpora"]}, "W13-2708": {"title": ["55"], "abstract": ["We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific complex concepts within large amounts of text", "Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for NLP", "It is of particular importance to find a common language between the different disciplines", "Therefore we use an interactive webinterface which is easily usable by nonexperts", "It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection", "Political scientists are thus enabled to use their own intuitions to find custom concepts"], "inroduction": ["In this paper we give examples of how NLP methods and tools can be used to provide support for complex tasks in political sciences", "Many concepts of political science are complex and faceted they tend to come in different linguistic realizations often in complex ones many concepts are not directly identifiable by means of a small set of individual lexical items but require some interpretation", "Many researchers in political sciences either work qualitatively on small amounts of data which precise results due to a rather unspecific search as well as semantically invalid or ambigious search words", "On the other hand large amounts of eg news texts are available also over longer periods of time such that eg tendencies over time can be derived", "The corpora we are currently working on contain ca", "700000 articles from British Irish German and Austrian newspapers as well as yet unexplored material in French", "Figure 1 depicts a simple example of a quantitative analysis1 The example shows how often two terms Friedensmissionpeace operation and Auslandseinsatzforeign intervention are used in the last two decades in newspaper texts about interventions and wars", "The longterm goal of the project is to provide similar analysis for complex concepts", "An example of a complex concept is the evocation of collective identities in political contexts as indirect in the news", "Examples for such collective identities are the Europeans the French the Catholics", "The objective of the work we are going to discuss in this paper is to provide NLP methods and tools for assisting political scientists in the exploration of large data sets with a view to both a detailed qualitative analysis of text instances and a quantitative overview of trends over time at the level of corpora", "The examples discussed here have to do with possibly multiple collective identities", "Typical context of such identities tend to report communication as direct or as indirect speech", "Examples of such contexts are given in 1they interpret instancewise or if they are in 1 Die Europaer wu rden die Lu cke fu llen terested in quantitative trends they use comparatively simple tools such as keywordbased search The Europeans would the gap fill in corpora or text classification on the basis of terms only this latter approach may lead to im 1 The figure shows a screenshot of our webbased prototype", "55 Proceedings of the 7th Workshop on Language Technology for Cultural Heritage Social Sciences and Humanities pages 5564 Sofia Bulgaria August 8 2013", "Qc 2013 Association for Computational Linguistics Figure 1 The screenshot of our webbased system shows a simple quantitative analysis of the frequency of two terms in news articles over time", "While in the 90s the term Friedensmission peace operation was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz foreign intervention being now frequently used", "sagte Ru he", "lated to the infrastructural standards in use in the said Ru he", "CLARIN communit y In section 4 we exemplify The Europeans would fill the gap Ru he said The tool support is meant to be semiautomatic as the automatic tools propose candidates that need to be validated or refused by the political scientists", "We combine a chain of corpus processing tools with classifierbased tools eg for topic classifiers commentaryreport classifiers etc make the tools interoperable to ensure flexible data exchange and multiple usage scenarios and we embed the tool collection under a web service  based user interface", "The remainder of this paper is structured as follows", "In section 2 we present an outline of the architecture of our tool collection and we motivate the architecture", "Section 3 presents examples of implemented modules both from corpus processing and search and retrieval of instances of complex concepts", "We also show how our tools are re the intended use of the methods with case studies about steps necessary for identifying evocation being able to separate reports from comments and strategies for identifying indirect speech", "Section 6 is devoted to a conclusion and to the discussion of future work"]}, "W13-3208": {"title": ["Determining Compositionality of Word Expressions Using"], "abstract": ["This paper presents a comparative study of 5 different types of Word Space Models WSMs combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions", "Many combinations of WSMs and mea"], "inroduction": ["before", "The study follows Biemann and Giesbrecht 2011 who attempted to find a list of expressions for which the composition ality assumption  the meaning of an expression is determined by the meaning of its constituents and their combination  does not hold", "Our results are very promising and can be appreciated by those interested in WSMs compositionality andor relevant evaluation methods", "1 Introduction", "Our understanding of WSM is in agreement with Sahlgren 2006 The word space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terms of spatial proximity", "There are many types of WSMs built by different algorithms", "WSMs are based on the Harris distributional hypothesis Harris 1954 which assumes that words are similar to the extent to which they share similar linguistic contexts", "WSM can be viewed as a set of words associated with vectors representing contexts in which the words occur", "Then similar vectors imply semantic similarity of the words and vice versa", "Consequently WSMs provide a means to find words semantically similar to a given word", "This capability of WSMs is exploited by many Natural Language Processing NLP applications as listed eg by Turney and Pantel 2010", "This study follows Biemann and Giesbrecht 2011 who attempted to find a list of non compositional expressions whose meaning is not fully determined by the meaning of its constituents and their combination", "The task turned out to be frustratingly hard Johannsen et al 2011", "Biemanns idea and motivation is that non compositional expressions could be treated as single units in many NLP applications such as Information Retrieval Acosta et al 2011 or Machine Translation Carpuat and Diab 2010", "We extend this motivation by stating that WSMs could also benefit from a set of noncompositional expressions", "Specifically WSMs could treat semantically noncompositional expressions as single units", "As an example consider kick the bucket hot dog or zebra crossing", "Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents kick bucket dog or zebra but instead to the whole expressions", "Recent works including that of Lin 1999 Baldwin et al", "2003 Biemann and Giesbrecht 2011 Johannsen et al", "2011 Reddy et al", "2011a Krcmar et al", "2012 and Krcmar et al", "2013 show the applicability of WSMs in determining the compositionality of word expressions", "The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets", "First this leads to nondirectly comparable results and second many combinations of 64 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality pages 6473 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics WSMs and measures have never before been applied to the task", "The main contribution and novelty of our study lies in systematic research of several basic and also advanced WSMs combined with all the so far to the best of our knowledge proposed WSMbased measures for determining the semantic compositionality", "The explored WSMs described in more detail in Section 2 include the Vector Space Model noted as log or cij denoted as sqrt", "The purpose of local weighting is to lower the importance of highly occurring words in the document", "The global function weights every value in row i of C by the same value calculated for row i Typically none denoted as N o Inverse Document Frequency denoted as I df  or a function referred to as Entropy Ent", "I df is calculated as 1  logndocsdf i and Ent Latent Semantic Analysis Hyperspace Analogue as 1   j pi j log pi j log ndocs where to Language Correlated Occurrence Analogue to Lexical Semantics and Random Indexing", "The measures including substitutability endocentricity compositionality and neighborsincommon based are described in detail in Section 3", "Section 4 describes our experiments performed on the manually annotated datasets  Distributional Semantics and Compositionality dataset DISCO and the dataset built by Reddy et al", "2011a", "Section 5 summarizes the results and Section 6 concludes the paper"]}, "W13-3209": {"title": ["Not not bad is not bad A distributional  account of negation"], "abstract": ["With the increasing empirical success of distributional models of compositional semantics it is timely to consider the types of textual logic that such models are capable of capturing", "In this paper we address shortcomings in the ability of current models to capture logical operations such as negation", "As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models"], "inroduction": ["Distributional models of semantics characterize the meanings of words as a function of the words they cooccur with Firth 1957", "These models mathematically instantiated as sets of vectors in high dimensional vector spaces have been applied to tasks such as thesaurus extraction Grefenstette 1994 Curran 2004 wordsense discrimination Schu tze 1998 automated essay marking Landauer and Dumais 1997 and so on", "During the past few years research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences", "This move from word to sentence has yielded models applied to tasks such as paraphrase detection Mitchell and Lapata 2008 Mitchell and Lapata 2010 Grefenstette and Sadrzadeh 2011 Blacoe and Lapata 2012 sentiment analysis Socher et al 2012 Hermann and Blunsom 2013 and semantic relation classification ibid", "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations Mitchell and Lapata 2008 Mitchell and Lapata 2010 Zanzotto et al 2010 matrix or tensorbased approaches Baroni and Zamparelli 2010 Coecke et al 2010 Grefenstette et al 2013 Kartsaklis et al 2012 or through the use of recursive autoencoding Socher et al 2011 Hermann and Blunsom 2013 or neuralnetworks Socher et al 2012", "On the noncompositional front Erk and Pado 2008 keep word vectors separate using syntactic information from sentences to disambiguate words in context likewise Turney 2012 treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition", "These compositional distributional approaches often portray themselves as attempts to reconcile the empirical aspects of distributional semantics with the structured aspects of formal semantics", "However they in fact only principally coopt the syntaxsensitivity of formal semantics while mostly eschewing the logical aspects", "Expressing the effect of logical operations in high dimensional distributional semantic models is a very different task than in boolean logic", "For example whereas predicates such as red are seen in predicate calculi as functions mapping elementsof some set Mred to T and all other domain elements to  in compositional distributional mod els we give the meaning of red a vectorlike representation and devise some combination operation with noun representations to obtain the representation for an adjectivenoun pair", "Under the logical view negation of a predicate therefore yields a new truthfunction mapping elements ofthe complement of Mred to T and all other do main elements to  but the effect of negation and other logical operations in distributional models is not so sharp we expect the representation for not red to remain close to other objects of the same domain of discourse ie other colours while being sufficiently different from the representation of red in some manner", "Exactly how textual logic 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality pages 7482 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics would best be represented in a continuous vector space model remains an open problem", "In this paper we propose one possible formulation for a continuous vector space based representation of semantics", "We use this formulation as the basis for providing an account of logical operations for distributional models", "In particular we focus on the case of negation and how it might work in higher dimensional distributional models", "Our formulation separates domain value and functional representation in such a way as to allow negation to be handled naturally", "We explain the linguistic and modelrelated impacts of this mode of representation and discuss how this approach could be generalised to other semantic functions", "In Section 2 we provide an overview of work relating to that presented in this paper covering the integration of logical elements in distributional models and the integration of distributional elements in logical models", "In Section 3 we introduce and argue for a tripartite representation in distributional semantics and discuss the issues relating to providing a linguistically sensible notion of negation for such representations", "In Section 4 we present matrixvector models similar to that of Socher et al", "2012 as a good candidate for expressing this tripartite representation", "We argue for the elimination of nonlinearities from such models and thus show that negation cannot adequately be captured", "In Section 5 we present a short analysis of the limitation of these matrix vector models with regard to the task of modelling nonboolean logical operations and present an improved model bypassing these limitations in Section 6", "Finally in Section 7 we conclude by suggesting future work which will extend and build upon the theoretical foundations presented in this paper"]}, "W13-3306": {"title": ["43"], "abstract": ["The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives", "The goldstandard discourse relation annotation leads to better translation performance in ranges of 460 for some ambiguous English connectives and helps to find correct syntactical constructs in Czech for less ambiguous connectives", "Automatic scoring confirms the stability of the newly built discourse aware translation systems", "Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful"], "inroduction": ["Recently research in statistical machine translation SMT has renewed interest in the fact that for a variety of linguistic phenomena one needs information from a longerrange context", "Current statistical translation models and decoding algorithms operate at the sentence andor phrase level only not considering already translated context from previous sentences", "This local distance is in many cases too restrictive to correctly model lexical cohesion referential expressions noun phrases pronouns and discourse markers all of which relate to the sentences before the one to be translated", "Discourse relations between sentences are often conveyed by explicit discourse connectives DC such as although because but since while", "DCs play a significant role in coherence and readability of a text", "Likewise if a wrong connective is used in translation the target text can be fully incomprehensible or not conveying the same meaning as was established by the discourse relations in the source text", "In English about 100 types of such explicit connectives have been annotated in the Penn Discourse TreeBank PDTB see Section 4 signaling discourse relations such as temporality or contrast between two spans of text", "Depending on the set of relations used there can be up to 130 such relations and combinations thereof", "Discourse relations can also be present implicitly inferred from the context without any explicit marker being present", "Although annotation for implicit DCs exists as well we only deal with explicit DCs in this paper", "DCs are difficult to translate mainly because a same English connective can signal different discourse relations in different contexts and when the target language has either different connectives according to the source relations signaled or uses different lexical or syntactical constructs in place of the English connective", "In this paper we present MT experiments from English EN to Czech CZ with a large amount of manually annotated DCs", "The corpus the parallel Prague CzechEnglish Dependency Treebank PCEDT Section 4 is directly usable for MT experiments the entire discourse annotation in EN is paralleled with a human CZ translation", "This means that we can build and evaluate against the CZ reference a translation system that learns from the EN gold standard discourse relations", "These then have no distortion from wrongly labeled connectives as it is given in related work Section 3 where automatic classifiers have been used to label the connectives with a certain error rate", "Furthermore we can use the sense labels for 100 types of EN connectives whereas related work only focused on a few highly ambiguous connectives that are especially problematic for translation", "The paper starts by illustrating difficult translations involving connectives Section 2 and discusses related work in Section 3", "The resources and data used are introduced in Section 4", "The MT experiments are explained in Section 5 and 43 Proceedings of the Workshop on Discourse in Machine Translation DiscoMT pages 4350 Sofia Bulgaria August 9 2013", "Qc 2013 Association for Computational Linguistics automatic evaluation is given in Section 6", "We further provide a detailed manual evaluation and error analysis for the CZ translations generated by our SMT systems Section 7", "Future work described in Section 8 concludes the paper"]}, "W13-4904": {"title": ["Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 34 45 Seattle Washington USA 18 October 2013 c  2013 Association for Computational Linguistics A CrossTask Flexible Transition Model for Arabic Tokenization Affix Detection Affix Labeling POS Tagging and Dependency Parsing Stephen Tratz Army Research Laboratory Adelphi Laboratory Center 2800 Powder Mill Road Adelphi MD 20783 stephenctratzcivmailmil Abstract This paper describes crosstask flexible transition models CTFTMs and demonstrates their effectiveness for Arabic natural language processing NLP NLP pipelines often suffer from error propagation as errors committed in lowerlevel tasks cascade through the remainder of the processing pipeline By allowing a flexible order of operations across and within multiple NLP tasks a CTFTM can mitigate both crosstask and withintask error propagation Our Arabic CTFTM models tokenization affix detection affix labeling partofspeech tagging and dependency parsing achieving stateoftheart results We present the details of our general framework our Arabic CTFTM and the setup and results of our experiments 1 Introduction Natural Language Processing NLP systems often consist of a series of NLP components each trained to perform a specific task such as parsing These pipelines tend to suffer from error propagation errors introduced by early components cascade through the remainder of the pipeline causing subsequent components to commit additional errors Partial solutions from higherlevel tasks eg parsing can aid in resolving the difficult decisions that must be made in solving lowerlevel tasks as with partofspeech tagging the classic  garden path sentence example  The horse raced past the barn fell To this end this paper presents crosstask flexible transition models CTFTMs which model multiple tasks and solve these tasks in a more flexible order than pipeline approaches We implement and experiment with a CTFTM for Arabic1 language processing and report experimental results for it on Arabic tokenization ie clitic separation affix detection affix labeling partofspeech tagging and dependency parsing In addition to error propagation between modules within a parsing pipeline errors may propagate within the parsing process itself due to the fixed order of operations of the parser This is common for standard transitionbased dependency parsing models McDonald and Nivre 2007 such as shiftreduce parsers which incrementally construct a parse by processing the input in a fixed lefttoright or righttoleft fashion However using a transition model that allows a more flexible order of operations such as Goldberg and Elhadad s 2010 parser allows difficult decisions to be postponed until later when more of the solution has been constructed CTFTMs extend this approach by modeling multiple tasks and providing this flexibility across tasks so that no one task needs to be complete before an other can be partially solved As a morphologically rich language Arabic requires a significant number of processing steps Arabic uses a variety of affixes to inflect for case gender number including dual and mood has clitics that attach to other words permits both VSO and SVO constructions and rarely includes short vowels in written form The presence of clitics and the absence of written short vowels are particularly significant sources of ambiguity As Tsarfaty 2006 argues for Modern Hebrew a Semitic language that shares these characteristics we contend that mor1This paper focuses on Modern Standard Arabic rather than any of the dialects 34 phological analysis and parsing should be done in a unified framework such as a CTFTM rather than by separate components In this paper we describe CTFTMs which can be used for a wide variety of NLP tasks and present our Arabic CTFTM for Arabic tokenization affix detection affix labeling partofspeech tagging and dependency parsing as well as the results obtained in applying it to our dependency conversion of the Penn Arabic Treebank ATB Maamouri et al 2004 Maamouri and Bies 2004 We find that our Arabic CTFTM for tokenization affix detection affix labeling POS tagging and parsing achieves slightly better results than a similar CTFTM that performs all the tasks except parsing The CTFTM that supports parsing appears to be more accurate at distinguishing between passive and active verbs as well as between nouns and adjectives cases where the context is crucial for proper interpretation due to Arabic s ambiguities Our system achieves tokenization accuracy similar to Kulick s 2011 stateoftheart system for a standard split of the ATB part 3 and in our experiments using ATB parts 13 our system achieves the highest labeled attachment unlabeled attachment and clitic separation figures including pronomial clitics for Arabic yet reported although no other work can be compared directly 2 Relevant Arabic Linguistics Arabic has rich morphology with a wide array of affixes and clitics and inflecting for case number gender and occasionally mood Coordinating conjunctions pronouns and most true prepositions along with some other particles and the definite article usually occur as clitics in Arabic Thus a spacedelimited2 sequence of Arabic characters may con sist of multiple words and identifying the boundaries between these must be done in order to produce syntactic parses These boundaries can t be detected perfectly using simple deterministic rules Significantly short vowels which are expressed using diacritics are not typically written in Arabic resulting in pervasive ambiguity For example active and passive forms of verbs vary only in their diacritics and nouns and adjectives are both derived from Arabic 2Technically spaceand punctuationdeliminated roots using the same templates and thus look sim ilar A single Arabic token may permit a variety of different analyses as the example in Table 1 illustrates  wAlY  ruler "], "abstract": ["This paper describes crosstask flexible transition models CTFTMs and demonstrates their effectiveness for Arabic natural language processing NLP", "NLP pipelines often suffer from error propagation as errors committed in lowerlevel tasks cascade through the remainder of the processing pipeline", "By allowing a flexible order of operations across and within multiple NLP tasks a CTFTM can mitigate both crosstask and withintask error propagation", "Our Arabic CTFTM models to kenization affix detection affix labeling part ofspeech tagging and dependency parsing achieving stateoftheart results", "We present the details of our general framework our Arabic CTFTM and the setup and results of our experiments"], "inroduction": ["Natural Language Processing NLP systems often consist of a series of NLP components each trained to perform a specific task such as parsing", "These pipelines tend to suffer from error propagation errors introduced by early components cascade through the remainder of the pipeline causing subsequent components to commit additional errors", "Partial solutions from higherlevel tasks eg parsing can aid in resolving the difficult decisions that must be made in solving lowerlevel tasks as with part ofspeech tagging the classic garden path sentence example The horse raced past the barn fell To this end this paper presents crosstask flexible transition models CTFTMs which model multiple tasks and solve these tasks in a more flexible order than pipeline approaches", "We implement and experiment with a CTFTM for Arabic1 language processing and report experimental results for it on Arabic tokenization ie clitic separation affix detection affix labeling partofspeech tagging and dependency parsing", "In addition to error propagation between modules within a parsing pipeline errors may propagate within the parsing process itself due to the fixed order of operations of the parser", "This is common for standard transitionbased dependency parsing models McDonald and Nivre 2007 such as shiftreduce parsers which incrementally construct a parse by processing the input in a fixed lefttoright or righttoleft fashion", "However using a transition model that allows a more flexible order of operations such as Goldberg and Elhadads 2010 parser allows difficult decisions to be postponed until later when more of the solution has been constructed", "CTFTMs extend this approach by modeling multiple tasks and providing this flexibility across tasks so that no one task needs to be complete before another can be partially solved", "As a morphologically rich language Arabic requires a significant number of processing steps", "Arabic uses a variety of affixes to inflect for case gender number including dual and mood has clitics that attach to other words permits both VSO and SVO constructions and rarely includes short vowels in written form", "The presence of clitics and the absence of written short vowels are particularly significant sources of ambiguity", "As Tsarfaty 2006 argues for Modern Hebrew a Semitic language that shares these characteristics we contend that mor 1 This paper focuses on Modern Standard Arabic rather than any of the dialects", "34 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 3445 Seattle Washington USA 18 October 2013", "Qc 2013 Association for Computational Linguistics phological analysis and parsing should be done in a unified framework such as a CTFTM rather than by separate components", "In this paper we describe CTFTMs which can roots using the same templates and thus look similar", "A single Arabic token may permit a variety of different analyses as the example in Table 1 illustrates", "be used for a wide variety of NLP tasks and present our Arabic CTFTM for Arabic tokenization affix detection affix labeling partofspeech tagging and dependency parsing as well as the results obtained in applying it to our dependency conversion of the Penn Arabic Treebank ATB Maamouri et al 2004 Maamouri and Bies 2004", "We find that our Arabic CTFTM for tokenization affix detection affix labeling POS tagging and parsing achieves slightly better results than a similar CTFTM that performs all the tasks except parsing", "The CTFTM that supports parsing appears to be more accurate at distinguishing between passive and active verbs as well as between nouns and adjectives cases where the context is crucial for proper interpretation due to Arabics ambiguities", "Our system achieves tokenization accuracy similar to Kulicks 2011 stateoftheart system for a standard split of the ATB part 3 and in our experiments using ATB parts 13 our system achieves the highest labeled attachment unlabeled attachment and clitic separation figures including pronomial clitics for Arabic yet reported although no other work can be compared directly"]}, "W13-4916": {"title": ["Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 135 145 Seattle Washington USA 18 October 2013 c  2013 Association for Computational Linguistics Reranking Meets Morphosyntax Stateoftheart Results from the SPMRL 2013 Shared Task  Anders Bjo rkelund Ozlem C etinog lu Richa rd Farkas  Thomas Mu ller   and Wolfgang Seeker Institute for Natural Language Processing  University of Stuttgart Germany Department of Informatics University of Szeged Hungary Center for Information and Language Processing University of Munich Germany andersozlemmuelletsseekerimsunistuttgartde rfarkasinfuszegedhu Abstract This paper describes the IMSSZEGEDCIS contribution to the SPMRL 2013 Shared Task We participate in both the constituency and dependency tracks and achieve stateoftheart for all languages For both tracks we make significant improvements through high quality preprocessing and reranking on top of strong baselines Our system came out first for both tracks 1 Introduction In this paper we present our contribution to the 2013 Shared Task on Parsing Morphologically Rich Languages MRLs MRLs pose a number of interesting challenges to today s standard parsing algorithms for example a free word order and due to their rich morphology greater lexical variation that aggravates outofvocabulary problems considerably Tsarfaty et al 2010 Given the wide range of languages encompassed by the term MRL there is as of yet no clear consensus on what approaches and features are generally important for parsing MRLs However developing tailored solutions for each language is timeconsuming and requires a good understanding of the language in question In our contribution to the SPMRL 2013 Shared Task Seddah et al 2013 we therefore chose an approach that we could apply to all languages in the Shared Task but that would also allow us to finetune it for individual languages by varying certain components Authors in alphabetical order For the dependency track we combined the nbest output of multiple parsers and subsequently ranked them to obtain the best parse While this approach has been studied for constituency parsing Zhang et al 2009 Johnson and Ural 2010 Wang and Zong 2011 it is to our knowledge the first time this has been applied successfully within de pendency parsing We experimented with different kinds of features in the ranker and developed feature models for each language Our system ranked first out of seven systems for all languages except French For the constituency track we experimented with an alternative way of handling unknown words and applied a products of Context Free Grammars with Latent Annotations PCFGLA Petrov et al 2006 whose output was reranked to select the best analysis The additional reranking step improved results for all languages Our system beats various baselines provided by the organizers for all languages Unfortunately no one else participated in this track For both settings we made an effort to automatically annotate our data with the best possible preprocessing POS morphological information We used a multilayered CRF Mu ller et al 2013 to annotate each data set stacking with the information provided by the organizers when this was beneficial The high quality of our preprocessing considerably improved the performance of our systems The Shared Task involved a variety of settings as to whether gold or predicted partofspeech tags and morphological information were available as well as whether the full training set or a smaller 5k sen135 Arabic Basque French German Hebrew Hungarian Korean Polish Swedish MarMoT 97389222 97028708 97619092 98109180 97099767 98729759 94038768 98129084 97279713 Stacked 98238905 98569263 97839762 Table 1 POSmorphological feature accuracies on the development sets tences training set was used for training Through out this paper we focus on the settings with predicted preprocessing information with gold segmentation and the full1 training sets Unless stated otherwise all given numbers are drawn from experiments in this setting For all other settings we refer the reader to the Shared Task overview paper Seddah et al 2013 The remainder of the paper is structured as follows We present our preprocessing in Section 2 and afterwards describe both our systems for the con stituency Section 3 and for the dependency tracks Section 4 Section 5 discusses the results on the Shared Task test sets We conclude with Section 6 2 Preprocessing We first spent some time on preparing the data sets in particular we automatically annotated the data with highquality POS and morphological information We consider this kind of preprocessing to be an essential part of a parsing system since the quality of the automatic preprocessing strongly affects the performance of the parsers Because our tools work on CoNLL09 format we first converted the training data from the CoNLL06 format to CoNLL09 We thus had to decide whether to use coarse or fine partofspeech POS tags In a preliminary experiment we found that fine tags are the better option for all languages but Basque and Korean For Korean the reason seems to be that the fine tag set is huge  900 and that the same information is also provided in the feature column We predict POS tags and morphological features jointly using the Conditional Random Field CRF tagger MarMoT2 Mu ller et al 2013 MarMoT incrementally creates forwardbackward lattices of increasing order to prune the sizable space of possible morphological analyses We use MarMoT with the default parameters 1Although for Hebrew and Swedish only 5k sentences were available for training and the two settings thus coincide 2httpscodegooglecompcistern Since morphological dictionaries can improve au tomatic POS tagging considerably we also created such dictionaries for each language For this we an alyzed the word forms provided in the data sets with languagespecific morphological analyzers except for Hebrew and German where we just extracted the morphological information from the lattice files provided by the organizers For the other languages we used the following tools Arabic AraMorph a reimplementation of Buckwalter 2002 Basque Apertium Forcada et al 2011 French an IMS internal tool3 Hungarian Magyarlanc Zsibrita et al 2013 Korean HanNanum Park et al 2010 Polish Morfeusz Wolin ski 2006 and Swedish Granska Domeij et al 2000 The created dictionaries were shared with the other Shared Task participants We used these dictionaries as additional features for MarMoT For some languages we also integrated the predicted tags provided by the organizers into the feature model These stacked models gave improvements for Swedish Polish and Basque cf Table 1 for accuracies For the full setting the training data was annotated using 5fold jackknifing In the 5k setting we additionally added all sentences not present in the parser training data to the training data sets of the tagger This is similar to the predicted 5k files provided by the organizers where more training data than the 5k was also used for prediction Table 3 presents a comparison between our graphbased baseline parser using the preprocessing ex plained in this section denoted mate and the preprocessing provided by the organizers denoted mate Our preprocessing yields improvements for all languages but Swedish The worse perfor mance for Swedish is due to the fact that the predictions provided by the organizers were produced by models that were trained on a much larger data 3The French morphology was written by Zhenxia Zhou Max Kisselew and Helmut Schmid It is an extension of Zhou 2007 and implemented in SFST Schmid 2005 136 Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Berkeley 7824 6917 7974 8174 8783 8390 7097 8411 7450 Replaced 7870 8433 7968 8274 8955 8908 8284 8712 7552 Product 8030 8621 8142 8456 9049 8980 8415 8832 7925 Reranked 8124 8735 8249 8501 9049 9107 8463 8840 7953 Table 2 PARSEVAL scores on the development sets set The comparison with other parsers demonstrates that for some languages eg Hebrew or Korean the improvements due to better preprocessing can be greater than the improvements due to a better parser For instance for Hebrew the parser trained on the provided preprocessing is more than three points LAS behind the three parsers trained on our own preprocessing However the difference be tween these three parsers is less than a point 3 Constituency Parsing The phrase structure parsing pipeline is based on products of Context Free Grammars with Latent Annotations PCFGLA Petrov et al 2006 and discriminative reranking We further replace rare words by their predicted morphological analysis We preprocess the treebank trees by removing the morphological annotation of the POS tags and the function labels of all nonterminals We also reduce the 177 compositional Korean POS tags to their first atomic tag which results in a POS tag set of 9 tags PCFG LAs are incrementally built by splitting nonterminals refining parameters using EMtraining and reversing splits that only cause small increases in likelihood Running the Berkeley Parser4  the reference implementation of PCFG LAs  on the data sets results in the PARSEVAL scores given in Table 2 Berkeley The Berkeley parser only implements a simple signaturebased unknown word model that seems to be ineffective for some of the languages especially Basque and Korean We thus replace rare words frequency  20 by the predicted morphological tags of Section 2 or the true morphological tag for the gold setup The intuition is that our discriminative tagger has a more sophisticated unknown word treatment than the Berkeley parser taking for example prefixes suffixes and 4httpcodegooglecomp berkeleyparser the immediate lexical context into account Further more the morphological tag contains most of the necessary syntactic information An exception for instance might be the semantic information needed to disambiguate prepositional attachment We think that replacing rare words by tags has an advantage over constraining the preterminal layer of the parser because the parser can still decide to assign a different tag for example in cases were the tagger produces errors due to longdistance dependencies The used frequency threshold of 20 results in token replacement rates of 18 French to 57 Korean and Polish which correspond to 209 for Polish to 3221 for Arabic word types that are not replaced The PARSEVAL scores for the described method are again given in Table 2 Replaced The method yields improvements for all languages ex cept for French where we observe a drop of 006 The improvements range from 046 for Arabic to"], "abstract": ["This paper describes the IMSSZEGEDCIS contribution to the SPMRL 2013 Shared Task", "We participate in both the constituency and dependency tracks and achieve stateofthe art for all languages", "For both tracks we make significant improvements through high quality preprocessing and reranking on top of strong baselines", "Our system came out first for both tracks"], "inroduction": ["In this paper we present our contribution to the 2013 Shared Task on Parsing Morphologically Rich Languages MRLs", "MRLs pose a number of interesting challenges to todays standard parsing algorithms for example a free word order and due to their rich morphology greater lexical variation that aggravates outofvocabulary problems considerably Tsarfaty et al 2010", "Given the wide range of languages encompassed by the term MRL there is as of yet no clear consensus on what approaches and features are generally important for parsing MRLs", "However developing tailored solutions for each language is time consuming and requires a good understanding of the language in question", "In our contribution to the SPMRL 2013 Shared Task Seddah et al 2013 we therefore chose an approach that we could apply to all languages in the Shared Task but that would also allow us to finetune it for individual languages by varying certain components", "Authors in alphabetical order", "For the dependency track we combined the n best output of multiple parsers and subsequently ranked them to obtain the best parse", "While this approach has been studied for constituency parsing Zhang et al 2009 Johnson and Ural 2010 Wang and Zong 2011 it is to our knowledge the first time this has been applied successfully within dependency parsing", "We experimented with different kinds of features in the ranker and developed feature models for each language", "Our system ranked first out of seven systems for all languages except French", "For the constituency track we experimented with an alternative way of handling unknown words and applied a products of Context Free Grammars with Latent Annotations PCFGLA Petrov et al 2006 whose output was reranked to select the best analysis", "The additional reranking step improved results for all languages", "Our system beats various baselines provided by the organizers for all languages", "Unfortunately no one else participated in this track", "For both settings we made an effort to automatically annotate our data with the best possible pre processing POS morphological information", "We used a multilayered CRF Mu ller et al 2013 to annotate each data set stacking with the information provided by the organizers when this was beneficial", "The high quality of our preprocessing considerably improved the performance of our systems", "The Shared Task involved a variety of settings as to whether gold or predicted partofspeech tags and morphological information were available as well as whether the full training set or a smaller 5k sen 135 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 135145 Seattle Washington USA 18 October 2013", "Qc 2013 Association for Computational Linguistics Arabic Basque French German Hebrew Hungarian Korean Polish Swedish MarMoT 97389222 97028708 97619092 98109180 97099767 98729759 94038768 98129084 97279713 Stacked 98238905 98569263 97839762 Table 1 POSmorphological feature accuracies on the development sets", "tences training set was used for training", "Throughout this paper we focus on the settings with predicted preprocessing information with gold segmentation and the full1 training sets", "Unless stated otherwise all given numbers are drawn from experiments in this setting", "For all other settings we refer the reader to the Shared Task overview paper Seddah et al 2013", "The remainder of the paper is structured as follows We present our preprocessing in Section 2 and afterwards describe both our systems for the constituency Section 3 and for the dependency tracks Section 4", "Section 5 discusses the results on the Shared Task test sets", "We conclude with Section 6"]}, "W13-4917": {"title": ["Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 146 182 Seattle Washington USA 18 October 2013 c  2013 Association for Computational Linguistics Overview of the SPMRL 2013 Shared Task CrossFramework Evaluation of Parsing Morphologically Rich Languages Djam Seddaha Reut Tsarfatyb Sandra Kblerc Marie Canditod Jinho D Choie Rich rd Farkasf  Jennifer Fosterg Iakes Goenagah Koldo Gojenolai Yoav Goldbergj  Spence Greenk Nizar Habashl Marco Kuhlmannm Wolfgang Maiern Joakim Nivreo Adam Przepirkowskip Ryan Rothq Wolfgang Seekerr Yannick Versleys Veronika Vinczet Marcin Wolinskiu Alina Wr blewskav Eric Villemonte de la Clrgeriew aU ParisSorbonneINRIA bWeizman Institute cIndiana U dU ParisDiderotINRIA eIPsoft Inc ftU of Szeged gDublin City U hiU of the Basque Country jBar Ilan U kStanford U lqColumbia U moUppsala U nD sseldorf U puvPolish Academy of Sciences rStuttgart U sHeidelberg U wINRIA Abstract This paper reports on the first shared task on statistical parsing of morphologically rich languages MRLs The task features data sets from nine languages each available both in constituency and dependency annotation We report on the preparation of the data sets on the proposed parsing scenarios and on the evaluation metrics for parsing MRLs given different representation types We present and analyze parsing results obtained by the task participants and then provide an analysis and comparison of the parsers across languages and frameworks reported for gold input as well as more realistic parsing scenarios 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure Datadriven approaches to this problem both for constituencybased and dependencybased parsing have seen a surge of interest in the last two decades These datadriven parsing approaches obtain stateoftheart results on the de facto standard Wall Street Journal data set Marcus et al 1993 of English Charniak 2000 Collins 2003 Charniak and Johnson 2005 McDonald et al 2005 McClosky et al 2006 Petrov et al 2006 Nivre et al 2007b Carreras et al 2008 Finkel et al 2008 Contact authors djameseddahparissorbonnefr reuttsarfatyweizmannacil skueblerindianaedu Huang 2008 Huang et al 2010 Zhang and Nivre 2011 Bohnet and Nivre 2012 Shindo et al 2012 and provide a foundation on which many tasks operating on semantic structure eg recognizing textual entailments or even discourse structure coreference summarization crucially depend While progress on parsing English  the main language of focus for the ACL community  has inspired some advances on other languages it has not by itself yielded highquality parsing for other languages and domains This holds in particular for morphologically rich languages MRLs where important information concerning the predicateargument structure of sentences is expressed through word formation rather than constituentorder patterns as is the case in English and other configurational languages MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level via phenomena such as inflectional affixes pronominal clitics and so on Tsarfaty et al 2012c The nonrigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs In addition insufficient language resources were shown to also contribute to parsing difficulty Tsarfaty et al 2010 Tsarfaty et al 2012c and references therein These challenges have initially been addressed by nativespeaking experts using strong indomain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models More 146 recently advances in PCFG LA parsing Petrov et al 2006 and languageagnostic datadriven dependency parsing McDonald et al 2005 Nivre et al 2007b have made it possible to reach high accuracy with classical feature engineering techniques in addition to or instead of languagespecific knowledge With these recent advances the time has come for establishing the state of the art and assessing strengths and weaknesses of parsers across different MRLs This paper reports on the first shared task on statistical parsing of morphologically rich languages the SPMRL Shared Task organized in collaboration with the 4th SPMRL meeting and colocated with the conference on Empirical Methods in Natural Language Processing EMNLP In defining and executing this shared task we pursue several goals First we wish to provide standard training and test sets for MRLs in different representation types and parsing scenarios so that researchers can exploit them for testing existing parsers across different MRLs Second we wish to standardize the evaluation protocol and metrics on morphologically ambiguous input an understudied challenge which is also present in English when parsing speech data or webbased nonstandard texts Finally we aim to raise the awareness of the community to the challenges of parsing MRLs and to provide a set of strong baseline results for further improvement The task features data from nine typologically diverse languages Unlike previous shared tasks on parsing we include data in both dependencybased and constituencybased formats and in addition to the full data setup complete training data we provide a small setup a training subset of 5000 sentences We provide three parsing scenarios one in which gold segmentation POS tags and morphological features are provided one in which segmentation POS tags and features are automatically predicted by an external resource and one in which we provide a lattice of multiple possible morphological analyses and allow for joint disambiguation of the morphological analysis and syntactic structure These scenarios allow us to obtain the performance upper bound of the systems in lab settings using gold input as well as the expected level of performance in realistic parsing scenarios  where the parser follows a morphological analyzer and is a part of a fullfledged NLP pipeline The remainder of this paper is organized as follows We first survey previous work on parsing MRLs 2 and provide a detailed description of the present task parsing scenarios and evaluation metrics  3 We then describe the data sets for the nine languages  4 present the different systems 5 and empirical results  6 Then we compare the systems along different axes  7 in order to analyze their strengths and weaknesses Finally we summarize and con clude with challenges to address in future shared tasks  8 2 Background"], "abstract": ["This paper reports on the first shared task on statistical parsing of morphologically rich languages MRLs", "The task features data sets from nine languages each available both in constituency and dependency annotation", "We report on the preparation of the data sets on the proposed parsing scenarios and on the evaluation metrics for parsing MRLs given different representation types", "We present and"], "inroduction": ["participants and then provide an analysis and comparison of the parsers across languages and frameworks reported for gold input as well as more realistic parsing scenarios", "1 Introduction", "Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure", "Datadriven approaches to this problem both for constituencybased and dependencybased parsing have seen a surge of interest in the last two decades", "These datadriven parsing approaches obtain stateoftheart results on the de facto standard Wall Street Journal data set Marcus et al 1993 of English Charniak 2000 Collins 2003 Charniak and Johnson 2005 McDonald et al 2005 McClosky et al 2006 Petrov et al 2006 Nivre et al 2007b Carreras et al 2008 Finkel et al 2008 Contact authors djameseddahparissorbonnefr reuttsarfatyweizmannacil skueblerindianaedu Huang 2008 Huang et al 2010 Zhang and Nivre 2011 Bohnet and Nivre 2012 Shindo et al 2012 and provide a foundation on which many tasks operating on semantic structure eg recognizing textual entailments or even discourse structure coreference summarization crucially depend", "While progress on parsing English  the main language of focus for the ACL community  has inspired some advances on other languages it has not by itself yielded highquality parsing for other languages and domains", "This holds in particular for morphologically rich languages MRLs where important information concerning the predicateargument structure of sentences is expressed through word formation rather than constituentorder patterns as is the case in English and other configurational languages", "MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level via phenomena such as inflectional affixes pronominal clitics and so on Tsarfaty et al 2012c", "The nonrigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs", "In addition insufficient language resources were shown to also contribute to parsing difficulty Tsarfaty et al 2010 Tsarfaty et al 2012c and references therein", "These challenges have initially been addressed by nativespeaking experts using strong indomain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models", "More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages pages 146182 Seattle Washington USA 18 October 2013", "Qc 2013 Association for Computational Linguistics recently advances in PCFGLA parsing Petrov et al 2006 and languageagnostic datadriven dependency parsing McDonald et al 2005 Nivre et al 2007b have made it possible to reach high accuracy with classical feature engineering techniques in addition to or instead of languagespecific knowledge", "With these recent advances the time has come for establishing the state of the art and assessing strengths and weaknesses of parsers across different MRLs", "This paper reports on the first shared task on statistical parsing of morphologically rich languages the SPMRL Shared Task organized in collaboration with the 4th SPMRL meeting and colocated with the conference on Empirical Methods in Natural Language Processing EMNLP", "In defining and executing this shared task we pursue several goals", "First we wish to provide standard training and test sets for MRLs in different representation types and parsing scenarios so that researchers can exploit them for testing existing parsers across different MRLs", "Second we wish to standardize the evaluation protocol and metrics on morphologically ambiguous input an understudied challenge which is also present in English when parsing speech data or webbased nonstandard texts", "Finally we aim to raise the awareness of the community to the challenges of parsing MRLs and to provide a set of strong baseline results for further improvement", "The task features data from nine typologically diverse languages", "Unlike previous shared tasks on parsing we include data in both dependencybased and constituencybased formats and in addition to the full data setup complete training data we provide a small setup a training subset of 5000 sentences", "We provide three parsing scenarios one in which gold segmentation POS tags and morphological features are provided one in which segmentation POS tags and features are automatically predicted by an external resource and one in which we provide a lattice of multiple possible morphological analyses and allow for joint disambiguation of the morphological analysis and syntactic structure", "These scenarios allow us to obtain the performance upper bound of the systems in lab settings using gold input as well as the expected level of performance in realistic parsing scenarios  where the parser follows a morphological analyzer and is a part of a fullfledged NLP pipeline", "The remainder of this paper is organized as follows", "We first survey previous work on parsing MRLs 2 and provide a detailed description of the present task parsing scenarios and evaluation metrics 3", "We then describe the data sets for the nine languages 4 present the different systems 5 and empirical results 6", "Then we compare the systems along different axes 7 in order to analyze their strengths and weaknesses", "Finally we summarize and conclude with challenges to address in future shared tasks 8"]}, "W15-0909": {"title": ["The Impact of Multiword Expression Compositionality  on Machine"], "abstract": ["In this paper we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric", "The attempt is marginally successful and we speculate on whether a largerscale attempt is likely to have greater impact"], "inroduction": ["While the explicit identification of multiword expressions MWEs Sag et al", "2002 Baldwin and Kim 2009 has been shown to be useful in various NLP applications Ramisch 2012 recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval IR Acosta et al", "2011 and machine translation MT Weller et al", "2014 Carpuat and Diab 2010 and Venkatapathy and Joshi 2006", "For instance Acosta et al", "2011 showed that by considering noncompositional MWEs as a single unit the effectiveness of document ranking in an IR system improves and Carpuat and Diab 2010 showed that by adding compositionality scores to the Moses SMT system Koehn et al 2007 they could improve translation quality", "This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs", "The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations based on compositionality", "For example an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation as tower here is most naturally interpreted compositionally in the MT output but noncompositionally in the reference translation", "On the other hand a partial mismatch between traffic signal and traffic light should be rewarded as the usage of traffic is highly compositional in both cases", "That is we ask the question can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs focusing on compound nouns and the TESLA machine translation metric Liu et al 2010"]}, "W96-0108": {"title": [""], "abstract": ["This paper describes an automatic contextsensitive worderror correction system based on statistical language modeling SLM as applied to optical character recognition OCR post processing", "The system exploits information from multiple sources including letter ngrams character confusion probabilities and wordbigram probabilities", "Letter ngrams are used to index the words in the lexicon", "Given a sentence to be corrected the system decomposes each string in the sentence into letter ngrams and retrieves word candidates from the lexicon by comparing string ngrams with lexiconentry ngrams", "The retrieved candidates are ranked by the conditional probability of matches with the string given character confusion probabilities", "Finally the wordbigram model and Viterbi algorithm are used to determine the best scoring word sequence for the sentence", "The system can correct nonword errors as well as realword errors and achieves a 602 error reduction rate for real OCR text", "In addition the system can learn the character confusion probabilities for a specific OCR environment and use them in selfcalibration to achieve better performance"], "inroduction": ["Word errors present problems for various text or speechbased applications such as optical char acter recognition OCR and voiceinput computer interfaces", "In particular though current OCR technology is quite refined and robust sources such as old books poorquality nthgeneration photocopies and faxes can still be difficult to process and may cause many OCR errors", "For OCR to be truly useful in a wide range of applications such as office automation and information retrieval systems OCR reliability must be improved", "A method for the automatic correction of OCR errors would be clearly beneficial", "Essentially there are two types of word errors nonword errors and realword errors", "A non word error occurs when a word in a source text is interpreted under OCR as a string that does not correspond to any valid word in a given word list or dictionary", "A realword error occurs when a sourcetext word is interpreted as a string that actually does occur in the dictionary but is not identical with the sourcetext word", "For example if the source text John found the man is rendered as John fomd he man by an OCR device then fomd is a nonword error and he is a realword error", "In general nonword errors will never correspond to any dictionary entries and will include wildly incorrect strings such as  as well as misrecognized alphanumeric sequences such as BN234 for 8N234", "However some nonword errors might become real word errors if the size of the word list or dictionary increases", "For example the word ruel1 might count as a nonword error for the sourcetext word rut if a small dictionary is used for reference but count as a realword error if an unabridged dictionary is used", "While nonword errors might be corrected without considering the context in which the error occurs a realword error can only be corrected by taking context into account", "The problems of worderror detection and correction have been studied for several decades", "A good survey in this area can be found in Kukich 1992", "Most traditional wordcorrection techniques concentrate on nonword error correction and do not consider the context in which the error appears", "Recently statistical language models SLMs and featurebased methods have been used for contextsensitive spellingerror correction", "For example Atwell and Elliittm 1987 have used a partofspeech POS tagging method to detect the realword errors in text", "Mays and colleagues 1991 have exploited word trigrams to detect and correct both the nonword and realword errors that were artificially generated from 100 sentences", "Church and Gale 1991 have used a Bayesian classifier method to improve the performance for nonword error correction", "Golding 1995 has applied a hybrid Bayesian method for realword error correction and Golding and Schabes 1996 have combined a POS trigram and Bayesian methods for the same purpose", "The goal of the work described here is to investigate the effectiveness and efficiency of SLM based methods applied to the problem of OCR error correction", "Since POSbased methods are not effective in distinguishing among candidates with the same POS tags and since methods based on wordtrigram models involve extensive training data and require that huge wordtrigram tables be available at run time we used a wordbigram SLM as the first step in our investigation", "In this paper we describe a system that uses a wordbigram SLM technique to correct OCR errors", "The system takes advantage of information from multiple sources including letter n grams character confusion probabilities and word bigram probabilities to effect contextbased word error correction", "It can correct nonword as well as realword errors", "In addition the system can learn the character confusion probability table for a specific OCR environment and use it to achieve better performance"]}, "W98-1234": {"title": ["Methods and tricks used in an attempt to pass the Turing Test"], "abstract": ["This paper describes differents methods and tricks in connection with our program which has been entered in the Loebner Prize competition that will happen on Sunday 11 January 1998 at the PowerHouse Museum in Sydney", "Of course this isnt exhaustive there are other possible techniques but we aim to give the main ideas", "Well speak about the main modules of our program  Spelling correction Different uses of WordNet and Generation of comments", "Our module used for spelling correction was developed on the basis of works by Brill 1 Brill and Marcus 2 Golding 3 Golding and Schabes 4 and Powers 5"], "inroduction": ["Alan Turing was a brilliant British mathematician who played an important role in the development of partofspeech verbs nouns adjectives adverbs", "These sets are divided into semanticals categories eg synonymous for nouns", "WordNet is completely described in the URL httpwwwspeechcscmuedulcompspeechSectionl I LexicaVwoidnethtml", "3", "Architecture", "To mimic some parts of human thought we created different principal modules  Spelling Correction Disambiguation between words Generation of comments Simulating human typing", " computers and developed a test that would serve as an indicator of intelligence for machines", "A lot of researchers posed the Loebner Prize as the first formal instantiation of the Turing Test", "To participate in this competition we conceived a program that attempts to simulate the responses of a human being", "Well begin to describe WordNet which includes a classification of English words", "Afterwards well present the architecture of our system which we are programming at the moment", "In this section well briefly explain every module", "Next well give an Gecnomemraetinotnof 31", "Spelling Correction", " r J ", "example of interaction between our program and one human", "In the same section well show different processes of generating a response from the input of the user", "Finally well conclude by indicating our own position on this test using knowledge that we have acquired during only two months of work in this area"]}, "W99-0503": {"title": ["Supervised Learning of Lexical Setuantic Verb  Classes Using  Frequency Distributions"], "abstract": ["We report a number of computational ex periments in supervised learnig whose goal IS to automatically classify a set of verbs mto lexrcal semantic classes based on frequency drstrrbut1on approx1mat1ons of grammatical features extracted from a very large annotated corpus D1strrbutrons of five syntactic features that approx1mate trans1trv1ty alternations and thematic role ass1gnments are sufficient to reduce error rate by 56 over chance We conclude that corpus data IS a usable repos1tory of verb class mformation and that corpus drrven extractiOn of grammatical features IS a promismg methodology for automatic lexical acquisition"], "inroduction": ["Recent years have Witnessed a shift m grammar de velopment methodology from craftmg large gram mars to annotation of corpora Correspondmgly there has been a change from developmg rulebased parsers to developmg stat1st1cal methods for mduc mg grammatical knowledge from annotated corpus data The sh1ft has mostly occurred because buld mg wrdecoverage grammars 1s t1meconsummg er ror prone and difficult The same can be sa1d for craftmg the nch lexrcal representatiOns that are a central component of hngu1strc knowledge and re search m automatic le1cal acqu1s1t1on has sought to address th1s Dorr and Jones 1996 Dorr 1997 among others Yet there have been few attempts to learn finegramed lexical classificatiOns from the sta tistical analysiS of d1stnbutwnal data analogously to the mduct1on of syntactic knowledge though see e g Brent 1993 Klavans and Chodorow 1992 Resml 1992 In th1s paper", "we propose uth an approach for the automatic classlficatwn of erb mto lex1cal semantic classes 1 We can express the Issues raised by th1s approach as follows Wh1ch hngu1st1c d1stmctwns among leIcal classes can we epect to find m a corpus"]}, "W99-0632": {"title": ["Using Subcategorization to Resolve Verb Class Ambiguity"], "abstract": ["Levins 1993 taxonomy of verbs and their classes is a widely used resource for lexical semantics", "In her framework some verbs such as give exhibit no class ambiguity", "But other verbs such as write can inhabit more than one class", "In some of these am Chris Brew HCRC Language Technology Group Division of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW UK chrisbrcogsciedacuk cf", "3a and the prepositional frame NPVNP PP10 cf", "3b", "l a Janet broke the cup", "b The cup broke", "2 a Bill sold a car to Tom", "b Bill sold Tom a car", "biguous cases the appropriate class for a particular token of a verb is immediately obvious from inspec tion of the surrounding context", "In others it is not 3 a b Martha carved the baby a toy", "Martha carved a toy for the baby", "and an application which wants to recover this infor mation will be forced to rely on some more or Jess elaborate process of inference", "We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference"], "inroduction": ["The relation between the syntactic realization of a verbs arguments and its meaning has been exten sively studied in Levin 1993", "Levins work re lies on the hypothesis that the behavior of a verb particularly with respect to the expression and in terpretation of its arguments is to a large extent determined by its meaning Levin 1993 p I", "Verbs which display the same diathesis alterna tionsalternations in the realization of their argu ment structureare assumed to share certain mean ing components and are organized into a semanti cally coherent class", "As an example consider sentences 13 taken from Levin", "Example l iiiustrates the causativeinchoative alternation", "Verbs undergoing this alternation can be manifested either as transi tive with a causative reading cf", "I a or as intransi tive with an inchoative reading cf", "lb", "Examples 2 and 3 illustrate the dative and benefactive al ternations respectively", "Verbs which license the for mer alternate between the prepositional frame NP VNPPPro cf", "2a and the double object frame VNPNP cf", "2b whereas verbs which undergo the latter alternate between the double object frame Verbs like crack and chip pattern with break in li censing the causativeinchoative alternation and are associated with the semantic class of BREAK verbs", "Verbs make and build behave similar to carve in licensing the benefactive alternation and are mem bers of the class of BUILD verbs whereas sell and give undergo the dative alternation and participate in the GIVE class", "By grouping together verbs which pattern together with respect to diathesis alterna tions Levin defines approximately 200 verb classes which she argues reflect important semantic regu larities"]}}